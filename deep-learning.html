<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 20 Deep Learning | STAT 362 R for Data Science</title>
  <meta name="description" content="Notes for STAT 362" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 20 Deep Learning | STAT 362 R for Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes for STAT 362" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 20 Deep Learning | STAT 362 R for Data Science" />
  
  <meta name="twitter:description" content="Notes for STAT 362" />
  

<meta name="author" content="Brian Ling" />


<meta name="date" content="2023-01-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ensemble-methods.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Data Science</a></li>

<li class="divider"></li>
<li><a href="index.html#syllabus">Syllabus<span></span></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> What is R and RStudio?<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#why-r"><i class="fa fa-check"></i><b>1.2</b> Why R?<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#what-will-you-learn-in-this-course"><i class="fa fa-check"></i><b>1.3</b> What will you learn in this course?<span></span></a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#r-and-r-as-a-programming-language"><i class="fa fa-check"></i><b>1.3.1</b> R and R as a programming language<span></span></a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.2</b> Data Wrangling<span></span></a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#data-visualization"><i class="fa fa-check"></i><b>1.3.3</b> Data Visualization<span></span></a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#statistical-inference"><i class="fa fa-check"></i><b>1.3.4</b> Statistical Inference<span></span></a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#machine-learning"><i class="fa fa-check"></i><b>1.3.5</b> Machine Learning<span></span></a></li>
<li class="chapter" data-level="1.3.6" data-path="introduction.html"><a href="introduction.html#some-numerical-methods"><i class="fa fa-check"></i><b>1.3.6</b> Some Numerical Methods<span></span></a></li>
<li class="chapter" data-level="1.3.7" data-path="introduction.html"><a href="introduction.html#lastly"><i class="fa fa-check"></i><b>1.3.7</b> Lastly<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#lets-get-started"><i class="fa fa-check"></i><b>1.4</b> Let’s Get Started<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#r-data-structures"><i class="fa fa-check"></i><b>1.5</b> R Data Structures<span></span></a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#vectors"><i class="fa fa-check"></i><b>1.5.1</b> Vectors<span></span></a></li>
<li class="chapter" data-level="1.5.2" data-path="introduction.html"><a href="introduction.html#factors"><i class="fa fa-check"></i><b>1.5.2</b> Factors<span></span></a></li>
<li class="chapter" data-level="1.5.3" data-path="introduction.html"><a href="introduction.html#matrix"><i class="fa fa-check"></i><b>1.5.3</b> Matrix<span></span></a></li>
<li class="chapter" data-level="1.5.4" data-path="introduction.html"><a href="introduction.html#lists"><i class="fa fa-check"></i><b>1.5.4</b> Lists<span></span></a></li>
<li class="chapter" data-level="1.5.5" data-path="introduction.html"><a href="introduction.html#data-frames"><i class="fa fa-check"></i><b>1.5.5</b> Data frames<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#operators"><i class="fa fa-check"></i><b>1.6</b> Operators<span></span></a><ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#vectorized-operators"><i class="fa fa-check"></i><b>1.6.1</b> Vectorized Operators<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#built-in-functions"><i class="fa fa-check"></i><b>1.7</b> Built-in Functions<span></span></a><ul>
<li class="chapter" data-level="1.7.1" data-path="introduction.html"><a href="introduction.html#sort"><i class="fa fa-check"></i><b>1.7.1</b> <code>sort()</code><span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="introduction.html"><a href="introduction.html#seq"><i class="fa fa-check"></i><b>1.7.2</b> <code>seq()</code><span></span></a></li>
<li class="chapter" data-level="1.7.3" data-path="introduction.html"><a href="introduction.html#rep"><i class="fa fa-check"></i><b>1.7.3</b> <code>rep()</code><span></span></a></li>
<li class="chapter" data-level="1.7.4" data-path="introduction.html"><a href="introduction.html#pmax-pmin"><i class="fa fa-check"></i><b>1.7.4</b> <code>pmax</code>, <code>pmin</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#some-useful-rstudio-shortcuts"><i class="fa fa-check"></i><b>1.8</b> Some Useful RStudio Shortcuts<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#exercises"><i class="fa fa-check"></i><b>1.9</b> Exercises<span></span></a></li>
<li class="chapter" data-level="1.10" data-path="introduction.html"><a href="introduction.html#comments-to-exercises"><i class="fa fa-check"></i><b>1.10</b> Comments to Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>2</b> Probability<span></span></a><ul>
<li class="chapter" data-level="2.1" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Probability Distributions<span></span></a><ul>
<li class="chapter" data-level="2.1.1" data-path="probability.html"><a href="probability.html#common-distributions"><i class="fa fa-check"></i><b>2.1.1</b> Common Distributions<span></span></a></li>
<li class="chapter" data-level="2.1.2" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>2.1.2</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability.html"><a href="probability.html#simulation"><i class="fa fa-check"></i><b>2.2</b> Simulation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>3</b> Programming in R<span></span></a><ul>
<li class="chapter" data-level="3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#writing-functions-in-r"><i class="fa fa-check"></i><b>3.1</b> Writing functions in R<span></span></a><ul>
<li class="chapter" data-level="3.1.1" data-path="programming-in-r.html"><a href="programming-in-r.html#argument-matching"><i class="fa fa-check"></i><b>3.1.1</b> Argument Matching<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="programming-in-r.html"><a href="programming-in-r.html#control-flow"><i class="fa fa-check"></i><b>3.2</b> Control Flow<span></span></a><ul>
<li class="chapter" data-level="3.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#for-loop"><i class="fa fa-check"></i><b>3.2.1</b> for loop<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#nested-for-loop"><i class="fa fa-check"></i><b>3.2.2</b> nested for loop<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#while-loop"><i class="fa fa-check"></i><b>3.2.3</b> while loop<span></span></a></li>
<li class="chapter" data-level="3.2.4" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond"><i class="fa fa-check"></i><b>3.2.4</b> if (cond)<span></span></a></li>
<li class="chapter" data-level="3.2.5" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond-else-expr"><i class="fa fa-check"></i><b>3.2.5</b> if (cond) else expr<span></span></a></li>
<li class="chapter" data-level="3.2.6" data-path="programming-in-r.html"><a href="programming-in-r.html#if-else-ladder"><i class="fa fa-check"></i><b>3.2.6</b> If else ladder<span></span></a></li>
<li class="chapter" data-level="3.2.7" data-path="programming-in-r.html"><a href="programming-in-r.html#switch"><i class="fa fa-check"></i><b>3.2.7</b> <code>switch</code><span></span></a></li>
<li class="chapter" data-level="3.2.8" data-path="programming-in-r.html"><a href="programming-in-r.html#next-break"><i class="fa fa-check"></i><b>3.2.8</b> <code>next</code>, <code>break</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="programming-in-r.html"><a href="programming-in-r.html#loop-functions"><i class="fa fa-check"></i><b>3.3</b> Loop functions<span></span></a><ul>
<li class="chapter" data-level="3.3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#apply"><i class="fa fa-check"></i><b>3.3.1</b> <code>apply()</code><span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="programming-in-r.html"><a href="programming-in-r.html#lapply"><i class="fa fa-check"></i><b>3.3.2</b> <code>lapply()</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="programming-in-r.html"><a href="programming-in-r.html#automatically-reindent-code"><i class="fa fa-check"></i><b>3.4</b> Automatically Reindent Code<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="programming-in-r.html"><a href="programming-in-r.html#speed-consideration"><i class="fa fa-check"></i><b>3.5</b> Speed Consideration<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="programming-in-r.html"><a href="programming-in-r.html#another-simulation-example"><i class="fa fa-check"></i><b>3.6</b> Another Simulation Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html"><i class="fa fa-check"></i><b>4</b> Creating Some Basic Plots<span></span></a><ul>
<li class="chapter" data-level="4.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#scatter-plot"><i class="fa fa-check"></i><b>4.1</b> Scatter Plot<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#line-graph"><i class="fa fa-check"></i><b>4.2</b> Line Graph<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#bar-chart"><i class="fa fa-check"></i><b>4.3</b> Bar Chart<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#histogram"><i class="fa fa-check"></i><b>4.4</b> Histogram<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#box-plot"><i class="fa fa-check"></i><b>4.5</b> Box Plot<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#plotting-a-function-curve"><i class="fa fa-check"></i><b>4.6</b> Plotting a function curve<span></span></a></li>
<li class="chapter" data-level="4.7" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#more-on-plots-with-base-r"><i class="fa fa-check"></i><b>4.7</b> More on plots with base R<span></span></a><ul>
<li class="chapter" data-level="4.7.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#multi-frame-plot"><i class="fa fa-check"></i><b>4.7.1</b> Multi-frame plot<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#subsubsec:type_of_plot"><i class="fa fa-check"></i><b>4.7.2</b> Type of Plot<span></span></a></li>
<li class="chapter" data-level="4.7.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#parameters-of-a-plot"><i class="fa fa-check"></i><b>4.7.3</b> Parameters of a plot<span></span></a></li>
<li class="chapter" data-level="4.7.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#elements-on-plot"><i class="fa fa-check"></i><b>4.7.4</b> Elements on plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#summary-of-ggplot"><i class="fa fa-check"></i><b>4.8</b> Summary of <code>ggplot</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html"><i class="fa fa-check"></i><b>5</b> Managing Data with R<span></span></a><ul>
<li class="chapter" data-level="5.1" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#saving-loading-and-removing-r-data-structures"><i class="fa fa-check"></i><b>5.2</b> Saving, loading, and removing R data structures<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#importing-and-saving-data-from-csv-files"><i class="fa fa-check"></i><b>5.3</b> Importing and saving data from CSV files<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html"><i class="fa fa-check"></i><b>6</b> Review (Chapter 1-5)<span></span></a><ul>
<li class="chapter" data-level="6.1" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#simulation-1"><i class="fa fa-check"></i><b>6.1</b> Simulation<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#matrix-1"><i class="fa fa-check"></i><b>6.2</b> Matrix<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#basic-operation"><i class="fa fa-check"></i><b>6.3</b> Basic Operation<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#some-basic-plots-in-r"><i class="fa fa-check"></i><b>6.4</b> Some basic plots in R<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html"><i class="fa fa-check"></i><b>7</b> Data Transformation with <code>dplyr</code><span></span></a><ul>
<li class="chapter" data-level="7.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#arrange"><i class="fa fa-check"></i><b>7.2</b> <code>arrange()</code><span></span></a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-2"><i class="fa fa-check"></i><b>7.2.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#filter"><i class="fa fa-check"></i><b>7.3</b> <code>filter()</code><span></span></a><ul>
<li class="chapter" data-level="7.3.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-3"><i class="fa fa-check"></i><b>7.3.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#select"><i class="fa fa-check"></i><b>7.4</b> <code>select()</code><span></span></a><ul>
<li class="chapter" data-level="7.4.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-4"><i class="fa fa-check"></i><b>7.4.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#mutate"><i class="fa fa-check"></i><b>7.5</b> <code>mutate()</code><span></span></a><ul>
<li class="chapter" data-level="7.5.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-5"><i class="fa fa-check"></i><b>7.5.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summarize-group_by"><i class="fa fa-check"></i><b>7.6</b> <code>summarize(), group_by()</code><span></span></a></li>
<li class="chapter" data-level="7.7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#combining-multiple-operations-with-pipe"><i class="fa fa-check"></i><b>7.7</b> Combining Multiple Operations with Pipe <code>%&gt;%</code><span></span></a></li>
<li class="chapter" data-level="7.8" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summary"><i class="fa fa-check"></i><b>7.8</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html"><i class="fa fa-check"></i><b>8</b> Data Visualization with ggplot2<span></span></a><ul>
<li class="chapter" data-level="8.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts"><i class="fa fa-check"></i><b>8.1</b> Bar charts<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graph-1"><i class="fa fa-check"></i><b>8.2</b> Line Graph<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plots"><i class="fa fa-check"></i><b>8.3</b> Scatter Plots<span></span></a><ul>
<li class="chapter" data-level="8.3.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#overplotting"><i class="fa fa-check"></i><b>8.3.1</b> Overplotting<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#labelling-points-in-a-scatter-plot"><i class="fa fa-check"></i><b>8.3.2</b> Labelling points in a scatter plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions"><i class="fa fa-check"></i><b>8.4</b> Summarizing Data Distributions<span></span></a><ul>
<li class="chapter" data-level="8.4.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#histogram-1"><i class="fa fa-check"></i><b>8.4.1</b> Histogram<span></span></a></li>
<li class="chapter" data-level="8.4.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#kernel-density-estimate"><i class="fa fa-check"></i><b>8.4.2</b> Kernel Density Estimate<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots"><i class="fa fa-check"></i><b>8.5</b> Saving your plots<span></span></a><ul>
<li class="chapter" data-level="8.5.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-pdf-vector-files"><i class="fa fa-check"></i><b>8.5.1</b> Outputting to pdf vector files<span></span></a></li>
<li class="chapter" data-level="8.5.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-bitmap-files"><i class="fa fa-check"></i><b>8.5.2</b> Outputting to bitmap files<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summary-1"><i class="fa fa-check"></i><b>8.6</b> Summary<span></span></a><ul>
<li class="chapter" data-level="8.6.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#combining-multiple-operations-with-pipe-1"><i class="fa fa-check"></i><b>8.6.1</b> Combining multiple operations with pipe <code>%&gt;%</code><span></span></a></li>
<li class="chapter" data-level="8.6.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts-1"><i class="fa fa-check"></i><b>8.6.2</b> Bar charts<span></span></a></li>
<li class="chapter" data-level="8.6.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graphs"><i class="fa fa-check"></i><b>8.6.3</b> Line graphs<span></span></a></li>
<li class="chapter" data-level="8.6.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plot-1"><i class="fa fa-check"></i><b>8.6.4</b> Scatter plot<span></span></a></li>
<li class="chapter" data-level="8.6.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions-1"><i class="fa fa-check"></i><b>8.6.5</b> Summarizing data distributions<span></span></a></li>
<li class="chapter" data-level="8.6.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots-1"><i class="fa fa-check"></i><b>8.6.6</b> Saving your plots<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference in R<span></span></a><ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.1</b> Maximum Likelihood Estimation<span></span></a><ul>
<li class="chapter" data-level="9.1.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#exercises-on-mle"><i class="fa fa-check"></i><b>9.1.1</b> Exercises on MLE<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#summary-2"><i class="fa fa-check"></i><b>9.1.2</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#interval-estimation-and-hypothesis-testing"><i class="fa fa-check"></i><b>9.2</b> Interval Estimation and Hypothesis Testing<span></span></a><ul>
<li class="chapter" data-level="9.2.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.2.1</b> Examples of Hypothesis Testing<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#null-hypotheses-alternative-hypotheses-and-p-values"><i class="fa fa-check"></i><b>9.2.2</b> Null Hypotheses, Alternative Hypotheses, and p-values<span></span></a></li>
<li class="chapter" data-level="9.2.3" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#type-i-error-and-type-ii-error"><i class="fa fa-check"></i><b>9.2.3</b> Type I error and Type II error<span></span></a></li>
<li class="chapter" data-level="9.2.4" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-for-mean-of-one-sample"><i class="fa fa-check"></i><b>9.2.4</b> Inference for Mean of One Sample<span></span></a></li>
<li class="chapter" data-level="9.2.5" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#comparing-the-means-of-two-samples"><i class="fa fa-check"></i><b>9.2.5</b> Comparing the means of two samples<span></span></a></li>
<li class="chapter" data-level="9.2.6" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-of-a-sample-proportion"><i class="fa fa-check"></i><b>9.2.6</b> Inference of a Sample Proportion<span></span></a></li>
<li class="chapter" data-level="9.2.7" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-groups-for-equal-proportions"><i class="fa fa-check"></i><b>9.2.7</b> Testing groups for equal proportions<span></span></a></li>
<li class="chapter" data-level="9.2.8" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-if-two-samples-have-the-same-underlying-distribution"><i class="fa fa-check"></i><b>9.2.8</b> Testing if two samples have the same underlying distribution<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html"><i class="fa fa-check"></i><b>10</b> Root finding and optimization<span></span></a><ul>
<li class="chapter" data-level="10.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#root-finding"><i class="fa fa-check"></i><b>10.1</b> Root Finding<span></span></a><ul>
<li class="chapter" data-level="10.1.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#bisection-method"><i class="fa fa-check"></i><b>10.1.1</b> Bisection Method<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method"><i class="fa fa-check"></i><b>10.2</b> Newton-Raphson Method<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#minimization-and-maximization"><i class="fa fa-check"></i><b>10.3</b> Minimization and Maximization<span></span></a><ul>
<li class="chapter" data-level="10.3.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method-1"><i class="fa fa-check"></i><b>10.3.1</b> Newton-Raphson Method<span></span></a></li>
<li class="chapter" data-level="10.3.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#gradient-descent"><i class="fa fa-check"></i><b>10.3.2</b> Gradient Descent<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#optim"><i class="fa fa-check"></i><b>10.4</b> <code>optim</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>11</b> k-nearest neighbors<span></span></a><ul>
<li class="chapter" data-level="11.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#feature-scaling"><i class="fa fa-check"></i><b>11.2</b> Feature Scaling<span></span></a></li>
<li class="chapter" data-level="11.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-classifying-breast-cancers"><i class="fa fa-check"></i><b>11.3</b> Example: Classifying Breast Cancers<span></span></a><ul>
<li class="chapter" data-level="11.3.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#evaluating-model-performance"><i class="fa fa-check"></i><b>11.3.1</b> Evaluating Model Performance<span></span></a></li>
<li class="chapter" data-level="11.3.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#using-z-score-standardization"><i class="fa fa-check"></i><b>11.3.2</b> Using <span class="math inline">\(z\)</span>-score standardization<span></span></a></li>
<li class="chapter" data-level="11.3.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#testing-alternative-values-of-k"><i class="fa fa-check"></i><b>11.3.3</b> Testing alternative values of <span class="math inline">\(k\)</span><span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>12</b> Linear Regression Models<span></span></a><ul>
<li class="chapter" data-level="12.1" data-path="linear-regression-models.html"><a href="linear-regression-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Simple Linear Regression<span></span></a></li>
<li class="chapter" data-level="12.2" data-path="linear-regression-models.html"><a href="linear-regression-models.html#smoothed-conditional-means"><i class="fa fa-check"></i><b>12.2</b> Smoothed Conditional Means<span></span></a></li>
<li class="chapter" data-level="12.3" data-path="linear-regression-models.html"><a href="linear-regression-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>12.3</b> Multiple Linear Regression<span></span></a></li>
<li class="chapter" data-level="12.4" data-path="linear-regression-models.html"><a href="linear-regression-models.html#example-diamonds"><i class="fa fa-check"></i><b>12.4</b> Example: <code>diamonds</code><span></span></a></li>
<li class="chapter" data-level="12.5" data-path="linear-regression-models.html"><a href="linear-regression-models.html#categorical-predictors"><i class="fa fa-check"></i><b>12.5</b> Categorical Predictors<span></span></a></li>
<li class="chapter" data-level="12.6" data-path="linear-regression-models.html"><a href="linear-regression-models.html#compare-models-using-anova"><i class="fa fa-check"></i><b>12.6</b> Compare models using ANOVA<span></span></a></li>
<li class="chapter" data-level="12.7" data-path="linear-regression-models.html"><a href="linear-regression-models.html#prediction"><i class="fa fa-check"></i><b>12.7</b> Prediction<span></span></a></li>
<li class="chapter" data-level="12.8" data-path="linear-regression-models.html"><a href="linear-regression-models.html#interaction-terms"><i class="fa fa-check"></i><b>12.8</b> Interaction Terms<span></span></a></li>
<li class="chapter" data-level="12.9" data-path="linear-regression-models.html"><a href="linear-regression-models.html#variable-transformation"><i class="fa fa-check"></i><b>12.9</b> Variable Transformation<span></span></a></li>
<li class="chapter" data-level="12.10" data-path="linear-regression-models.html"><a href="linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>12.10</b> Polynomial Regression<span></span></a></li>
<li class="chapter" data-level="12.11" data-path="linear-regression-models.html"><a href="linear-regression-models.html#stepwise-regression"><i class="fa fa-check"></i><b>12.11</b> Stepwise regression<span></span></a></li>
<li class="chapter" data-level="12.12" data-path="linear-regression-models.html"><a href="linear-regression-models.html#best-subset"><i class="fa fa-check"></i><b>12.12</b> Best subset<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logistic-regression-model.html"><a href="logistic-regression-model.html"><i class="fa fa-check"></i><b>13</b> Logistic Regression Model<span></span></a></li>
<li class="chapter" data-level="14" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>14</b> k-means Clustering<span></span></a><ul>
<li class="chapter" data-level="14.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-3"><i class="fa fa-check"></i><b>14.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="14.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications"><i class="fa fa-check"></i><b>14.2</b> Applications<span></span></a><ul>
<li class="chapter" data-level="14.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#cluster-analysis"><i class="fa fa-check"></i><b>14.2.1</b> Cluster Analysis<span></span></a></li>
<li class="chapter" data-level="14.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#image-segementation-and-image-compression"><i class="fa fa-check"></i><b>14.2.2</b> Image Segementation and Image Compression<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>15</b> Hierarchical Clustering<span></span></a><ul>
<li class="chapter" data-level="15.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#dissimilarity-measure-and-linkage"><i class="fa fa-check"></i><b>15.1</b> Dissimilarity measure and Linkage<span></span></a></li>
<li class="chapter" data-level="15.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#alogrithm"><i class="fa fa-check"></i><b>15.2</b> Alogrithm<span></span></a></li>
<li class="chapter" data-level="15.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#applications-1"><i class="fa fa-check"></i><b>15.3</b> Applications<span></span></a><ul>
<li class="chapter" data-level="15.3.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#nci60-data"><i class="fa fa-check"></i><b>15.3.1</b> NCI60 Data<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>16</b> Resampling Methods<span></span></a><ul>
<li class="chapter" data-level="16.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>16.1</b> Cross-validation<span></span></a><ul>
<li class="chapter" data-level="16.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cv-in-glm"><i class="fa fa-check"></i><b>16.1.1</b> CV in GLM<span></span></a></li>
<li class="chapter" data-level="16.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#general-implementation"><i class="fa fa-check"></i><b>16.1.2</b> General Implementation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="resampling-methods.html"><a href="resampling-methods.html#bootstrap"><i class="fa fa-check"></i><b>16.2</b> Bootstrap<span></span></a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>17</b> Regularization<span></span></a><ul>
<li class="chapter" data-level="17.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>17.1</b> Ridge Regression<span></span></a></li>
<li class="chapter" data-level="17.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>17.2</b> LASSO<span></span></a></li>
<li class="chapter" data-level="17.3" data-path="regularization.html"><a href="regularization.html#selecting-the-tuning-parameter"><i class="fa fa-check"></i><b>17.3</b> Selecting the tuning parameter<span></span></a></li>
<li class="chapter" data-level="17.4" data-path="regularization.html"><a href="regularization.html#glmnet"><i class="fa fa-check"></i><b>17.4</b> <code>glmnet</code><span></span></a><ul>
<li class="chapter" data-level="17.4.1" data-path="regularization.html"><a href="regularization.html#ridge-regression-1"><i class="fa fa-check"></i><b>17.4.1</b> Ridge Regression<span></span></a></li>
<li class="chapter" data-level="17.4.2" data-path="regularization.html"><a href="regularization.html#lasso-1"><i class="fa fa-check"></i><b>17.4.2</b> LASSO<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>18</b> Decision Trees<span></span></a><ul>
<li class="chapter" data-level="18.1" data-path="decision-trees.html"><a href="decision-trees.html#introduction-to-classification-tree"><i class="fa fa-check"></i><b>18.1</b> Introduction to Classification Tree<span></span></a></li>
<li class="chapter" data-level="18.2" data-path="decision-trees.html"><a href="decision-trees.html#introduction-to-regression-tree"><i class="fa fa-check"></i><b>18.2</b> Introduction to regression tree<span></span></a></li>
<li class="chapter" data-level="18.3" data-path="decision-trees.html"><a href="decision-trees.html#mathematical-formulation"><i class="fa fa-check"></i><b>18.3</b> Mathematical Formulation<span></span></a></li>
<li class="chapter" data-level="18.4" data-path="decision-trees.html"><a href="decision-trees.html#examples"><i class="fa fa-check"></i><b>18.4</b> Examples<span></span></a><ul>
<li class="chapter" data-level="18.4.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-tree"><i class="fa fa-check"></i><b>18.4.1</b> Classification Tree<span></span></a></li>
<li class="chapter" data-level="18.4.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-tree"><i class="fa fa-check"></i><b>18.4.2</b> Regression Tree<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>19</b> Ensemble Methods<span></span></a><ul>
<li class="chapter" data-level="19.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>19.1</b> Bagging<span></span></a></li>
<li class="chapter" data-level="19.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>19.2</b> Random Forest<span></span></a></li>
<li class="chapter" data-level="19.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#example"><i class="fa fa-check"></i><b>19.3</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>20</b> Deep Learning<span></span></a><ul>
<li class="chapter" data-level="20.1" data-path="deep-learning.html"><a href="deep-learning.html#introduction-4"><i class="fa fa-check"></i><b>20.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="20.2" data-path="deep-learning.html"><a href="deep-learning.html#single-layer-neural-netowrks"><i class="fa fa-check"></i><b>20.2</b> Single Layer Neural Netowrks<span></span></a></li>
<li class="chapter" data-level="20.3" data-path="deep-learning.html"><a href="deep-learning.html#multilayer-neural-networks"><i class="fa fa-check"></i><b>20.3</b> Multilayer Neural Networks<span></span></a></li>
<li class="chapter" data-level="20.4" data-path="deep-learning.html"><a href="deep-learning.html#examples-1"><i class="fa fa-check"></i><b>20.4</b> Examples<span></span></a><ul>
<li class="chapter" data-level="20.4.1" data-path="deep-learning.html"><a href="deep-learning.html#mnist"><i class="fa fa-check"></i><b>20.4.1</b> MNIST<span></span></a></li>
<li class="chapter" data-level="20.4.2" data-path="deep-learning.html"><a href="deep-learning.html#encoding-the-data"><i class="fa fa-check"></i><b>20.4.2</b> Encoding the data<span></span></a></li>
<li class="chapter" data-level="20.4.3" data-path="deep-learning.html"><a href="deep-learning.html#implementation"><i class="fa fa-check"></i><b>20.4.3</b> Implementation<span></span></a></li>
<li class="chapter" data-level="20.4.4" data-path="deep-learning.html"><a href="deep-learning.html#multilayer-neural-network"><i class="fa fa-check"></i><b>20.4.4</b> Multilayer Neural Network<span></span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 362 R for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deep-learning" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 20</span> Deep Learning<a href="deep-learning.html#deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>References:</p>
<ul>
<li><p>Ch10: in An introduction to Statistical Leraning with applications in R by James, Witten, Hastie and Tibshirani (some basic introduction with some math equations)</p></li>
<li><p>Deep Learning with R by Francois Chollet with J.J. Allaire. (no math equations but offer practical guildlines in implementation)</p></li>
<li><p><a href="https://tensorflow.rstudio.com/tutorials/beginners/" class="uri">https://tensorflow.rstudio.com/tutorials/beginners/</a></p></li>
</ul>
<div id="introduction-4" class="section level2 hasAnchor">
<h2><span class="header-section-number">20.1</span> Introduction<a href="deep-learning.html#introduction-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the past few years, artificial intelligence (AI), machine learning, deep learning have been subjects of intense media hype.</p>
<p>One may define AI as the study to automate intellectual tasks normally performed by humans. AI encompasses machine learning but also includes other approaches that don’t involve any learning. For example, early chess programs only involved hardcoded rules crafted by programmers. For a fairly long time, many experts believed that human-level AI could be achieved by having programmers handcraft a sufficiently large set of explicit rules for manipulating knowledge. This approach is known as symbolic AI.</p>
<p>It turns out it is intractable to figure out explicit rules for solving more complex problems, such as image classification, speech recognition, and language translation. A new appraoch is to use machine learning.</p>
<p>You have already studied various machine learning methods and how to apply them with R: <span class="math inline">\(k\)</span>-NN, linear regression, logistic regression, <span class="math inline">\(k\)</span>-means clustering, hierarchical clustering, ridge regression, LASSO, decision trees, random forest. These approaches are also examples of shallow learning.</p>
<p>So, what is deep learning?</p>
<ul>
<li><p>Deep learning is a subset of machine learning</p></li>
<li><p>The deep in deep learning refers to the idea of successive layers of representations (see next section)</p></li>
<li><p>These layered representations are learned via neural networks, which is the cornerstone of deep learning</p></li>
<li><p>Some important factors that drive the advances in machine learning</p>
<ul>
<li>Hardware: faster CPU (central processing units) and GPU (graphics processing unit), TPU (tensor processing unit)</li>
<li>Datasets: the rise of internet allows people to collect many data</li>
<li>Benchmarks: competitions such as Kaggle allow people to have benchmarks that researchers compete to beat</li>
<li>Algorithmic advances</li>
</ul></li>
</ul>
</div>
<div id="single-layer-neural-netowrks" class="section level2 hasAnchor">
<h2><span class="header-section-number">20.2</span> Single Layer Neural Netowrks<a href="deep-learning.html#single-layer-neural-netowrks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <strong>neural network</strong> takes an input vector of <span class="math inline">\(p\)</span> variables <span class="math inline">\(X = (X_1,\ldots,X_p)\)</span> and builds a nonlinear function <span class="math inline">\(f(X)\)</span> to predict the response <span class="math inline">\(Y\)</span>. A linear function is a function of the form <span class="math display">\[f(X) = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p.\]</span> In general, a linear function is not enough to describe the complex relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Graphical illustration of a single layer neural network:</p>
<p><img src="image/single_layer_NN.PNG" width="60%" style="display: block; margin: auto;" /></p>
<ul>
<li>Input layer: consists of <span class="math inline">\(p\)</span> input variables (the picture shows <span class="math inline">\(4\)</span> but we can have many more)</li>
<li>Hidden layer: consists of <span class="math inline">\(K\)</span> hidden units (the picture shows <span class="math inline">\(5\)</span> but we can have many more)</li>
<li>Output layer: prediction of the response (we can have more than one output variable)</li>
</ul>
<p>Activations <span class="math inline">\(A_k\)</span>, <span class="math inline">\(k=1,\ldots,K\)</span> are computed as functions of the input features
<span class="math display">\[\begin{equation*}
A_k = g\left(w_{k0} + \sum^p_{j=1} w_{kj}X_j \right),
\end{equation*}\]</span>
where <span class="math inline">\(g\)</span> is a nonlinear activation function that is specified in advance and the weights <span class="math inline">\(w_{k0},\ldots,w_{kp}\)</span> have to be estimated from the data. For the function <span class="math inline">\(g\)</span>, it is common to use ReLU (rectified linear unit), which takes the form
<span class="math display">\[\begin{equation*}
g(z) = z I(z \geq 0).
\end{equation*}\]</span>
With nonlinear activation functions, it is possible for the model to capture complex nonlinearities and interaction effects (e.g. <span class="math inline">\(X_1 X_2\)</span>).</p>
<p>Representation: We can think of each <span class="math inline">\(A_k\)</span> as a different transformation <span class="math inline">\(h_k(X) = g\left(w_{k0} + \sum^p_{j=1} w_{kj}X_j \right)\)</span> of the original features.</p>
<p>For regression problem, the output is
<span class="math display">\[\begin{equation*}
f(X) = \beta_0 + \sum^K_{k=1} \beta_k A_k.
\end{equation*}\]</span></p>
<p>If you want to write in terms of the original <span class="math inline">\(X\)</span>:
<span class="math display">\[\begin{equation*}
f(X) = \beta_0 + \sum^K_{k=1} \beta_k g\left(w_{k0} + \sum^p_{j=1} w_{kj}X_j \right).
\end{equation*}\]</span></p>
<p><strong>Estimation</strong></p>
<p>Parameters in the model: <span class="math inline">\(\beta_0,\ldots,\beta_K, w_{10},\ldots,w_{Kp}\)</span>.</p>
<p>Suppose we have <span class="math inline">\(n\)</span> data <span class="math inline">\(\{ (x_i, y_i)\}^n_{i=1}\)</span>. To estimate these parameters, the squared-error loss is typically used. That is, we wish to find parameters to minimize
<span class="math display">\[\begin{equation*}
\sum^n_{i=1} (y_i - f(x_i))^2.
\end{equation*}\]</span></p>
<p>We will describe how to use neural network in classification problems in the next section.</p>
<p><strong>Remark</strong></p>
<p>The name neural network originally derived from thinking of the model as analogous to neurons in the brain. But there is no need to think in that way.</p>
</div>
<div id="multilayer-neural-networks" class="section level2 hasAnchor">
<h2><span class="header-section-number">20.3</span> Multilayer Neural Networks<a href="deep-learning.html#multilayer-neural-networks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Modern neural networks typically have more than one hidden layer, and many units per layer. In general, you can also have more than one output variable. In particular, if you have a classification problem at hand with <span class="math inline">\(M\)</span> classes. You will need <span class="math inline">\(M\)</span> output variables to model the probabilities of the response being in the <span class="math inline">\(M\)</span> classes.</p>
<p>A graphical illustration of a neural network with <span class="math inline">\(2\)</span> hidden layers:</p>
<p><img src="image/multilayer_NN.PNG" width="60%" style="display: block; margin: auto;" /></p>
<ul>
<li>Input layer: consists of <span class="math inline">\(p\)</span> input variables</li>
<li><span class="math inline">\(1\)</span>st Hidden layer: consists of <span class="math inline">\(K_1\)</span> hidden units</li>
<li><span class="math inline">\(2\)</span>st Hidden layer: consists of <span class="math inline">\(K_2\)</span> hidden units</li>
<li>Output layer: prediction of the responses</li>
</ul>
<p>The activations in the first hidden layer:
<span class="math display">\[\begin{equation*}
A^{(1)}_k = g\left(w^{(1)}_{k0} + \sum^p_{j=1} w^{(1)}_{kj} X_j\right)
\end{equation*}\]</span>
for <span class="math inline">\(k=1,\ldots,K_1\)</span>.</p>
<p>The activations in the second hidden layer treats the activations <span class="math inline">\(A^{(1)}_k\)</span> of the first hidden layer as inputs and computes new activations:</p>
<p><span class="math display">\[\begin{equation*}
A^{(2)}_k = g\left(w^{(2)}_{k0} + \sum^{K_1}_{k=1} w^{(2)}_{kj} A_k^{(1)}\right)
\end{equation*}\]</span></p>
<p>Output layer:</p>
<p>For regression problem:
<span class="math display">\[\begin{equation*}
f_m(X) = \beta_{m0} + \sum^{K_2}_{l=1} \beta_{ml} A^{(2)}_l,\quad \text{for } m = 1,\ldots,M.
\end{equation*}\]</span>
In terms of the original <span class="math inline">\(X\)</span>, we have
<span class="math display">\[\begin{equation*}
f_m(X)= \beta_{m0} + \sum^{K_2}_{l=1} \beta_{ml} g\left(w^{(2)}_{k0} + \sum^{K_1}_{k=1} w^{(2)}_{kj} g\left(w^{(1)}_{k0} + \sum^p_{j=1} w^{(1)}_{kj} X_j\right)\right).
\end{equation*}\]</span></p>
<p>For classification problem, we do not directly output the predicted class but we try to predict the probability <span class="math inline">\(P(Y=m|X)\)</span>. Hence, we use
<span class="math display">\[\begin{equation*}
Z_m = \beta_{m0} + \sum^{K_2}_{l=1} \beta_{ml} A^{(2)}_l,\quad \text{for } m = 1,\ldots,M
\end{equation*}\]</span>
and set
<span class="math display">\[\begin{equation*}
f_m(Z) =  \frac{e^{Z_m}}{\sum^M_{m=1} e^{Z_m}}, \text{for } m=1,\ldots,M.
\end{equation*}\]</span>
The function in the last displayed equation is called the softmax activation function. This ensures all the outputs behave like probabilities (non-negative and sum to one).</p>
<p>For classification problem, we use the one-hot encoding for <span class="math inline">\(y_i\)</span> so that <span class="math inline">\(y_{im} = 1\)</span> if <span class="math inline">\(y_i = m\)</span> and <span class="math inline">\(y_{il} = 0\)</span> for <span class="math inline">\(l \neq m\)</span>. Then we estimate the parameters using the cross-entropy:
<span class="math display">\[\begin{equation*}
- \sum^n_{i=1} \sum^M_{m=1} y_{im} \log (f_m(x_i)),
\end{equation*}\]</span>
which is the negative of the likelihood of a multinomial distribution.</p>
<p><strong>Remark</strong></p>
<p>The multinomial distribution is a generalization of the binomial distribution. Recall that a binomial distribution models the number of successes in <span class="math inline">\(n\)</span> independent Bernoulli trials. In a Bernoulli trial, there are only two possible outcomes, success and failure. When we have <span class="math inline">\(M\)</span> classes and <span class="math inline">\(n\)</span> independent trials, we obtain a multinomial distribution. A special case is when we only have one trial. In that case, it is called a categorical distribution and the probability mass function is
<span class="math display">\[\begin{equation*}
P(Y = m) = p_m, \quad m =1,\ldots,M,
\end{equation*}\]</span>
where <span class="math inline">\(p_m \geq 0, \sum^M_{m=1} p_m = 1\)</span>. When you observe <span class="math inline">\(y_i\)</span>, the likelihood becomes
<span class="math display">\[\begin{equation*}
\prod^M_{m=1} p_m^{I(y_i = m)}.
\end{equation*}\]</span>
The log-likelihood is
<span class="math display">\[\begin{equation*}
\sum^M_{m=1} I(y_i = m ) \log p_m = \sum^M_{m=1} y_{im} \log p_m.
\end{equation*}\]</span></p>
</div>
<div id="examples-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">20.4</span> Examples<a href="deep-learning.html#examples-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="mnist" class="section level3 hasAnchor">
<h3><span class="header-section-number">20.4.1</span> MNIST<a href="deep-learning.html#mnist" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will use Keras, a deep-learning framework that provides a convenient way to define and train deep-learning models. One of the backend implementations in Keras is TensorFlow, which is one of the primary platforms for deep learning today. Almost every recent deep-learning competition has been won using Keras models. For serious deep learning users, need to train deep learning models with GPUs.</p>
<p>To install Keras, follow the installation guide on <a href="https://tensorflow.rstudio.com/guide/keras/" class="uri">https://tensorflow.rstudio.com/guide/keras/</a></p>
<p>We will fit a neural network to recognize handwritten digits from the MNIST dataset. MNIST consists of <span class="math inline">\(28 \times 28\)</span> greayscale images of handwritten digits like these:</p>
<p><img src="image/MNIST.PNG" width="60%" style="display: block; margin: auto;" /></p>
<p>The MNIST dataset is a classic dataset in the machine-learning community which has been around almost as long as the field itself and has been intensively studied. It’s a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST as the “Hello World” of deep learning—it’s what you do to verify that your algorithms are working as expected.</p>
<p>Load the data:</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="deep-learning.html#cb508-1"></a><span class="kw">library</span>(keras)</span>
<span id="cb508-2"><a href="deep-learning.html#cb508-2"></a></span>
<span id="cb508-3"><a href="deep-learning.html#cb508-3"></a>mnist &lt;-<span class="st"> </span><span class="kw">dataset_mnist</span>()</span>
<span id="cb508-4"><a href="deep-learning.html#cb508-4"></a>train_images &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>x</span>
<span id="cb508-5"><a href="deep-learning.html#cb508-5"></a>train_labels &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>y</span>
<span id="cb508-6"><a href="deep-learning.html#cb508-6"></a>test_images &lt;-<span class="st"> </span>mnist<span class="op">$</span>test<span class="op">$</span>x</span>
<span id="cb508-7"><a href="deep-learning.html#cb508-7"></a>test_labels &lt;-<span class="st"> </span>mnist<span class="op">$</span>test<span class="op">$</span>y</span></code></pre></div>
<p>The images are encoded as 3D arrays, and the labels are a 1D array of digits, ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(9\)</span>. There is a one-to-one correspondence between the images and the labels.</p>
<p>To view the data of the first image (which is a <span class="math inline">\(28 \times 28\)</span> matrix of integers in <span class="math inline">\([0, 255]\)</span>):</p>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="deep-learning.html#cb509-1"></a><span class="co"># this is how we access the 1st element in the 1st dimension of an 3D array</span></span>
<span id="cb509-2"><a href="deep-learning.html#cb509-2"></a>train_images[<span class="dv">1</span>, , ] </span></code></pre></div>
</div>
<div id="encoding-the-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">20.4.2</span> Encoding the data<a href="deep-learning.html#encoding-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For each image, we first turn the matrix of values into a vector of values. We also rescale the data to have values in <span class="math inline">\([0, 1]\)</span>. The data originally take integer values in <span class="math inline">\([0, 255]\)</span>.</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="deep-learning.html#cb510-1"></a>train_images &lt;-<span class="st"> </span><span class="kw">array_reshape</span>(train_images, <span class="kw">c</span>(<span class="dv">60000</span>, <span class="dv">28</span> <span class="op">*</span><span class="st"> </span><span class="dv">28</span>))</span>
<span id="cb510-2"><a href="deep-learning.html#cb510-2"></a>train_images &lt;-<span class="st"> </span>train_images <span class="op">/</span><span class="st"> </span><span class="dv">255</span></span>
<span id="cb510-3"><a href="deep-learning.html#cb510-3"></a></span>
<span id="cb510-4"><a href="deep-learning.html#cb510-4"></a>test_images &lt;-<span class="st"> </span><span class="kw">array_reshape</span>(test_images, <span class="kw">c</span>(<span class="dv">10000</span>, <span class="dv">28</span> <span class="op">*</span><span class="st"> </span><span class="dv">28</span>))</span>
<span id="cb510-5"><a href="deep-learning.html#cb510-5"></a>test_images &lt;-<span class="st"> </span>test_images <span class="op">/</span><span class="st"> </span><span class="dv">255</span></span></code></pre></div>
<p>The original labels take integer values from <span class="math inline">\(0\)</span> to <span class="math inline">\(9\)</span>. We will apply one-hot encoding to the labels:</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="deep-learning.html#cb511-1"></a>train_labels &lt;-<span class="st"> </span><span class="kw">to_categorical</span>(train_labels)</span>
<span id="cb511-2"><a href="deep-learning.html#cb511-2"></a>test_labels &lt;-<span class="st"> </span><span class="kw">to_categorical</span>(test_labels)</span></code></pre></div>
<p>For example, if there are only <span class="math inline">\(3\)</span> classes labeled as <span class="math inline">\(1,2,3\)</span> and <span class="math inline">\(y = 2\)</span>. Then we want to turn <span class="math inline">\(y\)</span> into <span class="math inline">\((0, 1, 0)\)</span>. If <span class="math inline">\(y = 1\)</span>, we want to turn it into <span class="math inline">\((1, 0, 0)\)</span>.</p>
</div>
<div id="implementation" class="section level3 hasAnchor">
<h3><span class="header-section-number">20.4.3</span> Implementation<a href="deep-learning.html#implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Model specification</strong></p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="deep-learning.html#cb512-1"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb512-2"><a href="deep-learning.html#cb512-2"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">512</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">28</span> <span class="op">*</span><span class="st"> </span><span class="dv">28</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb512-3"><a href="deep-learning.html#cb512-3"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb512-4"><a href="deep-learning.html#cb512-4"></a></span>
<span id="cb512-5"><a href="deep-learning.html#cb512-5"></a>model</span>
<span id="cb512-6"><a href="deep-learning.html#cb512-6"></a><span class="co">## Model: &quot;sequential&quot;</span></span>
<span id="cb512-7"><a href="deep-learning.html#cb512-7"></a><span class="co">## ________________________________________________________________________________________________</span></span>
<span id="cb512-8"><a href="deep-learning.html#cb512-8"></a><span class="co">##  Layer (type)                              Output Shape                          Param #        </span></span>
<span id="cb512-9"><a href="deep-learning.html#cb512-9"></a><span class="co">## ================================================================================================</span></span>
<span id="cb512-10"><a href="deep-learning.html#cb512-10"></a><span class="co">##  dense_1 (Dense)                           (None, 512)                           401920         </span></span>
<span id="cb512-11"><a href="deep-learning.html#cb512-11"></a><span class="co">##                                                                                                 </span></span>
<span id="cb512-12"><a href="deep-learning.html#cb512-12"></a><span class="co">##  dense (Dense)                             (None, 10)                            5130           </span></span>
<span id="cb512-13"><a href="deep-learning.html#cb512-13"></a><span class="co">##                                                                                                 </span></span>
<span id="cb512-14"><a href="deep-learning.html#cb512-14"></a><span class="co">## ================================================================================================</span></span>
<span id="cb512-15"><a href="deep-learning.html#cb512-15"></a><span class="co">## Total params: 407,050</span></span>
<span id="cb512-16"><a href="deep-learning.html#cb512-16"></a><span class="co">## Trainable params: 407,050</span></span>
<span id="cb512-17"><a href="deep-learning.html#cb512-17"></a><span class="co">## Non-trainable params: 0</span></span>
<span id="cb512-18"><a href="deep-learning.html#cb512-18"></a><span class="co">## ________________________________________________________________________________________________</span></span></code></pre></div>
<p>There are <span class="math inline">\(407,050\)</span> parameters! The above code means we specify a model with</p>
<ul>
<li>One hidden layer with <span class="math inline">\(512\)</span> units, the activation function is ReLU (rectified linear unit)</li>
<li>The output layer has <span class="math inline">\(10\)</span> units, the activation is the softmax function because we are dealing with a multiclass classification problem.</li>
</ul>
<p><strong>Compiling the model</strong></p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="deep-learning.html#cb513-1"></a>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</span>
<span id="cb513-2"><a href="deep-learning.html#cb513-2"></a>  <span class="dt">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb513-3"><a href="deep-learning.html#cb513-3"></a>  <span class="dt">loss =</span> <span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb513-4"><a href="deep-learning.html#cb513-4"></a>  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb513-5"><a href="deep-learning.html#cb513-5"></a>)</span></code></pre></div>
<p>For multiclass classification, the above setting is in general ok.</p>
<p><strong>Setting aside a validation set</strong></p>
<p>Setting aside a validation set to choose the number of epochs (each iteration over all the training data is called an <strong>epoch</strong>). The number of epochs is an important tunning parameter that one has to decide in order not to overfit the training data.</p>
<p>Note: since we have a lot of data, we use a simple validation approach instead of the cross-validation.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="deep-learning.html#cb514-1"></a>index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">dim</span>(train_images)[<span class="dv">1</span>], <span class="dv">10000</span>)</span>
<span id="cb514-2"><a href="deep-learning.html#cb514-2"></a>partial_train_images &lt;-<span class="st"> </span>train_images[<span class="op">-</span>index, ]</span>
<span id="cb514-3"><a href="deep-learning.html#cb514-3"></a>partial_train_labels &lt;-<span class="st"> </span>train_labels[<span class="op">-</span>index, ]</span>
<span id="cb514-4"><a href="deep-learning.html#cb514-4"></a>validation_images &lt;-<span class="st"> </span>train_images[index, ]</span>
<span id="cb514-5"><a href="deep-learning.html#cb514-5"></a>validation_labels &lt;-<span class="st"> </span>train_labels[index, ]</span></code></pre></div>
<p>Train the model with <span class="math inline">\(15\)</span> epochs and evaluate the loss using the validation data:</p>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="deep-learning.html#cb515-1"></a>history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(</span>
<span id="cb515-2"><a href="deep-learning.html#cb515-2"></a>  partial_train_images,</span>
<span id="cb515-3"><a href="deep-learning.html#cb515-3"></a>  partial_train_labels,</span>
<span id="cb515-4"><a href="deep-learning.html#cb515-4"></a>  <span class="dt">epochs =</span> <span class="dv">15</span>,</span>
<span id="cb515-5"><a href="deep-learning.html#cb515-5"></a>  <span class="dt">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb515-6"><a href="deep-learning.html#cb515-6"></a>  <span class="dt">validation_data =</span> <span class="kw">list</span>(validation_images, validation_labels)</span>
<span id="cb515-7"><a href="deep-learning.html#cb515-7"></a>)</span></code></pre></div>
<p>Create a plot to visualize the relationship between the number of epoch and loss as well as accuracy in the validation data (and training data).</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="deep-learning.html#cb516-1"></a><span class="kw">plot</span>(history)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-580-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>From the above plot, we can see that the training error keeps on decreasing when we train the model with more epochs. However, it seems that after <span class="math inline">\(5\)</span> epochs, the model starts to overfit the data as the validation loss starts to increase. Therefore, we shall only train the model with <span class="math inline">\(5\)</span> epochs. Now, we will retrain our model using all the training data but set <code>epochs = 5</code>.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="deep-learning.html#cb517-1"></a><span class="co"># same as before</span></span>
<span id="cb517-2"><a href="deep-learning.html#cb517-2"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb517-3"><a href="deep-learning.html#cb517-3"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">512</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">28</span> <span class="op">*</span><span class="st"> </span><span class="dv">28</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb517-4"><a href="deep-learning.html#cb517-4"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb517-5"><a href="deep-learning.html#cb517-5"></a></span>
<span id="cb517-6"><a href="deep-learning.html#cb517-6"></a><span class="co"># same as before</span></span>
<span id="cb517-7"><a href="deep-learning.html#cb517-7"></a>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</span>
<span id="cb517-8"><a href="deep-learning.html#cb517-8"></a>  <span class="dt">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</span>
<span id="cb517-9"><a href="deep-learning.html#cb517-9"></a>  <span class="dt">loss =</span> <span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb517-10"><a href="deep-learning.html#cb517-10"></a>  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb517-11"><a href="deep-learning.html#cb517-11"></a>)</span>
<span id="cb517-12"><a href="deep-learning.html#cb517-12"></a></span>
<span id="cb517-13"><a href="deep-learning.html#cb517-13"></a><span class="co"># no need to use validation data now</span></span>
<span id="cb517-14"><a href="deep-learning.html#cb517-14"></a>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(</span>
<span id="cb517-15"><a href="deep-learning.html#cb517-15"></a>  train_images,</span>
<span id="cb517-16"><a href="deep-learning.html#cb517-16"></a>  train_labels,</span>
<span id="cb517-17"><a href="deep-learning.html#cb517-17"></a>  <span class="dt">epochs =</span> <span class="dv">5</span>,</span>
<span id="cb517-18"><a href="deep-learning.html#cb517-18"></a>  <span class="dt">batch_size =</span> <span class="dv">128</span></span>
<span id="cb517-19"><a href="deep-learning.html#cb517-19"></a>)</span></code></pre></div>
<p>To form the predictions:</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="deep-learning.html#cb518-1"></a><span class="co"># prediction</span></span>
<span id="cb518-2"><a href="deep-learning.html#cb518-2"></a>prediction &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(test_images)</span>
<span id="cb518-3"><a href="deep-learning.html#cb518-3"></a></span>
<span id="cb518-4"><a href="deep-learning.html#cb518-4"></a><span class="co"># these are the predicted probabilities of each class for the 1st observation</span></span>
<span id="cb518-5"><a href="deep-learning.html#cb518-5"></a>prediction[<span class="dv">1</span>, ] </span>
<span id="cb518-6"><a href="deep-learning.html#cb518-6"></a><span class="co">##  [1] 5.189253e-09 1.674244e-10 2.472562e-07 2.095135e-05 4.843469e-13 2.812344e-08 5.421178e-14</span></span>
<span id="cb518-7"><a href="deep-learning.html#cb518-7"></a><span class="co">##  [8] 9.999783e-01 4.156804e-08 4.363555e-07</span></span></code></pre></div>
<p>Evaluate the performance:</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="deep-learning.html#cb519-1"></a>result &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">evaluate</span>(test_images, test_labels)</span>
<span id="cb519-2"><a href="deep-learning.html#cb519-2"></a>result</span>
<span id="cb519-3"><a href="deep-learning.html#cb519-3"></a><span class="co">##      loss  accuracy </span></span>
<span id="cb519-4"><a href="deep-learning.html#cb519-4"></a><span class="co">## 0.0641778 0.9799000</span></span></code></pre></div>
<p>We see that our basic neural network model is able to achieve 0.98 accuracy in a <span class="math inline">\(10\)</span>-class classification problem.</p>
</div>
<div id="multilayer-neural-network" class="section level3 hasAnchor">
<h3><span class="header-section-number">20.4.4</span> Multilayer Neural Network<a href="deep-learning.html#multilayer-neural-network" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you want to fit a model with two hidden layers, where the first hidden layer has <span class="math inline">\(512\)</span> units and the second layer has <span class="math inline">\(128\)</span> units:</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="deep-learning.html#cb520-1"></a>model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb520-2"><a href="deep-learning.html#cb520-2"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">512</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">28</span> <span class="op">*</span><span class="st"> </span><span class="dv">28</span>)) <span class="op">%&gt;%</span></span>
<span id="cb520-3"><a href="deep-learning.html#cb520-3"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">128</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">28</span> <span class="op">*</span><span class="st"> </span><span class="dv">28</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb520-4"><a href="deep-learning.html#cb520-4"></a><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">&quot;softmax&quot;</span>)</span></code></pre></div>
<p>Now, you should understand how to include more layers and change the number of units.</p>
<!-- ## Keras -->
<!-- Keras is a deep-learning framework that provides a convenient way to define and train deep-learning models. -->
<!-- - allow the code to run on CPU or GPU -->
<!-- almost every recent deep-learning competition has been won using Keras models -->
<!-- One of the backend implementations in Keras is TensorFlow, which is one of the primary platforms for deep learning today. -->
<!-- Installation: -->
<!-- For serious deep learning users, need to train deep learning using GPUs. -->
<!-- ## Two-class classification (binary classification) -->
<!-- The IMDB dataset -->
<!-- We'll be working with "IMDB dataset", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews. -->
<!-- the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary. -->
<!-- The following code will load the dataset (when you run it for the first time, about 80MB of data will be downloaded to your machine): -->
<!-- ```{r, results='hide'} -->
<!-- library(keras) -->
<!-- imdb <- dataset_imdb(num_words = 10000) -->
<!-- c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb -->
<!-- ``` -->
<!-- The argument `num_words = 10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows us to work with vector data of manageable size. -->
<!-- The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for "negative" and 1 stands for "positive": -->
<!-- Layers -->
<!-- Input data -->
<!-- Loss function -->
<!-- ## Binary Classification -->
<!-- The goal is to classify movie reviews as positive or negative. -->
<!-- ```{r} -->
<!-- library(keras) -->
<!-- imdb <- dataset_imdb(num_words = 10000) -->
<!-- train_data <- imdb$train$x -->
<!-- train_labels <- imdb$train$y -->
<!-- test_data <- imdb$test$x -->
<!-- test_labels <- imdb$test$y -->
<!-- ``` -->
<!-- The argument `num_words = 10000` means that we will only keep the top $10,000$ most frequently occurring words in the training data. Rare words will be discarded. This allows us to work with vector data of manageable size. -->
<!-- The variables `train_data` and `test_data` are lists of reviews, each review being a vector of word indices (encoding a sequence of words).  -->
<!-- For example, -->
<!-- ```{r} -->
<!-- str(train_data[[1]])  -->
<!-- ``` -->
<!-- The idea is to use an integer to represent a word. For example, `14` corresponds to `this`, `22` corresponds to `film`, `16` corresponds to `was`, etc.  -->
<!-- `train_labels` and `test_labels` are vectors of $0$s and $1$s, where $0$ stands for "negative" and $1$ stands for "positive": -->
<!-- **Encoding the data** -->
<!-- As a basic illustration, we will not make use of the order of the words (the order of the words in a text can be meaningful). Each review now is a vector of integers. Since we do not make use of the order of the words, we only need to record which words appear for each review. We assign each appeared word by $1$. -->
<!-- For example, suppose you have two reviews and there are only $5$ possible words. Suppose that the first review is `c(1, 5, 3)` and the second review is `c(2, 5, 4)`. Then we want to encode the two reviews into a binary matrix -->
<!-- ```{r echo =FALSE} -->
<!-- vectorize_sequences <- function(sequences, dimension = 10000) { -->
<!--   results <- matrix(0, nrow = length(sequences), ncol = dimension) -->
<!--   for (i in 1:length(sequences)) { -->
<!--     results[i, sequences[[i]]] <- 1 -->
<!--   } -->
<!--   return(results) -->
<!-- } -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # the data  -->
<!-- reviews <- list() -->
<!-- # e.g. 1 = not, 2 = is, 3 = good, 4 = bad, 5 = very  -->
<!-- reviews[[1]] <- c(1, 5, 3) -->
<!-- reviews[[2]] <- c(2, 5, 4) -->
<!-- vectorize_sequences(reviews, dimension = 5) -->
<!-- ``` -->
<!-- The function `vecctorize_sequences` can be written as: -->
<!-- ```{r} -->
<!-- vectorize_sequences <- function(sequences, dimension = 10000) { -->
<!--   results <- matrix(0, nrow = length(sequences), ncol = dimension) -->
<!--   for (i in 1:length(sequences)) { -->
<!--     results[i, sequences[[i]]] <- 1 -->
<!--   } -->
<!--   return(results) -->
<!-- } -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # Our vectorized training data -->
<!-- x_train <- vectorize_sequences(train_data) -->
<!-- # Our vectorized test data -->
<!-- x_test <- vectorize_sequences(test_data) -->
<!-- y_train <- train_labels -->
<!-- y_test <- test_labels -->
<!-- y_train <- as.numeric(train_labels) -->
<!-- y_test <- as.numeric(test_labels) -->
<!-- ``` -->
<!-- Define your model: -->
<!-- ```{r} -->
<!-- model <- keras_model_sequential() %>%  -->
<!--   layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%  -->
<!--   layer_dense(units = 16, activation = "relu") %>%  -->
<!--   layer_dense(units = 1, activation = "sigmoid") -->
<!-- ``` -->
<!-- ```{r} -->
<!-- model -->
<!-- ``` -->
<!-- There are $160,305$ parameters in the model! -->
<!-- Compile the model. -->
<!-- ```{r} -->
<!-- model %>% compile( -->
<!--   optimizer = "rmsprop", -->
<!--   loss = "binary_crossentropy", -->
<!--   metrics = c("accuracy") -->
<!-- ) -->
<!-- ``` -->
<!-- Setting aside a validation set -->
<!-- ```{r} -->
<!-- set.seed(1) -->
<!-- index <- sample(nrow(x_train), 10000) -->
<!-- x_val <- x_train[index,] -->
<!-- partial_x_train <- x_train[-index,] -->
<!-- y_val <- y_train[index] -->
<!-- partial_y_train <- y_train[-index] -->
<!-- ``` -->
<!-- Train the model -->
<!-- ```{r} -->
<!-- history <- model %>% fit( -->
<!--   partial_x_train, -->
<!--   partial_y_train, -->
<!--   epochs = 20, -->
<!--   batch_size = 512, -->
<!--   validation_data = list(x_val, y_val) -->
<!-- ) -->
<!-- ``` -->
<!-- Epoch: 1 epoch =  -->
<!-- ![3-layer network](https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png) -->
<!-- ```{r} -->
<!-- str(history) -->
<!-- plot(history) -->
<!-- ``` -->
<!-- Since the validation error starts to increase at $5$ epoch, we shall only train our model with $4$ epoch. -->
<!-- ```{r} -->
<!-- model <- keras_model_sequential() %>%  -->
<!--   layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%  -->
<!--   layer_dense(units = 16, activation = "relu") %>%  -->
<!--   layer_dense(units = 1, activation = "sigmoid") -->
<!-- model %>% compile( -->
<!--   optimizer = "rmsprop", -->
<!--   loss = "binary_crossentropy", -->
<!--   metrics = c("accuracy") -->
<!-- ) -->
<!-- model %>% fit(x_train, y_train, epochs = 4, batch_size = 512) -->
<!-- ``` -->
<!-- **Prediction Accuracy in test data** -->
<!-- ```{r} -->
<!-- (results <- model %>% evaluate(x_test, y_test)) -->
<!-- ``` -->
<!-- **Predicted probability of "positive" for the first $10$ test data** -->
<!-- ```{r} -->
<!-- model %>% predict(x_test[1:10,]) -->
<!-- ``` -->
<!-- ## Multiclass Classification -->
<!-- Classifying news wires by topic -->
<!-- ## Regression -->
<!-- Estimating the price of a house, given real-estate data -->
<!-- ## Appendix -->
<!-- To decode the text in `imdb`: -->
<!-- You can decode the data using the following code: -->
<!-- ```{r} -->
<!-- # word_index is a dictionary mapping words to an integer index -->
<!-- word_index <- dataset_imdb_word_index() -->
<!-- # We reverse it, mapping integer indices to words -->
<!-- reverse_word_index <- names(word_index) -->
<!-- names(reverse_word_index) <- word_index -->
<!-- # We decode the review; note that our indices were offset by 3 -->
<!-- # because 0, 1 and 2 are reserved indices for "padding", "start of sequence", and "unknown". -->
<!-- decoded_review <- sapply(train_data[[1]], function(index) { -->
<!--   word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]] -->
<!--   if (!is.null(word)) word else "?" -->
<!-- }) -->
<!-- cat(decoded_review) -->
<!-- ``` -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ensemble-methods.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Book.pdf", "Book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
