<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Classification Trees | STAT 362 R for Data Science</title>
  <meta name="description" content="Notes for STAT 362" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Classification Trees | STAT 362 R for Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes for STAT 362" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Classification Trees | STAT 362 R for Data Science" />
  
  <meta name="twitter:description" content="Notes for STAT 362" />
  

<meta name="author" content="Brian Ling" />


<meta name="date" content="2022-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="k-nearest-neighbors.html"/>
<link rel="next" href="linear-regression-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> What is R and RStudio?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-will-you-learn-in-this-course"><i class="fa fa-check"></i><b>1.2</b> What will you learn in this course?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#r-and-r-as-a-programming-language"><i class="fa fa-check"></i><b>1.2.1</b> R and R as a programming language</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#data-wrangling"><i class="fa fa-check"></i><b>1.2.2</b> Data Wrangling</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#data-visualization"><i class="fa fa-check"></i><b>1.2.3</b> Data Visualization</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#statistical-inference"><i class="fa fa-check"></i><b>1.2.4</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2.5" data-path="introduction.html"><a href="introduction.html#machine-learning"><i class="fa fa-check"></i><b>1.2.5</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2.6" data-path="introduction.html"><a href="introduction.html#some-numerical-methods"><i class="fa fa-check"></i><b>1.2.6</b> Some Numerical Methods</a></li>
<li class="chapter" data-level="1.2.7" data-path="introduction.html"><a href="introduction.html#lastly"><i class="fa fa-check"></i><b>1.2.7</b> Lastly</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#lets-get-started"><i class="fa fa-check"></i><b>1.3</b> Let’s Get Started</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#r-data-structures"><i class="fa fa-check"></i><b>1.4</b> R Data Structures</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#vectors"><i class="fa fa-check"></i><b>1.4.1</b> Vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#factors"><i class="fa fa-check"></i><b>1.4.2</b> Factors</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction.html"><a href="introduction.html#matrix"><i class="fa fa-check"></i><b>1.4.3</b> Matrix</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction.html"><a href="introduction.html#lists"><i class="fa fa-check"></i><b>1.4.4</b> Lists</a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction.html"><a href="introduction.html#data-frames"><i class="fa fa-check"></i><b>1.4.5</b> Data frames</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#operators"><i class="fa fa-check"></i><b>1.5</b> Operators</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#vectorized-operators"><i class="fa fa-check"></i><b>1.5.1</b> Vectorized Operators</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#built-in-functions"><i class="fa fa-check"></i><b>1.6</b> Built-in Functions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#sort"><i class="fa fa-check"></i><b>1.6.1</b> <code>sort()</code></a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction.html"><a href="introduction.html#seq"><i class="fa fa-check"></i><b>1.6.2</b> <code>seq()</code></a></li>
<li class="chapter" data-level="1.6.3" data-path="introduction.html"><a href="introduction.html#rep"><i class="fa fa-check"></i><b>1.6.3</b> <code>rep()</code></a></li>
<li class="chapter" data-level="1.6.4" data-path="introduction.html"><a href="introduction.html#pmax-pmin"><i class="fa fa-check"></i><b>1.6.4</b> <code>pmax</code>, <code>pmin</code></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#some-useful-rstudio-shortcuts"><i class="fa fa-check"></i><b>1.7</b> Some Useful RStudio Shortcuts</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#comments-to-exercises"><i class="fa fa-check"></i><b>1.9</b> Comments to Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Probability Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="probability.html"><a href="probability.html#common-distributions"><i class="fa fa-check"></i><b>2.1.1</b> Common Distributions</a></li>
<li class="chapter" data-level="2.1.2" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>2.1.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability.html"><a href="probability.html#simulation"><i class="fa fa-check"></i><b>2.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>3</b> Programming in R</a><ul>
<li class="chapter" data-level="3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#writing-functions-in-r"><i class="fa fa-check"></i><b>3.1</b> Writing functions in R</a></li>
<li class="chapter" data-level="3.2" data-path="programming-in-r.html"><a href="programming-in-r.html#control-flow"><i class="fa fa-check"></i><b>3.2</b> Control Flow</a><ul>
<li class="chapter" data-level="3.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#for-loop"><i class="fa fa-check"></i><b>3.2.1</b> for loop</a></li>
<li class="chapter" data-level="3.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#while-loop"><i class="fa fa-check"></i><b>3.2.2</b> while loop</a></li>
<li class="chapter" data-level="3.2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond"><i class="fa fa-check"></i><b>3.2.3</b> if (cond)</a></li>
<li class="chapter" data-level="3.2.4" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond-else-expr"><i class="fa fa-check"></i><b>3.2.4</b> if (cond) else expr</a></li>
<li class="chapter" data-level="3.2.5" data-path="programming-in-r.html"><a href="programming-in-r.html#if-else-ladder"><i class="fa fa-check"></i><b>3.2.5</b> If else ladder</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="programming-in-r.html"><a href="programming-in-r.html#automatically-reindent-code"><i class="fa fa-check"></i><b>3.3</b> Automatically Reindent Code</a></li>
<li class="chapter" data-level="3.4" data-path="programming-in-r.html"><a href="programming-in-r.html#speed-consideration"><i class="fa fa-check"></i><b>3.4</b> Speed Consideration</a></li>
<li class="chapter" data-level="3.5" data-path="programming-in-r.html"><a href="programming-in-r.html#another-simulation-example"><i class="fa fa-check"></i><b>3.5</b> Another Simulation Example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html"><i class="fa fa-check"></i><b>4</b> Creating Some Basic Plots</a><ul>
<li class="chapter" data-level="4.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#scatter-plot"><i class="fa fa-check"></i><b>4.1</b> Scatter Plot</a></li>
<li class="chapter" data-level="4.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#line-graph"><i class="fa fa-check"></i><b>4.2</b> Line Graph</a></li>
<li class="chapter" data-level="4.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#bar-chart"><i class="fa fa-check"></i><b>4.3</b> Bar Chart</a></li>
<li class="chapter" data-level="4.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#histogram"><i class="fa fa-check"></i><b>4.4</b> Histogram</a></li>
<li class="chapter" data-level="4.5" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#box-plot"><i class="fa fa-check"></i><b>4.5</b> Box Plot</a></li>
<li class="chapter" data-level="4.6" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#plotting-a-function-curve"><i class="fa fa-check"></i><b>4.6</b> Plotting a function curve</a></li>
<li class="chapter" data-level="4.7" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#more-on-plots-with-base-r"><i class="fa fa-check"></i><b>4.7</b> More on plots with base R</a><ul>
<li class="chapter" data-level="4.7.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#multi-frame-plot"><i class="fa fa-check"></i><b>4.7.1</b> Multi-frame plot</a></li>
<li class="chapter" data-level="4.7.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#subsubsec:type_of_plot"><i class="fa fa-check"></i><b>4.7.2</b> Type of Plot</a></li>
<li class="chapter" data-level="4.7.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#parameters-of-a-plot"><i class="fa fa-check"></i><b>4.7.3</b> Parameters of a plot</a></li>
<li class="chapter" data-level="4.7.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#elements-on-plot"><i class="fa fa-check"></i><b>4.7.4</b> Elements on plot</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#summary-of-ggplot"><i class="fa fa-check"></i><b>4.8</b> Summary of <code>ggplot</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html"><i class="fa fa-check"></i><b>5</b> Managing Data with R</a><ul>
<li class="chapter" data-level="5.1" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a></li>
<li class="chapter" data-level="5.2" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#saving-loading-and-removing-r-data-structures"><i class="fa fa-check"></i><b>5.2</b> Saving, loading, and removing R data structures</a></li>
<li class="chapter" data-level="5.3" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#importing-and-saving-data-from-csv-files"><i class="fa fa-check"></i><b>5.3</b> Importing and saving data from CSV files</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html"><i class="fa fa-check"></i><b>6</b> Review (Chapter 1-5)</a><ul>
<li class="chapter" data-level="6.1" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#simulation-1"><i class="fa fa-check"></i><b>6.1</b> Simulation</a></li>
<li class="chapter" data-level="6.2" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#matrix-1"><i class="fa fa-check"></i><b>6.2</b> Matrix</a></li>
<li class="chapter" data-level="6.3" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#basic-operation"><i class="fa fa-check"></i><b>6.3</b> Basic Operation</a></li>
<li class="chapter" data-level="6.4" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#some-basic-plots-in-r"><i class="fa fa-check"></i><b>6.4</b> Some basic plots in R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html"><i class="fa fa-check"></i><b>7</b> Data Transformation with <code>dplyr</code></a><ul>
<li class="chapter" data-level="7.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#arrange"><i class="fa fa-check"></i><b>7.2</b> <code>arrange()</code></a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-2"><i class="fa fa-check"></i><b>7.2.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#filter"><i class="fa fa-check"></i><b>7.3</b> <code>filter()</code></a><ul>
<li class="chapter" data-level="7.3.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-3"><i class="fa fa-check"></i><b>7.3.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#select"><i class="fa fa-check"></i><b>7.4</b> <code>select()</code></a><ul>
<li class="chapter" data-level="7.4.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-4"><i class="fa fa-check"></i><b>7.4.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#mutate"><i class="fa fa-check"></i><b>7.5</b> <code>mutate()</code></a><ul>
<li class="chapter" data-level="7.5.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-5"><i class="fa fa-check"></i><b>7.5.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summarize-group_by"><i class="fa fa-check"></i><b>7.6</b> <code>summarize(), group_by()</code></a></li>
<li class="chapter" data-level="7.7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#combining-multiple-operations-with-pipe"><i class="fa fa-check"></i><b>7.7</b> Combining Multiple Operations with Pipe <code>%&gt;%</code></a></li>
<li class="chapter" data-level="7.8" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summary"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html"><i class="fa fa-check"></i><b>8</b> Data Visualization with ggplot2</a><ul>
<li class="chapter" data-level="8.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts"><i class="fa fa-check"></i><b>8.1</b> Bar charts</a></li>
<li class="chapter" data-level="8.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graph-1"><i class="fa fa-check"></i><b>8.2</b> Line Graph</a></li>
<li class="chapter" data-level="8.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plots"><i class="fa fa-check"></i><b>8.3</b> Scatter Plots</a><ul>
<li class="chapter" data-level="8.3.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#overplotting"><i class="fa fa-check"></i><b>8.3.1</b> Overplotting</a></li>
<li class="chapter" data-level="8.3.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#labelling-points-in-a-scatter-plot"><i class="fa fa-check"></i><b>8.3.2</b> Labelling points in a scatter plot</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions"><i class="fa fa-check"></i><b>8.4</b> Summarizing Data Distributions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#histogram-1"><i class="fa fa-check"></i><b>8.4.1</b> Histogram</a></li>
<li class="chapter" data-level="8.4.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#kernel-density-estimate"><i class="fa fa-check"></i><b>8.4.2</b> Kernel Density Estimate</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots"><i class="fa fa-check"></i><b>8.5</b> Saving your plots</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-pdf-vector-files"><i class="fa fa-check"></i><b>8.5.1</b> Outputting to pdf vector files</a></li>
<li class="chapter" data-level="8.5.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-bitmap-files"><i class="fa fa-check"></i><b>8.5.2</b> Outputting to bitmap files</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summary-1"><i class="fa fa-check"></i><b>8.6</b> Summary</a><ul>
<li class="chapter" data-level="8.6.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#combining-multiple-operations-with-pipe-1"><i class="fa fa-check"></i><b>8.6.1</b> Combining multiple operations with pipe <code>%&gt;%</code></a></li>
<li class="chapter" data-level="8.6.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts-1"><i class="fa fa-check"></i><b>8.6.2</b> Bar charts</a></li>
<li class="chapter" data-level="8.6.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graphs"><i class="fa fa-check"></i><b>8.6.3</b> Line graphs</a></li>
<li class="chapter" data-level="8.6.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plot-1"><i class="fa fa-check"></i><b>8.6.4</b> Scatter plot</a></li>
<li class="chapter" data-level="8.6.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions-1"><i class="fa fa-check"></i><b>8.6.5</b> Summarizing data distributions</a></li>
<li class="chapter" data-level="8.6.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots-1"><i class="fa fa-check"></i><b>8.6.6</b> Saving your plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference in R</a><ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.1</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#exercises-on-mle"><i class="fa fa-check"></i><b>9.1.1</b> Exercises on MLE</a></li>
<li class="chapter" data-level="9.1.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#summary-2"><i class="fa fa-check"></i><b>9.1.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#interval-estimation-and-hypothesis-testing"><i class="fa fa-check"></i><b>9.2</b> Interval Estimation and Hypothesis Testing</a><ul>
<li class="chapter" data-level="9.2.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.2.1</b> Examples of Hypothesis Testing</a></li>
<li class="chapter" data-level="9.2.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#null-hypotheses-alternative-hypotheses-and-p-values"><i class="fa fa-check"></i><b>9.2.2</b> Null Hypotheses, Alternative Hypotheses, and p-values</a></li>
<li class="chapter" data-level="9.2.3" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#type-i-error-and-type-ii-error"><i class="fa fa-check"></i><b>9.2.3</b> Type I error and Type II error</a></li>
<li class="chapter" data-level="9.2.4" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-for-mean-of-one-sample"><i class="fa fa-check"></i><b>9.2.4</b> Inference for Mean of One Sample</a></li>
<li class="chapter" data-level="9.2.5" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#comparing-the-means-of-two-samples"><i class="fa fa-check"></i><b>9.2.5</b> Comparing the means of two samples</a></li>
<li class="chapter" data-level="9.2.6" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-of-a-sample-proportion"><i class="fa fa-check"></i><b>9.2.6</b> Inference of a Sample Proportion</a></li>
<li class="chapter" data-level="9.2.7" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-groups-for-equal-proportions"><i class="fa fa-check"></i><b>9.2.7</b> Testing groups for equal proportions</a></li>
<li class="chapter" data-level="9.2.8" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-if-two-samples-have-the-same-underlying-distribution"><i class="fa fa-check"></i><b>9.2.8</b> Testing if two samples have the same underlying distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html"><i class="fa fa-check"></i><b>10</b> Root finding and optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#root-finding"><i class="fa fa-check"></i><b>10.1</b> Root Finding</a><ul>
<li class="chapter" data-level="10.1.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#bisection-method"><i class="fa fa-check"></i><b>10.1.1</b> Bisection Method</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method"><i class="fa fa-check"></i><b>10.2</b> Newton-Raphson Method</a></li>
<li class="chapter" data-level="10.3" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#minimization-and-maximization"><i class="fa fa-check"></i><b>10.3</b> Minimization and Maximization</a><ul>
<li class="chapter" data-level="10.3.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method-1"><i class="fa fa-check"></i><b>10.3.1</b> Newton-Raphson Method</a></li>
<li class="chapter" data-level="10.3.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#gradient-descent"><i class="fa fa-check"></i><b>10.3.2</b> Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#optim"><i class="fa fa-check"></i><b>10.4</b> <code>optim</code></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>11</b> k-nearest neighbors</a><ul>
<li class="chapter" data-level="11.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#feature-scaling"><i class="fa fa-check"></i><b>11.2</b> Feature Scaling</a></li>
<li class="chapter" data-level="11.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-classifying-breast-cancers"><i class="fa fa-check"></i><b>11.3</b> Example: Classifying Breast Cancers</a><ul>
<li class="chapter" data-level="11.3.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#evaluating-model-performance"><i class="fa fa-check"></i><b>11.3.1</b> Evaluating Model Performance</a></li>
<li class="chapter" data-level="11.3.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#using-z-score-standardization"><i class="fa fa-check"></i><b>11.3.2</b> Using <span class="math inline">\(z\)</span>-score standardization</a></li>
<li class="chapter" data-level="11.3.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#testing-alternative-values-of-k"><i class="fa fa-check"></i><b>11.3.3</b> Testing alternative values of <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="classification-trees.html"><a href="classification-trees.html"><i class="fa fa-check"></i><b>12</b> Classification Trees</a><ul>
<li class="chapter" data-level="12.1" data-path="classification-trees.html"><a href="classification-trees.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="classification-trees.html"><a href="classification-trees.html#the-c5.0-classification-tree-algorithm"><i class="fa fa-check"></i><b>12.2</b> The C5.0 classification tree algorithm</a><ul>
<li class="chapter" data-level="12.2.1" data-path="classification-trees.html"><a href="classification-trees.html#choosing-the-best-split"><i class="fa fa-check"></i><b>12.2.1</b> Choosing the best split</a></li>
<li class="chapter" data-level="12.2.2" data-path="classification-trees.html"><a href="classification-trees.html#pruning-the-decision-tree"><i class="fa fa-check"></i><b>12.2.2</b> Pruning the decision tree</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="classification-trees.html"><a href="classification-trees.html#example-iris"><i class="fa fa-check"></i><b>12.3</b> Example: <code>iris</code></a></li>
<li class="chapter" data-level="12.4" data-path="classification-trees.html"><a href="classification-trees.html#example-identifying-risky-bank-loans"><i class="fa fa-check"></i><b>12.4</b> Example: identifying risky bank loans</a><ul>
<li class="chapter" data-level="12.4.1" data-path="classification-trees.html"><a href="classification-trees.html#adaptive-boosting"><i class="fa fa-check"></i><b>12.4.1</b> Adaptive boosting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>13</b> Linear Regression Models</a><ul>
<li class="chapter" data-level="13.1" data-path="linear-regression-models.html"><a href="linear-regression-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>13.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="13.2" data-path="linear-regression-models.html"><a href="linear-regression-models.html#smoothed-conditional-means"><i class="fa fa-check"></i><b>13.2</b> Smoothed Conditional Means</a></li>
<li class="chapter" data-level="13.3" data-path="linear-regression-models.html"><a href="linear-regression-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>13.3</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="13.4" data-path="linear-regression-models.html"><a href="linear-regression-models.html#example-diamonds"><i class="fa fa-check"></i><b>13.4</b> Example: <code>diamonds</code></a></li>
<li class="chapter" data-level="13.5" data-path="linear-regression-models.html"><a href="linear-regression-models.html#categorical-predictors"><i class="fa fa-check"></i><b>13.5</b> Categorical Predictors</a></li>
<li class="chapter" data-level="13.6" data-path="linear-regression-models.html"><a href="linear-regression-models.html#compare-models-using-anova"><i class="fa fa-check"></i><b>13.6</b> Compare models using ANOVA</a></li>
<li class="chapter" data-level="13.7" data-path="linear-regression-models.html"><a href="linear-regression-models.html#prediction"><i class="fa fa-check"></i><b>13.7</b> Prediction</a></li>
<li class="chapter" data-level="13.8" data-path="linear-regression-models.html"><a href="linear-regression-models.html#interaction-terms"><i class="fa fa-check"></i><b>13.8</b> Interaction Terms</a></li>
<li class="chapter" data-level="13.9" data-path="linear-regression-models.html"><a href="linear-regression-models.html#variable-transformation"><i class="fa fa-check"></i><b>13.9</b> Variable Transformation</a></li>
<li class="chapter" data-level="13.10" data-path="linear-regression-models.html"><a href="linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>13.10</b> Polynomial Regression</a></li>
<li class="chapter" data-level="13.11" data-path="linear-regression-models.html"><a href="linear-regression-models.html#stepwise-regression"><i class="fa fa-check"></i><b>13.11</b> Stepwise regression</a></li>
<li class="chapter" data-level="13.12" data-path="linear-regression-models.html"><a href="linear-regression-models.html#best-subset"><i class="fa fa-check"></i><b>13.12</b> Best subset</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="logistic-regression-model.html"><a href="logistic-regression-model.html"><i class="fa fa-check"></i><b>14</b> Logistic Regression Model</a></li>
<li class="chapter" data-level="15" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>15</b> k-means Clustering</a><ul>
<li class="chapter" data-level="15.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-4"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications"><i class="fa fa-check"></i><b>15.2</b> Applications</a><ul>
<li class="chapter" data-level="15.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#cluster-analysis"><i class="fa fa-check"></i><b>15.2.1</b> Cluster Analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#image-segementation"><i class="fa fa-check"></i><b>15.2.2</b> Image Segementation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>16</b> Neural Networks</a><ul>
<li class="chapter" data-level="16.1" data-path="neural-networks.html"><a href="neural-networks.html#introduction-5"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="neural-networks.html"><a href="neural-networks.html#regression-predict-the-strength-of-concrete"><i class="fa fa-check"></i><b>16.2</b> Regression: Predict the Strength of Concrete</a><ul>
<li class="chapter" data-level="16.2.1" data-path="neural-networks.html"><a href="neural-networks.html#data"><i class="fa fa-check"></i><b>16.2.1</b> Data</a></li>
<li class="chapter" data-level="16.2.2" data-path="neural-networks.html"><a href="neural-networks.html#training-a-model"><i class="fa fa-check"></i><b>16.2.2</b> Training a model</a></li>
<li class="chapter" data-level="16.2.3" data-path="neural-networks.html"><a href="neural-networks.html#understanding-the-model"><i class="fa fa-check"></i><b>16.2.3</b> Understanding the model</a></li>
<li class="chapter" data-level="16.2.4" data-path="neural-networks.html"><a href="neural-networks.html#evaluating-the-performance"><i class="fa fa-check"></i><b>16.2.4</b> Evaluating the Performance</a></li>
<li class="chapter" data-level="16.2.5" data-path="neural-networks.html"><a href="neural-networks.html#improving-the-model"><i class="fa fa-check"></i><b>16.2.5</b> Improving the Model</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="neural-networks.html"><a href="neural-networks.html#classification"><i class="fa fa-check"></i><b>16.3</b> Classification</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html"><i class="fa fa-check"></i><b>17</b> Evaluating Model Performance</a><ul>
<li class="chapter" data-level="17.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#accuracy-and-confusion-matrix"><i class="fa fa-check"></i><b>17.1</b> Accuracy and Confusion Matrix</a></li>
<li class="chapter" data-level="17.2" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#other-measures-of-performance"><i class="fa fa-check"></i><b>17.2</b> Other Measures of Performance</a><ul>
<li class="chapter" data-level="17.2.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#the-kappa-statistic"><i class="fa fa-check"></i><b>17.2.1</b> The kappa statistic</a></li>
<li class="chapter" data-level="17.2.2" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>17.2.2</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="17.2.3" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#roc-and-auc"><i class="fa fa-check"></i><b>17.2.3</b> ROC and AUC</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#estimating-future-performances"><i class="fa fa-check"></i><b>17.3</b> Estimating future performances</a><ul>
<li class="chapter" data-level="17.3.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#cross-validation"><i class="fa fa-check"></i><b>17.3.1</b> Cross-validation</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 362 R for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-trees" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Classification Trees</h1>
<p>Optional Reading: Chapter 5 in Machine Learning with R by Brett Lantz.</p>
<p>In this chapter, we will load the following libraries.</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="classification-trees.html#cb377-1"></a><span class="kw">library</span>(tidyverse) </span>
<span id="cb377-2"><a href="classification-trees.html#cb377-2"></a><span class="kw">library</span>(GGally) <span class="co"># to use ggpairs</span></span>
<span id="cb377-3"><a href="classification-trees.html#cb377-3"></a><span class="kw">library</span>(C50)</span>
<span id="cb377-4"><a href="classification-trees.html#cb377-4"></a><span class="kw">library</span>(ggpubr) <span class="co"># to use ggarrange</span></span></code></pre></div>
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">12.1</span> Introduction</h2>
<p>A decision tree utilizes a tree structure to model the relationship between the features and the outcomes. In each branching node of the tree, a specific feature of the data is examined. According to the value of the feature, one of the branches will be followed. The leaf node represents the class label. Below is a simplified example of determining if you should accept a new job offer.</p>
<p><img src="C:/Queens%20Teaching/Teaching/STAT%20362/01a_lect_notes_website/image/decision_tree_job_offer.jpg" width="80%" style="display: block; margin: auto;" /></p>
<p>In the above example, the leaf nodes are “Decline offer” and “Accept offer”. The paths from the root to leaf node represent classification rules.</p>
<p>In data science, there are two types of decision trees:</p>
<ul>
<li>classification tree - for discrete outcomes (our focus in this chapter)</li>
<li>regression tree - for continuous outcomes</li>
</ul>
<p>Classification trees are used for classification problems. Consider the <code>iris</code> dataset, which is available in base R. We have <span class="math inline">\(150\)</span> observations and <span class="math inline">\(5\)</span> variables named <code>Sepal.Length</code>, <code>Sepal.Wdith</code>, <code>Petal.Length</code>, <code>Petal.Width</code>, and <code>Species</code>.</p>
<p>Let’s take a look at the data.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="classification-trees.html#cb378-1"></a><span class="kw">str</span>(iris) </span>
<span id="cb378-2"><a href="classification-trees.html#cb378-2"></a><span class="co">## &#39;data.frame&#39;:    150 obs. of  5 variables:</span></span>
<span id="cb378-3"><a href="classification-trees.html#cb378-3"></a><span class="co">##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...</span></span>
<span id="cb378-4"><a href="classification-trees.html#cb378-4"></a><span class="co">##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...</span></span>
<span id="cb378-5"><a href="classification-trees.html#cb378-5"></a><span class="co">##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...</span></span>
<span id="cb378-6"><a href="classification-trees.html#cb378-6"></a><span class="co">##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...</span></span>
<span id="cb378-7"><a href="classification-trees.html#cb378-7"></a><span class="co">##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</span></span></code></pre></div>
<p>The first four variables are numeric features of the flowers. The last column in the dataset is the species of the flower. We will illustrate how to use the numeric features of the flowers to classify the flowers.</p>
<p>First, we will split the dataset into a training dataset and a testing dataset.</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="classification-trees.html#cb379-1"></a><span class="kw">set.seed</span>(<span class="dv">6</span>)</span>
<span id="cb379-2"><a href="classification-trees.html#cb379-2"></a>random_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(iris), <span class="dv">100</span>) <span class="co"># select 100 examples for our training dataset</span></span>
<span id="cb379-3"><a href="classification-trees.html#cb379-3"></a>iris_train &lt;-<span class="st"> </span>iris[random_index, ]</span>
<span id="cb379-4"><a href="classification-trees.html#cb379-4"></a>iris_test &lt;-<span class="st"> </span>iris[<span class="op">-</span>random_index, ]</span></code></pre></div>
<p>To create a scatterplot matrix, we can use <code>pairs</code> (base R graphics) or <code>ggpairs</code> (in the package <code>GGally</code>).</p>
<p>With <code>pairs()</code>:</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="classification-trees.html#cb380-1"></a><span class="co"># we only want to plot the numeric features</span></span>
<span id="cb380-2"><a href="classification-trees.html#cb380-2"></a><span class="co"># the 5th column is the species of the flowers</span></span>
<span id="cb380-3"><a href="classification-trees.html#cb380-3"></a><span class="kw">pairs</span>(iris_train[, <span class="dv">-5</span>])</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-410-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>With <code>ggpairs()</code>:</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="classification-trees.html#cb381-1"></a><span class="kw">library</span>(GGally) <span class="co"># to use ggpairs, install it first if you haven&#39;t done so</span></span>
<span id="cb381-2"><a href="classification-trees.html#cb381-2"></a><span class="kw">ggpairs</span>(<span class="dt">data =</span> iris_train, <span class="kw">aes</span>(<span class="dt">color =</span> Species, <span class="dt">alpha =</span> <span class="fl">0.8</span>), <span class="dt">columns =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-411-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>In <code>ggpairs()</code>:</p>
<ul>
<li><code>columns = 1:4</code> tells the function only uses the first <span class="math inline">\(4\)</span> columns of the dataset for plotting</li>
<li><code>color = Species</code> indicates that we map the color to the variable <code>Species</code></li>
<li><code>alpha = 0.8</code> controls the level of transparency so that the density estimates do not overlap each other completely</li>
</ul>
<p>From the plots, it seems that the variable <code>Petal.Width</code> and <code>Petal.Length</code> will be useful in classifying the flowers. Let’s focus on the scatterplot of these two variables.</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="classification-trees.html#cb382-1"></a><span class="kw">ggplot</span>(iris_train, <span class="kw">aes</span>(<span class="dt">x =</span> Petal.Width, <span class="dt">y =</span> Petal.Length, <span class="dt">color =</span> Species)) <span class="op">+</span></span>
<span id="cb382-2"><a href="classification-trees.html#cb382-2"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-412-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>From the above plot, we see that we can easily classify the flowers with high accuracy using these two features. For example, if we draw the horizontal line <span class="math inline">\(y = 1.9\)</span>, we could classify any iris lying on or below this line as setosa with <span class="math inline">\(100\%\)</span> accuracy.</p>
<p><img src="Book_files/figure-html/unnamed-chunk-413-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Next, we draw the vertical line <span class="math inline">\(x = 1.7\)</span>. We could classify any iris lying on or to the left of this line as versicolor.</p>
<p><img src="Book_files/figure-html/unnamed-chunk-414-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>We get a pretty good classification with only 3 mistakes.</p>
<p>The above classification is an example of classification trees (because the outcome is categorical) and can be visualized as</p>
<p><img src="Book_files/figure-html/unnamed-chunk-416-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>In the chapter, we will learn how to perform such a task using the C5.0 algorithm in R.</p>
</div>
<div id="the-c5.0-classification-tree-algorithm" class="section level2">
<h2><span class="header-section-number">12.2</span> The C5.0 classification tree algorithm</h2>
<p>There are different implementations of classification trees, we will use the C5.0 algorithm. Another popular algorithm is the CART (use the package <code>rpart</code> in R).</p>
<p><strong>Strengths of classification trees:</strong></p>
<ol style="list-style-type: decimal">
<li>an all-purpose classifier that does well on many types of problems</li>
<li>highly automatic learning process, which can handle numeric or nominal features, as well as missing data</li>
<li>excludes unimportant features</li>
<li>can be used on both small and large datasets</li>
<li>results in a model that can be interpreted without a mathematical background</li>
<li>more efficient than other complex models</li>
</ol>
<p><strong>Weaknesses of classification trees:</strong></p>
<ol style="list-style-type: decimal">
<li>decision tree models are often biased toward splits on features having a large number of levels</li>
<li>it is easy to overfit or underfit the model</li>
<li>can have trouble modeling some relationships due to reliance on axis-parallel splits</li>
<li>small changes in training data can result in large changes to decision logic</li>
<li>large trees can be difficult to interpret and the decisions they make may seem counterintuitive</li>
</ol>
<div id="choosing-the-best-split" class="section level3">
<h3><span class="header-section-number">12.2.1</span> Choosing the best split</h3>
<p>As we can see from the <code>iris</code> example, we first have to identify the feature to split upon. We choose the feature so that the resulting partitions <strong>contain examples primarily of a single class</strong>.</p>
<p>In machine learning, examples mean observations.</p>
<p>Definition: Purity = the degree to which a subset of examples contains only a single class</p>
<p>One common measure of purity is <strong>entropy</strong>. For <span class="math inline">\(c\)</span> classes (recall that we are considering classification problems), entropy ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(\log_2(c)\)</span>. Entropy is defined as
<span class="math display">\[\begin{equation*}
\text{Entropy}(S) = - \sum^c_{i=1} p_i \log_2 (p_i),
\end{equation*}\]</span>
where <span class="math inline">\(S\)</span> is a given set of data, <span class="math inline">\(c\)</span> is the number of class levels, <span class="math inline">\(p_i\)</span> is the proportion of data in class level <span class="math inline">\(i\)</span>, <span class="math inline">\(\log_2\)</span> is the logarithm to the base <span class="math inline">\(2\)</span>. By convention, <span class="math inline">\(0 \log_2 0 = 0\)</span>.</p>
<p><strong>Example:</strong> Suppose we have <span class="math inline">\(10\)</span> data where <span class="math inline">\(6\)</span> of them are in group A and the rest are in group B. The entropy is <span class="math inline">\(-0.6 \log_2(0.6) - 0.4 \log_2 (0.4) = 0.097\)</span>.</p>
<p>The smaller the entropy is, the purer the class is. If the Entropy is <span class="math inline">\(0\)</span>, it means the sample is completely homogeneous. That is, all the data belong to one group. If the entropy is <span class="math inline">\(log_2(c)\)</span>, it means the data are as diverse as possible.</p>
<p>The following figure shows the entropy as a function of the proportion of one class when there are only two classes. The maximum entropy is obtained when we have <span class="math inline">\(50\%\)</span> of data in one group and another <span class="math inline">\(50\%\)</span> of data in another group.</p>
<p><img src="Book_files/figure-html/unnamed-chunk-417-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p><span class="math inline">\(C5.0\)</span> algorithm uses entropy to determine the optimal feature to split upon. The algorithm calculates the <strong>information gain</strong> that would result from a split on each possible feature, which is a measure of the change in homogeneity. The information gain for a particular feature <span class="math inline">\(F\)</span> is calculated as the difference between the entropy before the split, denoted by <span class="math inline">\(\text{Entropy}(S)\)</span>, and the entropy after the split, denoted by <span class="math inline">\(\text{Entropy}(S|F)\)</span>:
<span class="math display">\[\begin{equation*}
\text{InfoGain}(F) = \text{Entropy}(S) - \text{Entropy}(S|F).
\end{equation*}\]</span>
Suppose the feature <span class="math inline">\(F\)</span> can take <span class="math inline">\(m\)</span> values (WLOG, assume the <span class="math inline">\(m\)</span> values are <span class="math inline">\(1,\ldots,m\)</span>), which splits the data into <span class="math inline">\(m\)</span> partitions. Then,
<span class="math display">\[\begin{equation*}
\text{Entropy}(S|F) = \sum^m_{i=1} w_i \text{Entropy}(S_F(i)),
\end{equation*}\]</span>
where <span class="math inline">\(w_i\)</span> is the proportion of data in the <span class="math inline">\(i\)</span>th partition and <span class="math inline">\(S_F(i)\)</span> is the set of data in the <span class="math inline">\(i\)</span>th partition.</p>
<p><strong>Example of Calculating Information Gain</strong></p>
<p>Consider the following data.</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="classification-trees.html#cb383-1"></a>animal</span>
<span id="cb383-2"><a href="classification-trees.html#cb383-2"></a><span class="co">## # A tibble: 15 x 4</span></span>
<span id="cb383-3"><a href="classification-trees.html#cb383-3"></a><span class="co">##    Animal    TravelsBy HasFur Mammal</span></span>
<span id="cb383-4"><a href="classification-trees.html#cb383-4"></a><span class="co">##    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; </span></span>
<span id="cb383-5"><a href="classification-trees.html#cb383-5"></a><span class="co">##  1 Bats      Air       Yes    Yes   </span></span>
<span id="cb383-6"><a href="classification-trees.html#cb383-6"></a><span class="co">##  2 Bears     Land      Yes    Yes   </span></span>
<span id="cb383-7"><a href="classification-trees.html#cb383-7"></a><span class="co">##  3 Birds     Air       No     No    </span></span>
<span id="cb383-8"><a href="classification-trees.html#cb383-8"></a><span class="co">##  4 Cats      Land      Yes    Yes   </span></span>
<span id="cb383-9"><a href="classification-trees.html#cb383-9"></a><span class="co">##  5 Dogs      Land      Yes    Yes   </span></span>
<span id="cb383-10"><a href="classification-trees.html#cb383-10"></a><span class="co">##  6 Eels      Sea       No     No    </span></span>
<span id="cb383-11"><a href="classification-trees.html#cb383-11"></a><span class="co">##  7 Elephants Land      No     Yes   </span></span>
<span id="cb383-12"><a href="classification-trees.html#cb383-12"></a><span class="co">##  8 Fish      Sea       No     No    </span></span>
<span id="cb383-13"><a href="classification-trees.html#cb383-13"></a><span class="co">##  9 Frogs     Land      No     No    </span></span>
<span id="cb383-14"><a href="classification-trees.html#cb383-14"></a><span class="co">## 10 Insects   Air       No     No    </span></span>
<span id="cb383-15"><a href="classification-trees.html#cb383-15"></a><span class="co">## 11 Pigs      Land      No     Yes   </span></span>
<span id="cb383-16"><a href="classification-trees.html#cb383-16"></a><span class="co">## 12 Rabbits   Land      Yes    Yes   </span></span>
<span id="cb383-17"><a href="classification-trees.html#cb383-17"></a><span class="co">## 13 Rats      Land      Yes    Yes   </span></span>
<span id="cb383-18"><a href="classification-trees.html#cb383-18"></a><span class="co">## 14 Rhinos    Land      No     Yes   </span></span>
<span id="cb383-19"><a href="classification-trees.html#cb383-19"></a><span class="co">## 15 Sharks    Sea       No     No</span></span></code></pre></div>
<p>Suppose we want to classify animals as mammal or non-mammal. Before any splitting, we have <span class="math inline">\(15\)</span> animals and <span class="math inline">\(9\)</span> of them are mammals.
The entropy is
<span class="math display">\[\begin{equation*}
- \frac{9}{15} \log_2 \bigg( \frac{9}{15} \bigg) - \frac{6}{15} \log_2 \bigg( \frac{6}{15} \bigg) = 0.971.
\end{equation*}\]</span>
Remark: you can use <code>log2(x)</code> to compute <span class="math inline">\(\log_2(x)\)</span>.</p>
<!-- ```{r} -->
<!-- -9 / 15 * log2(9 / 15) - 6 / 15 * log2(6 / 15) -->
<!-- ``` -->
<p>If we split the node by <code>TravelsBy</code>, we have <span class="math inline">\(3\)</span> partitions.</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="classification-trees.html#cb384-1"></a><span class="co"># 1st partition</span></span>
<span id="cb384-2"><a href="classification-trees.html#cb384-2"></a><span class="kw">filter</span>(animal, TravelsBy <span class="op">==</span><span class="st"> &quot;Air&quot;</span>)</span>
<span id="cb384-3"><a href="classification-trees.html#cb384-3"></a><span class="co">## # A tibble: 3 x 4</span></span>
<span id="cb384-4"><a href="classification-trees.html#cb384-4"></a><span class="co">##   Animal  TravelsBy HasFur Mammal</span></span>
<span id="cb384-5"><a href="classification-trees.html#cb384-5"></a><span class="co">##   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; </span></span>
<span id="cb384-6"><a href="classification-trees.html#cb384-6"></a><span class="co">## 1 Bats    Air       Yes    Yes   </span></span>
<span id="cb384-7"><a href="classification-trees.html#cb384-7"></a><span class="co">## 2 Birds   Air       No     No    </span></span>
<span id="cb384-8"><a href="classification-trees.html#cb384-8"></a><span class="co">## 3 Insects Air       No     No</span></span></code></pre></div>
<p><span class="math display">\[\begin{equation*}
\text{Entropy}(S_\text{TravelsBy}(\text{Air})) =  - \frac{1}{3} \log_2 \frac{1}{3} - \frac{2}{3} \log_2 \frac{2}{3} = 0.918.
\end{equation*}\]</span></p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="classification-trees.html#cb385-1"></a><span class="co"># 2nd partition</span></span>
<span id="cb385-2"><a href="classification-trees.html#cb385-2"></a><span class="kw">filter</span>(animal, TravelsBy <span class="op">==</span><span class="st"> &quot;Land&quot;</span>)</span>
<span id="cb385-3"><a href="classification-trees.html#cb385-3"></a><span class="co">## # A tibble: 9 x 4</span></span>
<span id="cb385-4"><a href="classification-trees.html#cb385-4"></a><span class="co">##   Animal    TravelsBy HasFur Mammal</span></span>
<span id="cb385-5"><a href="classification-trees.html#cb385-5"></a><span class="co">##   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; </span></span>
<span id="cb385-6"><a href="classification-trees.html#cb385-6"></a><span class="co">## 1 Bears     Land      Yes    Yes   </span></span>
<span id="cb385-7"><a href="classification-trees.html#cb385-7"></a><span class="co">## 2 Cats      Land      Yes    Yes   </span></span>
<span id="cb385-8"><a href="classification-trees.html#cb385-8"></a><span class="co">## 3 Dogs      Land      Yes    Yes   </span></span>
<span id="cb385-9"><a href="classification-trees.html#cb385-9"></a><span class="co">## 4 Elephants Land      No     Yes   </span></span>
<span id="cb385-10"><a href="classification-trees.html#cb385-10"></a><span class="co">## 5 Frogs     Land      No     No    </span></span>
<span id="cb385-11"><a href="classification-trees.html#cb385-11"></a><span class="co">## 6 Pigs      Land      No     Yes   </span></span>
<span id="cb385-12"><a href="classification-trees.html#cb385-12"></a><span class="co">## 7 Rabbits   Land      Yes    Yes   </span></span>
<span id="cb385-13"><a href="classification-trees.html#cb385-13"></a><span class="co">## 8 Rats      Land      Yes    Yes   </span></span>
<span id="cb385-14"><a href="classification-trees.html#cb385-14"></a><span class="co">## 9 Rhinos    Land      No     Yes</span></span></code></pre></div>
<p><span class="math display">\[\begin{equation*}
\text{Entropy}(S_\text{TravelsBy}(\text{Land})) = - \frac{8}{9} \log_2 \frac{8}{9} - \frac{1}{9} \log_2 \frac{1}{9} = 0.503
\end{equation*}\]</span></p>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="classification-trees.html#cb386-1"></a><span class="co"># 3rd partition</span></span>
<span id="cb386-2"><a href="classification-trees.html#cb386-2"></a><span class="kw">filter</span>(animal, TravelsBy <span class="op">==</span><span class="st"> &quot;Sea&quot;</span>)</span>
<span id="cb386-3"><a href="classification-trees.html#cb386-3"></a><span class="co">## # A tibble: 3 x 4</span></span>
<span id="cb386-4"><a href="classification-trees.html#cb386-4"></a><span class="co">##   Animal TravelsBy HasFur Mammal</span></span>
<span id="cb386-5"><a href="classification-trees.html#cb386-5"></a><span class="co">##   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; </span></span>
<span id="cb386-6"><a href="classification-trees.html#cb386-6"></a><span class="co">## 1 Eels   Sea       No     No    </span></span>
<span id="cb386-7"><a href="classification-trees.html#cb386-7"></a><span class="co">## 2 Fish   Sea       No     No    </span></span>
<span id="cb386-8"><a href="classification-trees.html#cb386-8"></a><span class="co">## 3 Sharks Sea       No     No</span></span></code></pre></div>
<p><span class="math display">\[\begin{equation*}
\text{Entropy}(S_\text{TravelsBy}(\text{Sea})) = 0.
\end{equation*}\]</span></p>
<p>Entropy after splitting:</p>
<p><span class="math display">\[\begin{eqnarray*}
\text{Entropy}(S|\text{TravelsBy}) &amp;=&amp; w_{\text{Air}} \text{Entropy}(S_\text{TravelsBy}(\text{Air})) +\\
&amp;&amp; \quad w_{\text{Land}} \text{Entropy}(S_\text{TravelsBy}(\text{Land})) + \\
&amp;&amp; \quad w_{\text{Sea}} \text{Entropy}(S_\text{TravelsBy}(\text{Sea})),
\end{eqnarray*}\]</span>
where <span class="math inline">\(w_{\text{Air}} = \frac{3}{15}\)</span>, <span class="math inline">\(w_{\text{Land}} = \frac{9}{15}\)</span>, <span class="math inline">\(w_{\text{Sea}} = \frac{3}{15}\)</span>.
<!-- \begin{eqnarray*} -->
<!-- \text{Entropy}(S_\text{TravelsBy}(\text{Air})) &=&  - \frac{1}{3} \log_2 \frac{1}{3} - \frac{2}{3} \log_2 \frac{2}{3} = 0.918 \\ -->
<!-- \text{Entropy}(S_\text{TravelsBy}(\text{Land})) &=& - \frac{8}{9} \log_2 \frac{8}{9} - \frac{1}{9} \log_2 \frac{1}{9} = 0.503 \\ -->
<!-- \text{Entropy}(S_\text{TravelsBy}(\text{Sea})) &=& 0. \\ -->
<!-- \end{eqnarray*} -->
Therefore, <span class="math inline">\(\text{Entropy}(S|\text{TravelsBy})\)</span> is <span class="math inline">\(0.486\)</span>. The information gain by splitting using <code>TravelsBy</code> is <span class="math inline">\(0.971 - 0.486 = 0.485\)</span>.</p>
<p>Now, let’s compute the information gain by splitting using <code>HasFur</code>. If we split the node by <code>HasFur</code>, then we have <span class="math inline">\(2\)</span> partitions.</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="classification-trees.html#cb387-1"></a><span class="co"># 1st partition</span></span>
<span id="cb387-2"><a href="classification-trees.html#cb387-2"></a><span class="kw">filter</span>(animal, HasFur <span class="op">==</span><span class="st"> &quot;Yes&quot;</span>)</span>
<span id="cb387-3"><a href="classification-trees.html#cb387-3"></a><span class="co">## # A tibble: 6 x 4</span></span>
<span id="cb387-4"><a href="classification-trees.html#cb387-4"></a><span class="co">##   Animal  TravelsBy HasFur Mammal</span></span>
<span id="cb387-5"><a href="classification-trees.html#cb387-5"></a><span class="co">##   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; </span></span>
<span id="cb387-6"><a href="classification-trees.html#cb387-6"></a><span class="co">## 1 Bats    Air       Yes    Yes   </span></span>
<span id="cb387-7"><a href="classification-trees.html#cb387-7"></a><span class="co">## 2 Bears   Land      Yes    Yes   </span></span>
<span id="cb387-8"><a href="classification-trees.html#cb387-8"></a><span class="co">## 3 Cats    Land      Yes    Yes   </span></span>
<span id="cb387-9"><a href="classification-trees.html#cb387-9"></a><span class="co">## 4 Dogs    Land      Yes    Yes   </span></span>
<span id="cb387-10"><a href="classification-trees.html#cb387-10"></a><span class="co">## 5 Rabbits Land      Yes    Yes   </span></span>
<span id="cb387-11"><a href="classification-trees.html#cb387-11"></a><span class="co">## 6 Rats    Land      Yes    Yes</span></span></code></pre></div>
<p><span class="math display">\[\begin{equation*}
\text{Entropy}(S_\text{HasFur}(\text{Yes})) = 0.
\end{equation*}\]</span></p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="classification-trees.html#cb388-1"></a><span class="co"># 2nd partition</span></span>
<span id="cb388-2"><a href="classification-trees.html#cb388-2"></a><span class="kw">filter</span>(animal, HasFur <span class="op">==</span><span class="st"> &quot;No&quot;</span>)</span>
<span id="cb388-3"><a href="classification-trees.html#cb388-3"></a><span class="co">## # A tibble: 9 x 4</span></span>
<span id="cb388-4"><a href="classification-trees.html#cb388-4"></a><span class="co">##   Animal    TravelsBy HasFur Mammal</span></span>
<span id="cb388-5"><a href="classification-trees.html#cb388-5"></a><span class="co">##   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; </span></span>
<span id="cb388-6"><a href="classification-trees.html#cb388-6"></a><span class="co">## 1 Birds     Air       No     No    </span></span>
<span id="cb388-7"><a href="classification-trees.html#cb388-7"></a><span class="co">## 2 Eels      Sea       No     No    </span></span>
<span id="cb388-8"><a href="classification-trees.html#cb388-8"></a><span class="co">## 3 Elephants Land      No     Yes   </span></span>
<span id="cb388-9"><a href="classification-trees.html#cb388-9"></a><span class="co">## 4 Fish      Sea       No     No    </span></span>
<span id="cb388-10"><a href="classification-trees.html#cb388-10"></a><span class="co">## 5 Frogs     Land      No     No    </span></span>
<span id="cb388-11"><a href="classification-trees.html#cb388-11"></a><span class="co">## 6 Insects   Air       No     No    </span></span>
<span id="cb388-12"><a href="classification-trees.html#cb388-12"></a><span class="co">## 7 Pigs      Land      No     Yes   </span></span>
<span id="cb388-13"><a href="classification-trees.html#cb388-13"></a><span class="co">## 8 Rhinos    Land      No     Yes   </span></span>
<span id="cb388-14"><a href="classification-trees.html#cb388-14"></a><span class="co">## 9 Sharks    Sea       No     No</span></span></code></pre></div>
<p><span class="math display">\[\begin{equation*}
\text{Entropy}(S_\text{TravelsBy}(\text{Land})) = - \frac{3}{9} \log_2 \frac{3}{9} - \frac{6}{9} \log_2 \frac{6}{9} = 0.918.
\end{equation*}\]</span></p>
<p>Entropy after splitting:</p>
<p><span class="math display">\[\begin{eqnarray*}
\text{Entropy}(S|\text{HasFur}) &amp;=&amp; w_{\text{Yes}} \text{Entropy}(S_\text{HasFur}(\text{Yes})) +\\
&amp;&amp; \quad w_{\text{No}} \text{Entropy}(S_\text{HasFur}(\text{No})) \\
&amp;=&amp; 0 + \frac{9}{15} (0.918) = 0.551.
\end{eqnarray*}\]</span></p>
<p>The information gain by splitting using <code>HasFur</code> is <span class="math inline">\(0.971 - 0.551 = 0.42\)</span>. Since splitting using <code>TravelsBy</code> leads to a larger information gain, we should use this to create the first branch.</p>
<p><strong>For Continuous Features</strong></p>
<p>The formula for information gain assumes nominal features. For continuous features, a common practice is to test various splits that divide the values into groups greater than or less than a threshold. This reduces the continuous feature into a two-level categorical feature that allows information gain to be calculated as above. The numeric cut point yielding the largest information gain is chosen for the split.</p>
<p>For example, for the numeric feature <code>Petal.Length</code>, the split in the right plot results in a higher information gain.</p>
<p><img src="Book_files/figure-html/unnamed-chunk-425-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Remark: to arrange multiple ggplot objects in a single plot, you can use <code>ggarrange()</code> from the package <code>ggpubr</code> as follows:</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="classification-trees.html#cb389-1"></a><span class="kw">library</span>(ggpubr) <span class="co"># install it first if you haven&#39;t done so</span></span>
<span id="cb389-2"><a href="classification-trees.html#cb389-2"></a>g1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(iris_train, <span class="kw">aes</span>(<span class="dt">x =</span> Petal.Width, <span class="dt">y =</span> Petal.Length, <span class="dt">color =</span> Species)) <span class="op">+</span></span>
<span id="cb389-3"><a href="classification-trees.html#cb389-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb389-4"><a href="classification-trees.html#cb389-4"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="fl">1.5</span>)</span>
<span id="cb389-5"><a href="classification-trees.html#cb389-5"></a>g2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(iris_train, <span class="kw">aes</span>(<span class="dt">x =</span> Petal.Width, <span class="dt">y =</span> Petal.Length, <span class="dt">color =</span> Species)) <span class="op">+</span></span>
<span id="cb389-6"><a href="classification-trees.html#cb389-6"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb389-7"><a href="classification-trees.html#cb389-7"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">2</span>)</span>
<span id="cb389-8"><a href="classification-trees.html#cb389-8"></a></span>
<span id="cb389-9"><a href="classification-trees.html#cb389-9"></a><span class="kw">ggarrange</span>(g1, g2, <span class="dt">common.legend =</span> <span class="ot">TRUE</span>, <span class="dt">legend =</span> <span class="st">&quot;bottom&quot;</span>)</span></code></pre></div>
</div>
<div id="pruning-the-decision-tree" class="section level3">
<h3><span class="header-section-number">12.2.2</span> Pruning the decision tree</h3>
<p>A decision tree can continue to grow indefinitely, choosing splitting features and dividing into smaller and smaller partitions. If the tree grows overly large, many of the decisions it makes will be overly specific and will not generalize to new data (i.e., overfitted to the training data). The process of pruning a decision tree involves reducing its size so that it generalizes better to unseen data. For C5.0 algorithm, it first grows a large tree that overfits the training data. Then, the nodes and branches that have little effect on the classification errors are removed.</p>
</div>
</div>
<div id="example-iris" class="section level2">
<h2><span class="header-section-number">12.3</span> Example: <code>iris</code></h2>
<p>Install the package <code>C50</code> and load the library.</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="classification-trees.html#cb390-1"></a><span class="kw">library</span>(C50)</span></code></pre></div>
<p><strong>Model Training</strong></p>
<p>Basic usage of <code>C5.0()</code> from <code>C50</code>:</p>
<ul>
<li><code>x</code>: a data frame containing the training data without the class labels</li>
<li><code>y</code>: a factor vector of the class labels</li>
</ul>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="classification-trees.html#cb391-1"></a>iris_ct &lt;-<span class="st"> </span><span class="kw">C5.0</span>(<span class="dt">x =</span> iris_train[, <span class="dv">-5</span>], <span class="dt">y =</span> iris_train<span class="op">$</span>Species)</span></code></pre></div>
<p><strong>Classification of the training data</strong></p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="classification-trees.html#cb392-1"></a><span class="kw">table</span>(iris_train<span class="op">$</span>Species, <span class="kw">predict</span>(iris_ct, iris_train))</span>
<span id="cb392-2"><a href="classification-trees.html#cb392-2"></a><span class="co">##             </span></span>
<span id="cb392-3"><a href="classification-trees.html#cb392-3"></a><span class="co">##              setosa versicolor virginica</span></span>
<span id="cb392-4"><a href="classification-trees.html#cb392-4"></a><span class="co">##   setosa         37          0         0</span></span>
<span id="cb392-5"><a href="classification-trees.html#cb392-5"></a><span class="co">##   versicolor      0         33         0</span></span>
<span id="cb392-6"><a href="classification-trees.html#cb392-6"></a><span class="co">##   virginica       0          3        27</span></span></code></pre></div>
<p>Accuracy is <span class="math inline">\(97/100\)</span>.</p>
<p><strong>Visualize the decision tree</strong></p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="classification-trees.html#cb393-1"></a><span class="kw">plot</span>(iris_ct)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-430-1.png" width="75%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="classification-trees.html#cb394-1"></a><span class="kw">summary</span>(iris_ct)</span></code></pre></div>
<p><img src="C:/Queens%20Teaching/Teaching/STAT%20362/01a_lect_notes_website/image/ct_iris.JPG" width="50%" style="display: block; margin: auto;" /></p>
<p>From the summary, we see that</p>
<ol style="list-style-type: decimal">
<li>if <code>Petal.Length</code> is less than or equal to <span class="math inline">\(1.9\)</span>, then the example is classified as “setosa”. The numbers in the bracket indicates how many examples are classified as “setosa”.</li>
<li>if <code>Petal.Length</code> is greater than <span class="math inline">\(1.9\)</span> and<code>Petal.Width</code> is less than or equal to <span class="math inline">\(1.7\)</span>, the example is classified as versicolor. This time there are two numbers in the bracket. The first number <span class="math inline">\(36\)</span> indicates how many examples are classified as versicolor. The second number <span class="math inline">\(3\)</span> indicates that out of <span class="math inline">\(36\)</span> examples, <span class="math inline">\(3\)</span> of them are classified incorrectly.</li>
<li>if <code>Petal.Length</code> is greater than <span class="math inline">\(1.9\)</span> and <code>Petal.Width</code> is greater than <span class="math inline">\(1.7\)</span>, the example is classified as virginica. <span class="math inline">\(27\)</span> examples are classified as virginica.</li>
</ol>
<p><strong>Evaluate the performance using testing data</strong></p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="classification-trees.html#cb395-1"></a><span class="kw">table</span>(iris_test<span class="op">$</span>Species, <span class="kw">predict</span>(iris_ct, iris_test))</span>
<span id="cb395-2"><a href="classification-trees.html#cb395-2"></a><span class="co">##             </span></span>
<span id="cb395-3"><a href="classification-trees.html#cb395-3"></a><span class="co">##              setosa versicolor virginica</span></span>
<span id="cb395-4"><a href="classification-trees.html#cb395-4"></a><span class="co">##   setosa         13          0         0</span></span>
<span id="cb395-5"><a href="classification-trees.html#cb395-5"></a><span class="co">##   versicolor      0         16         1</span></span>
<span id="cb395-6"><a href="classification-trees.html#cb395-6"></a><span class="co">##   virginica       0          2        18</span></span></code></pre></div>
<p>Accuracy is <span class="math inline">\(47/50\)</span>. In general, the training accuracy will be higher than the testing accuracy.</p>
</div>
<div id="example-identifying-risky-bank-loans" class="section level2">
<h2><span class="header-section-number">12.4</span> Example: identifying risky bank loans</h2>
<p>In this subsection, we will apply classification tree to identify factors that are linked to a higher risk of loan default. The dataset is from
<a href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)" class="uri">https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</a></p>
<p>See also <a href="https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/credit.csv" class="uri">https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/credit.csv</a></p>
<p>The dataset contains <span class="math inline">\(1,000\)</span> examples of loans, together with <span class="math inline">\(20\)</span> features about the loans and the loan applicants.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="classification-trees.html#cb396-1"></a>credit &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/credit.csv&quot;</span>,</span>
<span id="cb396-2"><a href="classification-trees.html#cb396-2"></a>                   <span class="dt">stringsAsFactors =</span> <span class="ot">TRUE</span>)</span>
<span id="cb396-3"><a href="classification-trees.html#cb396-3"></a>credit<span class="op">$</span>default &lt;-<span class="st"> </span><span class="kw">recode_factor</span>(credit<span class="op">$</span>default, <span class="st">&quot;1&quot;</span> =<span class="st"> &quot;no&quot;</span>, <span class="st">&quot;2&quot;</span> =<span class="st"> &quot;yes&quot;</span>)</span></code></pre></div>
<p>The <code>default</code> variable indicates whether the loan applicant was able to meet the agreed payment terms or if they went into default.</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="classification-trees.html#cb397-1"></a><span class="kw">table</span>(credit<span class="op">$</span>default)</span>
<span id="cb397-2"><a href="classification-trees.html#cb397-2"></a><span class="co">## </span></span>
<span id="cb397-3"><a href="classification-trees.html#cb397-3"></a><span class="co">##  no yes </span></span>
<span id="cb397-4"><a href="classification-trees.html#cb397-4"></a><span class="co">## 700 300</span></span></code></pre></div>
<p>In this dataset, <span class="math inline">\(30\%\)</span> of the loans went into default. If the bank can successfully predict who is more likely to default, it can reject the loan application and reduce the losses due to default loans.</p>
<p>We now split our data into a training dataset and a testing dataset.</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="classification-trees.html#cb398-1"></a><span class="kw">set.seed</span>(<span class="dv">6</span>) </span>
<span id="cb398-2"><a href="classification-trees.html#cb398-2"></a>random_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(credit), <span class="dv">900</span>) <span class="co"># 90% of data</span></span>
<span id="cb398-3"><a href="classification-trees.html#cb398-3"></a></span>
<span id="cb398-4"><a href="classification-trees.html#cb398-4"></a>credit_train &lt;-<span class="st"> </span>credit[random_index, ]</span>
<span id="cb398-5"><a href="classification-trees.html#cb398-5"></a>credit_test &lt;-<span class="st"> </span>credit[<span class="op">-</span>random_index, ]</span></code></pre></div>
<p><strong>Model training</strong></p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="classification-trees.html#cb399-1"></a>credit_ct &lt;-<span class="st"> </span><span class="kw">C5.0</span>(<span class="dt">x =</span> credit_train[<span class="op">-</span><span class="dv">21</span>], <span class="dt">y =</span> credit_train<span class="op">$</span>default)</span></code></pre></div>
<p>You can find the decision tree by using <code>summary(credit_ct)</code>:</p>
<p><img src="C:/Queens%20Teaching/Teaching/STAT%20362/01a_lect_notes_website/image/ct_credit_small.JPG" width="100%" style="display: block; margin: auto;" /></p>
<p>The meaning of this is similar to that of the <code>iris</code> dataset.</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="classification-trees.html#cb400-1"></a><span class="kw">library</span>(gmodels)</span>
<span id="cb400-2"><a href="classification-trees.html#cb400-2"></a>credit_train_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(credit_ct, credit_train)</span>
<span id="cb400-3"><a href="classification-trees.html#cb400-3"></a></span>
<span id="cb400-4"><a href="classification-trees.html#cb400-4"></a><span class="kw">CrossTable</span>(credit_train<span class="op">$</span>default, credit_train_pred, <span class="dt">prop.chisq =</span> <span class="ot">FALSE</span>, <span class="dt">prop.c =</span> <span class="ot">FALSE</span>,</span>
<span id="cb400-5"><a href="classification-trees.html#cb400-5"></a>           <span class="dt">prop.r =</span> <span class="ot">FALSE</span>, <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;actual default&quot;</span>, <span class="st">&quot;predicted default&quot;</span>))</span>
<span id="cb400-6"><a href="classification-trees.html#cb400-6"></a><span class="co">## </span></span>
<span id="cb400-7"><a href="classification-trees.html#cb400-7"></a><span class="co">##  </span></span>
<span id="cb400-8"><a href="classification-trees.html#cb400-8"></a><span class="co">##    Cell Contents</span></span>
<span id="cb400-9"><a href="classification-trees.html#cb400-9"></a><span class="co">## |-------------------------|</span></span>
<span id="cb400-10"><a href="classification-trees.html#cb400-10"></a><span class="co">## |                       N |</span></span>
<span id="cb400-11"><a href="classification-trees.html#cb400-11"></a><span class="co">## |         N / Table Total |</span></span>
<span id="cb400-12"><a href="classification-trees.html#cb400-12"></a><span class="co">## |-------------------------|</span></span>
<span id="cb400-13"><a href="classification-trees.html#cb400-13"></a><span class="co">## </span></span>
<span id="cb400-14"><a href="classification-trees.html#cb400-14"></a><span class="co">##  </span></span>
<span id="cb400-15"><a href="classification-trees.html#cb400-15"></a><span class="co">## Total Observations in Table:  900 </span></span>
<span id="cb400-16"><a href="classification-trees.html#cb400-16"></a><span class="co">## </span></span>
<span id="cb400-17"><a href="classification-trees.html#cb400-17"></a><span class="co">##  </span></span>
<span id="cb400-18"><a href="classification-trees.html#cb400-18"></a><span class="co">##                | predicted default </span></span>
<span id="cb400-19"><a href="classification-trees.html#cb400-19"></a><span class="co">## actual default |        no |       yes | Row Total | </span></span>
<span id="cb400-20"><a href="classification-trees.html#cb400-20"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb400-21"><a href="classification-trees.html#cb400-21"></a><span class="co">##             no |       605 |        25 |       630 | </span></span>
<span id="cb400-22"><a href="classification-trees.html#cb400-22"></a><span class="co">##                |     0.672 |     0.028 |           | </span></span>
<span id="cb400-23"><a href="classification-trees.html#cb400-23"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb400-24"><a href="classification-trees.html#cb400-24"></a><span class="co">##            yes |        83 |       187 |       270 | </span></span>
<span id="cb400-25"><a href="classification-trees.html#cb400-25"></a><span class="co">##                |     0.092 |     0.208 |           | </span></span>
<span id="cb400-26"><a href="classification-trees.html#cb400-26"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb400-27"><a href="classification-trees.html#cb400-27"></a><span class="co">##   Column Total |       688 |       212 |       900 | </span></span>
<span id="cb400-28"><a href="classification-trees.html#cb400-28"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb400-29"><a href="classification-trees.html#cb400-29"></a><span class="co">## </span></span>
<span id="cb400-30"><a href="classification-trees.html#cb400-30"></a><span class="co">## </span></span></code></pre></div>
<p>The accuracy for the training data is <span class="math inline">\(0.672 + 0.208 = 0.88\)</span>.</p>
<p><strong>Evaluating model performance</strong></p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="classification-trees.html#cb401-1"></a>credit_test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(credit_ct, credit_test)</span>
<span id="cb401-2"><a href="classification-trees.html#cb401-2"></a></span>
<span id="cb401-3"><a href="classification-trees.html#cb401-3"></a><span class="kw">CrossTable</span>(credit_test<span class="op">$</span>default, credit_test_pred, <span class="dt">prop.chisq =</span> <span class="ot">FALSE</span>, <span class="dt">prop.c =</span> <span class="ot">FALSE</span>,</span>
<span id="cb401-4"><a href="classification-trees.html#cb401-4"></a>           <span class="dt">prop.r =</span> <span class="ot">FALSE</span>, <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;actual default&quot;</span>, <span class="st">&quot;predicted default&quot;</span>))</span>
<span id="cb401-5"><a href="classification-trees.html#cb401-5"></a><span class="co">## </span></span>
<span id="cb401-6"><a href="classification-trees.html#cb401-6"></a><span class="co">##  </span></span>
<span id="cb401-7"><a href="classification-trees.html#cb401-7"></a><span class="co">##    Cell Contents</span></span>
<span id="cb401-8"><a href="classification-trees.html#cb401-8"></a><span class="co">## |-------------------------|</span></span>
<span id="cb401-9"><a href="classification-trees.html#cb401-9"></a><span class="co">## |                       N |</span></span>
<span id="cb401-10"><a href="classification-trees.html#cb401-10"></a><span class="co">## |         N / Table Total |</span></span>
<span id="cb401-11"><a href="classification-trees.html#cb401-11"></a><span class="co">## |-------------------------|</span></span>
<span id="cb401-12"><a href="classification-trees.html#cb401-12"></a><span class="co">## </span></span>
<span id="cb401-13"><a href="classification-trees.html#cb401-13"></a><span class="co">##  </span></span>
<span id="cb401-14"><a href="classification-trees.html#cb401-14"></a><span class="co">## Total Observations in Table:  100 </span></span>
<span id="cb401-15"><a href="classification-trees.html#cb401-15"></a><span class="co">## </span></span>
<span id="cb401-16"><a href="classification-trees.html#cb401-16"></a><span class="co">##  </span></span>
<span id="cb401-17"><a href="classification-trees.html#cb401-17"></a><span class="co">##                | predicted default </span></span>
<span id="cb401-18"><a href="classification-trees.html#cb401-18"></a><span class="co">## actual default |        no |       yes | Row Total | </span></span>
<span id="cb401-19"><a href="classification-trees.html#cb401-19"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb401-20"><a href="classification-trees.html#cb401-20"></a><span class="co">##             no |        56 |        14 |        70 | </span></span>
<span id="cb401-21"><a href="classification-trees.html#cb401-21"></a><span class="co">##                |     0.560 |     0.140 |           | </span></span>
<span id="cb401-22"><a href="classification-trees.html#cb401-22"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb401-23"><a href="classification-trees.html#cb401-23"></a><span class="co">##            yes |        16 |        14 |        30 | </span></span>
<span id="cb401-24"><a href="classification-trees.html#cb401-24"></a><span class="co">##                |     0.160 |     0.140 |           | </span></span>
<span id="cb401-25"><a href="classification-trees.html#cb401-25"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb401-26"><a href="classification-trees.html#cb401-26"></a><span class="co">##   Column Total |        72 |        28 |       100 | </span></span>
<span id="cb401-27"><a href="classification-trees.html#cb401-27"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb401-28"><a href="classification-trees.html#cb401-28"></a><span class="co">## </span></span>
<span id="cb401-29"><a href="classification-trees.html#cb401-29"></a><span class="co">## </span></span></code></pre></div>
<p>The accuracy for the testing data is <span class="math inline">\(0.56 + 0.14 = 0.7\)</span>. Is this a good classifer? Let’s take a look at the proportion of defaults in the testing data (it should be close to <span class="math inline">\(0.7\)</span>):</p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="classification-trees.html#cb402-1"></a><span class="kw">summary</span>(credit_test<span class="op">$</span>default)</span>
<span id="cb402-2"><a href="classification-trees.html#cb402-2"></a><span class="co">##  no yes </span></span>
<span id="cb402-3"><a href="classification-trees.html#cb402-3"></a><span class="co">##  70  30</span></span></code></pre></div>
<p>It turns out to be exactly <span class="math inline">\(0.7\)</span> (recall that we randomly select <span class="math inline">\(100\)</span> examples for the testing dataset so that this proportion may not exactly be <span class="math inline">\(0.7\)</span>). If we predict “no default” for every loan, we would obtain the same accuracy as our classification tree.</p>
<p>Remark: accuracy is not the only measure for classification problems. We will discuss other measures later.</p>
<div id="adaptive-boosting" class="section level3">
<h3><span class="header-section-number">12.4.1</span> Adaptive boosting</h3>
<p>One way the C5.0 algorithm improved upon the C4.5 algorithm was through the addition of adaptive boosting. This is a process in which many classification trees are built and the trees vote on the best class for each example. To add boosting to our classification tree, we only need to include the additional parameter <code>trials</code> in the function <code>C5.0</code>. This indicates the number of separate classification trees to use in the boosted team.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="classification-trees.html#cb403-1"></a>credit_ct_boost10 &lt;-<span class="st"> </span><span class="kw">C5.0</span>(<span class="dt">x =</span> credit_train[<span class="op">-</span><span class="dv">21</span>], <span class="dt">y =</span> credit_train<span class="op">$</span>default, <span class="dt">trials =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Training error:</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="classification-trees.html#cb404-1"></a>credit_train_pred_boost10 &lt;-<span class="st"> </span><span class="kw">predict</span>(credit_ct_boost10, credit_train)</span>
<span id="cb404-2"><a href="classification-trees.html#cb404-2"></a></span>
<span id="cb404-3"><a href="classification-trees.html#cb404-3"></a><span class="kw">CrossTable</span>(credit_train<span class="op">$</span>default, credit_train_pred_boost10, <span class="dt">prop.chisq =</span> <span class="ot">FALSE</span>, </span>
<span id="cb404-4"><a href="classification-trees.html#cb404-4"></a>           <span class="dt">prop.c =</span> <span class="ot">FALSE</span>, <span class="dt">prop.r =</span> <span class="ot">FALSE</span>, <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;actual default&quot;</span>, <span class="st">&quot;predicted default&quot;</span>))</span>
<span id="cb404-5"><a href="classification-trees.html#cb404-5"></a><span class="co">## </span></span>
<span id="cb404-6"><a href="classification-trees.html#cb404-6"></a><span class="co">##  </span></span>
<span id="cb404-7"><a href="classification-trees.html#cb404-7"></a><span class="co">##    Cell Contents</span></span>
<span id="cb404-8"><a href="classification-trees.html#cb404-8"></a><span class="co">## |-------------------------|</span></span>
<span id="cb404-9"><a href="classification-trees.html#cb404-9"></a><span class="co">## |                       N |</span></span>
<span id="cb404-10"><a href="classification-trees.html#cb404-10"></a><span class="co">## |         N / Table Total |</span></span>
<span id="cb404-11"><a href="classification-trees.html#cb404-11"></a><span class="co">## |-------------------------|</span></span>
<span id="cb404-12"><a href="classification-trees.html#cb404-12"></a><span class="co">## </span></span>
<span id="cb404-13"><a href="classification-trees.html#cb404-13"></a><span class="co">##  </span></span>
<span id="cb404-14"><a href="classification-trees.html#cb404-14"></a><span class="co">## Total Observations in Table:  900 </span></span>
<span id="cb404-15"><a href="classification-trees.html#cb404-15"></a><span class="co">## </span></span>
<span id="cb404-16"><a href="classification-trees.html#cb404-16"></a><span class="co">##  </span></span>
<span id="cb404-17"><a href="classification-trees.html#cb404-17"></a><span class="co">##                | predicted default </span></span>
<span id="cb404-18"><a href="classification-trees.html#cb404-18"></a><span class="co">## actual default |        no |       yes | Row Total | </span></span>
<span id="cb404-19"><a href="classification-trees.html#cb404-19"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb404-20"><a href="classification-trees.html#cb404-20"></a><span class="co">##             no |       630 |         0 |       630 | </span></span>
<span id="cb404-21"><a href="classification-trees.html#cb404-21"></a><span class="co">##                |     0.700 |     0.000 |           | </span></span>
<span id="cb404-22"><a href="classification-trees.html#cb404-22"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb404-23"><a href="classification-trees.html#cb404-23"></a><span class="co">##            yes |        15 |       255 |       270 | </span></span>
<span id="cb404-24"><a href="classification-trees.html#cb404-24"></a><span class="co">##                |     0.017 |     0.283 |           | </span></span>
<span id="cb404-25"><a href="classification-trees.html#cb404-25"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb404-26"><a href="classification-trees.html#cb404-26"></a><span class="co">##   Column Total |       645 |       255 |       900 | </span></span>
<span id="cb404-27"><a href="classification-trees.html#cb404-27"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb404-28"><a href="classification-trees.html#cb404-28"></a><span class="co">## </span></span>
<span id="cb404-29"><a href="classification-trees.html#cb404-29"></a><span class="co">## </span></span></code></pre></div>
<p>The accuracy becomes <span class="math inline">\(0.7 + 0.283 = 0.983\)</span>. However, we are only interested in the testing accuracy.</p>
<p>Evaluating performance:</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="classification-trees.html#cb405-1"></a>credit_test_pred_boost10 &lt;-<span class="st"> </span><span class="kw">predict</span>(credit_ct_boost10, credit_test)</span>
<span id="cb405-2"><a href="classification-trees.html#cb405-2"></a></span>
<span id="cb405-3"><a href="classification-trees.html#cb405-3"></a><span class="kw">CrossTable</span>(credit_test<span class="op">$</span>default, credit_test_pred_boost10, <span class="dt">prop.chisq =</span> <span class="ot">FALSE</span>, </span>
<span id="cb405-4"><a href="classification-trees.html#cb405-4"></a>           <span class="dt">prop.c =</span> <span class="ot">FALSE</span>, <span class="dt">prop.r =</span> <span class="ot">FALSE</span>, <span class="dt">dnn =</span> <span class="kw">c</span>(<span class="st">&quot;actual default&quot;</span>, <span class="st">&quot;predicted default&quot;</span>))</span>
<span id="cb405-5"><a href="classification-trees.html#cb405-5"></a><span class="co">## </span></span>
<span id="cb405-6"><a href="classification-trees.html#cb405-6"></a><span class="co">##  </span></span>
<span id="cb405-7"><a href="classification-trees.html#cb405-7"></a><span class="co">##    Cell Contents</span></span>
<span id="cb405-8"><a href="classification-trees.html#cb405-8"></a><span class="co">## |-------------------------|</span></span>
<span id="cb405-9"><a href="classification-trees.html#cb405-9"></a><span class="co">## |                       N |</span></span>
<span id="cb405-10"><a href="classification-trees.html#cb405-10"></a><span class="co">## |         N / Table Total |</span></span>
<span id="cb405-11"><a href="classification-trees.html#cb405-11"></a><span class="co">## |-------------------------|</span></span>
<span id="cb405-12"><a href="classification-trees.html#cb405-12"></a><span class="co">## </span></span>
<span id="cb405-13"><a href="classification-trees.html#cb405-13"></a><span class="co">##  </span></span>
<span id="cb405-14"><a href="classification-trees.html#cb405-14"></a><span class="co">## Total Observations in Table:  100 </span></span>
<span id="cb405-15"><a href="classification-trees.html#cb405-15"></a><span class="co">## </span></span>
<span id="cb405-16"><a href="classification-trees.html#cb405-16"></a><span class="co">##  </span></span>
<span id="cb405-17"><a href="classification-trees.html#cb405-17"></a><span class="co">##                | predicted default </span></span>
<span id="cb405-18"><a href="classification-trees.html#cb405-18"></a><span class="co">## actual default |        no |       yes | Row Total | </span></span>
<span id="cb405-19"><a href="classification-trees.html#cb405-19"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb405-20"><a href="classification-trees.html#cb405-20"></a><span class="co">##             no |        59 |        11 |        70 | </span></span>
<span id="cb405-21"><a href="classification-trees.html#cb405-21"></a><span class="co">##                |     0.590 |     0.110 |           | </span></span>
<span id="cb405-22"><a href="classification-trees.html#cb405-22"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb405-23"><a href="classification-trees.html#cb405-23"></a><span class="co">##            yes |        17 |        13 |        30 | </span></span>
<span id="cb405-24"><a href="classification-trees.html#cb405-24"></a><span class="co">##                |     0.170 |     0.130 |           | </span></span>
<span id="cb405-25"><a href="classification-trees.html#cb405-25"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb405-26"><a href="classification-trees.html#cb405-26"></a><span class="co">##   Column Total |        76 |        24 |       100 | </span></span>
<span id="cb405-27"><a href="classification-trees.html#cb405-27"></a><span class="co">## ---------------|-----------|-----------|-----------|</span></span>
<span id="cb405-28"><a href="classification-trees.html#cb405-28"></a><span class="co">## </span></span>
<span id="cb405-29"><a href="classification-trees.html#cb405-29"></a><span class="co">## </span></span></code></pre></div>
<p>The testing accuracy only improves a little bit to <span class="math inline">\(0.72\)</span>.</p>
<p>The results are not very satisfactory. Here are some possible reasons:</p>
<ol style="list-style-type: decimal">
<li>this classification problem is intrinsically difficult for this dataset. For example, we do not have a large sample size and do not have enough information about the background of the loan applicants.</li>
<li>Other methods and algorithms may perform better.</li>
</ol>
<p><strong>We shall talk about a better way of measuring the testing accuracy and other ways to improve the classification later</strong>. At this point, only the most basic usages of classification tree are discussed.</p>
<!-- ### Making some mistakes cost more than others -->
<!-- ```{r} -->
<!-- matrix_dim <- list(predicted = c("no", "yes"), actual = c("no", "yes")) -->
<!-- error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dim) -->
<!-- ``` -->
<!-- We need to supply the values in a specific order: -->
<!-- ```{r} -->
<!-- credit_ct_boost10_cost <- C5.0(x = credit_train[-21], y = credit_train$default, costs = error_cost) -->
<!-- credit_test_pred_boost10 <- predict(credit_ct_boost10_cost, credit_test) -->
<!-- CrossTable(credit_test$default, credit_test_pred_boost10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c("actual default", "predicted default")) -->
<!-- ``` -->
<!-- ## Compared with `rpart` -->
<!-- ```{r} -->
<!-- library(rpart) -->
<!-- fit <- rpart(default ~ ., data = credit_train, parms = list(split = "information")) -->
<!-- table(credit_test$default, predict(fit, credit_test, type = "class")) -->
<!-- ``` -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="k-nearest-neighbors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Book.pdf", "Book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
