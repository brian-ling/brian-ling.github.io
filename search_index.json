[["index.html", "STAT 362 R for Data Science Syllabus", " STAT 362 R for Data Science Brian Ling 2021-03-04 Syllabus STAT 362 R for Data Science Department of Mathematics and Statistics, Queens University Course Description: Introduction to R, data creation and manipulation, data import and export, scripts and functions, control flow, debugging and profiling, data visualization, statistical inference, Monte Carlo methods, decision trees, support vector machines, neural network, numerical methods. Format and Time: Each week: Two asynchronous lectures will be uploaded on Tue and Wed. We will have one synchronous lecture (not mandatory) on Thu (10:30-11:20am) via zoom. Meeting link: see onQ. Recording of the synchronous lecture will be uploaded to onQ. All times are Kingston Time Annoucement, Schedule, Lecture notes: https://brian-ling.github.io/ The same annoucement will also be posted in onQ. Instructor: Brian Ling (bl90@queensu.ca) Teacher assistant: Xinyi GE (16xg3@queensu.ca), Na LI (18nl6@queensu.ca). They will be responsible for grading your assignments. Office Hours: Wed 9pm-10pm, Thu 1pm-2pm, Fri 10:00-10:30am, or by appointment (let me know if you cannot attend any of these) Meeting link: see onQ (same as the link for the synchronous lecture) Intended Student Learning Outcomes: Understand the fundamental concepts in R Be able to import and tidy data for further analysis Be able to visualize data and perform exploratory data analysis Be able to apply appropriate methods for statistical analysis and interpret the output Be able to apply common machine learning algorithms with real-world applications Understand the basics of numerical and Monte Carlo methods Prerequisite: (STAT 263/3.0 or STAT 268/3.0) and MATH 120/3.0 or MATH 121/3.0 or MATH 124/3.0 MATH 126/3.0 or MATH 110/6.0 or MATH 111/6.0 or (MATH 112/3.0 and MATH 212/3.0) or permission of the department. In words, you need to understand probability and statistics, linear algebra (matrix operations), and basic calculus. Lecture Notes: See https://brian-ling.github.io/. Pdf in onQ. Lecture Videos See onQ. Main Textbooks/Readings: R for Data Science, by Garrett Grolemund and Hadley Wickham (https://r4ds.had.co.nz/) Machine Learning with R, by Brett Lantz R Cookbook, by J.D. Long &amp; Paul Teetor (https://rc2e.com/) Other Reference Textbooks/Readings: R Markdown: The Definitive Guide, by Yihui Xie, J. J. Allaire, Garrett Grolemund (https://bookdown.org/yihui/rmarkdown/) R Graphics Cookbook, by Winston Chang (https://r-graphics.org/) An introduction to R, by W. N. Venables, D. M. Smith and the R Core Team (https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf) Course Outline: R Basic Installing R, Rstudio and packages Basic data types and operations Data structures: vector, matrix, array, list, dara frame Data input and output Scripts and functions Control flow Data visualization with ggplot2 Data manipulation with tidyverse Statistical inference in R: one/two-sample test of mean and proportion; correlation test; non-parametric test; regression analysis Some machine learning methods: nearest neighbors, naive Bayes, decision trees, support vector machines, neural network, k-means clustering Introduction to numerical methods: Monte Carlo simulation, gradient descent, Newton Raphson algorithm Note: actual topics may change slightly depending on our progress Grading Scheme: 30% assignments, 30% Quizzes, 40% Final Project Assignments (30%): There will be approximately 6 Assignments. Late submission: receive 80% of the points (within one day), receive 0 (after one day). No exception other than academic accommodation and academic consideration for an extenuating circumstance. Solution will be posted after one day of the due date. One assignment with the lowest score will be dropped. It is recommended to use R Markdown to write your assignment (https://rmarkdown.rstudio.com/lesson-1.html, R Cookbook Ch.16). A template for this will be discussed. Quizzes (30%): Weekly quizzes. Simple questions will be posted on Thursday each week and each quiz is due on Sunday midnight (receive 80% afterwards). You should be able to answer them after attending the lectures. Two quizzes with the lowest score will be dropped. We have 12 weeks. So each quiz is worth 3% after dropping 2 quizzes with the lowest score. Final Project (40%): Form a group of 5-6 students. TA will help with assignment of the group if you cannot find one. 20% is from the final report. 20% is from the final presentation. All members in the group have to present (more details later). The members are expected to know every part of the project. Send your group member list (first name, last name, Queens email, student ID) to Xinyi GE (16xg3@queensu.ca) by Feb 5. If you cannot find a group, tell Xinyi (first name ,last name, Queens email, student ID, time zone). We will try to form a group for you with the same time zone. Academic Integrity: See https://www.queensu.ca/artsci/node/22/mid/1247 Departures from academic integrity include, but are not limited to, plagiarism, use of unauthorized materials, facilitation, forgery and falsification. Actions which contravene the regulation on academic integrity carry sanctions that can range from a warning, to loss of grades on an assignment, to failure of a course, to requirement to withdraw from the university. "],["schedule.html", "Schedule", " Schedule Lecture Content Reading Asg Quiz Lect 1 Introduction of the course, installation of R and RStudio Syllabus, 1-1.4.1  Lect 2 Factors, Matrix, Lists, Data frames 1.4.1-1.4.5  Lect 3 Operators, built-in functions 1.5-1.6 Quiz 1 (Due Jan 17) Lect 4 Shortcuts, probability distributions 1.7-1.9, 2.1 Lect 5 simulation, functions, loops 2.2, 3.1, 3.2 Lect 6 control flow, speed consideration, simulation 3.2-3.5 Quiz 2 (Due Jan 24) Lect 7 Basic plots in R 4.1-4.6 Lect 8 Basic plots in R, Managing data in R 4.7, 5.1-5.3 Lect 9 Review of Lect 1- 8 6.1-6.4 Quiz 3 (Due Jan 31) Lect 10 dplyr, arrange, filter 7.1-7.3 Lect 11 filter 7.3 Lect 12 select, mutate 7.4-7.5 Quiz 4 (Due Feb 7) Lect 13 summarize, %&gt;%, bar chart 7.6, 8.1, 8.2 Lect 14 bar chart, line graph 8.2-8.3 Lect 15 scatterplot, histogram 8.4-8.5 Quiz 5 (Due Feb 26) Lect 16 kernel density, MLE review, logistic regression, optim() 8.5-8.6, 9.1 Lect 17 logistic regression, simulation 9.1 Lect 18 Interval estimation, hypothesis testing 9.2 Quiz 6 (Due Feb 28), Asg 3 (Due Mar 1) Week 6: Study Quiz 5 solution and comments Due date: always at 11:59pm Reading = lecture notes "],["annoucement.html", "Annoucement", " Annoucement Quiz All the quizzes are not timed. You should not discuss with your friends or seek any help online. Quiz 1 is now available: onQ-&gt; Assessements -&gt; Quizzes-&gt;Quiz 1. Please complete it by Jan 17 at 11:59pm (= Jan 18 at 12:00 am). Assignment Assignment 1: covers the materials in Lect 1-6. "],["introduction.html", "Chapter 1 Introduction 1.1 What is R and RStudio? 1.2 What will you learn in this course? 1.3 Lets Get Started 1.4 R Data Structures 1.5 Operators 1.6 Built-in Functions 1.7 Some Useful RStudio Shortcuts 1.8 Exercises 1.9 Comments to Exercises", " Chapter 1 Introduction 1.1 What is R and RStudio? R R is a language and environment for statistical computing and graphics. R is an interpreted language (individual language expressions are read and then executed immediately as soon as the command is entered) To download R, go to https://cloud.r-project.org/ RStudio is an integrated development environment (IDE) for R programming Install R first, then go to https://rstudio.com/products/rstudio/download/ and download RStudio While you can work in R directly, it is recommended to work in RStudio. 1.2 What will you learn in this course? Note: we do not assume you know R or any programming language before. 1.2.1 R and R as a programming language operators control flow (if..else.., for loop) defining a function 1.2.2 Data Wrangling Data wrangling = the process of tidying and transforming the data 1.2.3 Data Visualization Graphs are powerful to illustrate features of the data. You will learn how to create some basic plots as well as using the package ggplot2 to create more elegant plots. Consider a dataset about cars. library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 4.0.3 mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto~ f 18 29 p ## 2 audi a4 1.8 1999 4 manu~ f 21 29 p ## 3 audi a4 2 2008 4 manu~ f 20 31 p ## 4 audi a4 2 2008 4 auto~ f 21 30 p ## 5 audi a4 2.8 1999 6 auto~ f 16 26 p ## 6 audi a4 2.8 1999 6 manu~ f 18 26 p ## 7 audi a4 3.1 2008 6 auto~ f 18 27 p ## 8 audi a4 q~ 1.8 1999 4 manu~ 4 18 26 p ## 9 audi a4 q~ 1.8 1999 4 auto~ 4 16 25 p ## 10 audi a4 q~ 2 2008 4 manu~ 4 20 28 p ## # ... with 224 more rows, and 1 more variable: class &lt;chr&gt; Among the variables in mpg are: displ, a cars engine size, in litres. hwy, a cars fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. Scatterplot Scatterplot, points are labeled with colors according to the class variable Scatterplots Line Chart Bar chart Another Bar Chart Boxplot Histogram 1.2.4 Statistical Inference Many problems in different domains can be formulated into hypothesis testing problems. Are university graduates more likely to vote for Candidate A? Is a treatment effective in reducing weights? Is a drug effective in reducing mortality rate? We want to answer these questions that take into account of the intrinsic variability. Formally, we can perform hypothesis testing and compute the confidence intervals. These are what you learned in STAT 269. It is ok if you havent taken the STAT 269. The topics will be briefly reviewed. We will focus on the applications using R. 1.2.5 Machine Learning We will illustrate some machine learning methods using real datasets. For example, Diagnoising breast cancer with the k-NN algorithm Employ Naive Bayes to build an SMS junk message filter (text data) A wordcloud of text data Use neural network to predict the compressive strength of concrete 1.2.6 Some Numerical Methods Monte Carlo simulation (estimate probabilities, expectations, integrals) numerical optimizaiton methods (e.g. maximizing a multi-parameter likelihood function using optim) 1.2.7 Lastly It is important to communicate your results to other after performing the data analysis. Therefore, you will do a project with presentation and report. 1.3 Lets Get Started The best way to learn R is to get started immediately and try the code by yourselves. We will not discuss every topic in detail at the beginning, which is not interesting and unnecessary. We shall revisit the topics when we need additional knowledge. Simple arithmetic expression # can be used a simple calculator 3 + 5 ## [1] 8 4 * 2 ## [1] 8 10 / 2 ## [1] 5 Comment a code: use the hash mark # # this is a comment, R will not run the code behine # Function for combining c(4, 2, 3) # &quot;c&quot; is to &quot;combine&quot; the numbers ## [1] 4 2 3 Assignment (&lt;- is the assignment operator like = in many other programming languages) y &lt;- c(4, 2, 3) # create a vector called y with elements 4, 2, 3 c(1, 3, 5) -&gt; v # c(1,3,5) is assigned to v Output y ## [1] 4 2 3 v ## [1] 1 3 5 R is case-sensitive. When you type Y, you will see an error message: object Y not found Y ## Error in eval(expr, envir, enclos): object &#39;Y&#39; not found 1.4 R Data Structures Reading: ML with R Ch2 Most frequently used data structures in R: vectors, factors, lists, arrays, matrices, data frames 1.4.1 Vectors Vector fundamental R data structure stores an ordered set of values called elements elements must be of the same type Type: integer, double, character, logical Integer, double, logical, character vectors x &lt;- 1:2 # integer vector, we use a:b to form the sequence of integers from a to b typeof(x) # type of the vector ## [1] &quot;integer&quot; x &lt;- c(1.1, 1.2) # double vector typeof(x) ## [1] &quot;double&quot; length(x) # length of the vector x ## [1] 2 x &lt; 2 # logical (TRUE/FALSE) ## [1] TRUE TRUE p &lt;- c(TRUE, FALSE) subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) # character vector Combine two vectors y &lt;- c(2, 4, 6) c(x, y) # note that we created x above ## [1] 1.1 1.2 2.0 4.0 6.0 c(y, subject_name) # 2, 4, 6 become characters &quot;2&quot;, &quot;4&quot;, &quot;6&quot; ## [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; Assessing elements in the vectors y &lt;- c(2, 4, 6) y[2] # second element ## [1] 4 y[3] # third element ## [1] 6 1.4.2 Factors A factor is a special type of vector that is solely used for representing categorical (male, female/group 1, group 2, group 3) or ordinal (cold, warm, hot/ low, medium, high) variables. Reasons for using factor the category labels are stored only once. E.g., rather than storing MALE, MALE, MALE, FEMALE, the computer may store 1,1,1,2(save memory) many machine learning algorithms treat categorical/ordinal and numeric features differently and may require the input as a factor Create a factor gender &lt;- factor(c(&quot;MALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;)) gender ## [1] MALE MALE FEMALE MALE ## Levels: FEMALE MALE # compared with c(&quot;MALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;) ## [1] &quot;MALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; 1.4.3 Matrix Matrix a collection of numbers in a rectangular form A matrix with dimension n by m means the matrix has n rows and m columns. Create Matrix: To create a \\(3\\times 4\\) matrix with elements 1:12 filled in column-wise A &lt;- matrix(1:12, nrow = 3, ncol = 4) # note that we use = instead of &lt;- A ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Dimension, number of rows, number of columns of a matrix # again R is case-sensitive, a and A are different dim(A) # to find the dimension of A ## [1] 3 4 nrow(A) # to find the number of row in A ## [1] 3 ncol(A) # to find the number of column in A ## [1] 4 By default, the matrix is filled column-wise. You can change to row-wise by adding byrow = TRUE B &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE) B ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 Select rows, columns, submatrix, element A[1, 2] # select the element in the 1st row and 2nd column ## [1] 4 A[2, ] # select 2nd row ## [1] 2 5 8 11 A[, 3] # select 3rd column ## [1] 7 8 9 A[1:2, 3:4] # select a submatrix ## [,1] [,2] ## [1,] 7 10 ## [2,] 8 11 Try: A[c(1, 2), c(1, 3, 4)] A[-1, ] A[, -2] Combine Two Matrices cbind(A, B) # combine column-wise ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1 4 7 10 1 2 3 4 ## [2,] 2 5 8 11 5 6 7 8 ## [3,] 3 6 9 12 9 10 11 12 rbind(A, B) # combine row-wise ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [4,] 1 2 3 4 ## [5,] 5 6 7 8 ## [6,] 9 10 11 12 Try: rbind(B, A) Transpose x &lt;- c(1, 2, 3) t(x) # transpose ## [,1] [,2] [,3] ## [1,] 1 2 3 Q &lt;- matrix(1:4, 2, 2) Q ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 t(Q) # transpose ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 Matrix Addition A &lt;- matrix(1:6, nrow = 2, ncol = 3) B &lt;- matrix(2:7, nrow = 2, ncol = 3) A + B ## [,1] [,2] [,3] ## [1,] 3 7 11 ## [2,] 5 9 13 A - B ## [,1] [,2] [,3] ## [1,] -1 -1 -1 ## [2,] -1 -1 -1 A + 2 ## [,1] [,2] [,3] ## [1,] 3 5 7 ## [2,] 4 6 8 c &lt;- c(1, 2) A + c ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 4 6 8 Elementwise Product A &lt;- matrix(1:6, nrow = 2, ncol = 3) B &lt;- matrix(1:2, nrow = 2, ncol = 3) A * B ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 4 8 12 c &lt;- 2 A * c ## [,1] [,2] [,3] ## [1,] 2 6 10 ## [2,] 4 8 12 c &lt;- c(10, 100) A * c ## [,1] [,2] [,3] ## [1,] 10 30 50 ## [2,] 200 400 600 c &lt;- c(10, 100, 1000) A * c # do you notice the pattern? ## [,1] [,2] [,3] ## [1,] 10 3000 500 ## [2,] 200 40 6000 Matrix Multiplication A &lt;- matrix(1:12, nrow = 3, ncol = 4) # 3x4 matrix t(A) # 4x3 matrix ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 t(A) %*% A #3x3 matrix, %*% = matrix multiplication ## [,1] [,2] [,3] [,4] ## [1,] 14 32 50 68 ## [2,] 32 77 122 167 ## [3,] 50 122 194 266 ## [4,] 68 167 266 365 B &lt;- matrix(1:9, nrow = 3, ncol = 3) B %*% A ## [,1] [,2] [,3] [,4] ## [1,] 30 66 102 138 ## [2,] 36 81 126 171 ## [3,] 42 96 150 204 A %*% B # error, non-conformable arguments ## Error in A %*% B: non-conformable arguments Diagonal Matrix diag(1:4) # diagonal matrix with diagonal elements being 1:4 ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 2 0 0 ## [3,] 0 0 3 0 ## [4,] 0 0 0 4 A &lt;- matrix(1:9, 3, 3) A ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 diag(A) # find the diagonal of A ## [1] 1 5 9 How to create an identity matrix in R? Inverse The inverse of a \\(n \\times n\\) matrix \\(A\\), denoted by \\(A^{-1}\\), is a \\(n \\times n\\) matrix such that \\(AA^{-1} = A^{-1} A = I_n\\), where \\(I_n\\) is the \\(n\\times n\\) identity matrix. To find the inverse of \\(A\\) in R: solve A &lt;- matrix(c(1, 0, 0, 3), 2, 2) solve(A) ## [,1] [,2] ## [1,] 1 0.0000000 ## [2,] 0 0.3333333 Some Statistical Applications I will mention a few connections of matrices with statistics. A dataset is naturally a matrix. Suppose that you have \\(n\\) people. You collected their health information: blood pressure, height, weight, age, whether they smoke (1 if yes, 0 if no), whether they drink (1/0), etc. Linear regression: we observe \\((x,y)\\), where \\(x\\) is a vector of covariates and \\(y\\) is your response. For example, \\(y\\) is the blood pressure, \\(x\\) is the collection of other health information. The linear regression model assumes that \\[y = \\beta_0 + x^T \\beta_1 + \\varepsilon,\\] where \\(\\varepsilon\\) is the error term. Our goal is to estimate \\(\\beta:=(\\beta_0, \\beta_1)\\). Let \\(X\\) be the design matrix. That is \\(X\\) is a \\(n \\times p\\) matrix where \\(n\\) is the number of observation, \\(p-1\\) is the number of covariates. The least squares solution for \\(\\beta\\) is \\[\\hat{\\beta} = (X^T X)^{-1}X^TY.\\] We will revisit linear regression later (I know some of you may not know linear regression). Correlation matrix, Covariance matrix. Let \\(X\\) be a random vector (column vector). The covariance matrix is defined as \\[\\Sigma := E[(X-E(X))(X-E(X))^T].\\] 1.4.4 Lists store an ordered set of elements like a vector can store different R data types (unlike a vector) # let&#39;s create some vectors (of different types) subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) # at this point, you should notice that meaningful names should be used for the variables temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) # notice how we use _ to separate two words # this is one of the styles in coding, you should be consistent with your style data &lt;- list(fullname = subject_name, temperature = temperature, flu_status = flu_status) # you may wonder what is the meaning of temperature = temperature # in &quot;fullname = subject_name&quot; # on the left of = is the name of the 1st element of your list # on the right of = is the name of the variable that you want to # assign the value to the 1st element data ## $fullname ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; ## ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE To assess the element of a list: data$flu_status ## [1] FALSE FALSE TRUE data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE data[2:3] # if you don&#39;t have the names ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE 1.4.5 Data frames Data frame can be understood as a list of vectors, each having exactly the same number of values, arranged in a structure like a spreadsheet or database gender &lt;- c(&quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;) blood &lt;- c(&quot;O&quot;, &quot;AB&quot;, &quot;A&quot;) pt_data &lt;- data.frame(subject_name, temperature, flu_status, gender, blood) pt_data ## subject_name temperature flu_status gender blood ## 1 John 98.1 FALSE MALE O ## 2 Jane 98.6 FALSE FEMALE AB ## 3 Steve 101.4 TRUE MALE A colnames(pt_data) ## [1] &quot;subject_name&quot; &quot;temperature&quot; &quot;flu_status&quot; &quot;gender&quot; ## [5] &quot;blood&quot; pt_data$subject_name ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; pt_data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE pt_data[1, ] # like a matrix ## subject_name temperature flu_status gender blood ## 1 John 98.1 FALSE MALE O pt_data[, 2:3] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE Create a new column pt_data$temp_c &lt;- (pt_data$temperature - 32) * 5 / 9 pt_data ## subject_name temperature flu_status gender blood temp_c ## 1 John 98.1 FALSE MALE O 36.72222 ## 2 Jane 98.6 FALSE FEMALE AB 37.00000 ## 3 Steve 101.4 TRUE MALE A 38.55556 pt_data$fever &lt;- (pt_data$temp_c &gt; 37.6) pt_data ## subject_name temperature flu_status gender blood temp_c fever ## 1 John 98.1 FALSE MALE O 36.72222 FALSE ## 2 Jane 98.6 FALSE FEMALE AB 37.00000 FALSE ## 3 Steve 101.4 TRUE MALE A 38.55556 TRUE 1.5 Operators Priority Operator Meaning 1 $ component selection 2 [ [[ subscripts, elements 3 ^ (caret) exponentiation 4 - unary minus 5 : sequence operator 6 %% %/% %*% modulus, integer divide, matrix multiply 7 * / multiply, divide 8 + - add, subtract 9 &lt; &gt; &lt;= &gt;= == != comparison 10 ! not 11 &amp; | &amp;&amp; || logical and, logical or 12 &lt;- -&gt; = assignments # $ for list, data frame, etc subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) data &lt;- list(fullname = subject_name, temperature = temperature, flu_status = flu_status) data$fullname ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; pt_data &lt;- data.frame(subject_name, temperature, flu_status) pt_data$temperature ## [1] 98.1 98.6 101.4 # [ ], [[]] x &lt;- c(1, 5, 7) x[2] ## [1] 5 data[[1]] ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; # x^r = x to the power of r x &lt;- 2 x^4 # 16 ## [1] 16 # modulus 7 %% 2 # 7 divided by 2 equals 3 but it remains 1, modulus = reminder ## [1] 1 10 %% 3 ## [1] 1 20 %% 2 ## [1] 0 # integer division 7 %/% 2 ## [1] 3 20 %/% 3 ## [1] 6 Comparison # &lt;, &gt;, &lt;=, &gt;=, ==, != x &lt;- 2 x &gt; 3 ## [1] FALSE x &lt; 4 ## [1] TRUE x &lt;- c(1, 5, 7) x &lt; 3 # compare each element with 3 ## [1] TRUE FALSE FALSE x &gt;= 5 ## [1] FALSE TRUE TRUE x == 5 # if x is equal to 5, not x = 5 ## [1] FALSE TRUE FALSE x != 5 # if x is not equal to 5 ## [1] TRUE FALSE TRUE x &lt;- TRUE !x # not x ## [1] FALSE x &lt;- 2 x &lt;= 2 ## [1] TRUE !(x &lt;= 2) ## [1] FALSE &amp; and &amp;&amp; indicate logical AND. The shorter form performs elementwise comparisons. The longer form examine only the first element of each vector. x &lt;- c(TRUE, FALSE, TRUE) y &lt;- c(FALSE, FALSE, TRUE) x &amp; y ## [1] FALSE FALSE TRUE x &amp;&amp; y ## [1] FALSE z &lt;- c(TRUE) x &amp;&amp; z ## [1] TRUE | and || indicate logical OR. The shorter form performs elementwise comparisons. The longer form examine only the first element of each vector. x &lt;- c(TRUE, FALSE, TRUE) y &lt;- c(FALSE, FALSE, TRUE) x | y ## [1] TRUE FALSE TRUE x || y ## [1] TRUE z &lt;- c(TRUE) x || z ## [1] TRUE Assignment # these assignments are the same, it is recommended to use &lt;- a &lt;- 2 a ## [1] 2 2 -&gt; b b ## [1] 2 c = 2 c ## [1] 2 Do !(x &gt; 1) &amp; (x &lt; 4) and !((x &gt; 1) &amp; (x &lt; 4)) give different results? 1.5.1 Vectorized Operators An important property of many of the operators is that they are vectorized. This means that the operation will be performed elementwise. x &lt;- c(1, 2, 3) y &lt;- c(5, 6, 7) x + y ## [1] 6 8 10 x * y ## [1] 5 12 21 2 * x # you do not need to use c(2,2,2)*x ## [1] 2 4 6 y / 2 # you do not need to use y/c(2,2,2) ## [1] 2.5 3.0 3.5 A &lt;- matrix(1:9, nrow = 3, ncol = 3) B &lt;- matrix(1:9, nrow = 3, ncol = 3) A + B ## [,1] [,2] [,3] ## [1,] 2 8 14 ## [2,] 4 10 16 ## [3,] 6 12 18 A * B # this is not matrix multiplication, but elementiwse multiplication ## [,1] [,2] [,3] ## [1,] 1 16 49 ## [2,] 4 25 64 ## [3,] 9 36 81 x &lt;- c(1, 3, 5) y &lt;- c(2, 2, 9) x &lt; y ## [1] TRUE FALSE TRUE 1.6 Built-in Functions Common mathematical functions sqrt, abs, sin, cos, log, exp. To get help on the usage of a function. Use ?. For example, if you want to know more about log. Type ?log in the console. You will then see that by default, log computes the natrual logarithms. Other useful functions Name Operations ceiling smallest integer greater than or equal to element floor largest integer less than or equal to element trunc ignore the decimal part round round up for positive and round down for negative sort sort the vector in ascending or descending order sum, prod sum and produce of a vector cumsum, cumprod cumulative sum and product min, max return the smallest and largest values range return a vector of length 2 containing the min and max mean return the sample mean of a vector var return the sample variance of a vector sd return the sample standard deviation of a vector seq generate a sequence of number rep replicate elements in a vector Note: If you have data \\(x_1,\\ldots,x_n\\), the sample variance is defined as \\[ S^2_n := \\frac{1}{n-1} \\sum^n_{i=1}(x_i-\\overline{x}_n)^2. \\] Note that we divide the sum by \\(n-1\\) but not \\(n\\). The sample standard deviation is the square root of the sample variance. x &lt;- 1:5 y &lt;- sqrt(x) y ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 ceiling(y) ## [1] 1 2 2 2 3 sum(x) ## [1] 15 prod(x) ## [1] 120 cumsum(x) ## [1] 1 3 6 10 15 cumprod(x) ## [1] 1 2 6 24 120 min(x) ## [1] 1 max(x) ## [1] 5 range(x) ## [1] 1 5 mean(x) ## [1] 3 var(x) ## [1] 2.5 rep(0, 10) # create a vector of length 10 with all elements being 0 ## [1] 0 0 0 0 0 0 0 0 0 0 rep(1, 10) # create a vector of length 10 with all elements being 1 ## [1] 1 1 1 1 1 1 1 1 1 1 1.6.1 sort() x &lt;- c(1, 5, 3, 10) sort(x) # default = ascending order ## [1] 1 3 5 10 sort(x, decreasing = TRUE) # descending order ## [1] 10 5 3 1 1.6.2 seq() This is an example of function with more than one argument. # seq(from, to) seq(1:5) ## [1] 1 2 3 4 5 seq(from = 1, to = 5) ## [1] 1 2 3 4 5 # seq(from, to, by) seq(1, 5, 2) ## [1] 1 3 5 seq(from = 1, to = 5, by = 2) ## [1] 1 3 5 # seq(from, to, length) seq(0, 10, length = 21) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 ## [15] 7.0 7.5 8.0 8.5 9.0 9.5 10.0 seq(from = 0, to = 10, length = 21) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 ## [15] 7.0 7.5 8.0 8.5 9.0 9.5 10.0 1.6.3 rep() # rep(data, times), try ?rep rep(0, 10) ## [1] 0 0 0 0 0 0 0 0 0 0 rep(c(1, 2, 3), 3) ## [1] 1 2 3 1 2 3 1 2 3 1.6.4 pmax, pmin x &lt;- c(1, 3, 5) y &lt;- c(2, 4, 4) max(x, y) # maximum of x and y ## [1] 5 min(x, y) # minimum of x and y ## [1] 1 pmax(x, y) # elementwise comparison ## [1] 2 4 5 pmin(x, y) # elementwise comparison ## [1] 1 3 4 1.7 Some Useful RStudio Shortcuts See also https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts Ctrl + 1: Move focus to the Source Editor (when you are in the Console) Ctrl + 2: Move focus to the Console (when you are in the source window) \\(\\uparrow\\) (the up arrow key on the keyboard): go to the previous command (in the console) \\(\\downarrow\\) (the down arrow key on the keyboard): go to the next command (in the console) Esc: Delete the current command/ Interrupt currently executing command Ctrl + Tab: go to the next tab 1.8 Exercises To test your understanding, try to evaluate the following code by hand and then check with the output from R. x &lt;- (10:1)[c(-1, -4)] x &lt;- x^2 x[5] # what do you expect to see? a&lt;-c(1:3, 6, 3 + 3, &quot;sta&quot;, 3 &gt; 5) a #? typeof(a) #? length(a) #? x&lt;-rep(1:6, rep(1:3, 2)) x %% 2 == 0 #? x[x %% 2 == 0] #? round(-3.7) #? trunc(-3.7) #? floor(-3.7) #? ceiling(-3.7) #? round(3.8) #? trunc(3.8) #? floor(3.8) #? ceiling(3.8) #? x &lt;- c(4, 3, 8, 7, 5, 6, 2, 1) sort(x) #? order(x) #? sum(x) + prod(x) #? cumsum(x) + cumprod(x) #? max(x) + min(x) #? 1.9 Comments to Exercises These are comments but not answers but you can get the answers immediately by running the code. x &lt;- (10:1)[c(-1, -4)] # 10:1 will give you a vector with elements 10, 9, 8,...,1 10:1 ## [1] 10 9 8 7 6 5 4 3 2 1 # [c(-1, -4)] will remove the 1st and 4th elements in the vector # therefore 10 and 7 will be removed from 10:1 given x ## [1] 9 8 6 5 4 3 2 1 x &lt;- x^2 # ^2 will square each of the elements in the vector x[5] # 36 ## [1] 16 a&lt;-c(1:3, 6, 3 + 3, &quot;sta&quot; , 3 &gt; 5) a # when you combine numeric, characters and logical values, the results become character ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;6&quot; &quot;6&quot; &quot;sta&quot; &quot;FALSE&quot; typeof(a) # character ## [1] &quot;character&quot; length(a) # ## [1] 7 x&lt;-rep(1:6, rep(1:3, 2)) # we first evaluate rep(1:3, 2), which is # 1 2 3 1 2 3 # rep then replicates the elements by the corresponding number of times # 1 is repeated 1 time # 2 is repeated 2 times # 3 is repeated 3 times # 4 is repeated 1 time # 5 is repeated 2 times # 6 is repeated 3 times x %% 2 == 0 # check if the modulus is 0 when x is divided by 2 ## [1] FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE ## [12] TRUE # this is equivalent to ask if the elements are even x[x %% 2 == 0] # find out all the even elements ## [1] 2 2 4 6 6 6 # if you need to use these functions # you may try with a positive number and a negative number # to see if the results are what you want round(-3.7) # try round(-3.5), round(-3.4) ## [1] -4 trunc(-3.7) ## [1] -3 floor(-3.7) ## [1] -4 ceiling(-3.7) ## [1] -3 round(3.8) ## [1] 4 trunc(3.8) ## [1] 3 floor(3.8) ## [1] 3 ceiling(3.8) ## [1] 4 x&lt;-c(4, 3, 8, 7, 5, 6, 2, 1) sort(x) # asecending order ## [1] 1 2 3 4 5 6 7 8 order(x) # from the result, can you guess what it does? ## [1] 8 7 2 1 5 6 4 3 # order(x) returns a permutation which rearranges its first argument into ascending or descending order # that is, x[order(x)] = sort(x) sum(x) + prod(x) ## [1] 40356 cumsum(x) + cumprod(x) ## [1] 8 19 111 694 3387 20193 40355 40356 max(x) + min(x) ## [1] 9 "],["probability.html", "Chapter 2 Probability 2.1 Probability Distributions 2.2 Simulation", " Chapter 2 Probability Optional Reading: R Cookbook Ch8 2.1 Probability Distributions Using the normal distribution as an example: Function Purpose dnorm Normal density pnorm Normal CDF qnorm Normal quantile function rnorm Normal random variables Examples Density of \\(N(2, 3^2)\\) at \\(5\\). dnorm(5, mean = 2, sd = 3) ## [1] 0.08065691 \\(P(X \\leq 3)\\), where \\(X \\sim N(2, 3^2)\\) pnorm(3, mean = 2, sd = 3) ## [1] 0.6305587 # &quot;mean =&quot; and &quot;sd =&quot; are optional pnorm(3, 2, 3) ## [1] 0.6305587 Generate 10 random variables, each follows \\(N(3, 4^2)\\). rnorm(10, 3, 4) ## [1] -1.1627704 1.2299483 -1.8527734 1.0806251 6.3108719 4.4707855 ## [7] 4.3038223 4.6072161 3.8863634 -0.6679703 95th percenttile of \\(N(0, 1)\\). Find \\(q\\) such that \\(P(Z \\leq q) = 0.95\\) qnorm(0.95, 0, 1) ## [1] 1.644854 Plotting the normal density x &lt;- seq(-4, 4, by = 0.1) plot(x, dnorm(x), type = &quot;l&quot;, main = &quot;Density of N(0,1)&quot;) # &quot;l&quot; for lines 2.1.1 Common Distributions Common discrete distributions Discrete distribution R name Parameters Binomial binom n = number of trials; p = probability of success for one trial Geometric geom p = probability of success for one trial Negative binomial (NegBinomial) nbinom size = number of successful trials; either prob = probability of successful trial or mu = mean Poisson pois lambda = mean Common continuous distributions Continuous distribution R name Parameters Beta beta shape1; shape2 Cauchy cauchy location; scale Chi-squared (Chisquare) chisq df = degrees of freedom Exponential exp rate F f df1 and df2 = degrees of freedom Gamma gamma rate; either rate or scale Log-normal (Lognormal) lnorm meanlog = mean on logarithmic scale; sdlog = standard deviation on logarithmic scale Logistic logis location; scale Normal norm mean; sd = standard deviation Students t (TDist) t df = degrees of freedom Uniform unif min = lower limit; max = upper limit To get help on the distributions: ?dnorm ?dbeta ?dcauchy # the following distributions need to use different code ?TDist ?Chisquare ?Lognormal Examples (Using Binomial as an Example) dbinom(2, 10, 0.6) # p_X(2), p_X is the pmf of X, X ~ Bin(n=10, p=0.6) ## [1] 0.01061683 pbinom(2, 10, 0.6) # F_X(2), F_X is the CDF of X, X ~ Bin(n=10, p=0.6) ## [1] 0.01229455 qbinom(0.5, 10, 0.6) # 50th percentile of X ## [1] 6 rbinom(4, 10, 0.6) # generate 4 random variables from Bin(n=10, p=0.6) ## [1] 6 3 6 3 x &lt;- 0:10 plot(x, dbinom(x, 10, 0.6), type = &quot;h&quot;) # &quot;h&quot; for histogram like vertical lines 2.1.2 Exercises The average number of trucks arriving on any one day at a truck depot in a certain city is known to be 12. Assuming the number of trucks arriving on any one day has a Poisson distribution, what is the probability that on a given day fewer than 9 (strictly less than 9) trucks will arrive at this depot? ppois(8, 12) ## [1] 0.1550278 Let \\(Z \\sim N(0, 1)\\). Find \\(c\\) such that \\(P(Z \\leq c) = 0.1151\\) qnorm(0.1151) ## [1] -1.199844 \\(P(1\\leq Z \\leq c) = 0.1525\\) c &lt;- qnorm(pnorm(1) + 0.1525) # draw a graph # test the answer pnorm(c) - pnorm(1) ## [1] 0.1525 \\(P(-c \\leq Z \\leq c) = 0.8164\\). # P(0 &lt;= Z &lt;= c) = 0.8164/2 # P(Z &lt;= c) = 0.8164/2 + 0.5 c &lt;- qnorm(0.8164 / 2 + 0.5) # test our answer pnorm(c)- pnorm(-c) ## [1] 0.8164 Plot the density of a chi-squared distribution with degrees of freedom \\(4\\), from \\(x=0\\) to \\(x=10\\). Find the 95th percentile of this distribution. # note that a chi-squared r.v. is nonnegative x &lt;- seq(0, 10, by = 0.1) plot(x, dchisq(x, df = 4), type = &quot;l&quot;) qchisq(0.95, df = 4) ## [1] 9.487729 Simulate \\(10\\) Bernoulli random variables with parameter \\(0.6\\). # Bernoulli(p) = Bin(1, p) rbinom(10, size = 1, prob = 0.6) ## [1] 0 1 1 1 1 1 1 1 1 0 Plot the Poisson pmf with parameter \\(2\\) from \\(x = 0\\) to \\(x = 10\\). x &lt;- 0:10 plot(x, dpois(x, 2), type = &quot;h&quot;) Draw a plot to illustrate that the 97.5th percentile of the t distribution will be getting closer to that of the standard normal distribution when the degrees of freedom increases. x &lt;- 10:200 plot(x, qt(0.975, df = x), type = &quot;l&quot;, ylim = c(1.9,2.3)) # add a horizontal line with value at qnorm(0.975) # lty = 2 for dashed line, check ?par abline(h = qnorm(0.975), lty = 2) # Therefore, for a large sample, t-test and z-test will give you similar result. 2.2 Simulation We have already seen how to use functions like runif, rnorm, rbinom to generate random variables. R actually generates pseudo-random number sequence (deterministic sequence of numbers that approximates the properties of random numbers) The pseduo-random number sequence will be the same if it is initialized by the same seed (can be used to reproduce the same simulation results or used to debug). # every time you run the first two lines, you get the same result set.seed(1) runif(5) ## [1] 0.2655087 0.3721239 0.5728534 0.9082078 0.2016819 # every time you run the following code, you get a different result runif(5) ## [1] 0.89838968 0.94467527 0.66079779 0.62911404 0.06178627 Sampling from discrete distributions Usage of sample: sample(x, size, replace = FALSE, prob = NULL) See also ?sample. sample(10) # random permutation of integers from 1 to 10 ## [1] 3 1 5 8 2 6 10 9 4 7 sample(10, replace = T) # sample with replacement ## [1] 5 9 9 5 5 2 10 9 1 4 sample(c(1, 3, 5), 5, replace = T) ## [1] 5 3 3 3 3 # simulate 20 random variables from a discrete distribution sample(c(-1,0,1), size = 20, prob = c(0.25, 0.5, 0.25), replace = T) ## [1] -1 1 1 -1 0 0 1 1 0 -1 0 0 0 0 0 1 1 0 -1 0 Example: Suppose we have a fair coin and we play a game. We flip the coin. We win $1 if the result is head and lose $1 if the result is tail. You play the game 100 times. You are interested in the cumulative profit. set.seed(1) # R actually generates pseudo random numbers # setting the seed ensure that each time you will get the same result # for illustration, code debugging, reproducibility profit &lt;- sample(c(-1, 1), size = 100, replace = T) plot(cumsum(profit), type = &quot;l&quot;) set.seed(2) profit &lt;- sample(c(-1, 1), size = 100, replace = T) plot(cumsum(profit), type = &quot;l&quot;) Example: You have two dice \\(A\\) and \\(B\\). For die \\(A\\), there are \\(6\\) sides with numbers \\(1,2,3,4,5,6\\) and the corresponding probability of getting these values are \\(0.1,0.1,0.1,0.1,0.1,0.5\\). For die \\(B\\), there are \\(4\\) sides with numbers \\(1,2,3,7\\) and the corresponding probability of getting these values are \\(0.3,0.2,0.3,0.2\\). You roll the two dice independently. Estimate \\(P(X &gt; Y)\\) using simulation, where \\(X\\) is the result from die \\(A\\) and \\(Y\\) is the result from die \\(B\\). n &lt;- 10000 # number of simulations X &lt;- sample(1:6, size = n, replace = TRUE, prob = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.5)) Y &lt;- sample(c(1, 2, 3, 7), size = n, replace = TRUE, prob = c(0.3, 0.2, 0.3, 0.2)) mean(X &gt; Y) ## [1] 0.6408 Why the sample mean approximates the required probability? Recall the strong law of large numbers (SLLN). Let \\(X_1,\\ldots,X_n\\) be independent and identically distributed random variables with mean \\(\\mu:=E(X)\\). Let \\(\\overline{X}_n:= \\frac{1}{n} \\sum^n_{i=1}X_i\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\rightarrow} \\mu.\\] Note: a.s. means almost surely. The above convergence means \\(P(\\lim_{n\\rightarrow \\infty} \\overline{X}_n = \\mu) = 1\\). Note that (the expectation of an indicator random variable is the probability that the corresponding event will happen) \\[ P(X&gt;Y) = E(I(X&gt;Y)).\\] To apply SLLN, we just need to recognize the underlying random variable is \\(I(X&gt;Y)\\). Then, with probability \\(1\\), the sample mean \\[ \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i) \\rightarrow P(X&gt;Y).\\] The quantity on the LHS is what we compute in mean(X&gt;Y) (note that we are using vectorized comparison). We will see additional simulation examples after we talk about some programming in R There are many important topics that we will not discuss algorithms for simulating random variables inverse transform acceptance rejection Markov Chain Monte Carlo methods to reduce variance in simulation Control variates Antithetic variates Importance sampling "],["programming-in-r.html", "Chapter 3 Programming in R 3.1 Writing functions in R 3.2 Control Flow 3.3 Automatically Reindent Code 3.4 Speed Consideration 3.5 Another Simulation Example", " Chapter 3 Programming in R Optional reading: R Cookbook Ch 15, R for data science https://r4ds.had.co.nz/program-intro.html Hands-on programming with R: https://rstudio-education.github.io/hopr/ 3.1 Writing functions in R When you have to copy and paste some code more than 2 times, you should consider writing a function Writing a function can simplify your code and isolate the main part of your program General format of a function function_name &lt;- function(argument1, argument2) { statements } Example: Lets try to write a function to compute the sample variance. my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2)/(n - 1)) } y &lt;- 1:9 my_var(y) ## [1] 7.5 var(y) # compared with the bulit-in function ## [1] 7.5 x &lt;- rnorm(1000, mean = 0, sd = 2) my_var(x) ## [1] 4.262595 var(x) # why the result is not equal to 2? ## [1] 4.262595 We can also write my_var2 &lt;- function(x){sum((x - mean(x))^2)/(length(x) - 1)} The variable x is the argument to be passed into the function. The variables mean_x and n are local variables whose scope is within this function. y &lt;- 2 f &lt;- function(x) { y &lt;- x x &lt;- 4 y } y ## [1] 2 f(3) # output the value f(3) ## [1] 3 y # y is unchanged, y is defined in the global environment ## [1] 2 x ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found We shall write code using proper indentation (easier to read and debug) # with indentation (use this one) my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2)/(n - 1)) } # no indentation my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2)/(n - 1)) } The number of arguments passed to a function can be more than one Example: Write a function to compute the pooled sample standard deviation of two independent samples \\(x_1,\\ldots,x_n\\) and \\(y_1,\\ldots,y_m\\) of sizes \\(n\\) and \\(m\\). Recall that the pooled sample standard deviation is defined as: \\[ S_p := \\sqrt{\\frac{(n-1)S^2_X + (m-1)S^2_Y}{m+n-2}}, \\] where \\(S^2_X\\) and \\(S^2_Y\\) are the sample variances of \\(x_1,\\ldots,x_n\\) and \\(y_1,\\ldots,y_m\\), respectively. pooled_sd &lt;- function(x, y) { n &lt;- length(x) m &lt;- length(y) return(sqrt(((n - 1) * var(x) + (m - 1) * var(y))/(m + n - 2))) } Remark: if the final statement will output something, it will be the output of the function. You can also use return() as above. That is, pooled_sd and pooled_sd2 are exactly the same. pooled_sd2 &lt;- function(x, y) { n &lt;- length(x) m &lt;- length(y) sqrt(((n - 1) * var(x) + (m - 1) * var(y))/(m + n - 2)) } You can return more than one value in a function my_var_sd &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) my_var &lt;- sum((x - mean_x)^2)/(n - 1) return(c(my_var, sqrt(my_var))) } You may also return a list my_var_sd &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) my_var &lt;- sum((x - mean_x)^2)/(n - 1) output &lt;- list(var = my_var, sd = sqrt(my_var)) return(output) } Example: write a function called my_summary that will output a list with elements being the mean, sd, median, min and max of a given vector. my_summary &lt;- function(x){ output &lt;- list(mean = mean(x), sd = sd(x), median = median(x), min = min(x), max = max(x)) return(output) } my_summary(1:10) ## $mean ## [1] 5.5 ## ## $sd ## [1] 3.02765 ## ## $median ## [1] 5.5 ## ## $min ## [1] 1 ## ## $max ## [1] 10 Define a function with default value my_power = function(x, p = 2) { return(x^p) } my_power(3) # by default, p = 2, will compute 3^2 ## [1] 9 my_power(3, 3) # will compute 3^3 ## [1] 27 3.2 Control Flow 3.2.1 for loop You can use a for loop when you know how many times you will loop. Syntax: for (var in sequence) { statement # do this statement for each value of i } Examples: for (i in 1:5) { # note: you do not have to define i beforehand print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 for (i in c(1, 3, 6)) { print(i) } ## [1] 1 ## [1] 3 ## [1] 6 Examples Write R code to find \\(\\sum^{10}_{i=1} \\sum^4_{j=1} \\frac{i^2}{(i+j)^2}\\). sum &lt;- 0 for (i in 1:10) { for (j in 1:4) { sum &lt;- sum + i^2/(i + j)^2 } } sum ## [1] 18.26491 Write R code to find \\(\\sum^{10}_{i=1} \\sum^i_{j=1} \\frac{i^2}{(i+j)^3}\\). sum &lt;- 0 for (i in 1:10) { for (j in 1:i) { sum &lt;- sum + i^2/(i + j)^3 } } sum ## [1] 2.779252 3.2.2 while loop You can use a while loop if you want to loop until a specific condition is met. For example, when you minimize a function numerically using some iterative algorithm, you may want to stop when the objective value does not change much. You may not know how many loops are required in advance so that a while loop may be better than a for loop in this application. Syntax: while (condition) { statement # while the condition is TRUE, do this } A simple example: i &lt;- 1 while (i &lt; 6) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 # What happen if you change &quot;i &lt; 6&quot; to &quot;i &lt;= 6&quot;? Ans: will print 1 to 6 # What happen if you change &quot;i &lt; 6&quot; to &quot;i &lt;= 5&quot;? Ans: outputs are the same Another example: # find the smallest n such that 1^2+ 2^2+ ... + n^2 &gt; 65 sum &lt;- 0 i &lt;- 1 while (sum &lt; 65) { sum &lt;- sum + i^2 print(c(i, sum)) i &lt;- i + 1 } ## [1] 1 1 ## [1] 2 5 ## [1] 3 14 ## [1] 4 30 ## [1] 5 55 ## [1] 6 91 i # 6 ## [1] 7 3.2.3 if (cond) Syntax: if (condition) { statement # do this if the condition is TRUE } Example: write a function that outputs positive if a positive number is entered. # check if a number if positive my_pos &lt;- function(x) { if (x &gt; 0) { print(&quot;positive&quot;) } } my_pos(-2) my_pos(2) ## [1] &quot;positive&quot; 3.2.4 if (cond) else expr Syntax if (condition) { statement1 # do this if condition is TRUE } else { statement2 # do this if condition is FALSE } Example: # write my own absolute value function my_abs &lt;- function(x) { if (x&gt;=0) { return(x) } else { return(-x) } } my_abs(-2) ## [1] 2 my_abs(3) ## [1] 3 my_abs(0) ## [1] 0 Error-handling in a function: my_sqrt = function(x) { if (x &gt;= 0) { print(sqrt(x)) # do this if x &gt;= 0 } else { cat(&quot;Error: this is a negative number!&quot;) # do this otherwise } } my_sqrt(-2) ## Error: this is a negative number! 3.2.5 If else ladder Syntax # Example if (condition1) { statement1 } else if (condition2) { statement2 } else if (condition1) { statement3 } Example: score_to_grade = function(x) { if (x&gt;=90) { cat(&quot;A+&quot;) } else if (x &gt;= 85) { cat(&quot;A&quot;) } else if (x &gt;= 80) { cat(&quot;A-&quot;) } else { cat(&quot;B+ or below&quot;) } } # after you write the function, you should check each case carefully score_to_grade(92) ## A+ score_to_grade(88) ## A score_to_grade(83) ## A- score_to_grade(78) ## B+ or below 3.3 Automatically Reindent Code To indent a block of code, highlight the text in RStudio, then press Ctrl+i (Windows or Linux) or press Cmd+i (Mac). Poor indentation, difficult to read for (i in 1:5) { if (i &gt;= 3) { print(i*2) } else { print(i * 3) } } ## [1] 3 ## [1] 6 ## [1] 6 ## [1] 8 ## [1] 10 Highlight the block of code, press Ctrl+i or Cmd+i for (i in 1:5) { if (i &gt;= 3) { print(i*2) } else { print(i * 3) } } ## [1] 3 ## [1] 6 ## [1] 6 ## [1] 8 ## [1] 10 3.4 Speed Consideration While the computing power is getting stronger and stronger, we should still write code that runs efficiently. # suppose we want to simulate 200,000 normal random variables n &lt;- 200000 x &lt;- rep(0, n) # create a vector for storage of the values initial_time &lt;- proc.time() for (i in 1:n) { x[i] &lt;- rnorm(1) } proc.time() - initial_time ## user system elapsed ## 0.4 0.0 0.4 n &lt;- 200000 x &lt;- rep(0, n) # create a vector for storage of the values # Alternatively system.time({ for (i in 1:n) { x[i] &lt;- rnorm(1) } }) ## user system elapsed ## 0.33 0.00 0.33 The user time is the CPU time charged for the execution of user instructions of the calling process. The system time is the CPU time charged for execution by the system on behalf of the calling process. A much more efficient way for the same task is to use system.time({ n &lt;- 200000 x &lt;- rnorm(n) }) ## user system elapsed ## 0.01 0.00 0.01 Another example: set.seed(1) x &lt;- rnorm(2e6) y &lt;- rnorm(2e6) v &lt;- rep(0, 2e6) system.time({ for (i in 1:length(x)){ v[i] &lt;- x[i]+y[i] } }) ## user system elapsed ## 0.14 0.00 0.14 system.time(v &lt;- x + y) ## user system elapsed ## 0 0 0 The general rule is to use vectorized operations whenever possible and to avoid using for loops. We use a for loop when the code is not time-consuming or when the code is hard to write without using a for loop. A more advanced option is to combine C++ with R using the package rcpp. That is, we can write the most time-consuming part of the R code in C++, which could run many times faster (will not be discussed in this course). See also http://www.noamross.net/archives/2014-04-16-vectorization-in-r-why/ 3.5 Another Simulation Example A simple model on the stock return assumes that (i) \\[r_{t+1} := \\log \\frac{P_{t+1}}{P_t} \\sim N(\\mu,\\sigma^2), \\] where \\(r_{t+1}\\) is the log-return at Day \\(t+1\\), \\(P_t\\) is the stock price at the end of Day \\(t\\); (ii) \\(r_1,r_2,\\ldots\\) are iid. Simple algebra shows that \\[P_{t+1} = P_t e^{r_{t+1}}\\] and \\[P_{t+1} = P_0 e^{ \\sum^{t+1}_{i=1} r_i}.\\] Suppose that the current price of a certain stock \\(P_0\\) is \\(100\\), \\(\\mu = 0.0002\\) and \\(\\sigma = 0.015\\). Using simulation, estimate the probability that the price is below $95 at the close of at least one of the next 30 trading days. no_sim &lt;- 10000 # number of simulation below &lt;- rep(0, no_sim) for (i in 1:no_sim) { price &lt;- 100 * exp(cumsum(rnorm(30, mean = 0.0002, sd = 0.015))) below[i] &lt;- min(price) &lt; 95 } mean(below) ## [1] 0.4384 "],["creating-some-basic-plots.html", "Chapter 4 Creating Some Basic Plots 4.1 Scatter Plot 4.2 Line Graph 4.3 Bar Chart 4.4 Histogram 4.5 Box Plot 4.6 Plotting a function curve 4.7 More on plots with base R 4.8 Summary of ggplot", " Chapter 4 Creating Some Basic Plots The base R contains many basic methods for producing graphics. We will learn some of them in this chapter. For more elegant plots, we will use the package ggplot2. We will use some simple datasets in base R to illustrate how to create some basic plots in this chapter. In this next chapter, we will discuss how to input our own data and transform them. Reference: R graphics cookbook, R for data science, R Cookbook. We will load the package tidyverse. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. You have to install the package before you can use it. install.packages(&quot;tidyverse&quot;) Load the library library(tidyverse) # load the tidyverse package (which contains ggplot2) if we want to make it clear what package an object/ function comes from, use package name followed by two colons, like dplyr::mutate(). 4.1 Scatter Plot Lets take a look at the mtcars dataset. This dataset comes with base R. head(mtcars) # this is a data frame ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) # display the structure of the data frame ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... mpg: miles/gallon wt: weight (1000lbs) Scatter plot with base graphics # x-axis: mtcars$wt # y-axis: mtcars$mpg plot(x = mtcars$wt, y = mtcars$mpg) # &quot;x=&quot;, &quot;y=&quot; are optional You can produce the same plot with plot(mtcars$wt, mtcars$mpg) Scatter plot with ggplot2 ggplot(data = mtcars, aes(x = wt, y = mpg)) + geom_point() Scatter plot with base graphics (when you only have \\(x\\) and \\(y\\) but not a dataset) set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 2 * x + rnorm(100, 0, 1) plot(x, y) Scatter plot with ggplot2 (when you only have \\(x\\) and \\(y\\) but not a dataset) set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 2*x + rnorm(100, 0, 1) ggplot(mapping=aes(x = x, y = y)) + # by default, data = NULL geom_point() 4.2 Line Graph The dataset pressure (also in base R) contains the relation between temperature in degrees Celsius and vapor pressure of mercury in millimeters (of mercury). Line graph with base graphics # the only difference from a scatter plot is that we add type=&quot;l&quot; plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) #l = line Line graph with base graphics with points plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) points(pressure$temperature, pressure$pressure) # add some points Line graph with base graphics with another line and points (with color) plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) points(pressure$temperature, pressure$pressure) # the additional line may not have a physical meaningful # just an illustration of how to add a line wit base graphics lines(pressure$temperature, pressure$pressure / 2, col = &quot;red&quot;) points(pressure$temperature, pressure$pressure / 2, col = &quot;red&quot;) Colors in R You can go to https://www.r-graph-gallery.com/ggplot2-color.html and read more about colors in R. For example, you can specify the color by name, rgb, number and hex code. plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;, col = rgb(0.1, 0.2,0.5,1)) lines(pressure$temperature, pressure$pressure / 2, type = &quot;l&quot;, col = 2) lines(pressure$temperature, pressure$pressure * 2, col = &quot;#8B2813&quot;) lines(pressure$temperature, pressure$pressure * 3, col = &quot;cornflowerblue&quot;) Line graph with ggplot2 ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() Line graph with ggplot2 with points ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() + geom_point() Line graph with ggplot2 with another line and points (with color) ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() + geom_point() + geom_line(aes(x = temperature, y = pressure / 2), color =&quot;red&quot;) + geom_point(aes(x = temperature, y = pressure / 2), color = &quot;#8B2813&quot;) Remark: it is common with ggplot() to split the command on multiple lines, ending each line with a + so that R knows that the command will continue on the next line. 4.3 Bar Chart Two types of bar charts: Bar chart of values. x-axis: discrete variable, y-axis: numeric data (not necessarily count data) Bar chart of count. x-axis: discrete variable, y-axis: count of cases in the discrete variable Remark: for histogram: x-axis = continuous variable, y-axis = count of cases in the interval. The BOD data set has 6 rows and 2 columns giving the biochemical oxygen demand versus time in an evaluation of water quality. # instead of using head(BOD) and str(BOD), we can also change it to a &quot;tibble&quot; # we can view the first few lines and the data structure as_tibble(BOD) ## # A tibble: 6 x 2 ## Time demand ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 8.3 ## 2 2 10.3 ## 3 3 19 ## 4 4 16 ## 5 5 15.6 ## 6 7 19.8 Bar chart of values with base graphics # names.arg = a vector of names to be plotted below each bar or group of bars. barplot(BOD$demand, names.arg = BOD$Time) Bar chart of counts with base graphics In the dateset mtcars, cylis the number of cylinders in the car. The possible values are \\(4, 6\\), and \\(8\\). We first find the count of each unique value in mtcars$cyl: table(mtcars$cyl) ## ## 4 6 8 ## 11 7 14 To plot the bar chart, we use barplot(table(mtcars$cyl)) Bar chart with values with ggplot2 We first make the Time variable to a factor: BOD2 = BOD BOD2$Time = as.factor(BOD2$Time) ggplot(BOD2, aes(x = Time, y = demand)) + geom_col() What will happen if we do not make Time as a factor: ggplot(BOD, aes(x = Time, y = demand)) + geom_col() Bar chart of counts with ggplot2 # the y position is calculated by counting the number of rows for each value of cyl ggplot(mtcars, aes(x = cyl)) + geom_bar() 4.4 Histogram mpg in mtcars is the miles/gallon of the car. It is a continuous variable. Histogram with base graphics hist(mtcars$mpg) Histogram with base graphics # Specify approximate number of bins with &quot;breaks&quot; hist(mtcars$mpg, breaks = 10) Histogram with ggplo2 ggplot(mtcars, aes(x = mpg)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(mtcars, aes(x = mpg)) + geom_histogram(binwidth = 2.5) Remark: different bin widths will give you histograms with different looks. 4.5 Box Plot Lets take a look at another dataset ToothGrowth. In particular, supp is a factor. as_tibble(ToothGrowth) ## # A tibble: 60 x 3 ## len supp dose ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 ## 6 10 VC 0.5 ## 7 11.2 VC 0.5 ## 8 11.2 VC 0.5 ## 9 5.2 VC 0.5 ## 10 7 VC 0.5 ## # ... with 50 more rows Box plot with basic graphics (using plot) # if x is a factor, use the following code plot(x = ToothGrowth$supp, y = ToothGrowth$len) Box plot with basic graphics (using boxplot) # len ~ supp is an example of a &quot;formula&quot; (y ~ x) boxplot(len ~ supp, data = ToothGrowth) Box plot with basic graphics + interaction of two variables on x-axis (using boxplot) boxplot(len ~ supp + dose, data = ToothGrowth) Box plot with ggplot2 ggplot(ToothGrowth, aes(x = supp, y = len)) + geom_boxplot() Box plot with ggplot2 + interaction of two variables on x-axis ggplot(ToothGrowth, aes(x = interaction(supp, dose), y = len)) + geom_boxplot() What will happen if dose take a lot more values? ggplot(ToothGrowth, aes(x = interaction(supp, dose + 1:5), y = len)) + geom_boxplot() 4.6 Plotting a function curve curve(x^3 - 5 * x, from = -4, to = 4) Alternatively: x &lt;- seq(-4, 4, len = 1000) plot(x, x^3 - 5 * x, type = &quot;l&quot;) Plotting a built-in function curve(dnorm(x), from = -4, to = 4) Plotting a self-defined function my_function &lt;- function(x) { 1 / (1 + exp(-x + 10)) } curve(my_function, from = 0, to = 20) Plotting a function with additional arguments curve(dnorm(x, mean = 2, sd = 3), from = -4, to = 4) 4.7 More on plots with base R 4.7.1 Multi-frame plot To create a 3x2 multi-frame plot. Use par(mfrow = c(3, 2)). set.seed(1) x &lt;- rnorm(100, 50, 5) y &lt;- x + rnorm(100, 2, 2) # create a 2x2 multi-frame plot par(mfrow=c(2, 2)) hist(x) hist(y,breaks = 10) plot(x, y) boxplot(x, y) 4.7.2 Type of Plot Option Type type=\"p\" Points (default) type=\"l\" Lines connecting the data points type=\"b\" Points and non-overlapping lines type=\"h\" Height lines type=\"o\" Points and overlapping lines par(mfrow=c(3, 2)) x &lt;- -5:5 y &lt;- x^2 plot(x, y) plot(x, y, type = &quot;p&quot;) plot(x, y, type = &quot;l&quot;) plot(x, y, type = &quot;b&quot;) plot(x, y, type = &quot;h&quot;) plot(x, y, type = &quot;o&quot;) 4.7.3 Parameters of a plot Parameter Meaning type See Type of Plot main Title sub Subtitle xlab x-axis label ylab y-axis label xlim x-axis range ylim y-axis range pch Symbol of data points col Color of data points lty Type of the line To illustrate some of the components: set.seed(1) x &lt;- rnorm(100, 50, 15) y &lt;- x + rnorm(100, 2, 13) plot(x, y, col = &quot;red&quot;, pch = 15, main = &quot;This is the title&quot;, xlim = c(0, 100), ylim = c(0,100), xlab = &quot;name of x-axis&quot;, ylab = &quot;name of y-axis&quot;) 4.7.4 Elements on plot Function Description abline(c,m) plot the line y = mx +c abline(h = a) plot the line y = a abline(v = b) plot the line x = b lines(x,y) line joining points with coordinates (x,y) set.seed(1) x &lt;- rnorm(100, 50, 15) y &lt;- x + rnorm(100, 2, 13) plot(x, y) # connect the points (20, 20), (30, 80), (40, 40) with a line lines(x = c(20, 30, 40),y = c(20, 80, 40), col = &quot;red&quot;) abline(v = 60, col = &quot;blue&quot;) 4.8 Summary of ggplot Plot geom scatter plot geom_point() line graph geom_line() bar chart of values geom_col() bar chart of counts geom_bar() histogram geom_histogram() box plot geom_boxplot() "],["managing-data-with-r.html", "Chapter 5 Managing Data with R 5.1 Missing Values 5.2 Saving, loading, and removing R data structures 5.3 Importing and saving data from CSV files", " Chapter 5 Managing Data with R Optional Reading: ML with R Ch2 5.1 Missing Values Missing values are common in real datasets. NA is used to denote missing values. (x &lt;- c(1, 2, 3, NA, 4, NA, 4)) # we can use (x&lt;-1) to assign 1 to x and display x at the same time ## [1] 1 2 3 NA 4 NA 4 mean(x) # mean cannot be computed when missing values exist ## [1] NA mean(x, na.rm = TRUE) # NA values will be removed before computing mean ## [1] 2.8 sd(x) # sd cannot be computed when missing values exist ## [1] NA sd(x, na.rm = TRUE) # NA values will be removed before computing SD ## [1] 1.30384 is.na(x) # logical vector ## [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE x[!is.na(x)] #select the elements with non-missing valuess ## [1] 1 2 3 4 4 na.omit(x) # select the elements with non-missing valuess ## [1] 1 2 3 4 4 ## attr(,&quot;na.action&quot;) ## [1] 4 6 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; 5.2 Saving, loading, and removing R data structures Removing all objects in R: rm(list=ls()) ls() returns a vector of all data structures currently in memory x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) ls() ## [1] &quot;x&quot; &quot;y&quot; To remove x from the memory rm(x) x # because we have deleted x, an error message occurs ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found Saving objects to a file (regardless of whether they are vectors, factors, lists, etc) A &lt;- matrix(1:9, 3, 3) f &lt;- function(x){ return(1) } save(A, f, file = &quot;my_data.RData&quot;) Loading objects from a .RData file. rm(list=ls()) # remove everything load(&quot;my_data.RData&quot;) A ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 f ## function(x){ ## return(1) ## } 5.3 Importing and saving data from CSV files Finding current directory getwd() ## [1] &quot;C:/Queens Teaching/Teaching/STAT 362/01c_published_webiste&quot; If you use mac, you will probably see \"/Users/..../\". Setting working directory setwd(&quot;C:/Queens Teaching/Teaching/STAT 362/Notes&quot;) # use /, not \\ If you use mac, change the above code accordingly. Writing to a file my_data &lt;- data.frame(x = c(1, 2, 3), y = c(4, 5, 6)) my_data ## x y ## 1 1 4 ## 2 2 5 ## 3 3 6 library(tidyverse) write_csv(my_data, &quot;C:/Queens Teaching/Teaching/STAT 362/my_data.csv&quot;) Reading a csv file A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. If we use read_csv from the package tidyverse, the resulting object is a tibble. If we use read.csv from base R, the resulting object is a data frame. See R for data science (https://r4ds.had.co.nz/data-import.html) for a discussion on the differences between read.csv and read_csv. A tibble allows us to perform different types of transformation (next chapter). my_data &lt;- read_csv(&quot;C:/Queens Teaching/Teaching/STAT 362/my_data.csv&quot;) ## ## -- Column specification -------------------------------------------------- ## cols( ## x = col_double(), ## y = col_double() ## ) "],["review-1-lect-1-8.html", "Chapter 6 Review 1 (Lect 1-8) 6.1 Simulation 6.2 Matrix 6.3 Basic Operation 6.4 Some basic plots in R", " Chapter 6 Review 1 (Lect 1-8) 6.1 Simulation Basic vectorized comparison X &lt;- c(1, 1, 5) Y &lt;- c(5, 5, 1) X &gt; Y # FALSE FALSE TRUE ## [1] FALSE FALSE TRUE sum(X &gt; Y) # TRUE = 1, FALSE = 0 ## [1] 1 mean(X &gt; Y) # 1/3 ## [1] 0.3333333 The code mean(X &gt; Y) is computing \\[\\begin{equation*} \\frac{1}{3}( I(X_1 &gt; Y_1) + I(X_2 &gt; Y_2) + I(X_3 &gt; Y_3)), \\end{equation*}\\] where \\(I(\\cdot)\\) is the indicator function, that is, \\(I(X_1 &gt; Y_1) = 1\\) if \\(X_1 &gt; Y_1\\) and \\(I(X_1 &gt; Y_1) = 0\\) if \\(X_1 \\leq Y_1\\). Example: Let \\(X \\sim N(mean = 0, sd = 2)\\) and \\(Y \\sim Exp(rate = 3)\\). Estimate \\(P(X &gt; Y)\\) using simulation. n &lt;- 10000 # number of simulations X &lt;- rnorm(n, 0, 2) Y &lt;- rexp(n, 3) mean(X &gt; Y) ## [1] 0.4372 The code mean(X &gt; Y) is computing \\[\\begin{equation*} \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i), \\end{equation*}\\] which is an approximation of \\(P(X&gt;Y)\\). Theory Why the sample mean \\(\\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i)\\) approximates the required probability \\(P(X&gt;Y)\\)? Recall the strong law of large numbers (SLLN): Let \\(X_1,\\ldots,X_n\\) be independent and identically distributed random variables with mean \\(\\mu:=E(X)\\). Let \\(\\overline{X}_n:= \\frac{1}{n} \\sum^n_{i=1}X_i\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\rightarrow} \\mu.\\] Note: a.s. means almost surely (probability = 1). The above convergence means \\(P(\\lim_{n\\rightarrow \\infty} \\overline{X}_n = \\mu) = 1\\). Note that (the expectation of an indicator random variable is the probability that the corresponding event will happen) \\[ P(X&gt;Y) = E(I(X&gt;Y)).\\] To apply SLLN, we just need to recognize the underlying random variable is \\(I(X&gt;Y)\\) (which follows a Bernoulli distribution with parameter \\(P(X&gt;Y)\\)). Then, with probability \\(1\\), the sample mean \\[ \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i) \\rightarrow P(X&gt;Y).\\] The quantity on the LHS is what we compute in mean(X&gt;Y) (note that we are using vectorized comparison). This is the reason why we can use mean(X&gt;Y) to estimate \\(P(X&gt;Y)\\). Ex 1: Let X ~ N(mean = 2, sd =1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(\\max(X,Y)&gt;Z)\\). Recall the difference between pmax and max: # always try with simple examples when you test the usage of functions x &lt;- c(1, 2, 3) y &lt;- c(0, 5, 10) max(x, y) ## [1] 10 pmax(x, y) ## [1] 1 5 10 n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(pmax(x,y) &gt; z) ## [1] 0.5131 Ex 2 Let X ~ N(mean = 2, sd = 1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(\\min(X,Y)&gt;Z)\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(pmin(x, y) &gt; z) # what is the difference between pmin and min? ## [1] 0.11471 Ex 3 Let X ~ N(mean = 2, sd = 1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(X^2 Y&gt;Z)\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(x^2 * y &gt; z) ## [1] 0.41099 Ex 4 Person \\(A\\) generates a random variable \\(X \\sim N(2, 1)\\) and Person \\(B\\) generates a random variable \\(Z \\sim Unif(0, 4)\\). If \\(X &lt; Z\\), Person \\(A\\) will discard \\(X\\) and generate another random variable \\(Y \\sim Exp(0.5)\\). Find the probability that the number generated by \\(A\\) is greater than that by \\(B\\). n &lt;- 100000 greater &lt;- rep(0, n) for (i in 1:n) { X &lt;- rnorm(1, 2, 1) Z &lt;- runif(1, 0, 4) if (X&lt; Z) { Y &lt;- rexp(1, rate = 0.5) greater[i] = Y &gt; Z } else { greater[i] = 1 # 1 means A&#39;s no &gt; B&#39;s no } } mean(greater) ## [1] 0.63699 Remark: you may find that the following code gives you almost the same answer. Why? n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 0.5) z &lt;- runif(n, 0, 4) mean(pmax(x, y) &gt; z) ## [1] 0.63969 Ex 5 Let X ~ N(mean = 2, sd = 1) and Y ~ Exp(rate = 2). Estimate \\(E(min(X,Y))\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) mean(pmin(x, y)) ## [1] 0.445328 The code mean(pmin(x, y)) computes \\[\\begin{equation*} \\frac{1}{n} \\sum^n_{i=1} \\min(X_i, Y_i), \\end{equation*}\\] which approximates \\(E(\\min(X,Y))\\) by the SLLN. 6.2 Matrix Ex6: Write a function called matrix_times_vector to compute \\(X Y\\), where \\(X\\) is a matrix and \\(Y\\) is a vector. The output should be a vector. matrix_times_vector = function(X, Y) { as.vector(X %*% Y) } # e.g. X &lt;- matrix(1:12, 3, 4) Y &lt;- 1:4 X ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Y ## [1] 1 2 3 4 matrix_times_vector(X, Y) ## [1] 70 80 90 Note: X%*%Y will return a matrix. We can use as.vector to change it into a vector. It is common to see the error non-conformable arguments. This is because the dimensions of your matrices/vectors do not match. If you have a \\(n\\times p\\) matrix \\(A\\) and \\(m \\times q\\) matrix \\(B\\), you can do the matrix multiplication \\(AB\\) only if \\(p = m\\). In R, if this is not the case, there will be an error. Similarly, if you have a vector \\(d\\) of length \\(m\\). You can do the matrix multiplication \\(A d\\) only if \\(p = m\\). Ex7: Write a function called matrix_times_vector2 to compute \\(X Y\\), where \\(X\\) is a matrix and \\(Y\\) is a vector. The output should be a vector. However, you should check if the dimensions of the inputs are appropriate before you perform the calculation. Display an error message The dimensions do not match if this is not the case. matrix_times_vector2 = function(X, Y) { p &lt;- ncol(X) m &lt;- length(Y) if (p == m) { return(as.vector(X %*% Y)) } else { cat(&quot;The dimensions do not match&quot;) } } # e.g. X &lt;- matrix(1:12, 4, 3) Y &lt;- 1:4 matrix_times_vector2(X, Y) ## The dimensions do not match X &lt;- matrix(1:12, 3, 4) Y &lt;- 1:4 matrix_times_vector2(X, Y) ## [1] 70 80 90 Example: if A is matrix and x is a vector, you can find the sums of the elements in A and x by sum(A) and sum(x) respectively. x &lt;- 1:10 sum(x) ## [1] 55 A &lt;- matrix(1:10, 5, 2) sum(A) ## [1] 55 Ex 8: Find \\(\\sum^{5}_{x=1}\\sum^{4}_{y=1} \\frac{x}{x+y}\\) without any loops. (x &lt;- matrix(1:5, nrow = 4, ncol = 5, byrow = TRUE)) # define and display at the same time using (...) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 1 2 3 4 5 ## [3,] 1 2 3 4 5 ## [4,] 1 2 3 4 5 (y &lt;- matrix(1:4, nrow = 4, ncol = 5, byrow = FALSE)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1 ## [2,] 2 2 2 2 2 ## [3,] 3 3 3 3 3 ## [4,] 4 4 4 4 4 x / (x + y) # vectorized operation ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.5000000 0.6666667 0.7500000 0.8000000 0.8333333 ## [2,] 0.3333333 0.5000000 0.6000000 0.6666667 0.7142857 ## [3,] 0.2500000 0.4000000 0.5000000 0.5714286 0.6250000 ## [4,] 0.2000000 0.3333333 0.4285714 0.5000000 0.5555556 sum(x / (x + y)) ## [1] 10.72817 6.3 Basic Operation Example Let v be a vector of integers. Write a one-line R code to compute the product of all the even integers in v. To illustrate how to solve the question step by step: v &lt;- -10:10 # begin writing your code by setting some integers v %% 2 # find the remainder, if the remainder is 0, it is an even number ## [1] 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 v %% 2 == 0 # check which elements is 0 ## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE ## [12] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE v[v %% 2 == 0] # select the elements which are &quot;TRUE&quot; ## [1] -10 -8 -6 -4 -2 0 2 4 6 8 10 prod(v[v %% 2 == 0]) # find the product ## [1] 0 # the result is 0. v may not be a good example to check if the code is correct # let&#39;s change to some other vector v &lt;- 2:8 prod(v[v %% 2 == 0]) ## [1] 384 2 * 4 * 6 * 8 #check ## [1] 384 # now, the final answer is prod(v[v%%2 == 0]) Ex 9a: Given that x = 1:100. Write a one-line R code to copmute \\[\\begin{equation*} S := 1^2 - 2^2 + 3^2 - \\ldots + 99^2 - 100^2. \\end{equation*}\\] x &lt;- 1:100 sum((x[x %% 2 == 1])^2) - sum((x[x %% 2 == 0])^2) ## [1] -5050 Ex 9b (optional) Can you do Ex 9a without any program or calculator? Ex 10: Write a function with inputs n and p to compute \\[\\begin{equation*} S(n, p) := 1^p - 2^p + 3^p -\\ldots + (-1)^{n-1} (n-1)^p + (-1)^n n^p. \\end{equation*}\\] snp &lt;- function(n, p) { x &lt;- 1:n return(sum((x[x %% 2 == 1])^p) - sum((x[x %% 2 == 0])^p)) } 6.4 Some basic plots in R Example Write a function called my_summary_plot with input being two numeric vectors x,y and outputs a \\(2 \\times 2\\) multi-frame plot with (i) histogram of x, (ii) histogram of y, (iii) scatter plot of y versu x, and (iv) boxplots of x and y. # when you write a function, you can begin with some sample x and y x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) par(mfrow=c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) # after that, you wrap the code into a function without defining x, y my_summary_plot = function(x, y){ par(mfrow=c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) } # after you write the function, you should try if it will work or not rm(list=ls()) # let&#39;s remove everything in the memory my_summary_plot =function(x, y){ par(mfrow=c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) } x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) my_summary_plot(x,y) # try another example x &lt;- rnorm(100, 0, 1) y &lt;- 5 * x + rnorm(100, 0, 1) my_summary_plot(x, y) "],["data-transformation-with-dplyr.html", "Chapter 7 Data Transformation with dplyr 7.1 Introduction 7.2 arrange() 7.3 filter() 7.4 select() 7.5 mutate() 7.6 summarize() 7.7 Summary", " Chapter 7 Data Transformation with dplyr 7.1 Introduction Reference: see https://r4ds.had.co.nz/transform.html Preparation We will use a dataset in the package nycflights13. To install it: install.packages(&quot;nycflights13&quot;) To use the dataset or functions in the package, we first load the library: library(nycflights13) In Chapter 4, we have installed tidyverse, which contains the package dplyr. Now, load the package library(tidyverse) ## function (.data, ...) ## { ## UseMethod(&quot;select&quot;) ## } ## &lt;bytecode: 0x000001e7bca30680&gt; ## &lt;environment: namespace:dplyr&gt; nycflights13 The dataset flights in the package nyclfights13 contains all \\(336,776\\) flights that departed from New York City in 2013. Check ?flights for details. # let&#39;s view the dataset flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 336,766 more rows, and 12 more variables: ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; flights is a tibble. Tibbles are data frames with better properties. Optional: If you are interested in the differences between a data frame and a tibble, you can go to https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html To view the complete dataset, use View(flights). Five key dplyr functions arrange(): reorder the rows fliter(): pick observations by their values select(): pick variables by their names mutate(): create new variables with functions of existing variables summarize(): collapse many values down to a single summary All functions work similarly: The first argument is a data frame/ tibble The subsequent argument describe what to do with the data frame, using the variable names (without quotes). The result is a new data frame. Of course, it is also possible to perform the same tasks without using dplyr functions. We will also discuss briefly how to use the base subsetting with [] to select the data. In general, the functions in dplyr are designed to transform the data more easily. 7.2 arrange() arrange() orders your dataset. If more than one column name is provided, each additional column will be used to break ties in the values of preceding columns. To reorder by a column in ascending order: arrange(flights, year, month, day) Lets create a simple dataset to illustrate this because flights is already sorted in year, month and day. (data &lt;- tibble(x = c(2,2,1,4,5), y = c(2,3,10,10,10))) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 ## 2 2 3 ## 3 1 10 ## 4 4 10 ## 5 5 10 arrange(data, x) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 ## 2 2 2 ## 3 2 3 ## 4 4 10 ## 5 5 10 # first sort in ascending order of x, use y to break any ties and sort in descending order arrange(data, x, desc(y)) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 ## 2 2 3 ## 3 2 2 ## 4 4 10 ## 5 5 10 To reorder by a column in descending order, use desc(): arrange(flights, desc(arr_delay)) Missing values are always sorted at the end # create a tibble with one column called x with values 5,2,NA df &lt;- tibble(x = c(5, 2, NA)) arrange(df, x) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 2 ## 2 5 ## 3 NA arrange(df, desc(x)) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 5 ## 2 2 ## 3 NA 7.2.1 Exercises Sort flights to find the most delayed flights. Find the flights that left earliest. arrange(flights, desc(dep_delay)) arrange(flights, dep_delay) Which flights traveled the longest? Which traveled the shortest? arrange(flights, desc(distance)) arrange(flights, distance) 7.3 filter() filter() only includes rows where the condition is TRUE Select all flights on Jan 1st: # flights is the name of your data frame # month == 1, day == 1 is the condition (jan1 &lt;- filter(flights, month == 1, day == 1)) ## # A tibble: 842 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 832 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Lets use a simple dataset to see how to perform the same task without filter: # just a simple dataset (data &lt;- tibble(x = c(1,3,5,5,3), y = 1:5)) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 1 1 ## 2 3 2 ## 3 5 3 ## 4 5 4 ## 5 3 5 data$x == 5 # logical vector ## [1] FALSE FALSE TRUE TRUE FALSE data[data$x == 5,] # select the rows with value &quot;TRUE&quot; ## # A tibble: 2 x 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 5 3 ## 2 5 4 # returning to the flights dataset base_jan1 &lt;- flights[(flights$month == 1 &amp; flights$day == 1), ] (flights$month == 1 &amp; flights$day == 1) is a logical vector indicating if the corresponding flight was on Jan 1 (TRUE if yes). Now, lets check if jan1 and base_jan1 are the same using identical: identical(jan1, base_jan1) # TURE means they are the same, FALSE means they are not the same ## [1] TRUE More Examples Select flights that departed in Nov or Dec filter(flights, month == 11 | month == 12) # alternatively, simpler code filter(flights, month %in% c(11,12)) How to use the operator %in%? y &lt;- c(1,3,5) x &lt;- 1 x%in%y #whether 1 is in {1,3,5} ## [1] TRUE x &lt;- c(1,3,2,4,1) x%in%y # check if whether each element in x is in {1,3,5} ## [1] TRUE TRUE FALSE FALSE TRUE %in% also works with characters c(&quot;a&quot;, &quot;b&quot;) %in% c(&quot;a&quot;, &quot;c&quot;, &quot;d&quot;) ## [1] TRUE FALSE The result is TRUE FALSE because \"a\" is in c(\"a\", \"c\", \"d\") and \"b\" is not in c(\"a\", \"c\", \"d\"). Perform the same task without filter: flights[flights$month == 11 | flights$month == 12,] # or flights[flights$month %in% c(11, 12), ] Flights that were not delayed (on arrival or departure) by more than two hours: delay1 &lt;- filter(flights, arr_delay &lt;= 120, dep_delay &lt;= 120) Without using filter delay2 &lt;- flights[flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120, ] Lets check if delay1 and delay2 are the same. identical(delay1, delay2) ## [1] FALSE The result is FALSE, meaning delay1 and delay2 are not the same. Why? Because some elements in flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120 are NA. # to find out the number of NA values sum(is.na(flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120)) ## [1] 9304 As a result, with the base subsetting method, a row with all NA values will be selected. On the other hand, for filter, when a condition evaluates to NA, the row will be dropped. Lets create a small dataset to illustrate this. From now on, lets try to use tibble instead of data.frame. data &lt;- tibble(x = 1:4, y = c(1, 2, NA, 4)) data[data$y &lt;= 3, ] ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 NA NA # to avoid the above problem # which() returns which elements are TRUE which(data$y &lt;= 3) # the result is 1, 2 ## [1] 1 2 data[which(data$y &lt;= 3), ] # select row 1, row 2 ## # A tibble: 2 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 # using filter filter(data, y &lt;= 3) ## # A tibble: 2 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 # if we want to include the row where the value of y is NA # recall that | means &quot;or&quot; data[which(data$y &lt;= 3 | is.na(data$y)), ] ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA # using filter filter(data, y &lt;= 3 | is.na(y)) ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA If you want to drop the NA values with base subsetting[], you may use delay3 &lt;- flights[which((flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120) == TRUE), ] To see if delay1 and delay3 are exactly the same: identical(delay1, delay3) # TRUE means exactly the same ## [1] TRUE At this point, you should see that filter could perform the same tasks with simpler code. 7.3.1 Exercises 1a. Find all flights that had an arrival delay of two or more hours (drop the rows with NA in arr_delay). # Using &quot;filter&quot; filter(flights, arr_delay &gt;= 120) # Without using &quot;filter&quot; flights[which(flights$arr_delay &gt;= 120), ] 1b. Find all flights that flew to Houston (IAH or HOU). # Using &quot;filter&quot; filter(flights, dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)) # Without using &quot;filter&quot; flights[which(flights$dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)), ] 1c. Find all flights that were operated by United, American, or Delta # find all the sorted carrier codes in the dataset sort(unique(flights$carrier)) ## [1] &quot;9E&quot; &quot;AA&quot; &quot;AS&quot; &quot;B6&quot; &quot;DL&quot; &quot;EV&quot; &quot;F9&quot; &quot;FL&quot; &quot;HA&quot; &quot;MQ&quot; &quot;OO&quot; &quot;UA&quot; &quot;US&quot; &quot;VX&quot; ## [15] &quot;WN&quot; &quot;YV&quot; # look up airline names from their carrier codes airlines ## # A tibble: 16 x 2 ## carrier name ## &lt;chr&gt; &lt;chr&gt; ## 1 9E Endeavor Air Inc. ## 2 AA American Airlines Inc. ## 3 AS Alaska Airlines Inc. ## 4 B6 JetBlue Airways ## 5 DL Delta Air Lines Inc. ## 6 EV ExpressJet Airlines Inc. ## 7 F9 Frontier Airlines Inc. ## 8 FL AirTran Airways Corporation ## 9 HA Hawaiian Airlines Inc. ## 10 MQ Envoy Air ## 11 OO SkyWest Airlines Inc. ## 12 UA United Air Lines Inc. ## 13 US US Airways Inc. ## 14 VX Virgin America ## 15 WN Southwest Airlines Co. ## 16 YV Mesa Airlines Inc. # after looking up the names, we know UA = United, AA = American, DL = Delta filter(flights, carrier %in% c(&quot;UA&quot;, &quot;AA&quot;, &quot;DL&quot;)) ## # A tibble: 139,504 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 554 600 -6 812 ## 5 2013 1 1 554 558 -4 740 ## 6 2013 1 1 558 600 -2 753 ## 7 2013 1 1 558 600 -2 924 ## 8 2013 1 1 558 600 -2 923 ## 9 2013 1 1 559 600 -1 941 ## 10 2013 1 1 559 600 -1 854 ## # ... with 139,494 more rows, and 12 more variables: ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1d. Find all flights that departed in summer (July, August, and September) filter(flights, month %in% c(7, 8, 9)) # Alternative Method filter(flights, between(month, 7, 9)) 1e. Find all flights that arrived more than two hours late, but didnt leave late filter(flights, arr_delay &gt; 120, dep_delay &lt;= 0) The next two exercises are trickier. 1f. Find all flights that were delayed by at least an hour, but made up over 30 minutes in flight. First, the flight was delayed by at least an hour is the same as dep_delay &gt;=60. Second, if a flight made up over 30 minutes in flight, the arrival delay must be at least 30 minutes less than the departure delay, which is the same as dep_delay - arr_delay &gt; 30. filter(flights, dep_delay &gt;= 60, dep_delay - arr_delay &gt; 30) 1g. Find all flights that departed between midnight and 6 a.m. (inclusive). The first question that should come to your mind is how is midnight represented in the dataset? Lets take a look at summary(flights$dep_time). summary(flights$dep_time) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1 907 1401 1349 1744 2400 8255 The minimum is 1 and the maximum is 2400. Therefore, you know midnight is represented by 2400 instead of 0 in this dataset. The answer to the question would be filter(flights, dep_time &lt;= 600 | dep_time == 2400) 7.4 select() Very often, we are only interested in some variables in a dataset. In that case, we can focus on the variables by creating a new dataset with those variables only. select() is to select the columns in a dataset by the name of the columns Selecting Variables Suppose you want to select the following \\(3\\) columns in flights: year, month, day: select(flights, year, month, day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows The usual way without using select() is flights[c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] # or flights[,c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows Select all columns between year and day (inclusive) select(flights, year:day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows Excluding Variables Select all columns except those from year to day (inclusive) select(flights, -(year:day)) select(flights, -year, -month, -day) Without using select() flights[,!(colnames(flights) %in% c(&quot;year&quot;, &quot;day&quot;, &quot;month&quot;))] 7.4.1 Exercises You can also use starts_with(\"abc\") matches names that begin with \"abc\" ends_with(\"xyz\") matches names that end with \"xyz\" contains(\"ijk\") mathces names that contain \"ijk\" Ex: select all columns that end with \"delay\". select(flights, ends_with(&quot;delay&quot;)) ## # A tibble: 336,776 x 2 ## dep_delay arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 ## 2 4 20 ## 3 2 33 ## 4 -1 -18 ## 5 -6 -25 ## 6 -4 12 ## 7 -5 19 ## 8 -3 -14 ## 9 -3 -8 ## 10 -2 8 ## # ... with 336,766 more rows Ex: select all columns that start with \"a\". select(flights, starts_with(&quot;a&quot;)) ## # A tibble: 336,776 x 3 ## arr_time arr_delay air_time ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 830 11 227 ## 2 850 20 227 ## 3 923 33 160 ## 4 1004 -18 183 ## 5 812 -25 116 ## 6 740 12 150 ## 7 913 19 158 ## 8 709 -14 53 ## 9 838 -8 140 ## 10 753 8 138 ## # ... with 336,766 more rows Does the result of running the following code surprise you? select(flights, contains(&quot;TIME&quot;)) ## # A tibble: 336,776 x 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 830 819 227 ## 2 533 529 850 830 227 ## 3 542 540 923 850 160 ## 4 544 545 1004 1022 183 ## 5 554 600 812 837 116 ## 6 554 558 740 728 150 ## 7 555 600 913 854 158 ## 8 557 600 709 723 53 ## 9 557 600 838 846 140 ## 10 558 600 753 745 138 ## # ... with 336,766 more rows, and 1 more variable: time_hour &lt;dttm&gt; Yes, because we used TIME but not time and still get the selected columns. If you check ?contains, you can see that the default is to ignore the case. To change the default: select(flights, contains(&quot;TIME&quot;, ignore.case = FALSE)) ## # A tibble: 336,776 x 0 Ex: without using select(), select all the columns that contain time. flights[grep(&quot;time&quot;, names(flights))] ## # A tibble: 336,776 x 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 830 819 227 ## 2 533 529 850 830 227 ## 3 542 540 923 850 160 ## 4 544 545 1004 1022 183 ## 5 554 600 812 837 116 ## 6 554 558 740 728 150 ## 7 555 600 913 854 158 ## 8 557 600 709 723 53 ## 9 557 600 838 846 140 ## 10 558 600 753 745 138 ## # ... with 336,766 more rows, and 1 more variable: time_hour &lt;dttm&gt; Basic Usage of grep: grep(pattern, x). pattern: character string. e.g., time, delay, air x: a character vector where matches are sought. e.g., the colnames of a dataframe. Output: a vector of the indicaes of the elements of x that yielded a match. some_names &lt;- c(&quot;ab&quot;, &quot;bc&quot;, &quot;cd&quot;) grep(&quot;b&quot;, some_names) ## [1] 1 2 grep(&quot;d&quot;, some_names) ## [1] 3 grep(&quot;e&quot;, some_names) ## integer(0) 7.5 mutate() Add new variables with mutate() Very often, we want to create a new variable based on existing variables. For example, if we have distance and time, we can compute the speed by distance/time. mutate() always adds new columns at the end of the dataset. Lets create a narrower dataset so that we can see the result of mutate without use View(). # create a smaller dataset flights_sml &lt;- select(flights, year:day, arr_delay, dep_delay, distance, air_time) Now, lets use mutate() to create a variable called gain (how much time we gain in flight) defined as arr_delay-dep_delay and a variable called speed (miles/hour) defined as distance/air_time. mutate(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 x 9 ## year month day arr_delay dep_delay distance air_time gain speed ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 11 2 1400 227 9 370. ## 2 2013 1 1 20 4 1416 227 16 374. ## 3 2013 1 1 33 2 1089 160 31 408. ## 4 2013 1 1 -18 -1 1576 183 -17 517. ## 5 2013 1 1 -25 -6 762 116 -19 394. ## 6 2013 1 1 12 -4 719 150 16 288. ## 7 2013 1 1 19 -5 1065 158 24 404. ## 8 2013 1 1 -14 -3 229 53 -11 259. ## 9 2013 1 1 -8 -3 944 140 -5 405. ## 10 2013 1 1 8 -2 733 138 10 319. ## # ... with 336,766 more rows To keep the new variables only, use transmute(): transmute(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 x 2 ## gain speed ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9 370. ## 2 16 374. ## 3 31 408. ## 4 -17 517. ## 5 -19 394. ## 6 16 288. ## 7 24 404. ## 8 -11 259. ## 9 -5 405. ## 10 10 319. ## # ... with 336,766 more rows Without using select and mutate(), one may use flights_sml2 &lt;- flights[c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;arr_delay&quot;, &quot;dep_delay&quot;, &quot;distance&quot;, &quot;air_time&quot;)] flights_sml2$gain &lt;- flights_sml2$arr_delay - flights_sml2$dep_delay flights_sml2$speed &lt;- flights_sml2$distance/flights_sml2$air_time*60 7.5.1 Exercises Ex: Currently, dep_time and sched_dep_time are convenient to look at, but hard to compute with because they are not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Recall that dep_time and sched_dep_time are in HHMM format. For example, 1304 means 1:04pm. The number of minutes since midnight is \\(13\\times 60 + 4 = 784\\). In general, we can use the integer division to find the number of hours since midnight, then multiply by \\(60\\), and finally add the remainder for the minutes. For example, 1304 %/% 100 # get the number of hours since midnight ## [1] 13 1304 %% 100 # get the remainder ## [1] 4 1304 %/% 100 * 60 + 1304 %% 100 # number of minutes since midnight ## [1] 784 Recall that midnight is represented as 2400 in the dataset and the number of minutes since midnight should be 0. However, if we use the above method for midnight, we get 2400 %/% 100 * 60 + 2400 %% 100 # this is not correct for midnight ## [1] 1440 Therefore, we also have to deal with this case. One possible solution is to do another integer division by 1440 (24x60 = 1440): (1304 %/% 100 * 60 + 1304 %% 100) %% 1440 # will not change the result if the time is not midnight ## [1] 784 (2400 %/% 100 * 60 + 2400 %% 100) %% 1440 # this is correct for midnight ## [1] 0 Go back to flights: # let&#39;s keep only the dep_time and sched_dep_time flights_time &lt;- select(flights, dep_time, sched_dep_time) mutate(flights_time, min_dep_time = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, min_sched_dep_time = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440) ## # A tibble: 336,776 x 4 ## dep_time sched_dep_time min_dep_time min_sched_dep_time ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 515 317 315 ## 2 533 529 333 329 ## 3 542 540 342 340 ## 4 544 545 344 345 ## 5 554 600 354 360 ## 6 554 558 354 358 ## 7 555 600 355 360 ## 8 557 600 357 360 ## 9 557 600 357 360 ## 10 558 600 358 360 ## # ... with 336,766 more rows The above code doesnt look good because we have written the formula to complete the same task twice. We can define a function to do this: time_to_minutes &lt;- function(x) { (x %/% 100 * 60 + x %% 100) %% 1440 } With the function time_to_minutes, we have: mutate(flights, min_dep_time = time_to_minutes(dep_time), min_sched_dep_time = time_to_minutes(sched_dep_time)) If you cannot think of using %%1440 to deal with the midnight cases, we can write: # ind for indicator approach time_to_minutes_ind &lt;- function(x) { (x %/% 100 * 60 + x %% 100) * (x != 2400) } # Explanation # if x is 2400, (x!=2400) is FALSE, FALSE times a number y is 0 # if x is not 2400, (x!=2400) is TRUE, TRUE times a number y is y # because &quot;FALSE=0, TRUE=1&quot; mutate(flights, min_dep_time = time_to_minutes_ind(dep_time), min_sched_dep_time = time_to_minutes_ind(sched_dep_time)) An alternative way is to use ifelse (which is a vectorized function). Usage of ifelse: ifelse(test, yes, no) time_to_minutes_ifelse &lt;- function(x) { ifelse(x != 2400, x %/% 100 * 60 + x %% 100, 0) } mutate(flights, min_dep_time = time_to_minutes_ifelse(dep_time), min_sched_dep_time = time_to_minutes_ifelse(sched_dep_time)) ## # A tibble: 336,776 x 21 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 336,766 more rows, and 14 more variables: ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ## # min_dep_time &lt;dbl&gt;, min_sched_dep_time &lt;dbl&gt; A less efficient way with for loop and if-else: # if for if-else approach time_to_minutes_if &lt;- function(x) { n &lt;- length(x) output &lt;- rep(0, n) for (i in 1:n) { if(is.na(x[i])){ # check for NA output[i] &lt;- NA } else if (x[i] != 2400) { # if not equal to 2400 output[i] &lt;- x[i] %/% 100 * 60 + x[i] %% 100 } else { # if equal to 2400 output[i] &lt;- 0 } } return(output) } mutate(flights, min_dep_time = time_to_minutes_if(dep_time), min_sched_dep_time = time_to_minutes_if(sched_dep_time)) Compare the efficiency: system.time(mutate(flights, min_dep_time = time_to_minutes(dep_time), min_sched_dep_time = time_to_minutes(sched_dep_time))) ## user system elapsed ## 0.06 0.00 0.06 system.time(mutate(flights, min_dep_time = time_to_minutes_if(dep_time), min_sched_dep_time = time_to_minutes_if(sched_dep_time))) ## user system elapsed ## 0.47 0.01 0.48 7.6 summarize() Average delay over the year (not useful): summarize(flights, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 x 1 ## delay ## &lt;dbl&gt; ## 1 12.6 Using summarize with group_by can result in more useful statistics. Use group_by. First argument is your dataset. Subsequent arguments indicate how you want to group the data. In summarize, the dataset becomes the dataset from group_by. E.g.: Average delay per date (more useful): by_day &lt;- group_by(flights, year, month, day) summarize(by_day, delay = mean(dep_delay, na.rm = TRUE)) ## `summarise()` regrouping output by &#39;year&#39;, &#39;month&#39; (override with `.groups` argument) ## # A tibble: 365 x 4 ## # Groups: year, month [12] ## year month day delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.5 ## 2 2013 1 2 13.9 ## 3 2013 1 3 11.0 ## 4 2013 1 4 8.95 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.55 ## 9 2013 1 9 2.28 ## 10 2013 1 10 2.84 ## # ... with 355 more rows E.g: Average delay per month: by_month &lt;- group_by(flights, year, month) summarize(by_month, delay = mean(dep_delay, na.rm = TRUE)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) ## # A tibble: 12 x 3 ## # Groups: year [1] ## year month delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 10.0 ## 2 2013 2 10.8 ## 3 2013 3 13.2 ## 4 2013 4 13.9 ## 5 2013 5 13.0 ## 6 2013 6 20.8 ## 7 2013 7 21.7 ## 8 2013 8 12.6 ## 9 2013 9 6.72 ## 10 2013 10 6.24 ## 11 2013 11 5.44 ## 12 2013 12 16.6 # we only have 1 year, not necessary to use year by_month &lt;- group_by(flights, month) summarize(by_month, delay = mean(dep_delay,na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 12 x 2 ## month delay ## &lt;int&gt; &lt;dbl&gt; ## 1 1 10.0 ## 2 2 10.8 ## 3 3 13.2 ## 4 4 13.9 ## 5 5 13.0 ## 6 6 20.8 ## 7 7 21.7 ## 8 8 12.6 ## 9 9 6.72 ## 10 10 6.24 ## 11 11 5.44 ## 12 12 16.6 Ex: Find the average weights of the cars grouped by the number of gears. mtcars_gear &lt;- group_by(mtcars, gear) summarize(mtcars_gear, avg_wt = mean(wt, na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 2 ## gear avg_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3.89 ## 2 4 2.62 ## 3 5 2.63 Ex: Find the sample standard deviation of the weights of the cars grouped by the number of gears. mtcars_gear &lt;- group_by(mtcars, gear) summarize(mtcars_gear, avg_wt = sd(wt, na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 2 ## gear avg_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 0.833 ## 2 4 0.633 ## 3 5 0.819 7.7 Summary Functions and operators learned in this chapter: arrange, filter, identical, %in%, tibble, ifelse, mutate, transmute, select, grep, group_by, summarize "],["data-visualization-with-ggplot2.html", "Chapter 8 Data Visualization with ggplot2 8.1 Combining Multiple Operations with Pipe %&gt;% 8.2 Bar charts 8.3 Line Graph 8.4 Scatter Plots 8.5 Summarizing Data Distributions 8.6 Saving your plots 8.7 Summary", " Chapter 8 Data Visualization with ggplot2 Main reference for this chapter: R graphics cookbook (https://r-graphics.org/) In Chapter 4, we learned how to create some basic plots with base R and the function ggplot (in the package ggplot2, which is also contained in the package tidyverse). In this chapter, we will study how to use ggplot for data visualization in more detail. We will use some of the datasets from the the package gcookbook. Therefore, we will install it now. install.packages(&quot;gcookbook&quot;) Load the packages gcookbook, tidyverse and nycflights13. library(gcookbook) # contains some datasets for illustration library(tidyverse) # contains ggplot2 and dplyr library(nycflights13) # contains the dataset &quot;flights&quot; 8.1 Combining Multiple Operations with Pipe %&gt;% You can use the shortcut Ctrl/Cmd + Shift + M to insert the pipe operator %&gt;%. x %&gt;% f(y) turns into f(x,y) For example, flights %&gt;% filter(month == 1, day == 1) is the same as filter(flights, month == 1, day == 1) x %&gt;% f(y) %&gt;% g(z) turns into g(f(x,y),z). For example, flights %&gt;% group_by(year, month) %&gt;% summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) is the same as summarize(group_by(flights, year, month), mean_dep_delay = mean(dep_delay, na.rm = TRUE)) Using the pipe can avoid creating and naming intermediate objects that we dont need. Instead, the pipe focuses on the sequence of actions, not the object that the actions being performed on. It also tends to make the code easier to read. For the above example, you can read it as: for the dataset flights, we first group the data by year and month, then summarize the data by finding the mean. We can think of the pipe %&gt;% as then. Pipe %&gt;% can also be used with ggplot: flights %&gt;% group_by(year, month) %&gt;% summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = month, y = mean_dep_delay)) + geom_col() # notice the spacing for good indentation Notice the difference between %&gt;% and + in the above code. Note: if you have to manipulate some intermediate objects, it may make sense not to use the pipe in that situation. 8.2 Bar charts We will start with bar charts. Many of the usages discussed in this section can also be transferable to create other plots. Recall that there are two types of bar charts: Bar chart of values. x-axis: discrete variable, y-axis: numeric data (not necessarily count data) Bar chart of counts. x-axis: discrete variable, y-axis: count of cases in the discrete variable Using ggplot: For bar chart of values, we use geom_col(), which is the same as using geom_bar(stat = \"identity\"). For bar chart of counts, we use geom_bar(), which is the same as using geom_bar(stat = \"count\"). That is, the default for geom_bar() is to use stat = \"count\". Bar chart of values: pg_mean is a simple dataset with groupwise means of some plant growth data. pg_mean ## group weight ## 1 ctrl 5.032 ## 2 trt1 4.661 ## 3 trt2 5.526 ggplot(data = pg_mean, mapping = aes(x = group, y = weight)) + geom_col() Recall the mtcars dataset. Lets create a bar chart of values for the mean weights grouped by the number of gears. First, we summarize the data using summarize. by_gear &lt;- group_by(mtcars, gear) mtcars_wt &lt;- summarize(by_gear, mean_wt_by_gear = mean(wt)) # Alternatively, using %&gt;% mtcars_wt &lt;- mtcars %&gt;% group_by(gear) %&gt;% summarize(mean_wt_by_gear = mean(wt)) Create the bar chart: ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col() To change the colour of the bars, use fill. ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(fill = &quot;lightblue&quot;) By default, there is no outline around the fill. To add an outline, use colour (or color). ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(color = &quot;red&quot;) Of course, you can combine the two settings: ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(fill = &quot;lightblue&quot;, color = &quot;red&quot;) Graph with grouped bars The most basic bar chart of values have one categorical variable on the x-axis and one continuous variable on the y-axis. If you want to include another categorical variable to divide up the data, you can use a graph with grouped bars. In mtcars, vs represents the engine of the car with 0 = V-shaped and 1 = straight. We can use vc to divide up the data in addition to gear using fill. To create a grouped bar chart, set position = \"dodge\" in geom_col(); otherwise, you will get a stacked bar chart. # prepare the data by_gear_vs &lt;- group_by(mtcars, gear, vs) mtcars_wt2 &lt;- summarize(by_gear_vs, mean_wt = mean(wt)) # convert to factor in the data mtcars_wt2$vs &lt;- as.factor(mtcars_wt2$vs) # using pipe %&gt;% to prepare the data mtcars_wt2 &lt;- mtcars %&gt;% group_by(gear, vs) %&gt;% summarize(mean_wt = mean(wt)) %&gt;% ungroup() %&gt;% mutate(vs = as.factor(vs)) # plot ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) Without position = \"dodge\", we get a stacked bar chart: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col() You can also convert vs to factor in call to ggplot(): # prepare the data by_gear_vs &lt;- group_by(mtcars, gear, vs) mtcars_wt3 &lt;- summarize(by_gear_vs, mean_wt = mean(wt)) # plot ggplot(mtcars_wt3, aes(x = gear, y = mean_wt, fill = factor(vs))) + geom_col(position = &quot;dodge&quot;) To change the colours of the bars: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Pastel2&quot;) You can try with different palettes: library(RColorBrewer) display.brewer.all() Using palette = \"Oranges\": ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Oranges&quot;) Using a manually defined palette: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_manual(values = c(&quot;#cc6666&quot;, &quot;#66cccc&quot;)) # also see Section 4.2 for the color Bar Charts of Counts Creating a bar chart of counts is very similar to creating a bar chart of values. Bar chart of the number of cars by gear in mtcars: ggplot(mtcars, aes(x = gear)) + geom_bar() Bar chart of the number of flights by each month in nycflights13: ggplot(flights, aes(x = month)) + geom_bar(fill = &quot;lightblue&quot;) Controlling the width (by default, width = 0.9): ggplot(flights, aes(x = month)) + geom_bar(fill = &quot;lightblue&quot;, width = 0.5) Bar chart of the number of flights by origin and month: # have to convert month to factor ggplot(flights, aes(x = origin, fill = factor(month))) + geom_bar(position = &quot;dodge&quot;, color = &quot;black&quot;) 8.3 Line Graph Suppose you want to make a line graph of the daily average departure delay in flights. From now on, we will use %&gt;% whenever it is appropriate. avg_delay &lt;- flights %&gt;% group_by(month, day) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(Time = 1:365) ggplot(avg_delay, aes(x = Time, y = delay)) + geom_line() Labeling the graph: # notice how we put each argument on its own line when the arguments # do not all fit on one line ggplot(avg_delay, aes(x=Time, y=delay)) + geom_line() + labs( y = &quot;Average Delay&quot;, title = &quot;Daily Average Departure Delay of Flights from NYC in 2013&quot; ) By default, the range of the y-axis of a line graph is just enough to include all the y values in the data. Sometimes, you may want to change the range manually. For example, the range of the y-axis in the following graph does not include 0. ggplot(BOD, aes(x = Time, y = demand)) + geom_line() If you want to include 0 in the y range, you can use ylim: ggplot(BOD, aes(x = Time, y = demand)) + geom_line() + ylim(0, max(BOD$demand)) Line Graph with multiple lines Suppose we want to create a line graph showing the daily average departure delay from the 3 airports in flights. # prepare the data flights_delay &lt;- flights %&gt;% group_by(month, origin) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() Line Graph: ggplot(flights_delay, aes(x = month, y = delay, color = origin)) + geom_line() With different line types: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin)) + geom_line() Add the points on top of the lines: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin)) + geom_line() + geom_point() Change the point shapes according to origin: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin, shape = origin)) + geom_line() + geom_point() To use one single shape for the points, we can specify the shape in geom_point(). The default shape is shape = 16. The default size is size = 2. fill is only applicable for shape = 21 to 25. ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin)) + geom_line() + geom_point(shape = 22, size = 3, fill = &quot;white&quot;, color = &quot;darkred&quot;) Using another colour palette and changing the size of the lines: ggplot(flights_delay, aes(x = month, y = delay, color = origin)) + geom_line(size = 2) + geom_point(shape = 22, size = 3, fill = &quot;white&quot;, color = &quot;darkred&quot;)+ scale_colour_brewer(palette = &quot;Set2&quot;) 8.4 Scatter Plots Scatter plots are often used to visualize the relationship between two continuous variables. It is also possible to use a scatter plot when either or both variables are discrete. The dataset heightweight contains sex, age, height and weight of some schoolchildren. head(heightweight) ## sex ageYear ageMonth heightIn weightLb ## 1 f 11.92 143 56.3 85.0 ## 2 f 12.92 155 62.3 105.0 ## 3 f 12.75 153 63.3 108.0 ## 4 f 13.42 161 59.0 92.0 ## 5 f 15.92 191 62.5 112.5 ## 6 f 14.25 171 62.5 112.0 To create a basic scatter plot, use geom_point(): ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point() You can control the shape, size, and color of the points as illustrated in the last section. ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point(size = 1.5, shape = 4, color = &quot;blue&quot;) If shape = 21-25, you can control the color in the points and outline of the points using fill and color, respectively. ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point(size = 1.5, shape = 22, fill = &quot;red&quot;, color = &quot;blue&quot;) Visualizing an additional discrete variable Suppose you want to use different colours for the points according to different categories of sex. ggplot(heightweight, aes(x = ageYear, y = heightIn, color = sex)) + geom_point() Suppose you want to use different shapes for the points according to different categories of sex. ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex)) + geom_point() You can use colours and shapes at the same time: ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, color = sex)) + geom_point() You can change the shapes or colours manually: ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, color = sex)) + geom_point() + scale_shape_manual(values = c(21,22)) + scale_colour_brewer(palette = &quot;Set2&quot;) Visualizing an additional continuous variable You may map an additional continuous variable to color. ggplot(heightweight, aes(x = ageYear, y = heightIn, color = weightLb)) + geom_point() Visualizing two additional discrete variables Lets create a new column to indicate if the child weights &lt; 100 or &gt;= 100 pounds (this is a discrete variable). heightweight2 &lt;- heightweight %&gt;% mutate(weightgroup = ifelse(weightLb &lt; 100, &quot;&lt; 100&quot;, &quot;&gt;= 100&quot;)) Now, we can add both sex and weightgroup in the plot in the following way: ggplot(heightweight2, aes(x = ageYear, y = heightIn, shape = sex, fill = weightgroup)) + geom_point() + scale_shape_manual(values = c(21, 24)) + scale_fill_manual( values = c(&quot;red&quot;, &quot;black&quot;), guide = guide_legend(override.aes = list(shape = 21)) # to change the legend ) Changing the mark ticks, limits and labels of the x-axis and y-axis: ggplot(heightweight2, aes(x = ageYear, y = heightIn, shape = sex, fill = weightgroup)) + geom_point() + scale_shape_manual(values = c(21, 24)) + scale_fill_manual( values = c(&quot;red&quot;, &quot;black&quot;), guide = guide_legend(override.aes = list(shape = 21)) # to change the legend ) + scale_x_continuous(name = &quot;Age (Year)&quot;, breaks = 11:18, limits = c(11, 18)) + scale_y_continuous(name = &quot;Height (In)&quot;, breaks = seq(50, 70, 5), limits = c(50, 73)) 8.4.1 Overplotting Overplotting refers to the situation when you have a large dataset so that the points in a scatter plot overlap and obscure each other. # We can create a variable to store the &quot;ggplot&quot; diamonds_ggplot &lt;- ggplot(diamonds, aes(x = carat, y = price)) diamonds_ggplot + geom_point() Possible solutions for overplotting: Use smaller points (size) # with diamonds_ggplot, we do not have to type # ggplot(diamonds, aes(x = carat, y = price)) diamonds_ggplot + geom_point(size = 0.1) Make the points semitransparent (alpha) diamonds_ggplot + geom_point(alpha = 0.05, size = 0.1) # 0.05 = 95% transparent We can see some vertical bands at some values of carats, meaning that diamonds tend to be cut to those sizes. Bin the data into rectangles (stat_bin2d) bins controls the number of bins in the x and y directions. The color of the rectangle indicates how many data points there are in the region. # by default, bins = 30 diamonds_ggplot + stat_bin2d(bins = 3) With bins = 50: diamonds_ggplot + stat_bin2d(bins = 50) + scale_fill_gradient(low = &quot;lightblue&quot;, high = &quot;red&quot;) Overplotting can also occur when the data is discrete on one or both axes. In the following example, we use the dataset ChickWeight, where Time is a discrete variable. head(ChickWeight) ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 # create a base plot cw_ggplot &lt;- ggplot(ChickWeight, aes(x = Time, y = weight)) cw_ggplot + geom_point() You may randomly jitter the points: cw_ggplot + geom_point(position = &quot;jitter&quot;) Jittering the points means a small amount of random variation is added to the location of each point. If you only want to jitter in the x-direction: cw_ggplot + geom_point(position = position_jitter(width = 0.5, height = 0)) 8.4.2 Labelling points in a scatter plot We can use annotate() or geom_text_repel() to label points in a scatter plot. For the latter, we have to install the package ggrepel. We will use the countries dataset in the package gcookbook and visualize the relationship between health expenditures and infant mortality rate. We will consider a subset of data by focusing the data from 2009 and countries with more than \\(2,000\\) USD health expenditures per capita: countries_subset &lt;- countries %&gt;% filter(Year == 2009, healthexp &gt; 2000) Using annotate: # find out the x and y coordinates for the point corresponding to Canada canada_x &lt;- filter(countries_subset, Name == &quot;Canada&quot;)$healthexp canada_y &lt;- filter(countries_subset, Name == &quot;Canada&quot;)$infmortality ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + annotate(&quot;text&quot;, x = canada_x, y = canada_y + 0.2, label = &quot;Canada&quot;) # + 0.2 is to avoid the label placing on top of the point Label all the points with geom_text_repel: # to use geom_text_repel, load the package ggrepel library(ggrepel) ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + geom_text_repel(aes(label = Name), size = 3) Label all the points with geom_label_repel (with a box around the label): # geom_label_repel also depends on the package ggrepel ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + geom_label_repel(aes(label = Name), size = 3) 8.5 Summarizing Data Distributions 8.5.1 Histogram Histogram can be used to visualize the distribution of a variable. We will illustrate how to create histograms using the dataset birthwt from the package MASS. library(MASS) birthwt contains data of 189 birth weights with some covariates of the mothers. Take a look at the dataset: head(birthwt) ## low age lwt race smoke ptl ht ui ftv bwt ## 85 0 19 182 2 0 0 0 1 0 2523 ## 86 0 33 155 3 0 0 0 0 3 2551 ## 87 0 20 105 1 1 0 0 0 1 2557 ## 88 0 21 108 1 1 0 0 1 2 2594 ## 89 0 18 107 1 1 0 0 1 0 2600 ## 91 0 21 124 3 0 0 0 0 0 2622 Basic histogram: ggplot(birthwt, aes(x=bwt)) + geom_histogram() Plot a histogram with density (not frequency): ggplot(birthwt, aes(x=bwt)) + geom_histogram(aes(y = ..density..)) To compare two histograms Use facet_grid() to display two histograms in the same plot. Suppose we group the data according to the smoking status during pregnancy and we want to display the two histograms of the birth weight: ggplot(birthwt, aes(x = bwt)) + geom_histogram() + facet_grid(smoke ~ .) To change the label, we can change the content of the variable: # create another dataset birthwt_mod &lt;- birthwt birthwt_mod$smoke &lt;- ifelse(birthwt_mod$smoke == 1, &quot;Smoke&quot;, &quot;No Smoke&quot;) ggplot(birthwt_mod, aes(x = bwt)) + geom_histogram() + facet_grid(smoke ~ .) Alternatively, we can use recode_factor: birthwt_mod$smoke &lt;- recode_factor(birthwt_mod$smoke, &quot;0&quot; = &quot;No Smoke&quot;, &quot;1&quot; = &quot;Smoke&quot;) Use fill() to put two groups in the same plot with different colors. We need to set position = \"identity\"; otherwise, the bars will be stacked on top of each other vertically which is not what we want. ggplot(birthwt_mod, aes(x=bwt, fill=smoke)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) #+ # facet_grid(race~., scales=&quot;free&quot;) It is also possible to use both facet_grid and fill when we have want to group the data with two discrete variables. We will illustrate this with grouping according to the smoking status and the race. We also add scales = \"free\" so that the ranges of the y-axes will be adjusted according to the data in each histogram. # change the name so that the labels can be understood easily birthwt_mod$race[which(birthwt_mod$race==1)] = &quot;White&quot; birthwt_mod$race[which(birthwt_mod$race==2)] = &quot;Black&quot; birthwt_mod$race[which(birthwt_mod$race==3)] = &quot;Other&quot; ggplot(birthwt_mod, aes(x = bwt, fill = smoke)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) + facet_grid(race ~ ., scales = &quot;free&quot;) Note: we do not have a large dataset in this example so that grouping by two variables may not give us a very good understanding of the data. 8.5.2 Kernel Density Estimate Kernel density estimation is a nonparametric method to estimate the density of the samples. Nonparametric method means we do not impose a parametric model. A parametric model has a finite dimensional parameter \\(\\theta \\in \\mathbb{R}^d\\) for some finite \\(d\\). Let \\(X_1,\\ldots,X_n\\) be i.i.d. random variables from some distribution with density \\(f\\). The histogram for \\(f\\) at point \\(x_0\\) is \\[\\begin{equation*} \\hat{f}(x_0) = \\frac{\\text{number of $x_i$ in the bin containing $x_0$}}{n h}, \\end{equation*}\\] where the bin width is \\(h\\). As we already know, the histogram will not give a smooth estimate of the density. One may use another method called kernel density estimator, which could produce smooth estimate of the density. The kernel density estimator is \\[\\begin{equation*} \\hat{f}_n(x_0) = \\frac{1}{nh}\\sum^n_{i=1} K \\bigg( \\frac{x_0 - x_i}{h} \\bigg), \\end{equation*}\\] where \\(K\\) is a kernel and \\(h\\) is the bandwidth. For our purposes, a kernel is a non-negative symmetric function such that \\(\\int^\\infty_{-\\infty}K(x)dx = 1\\) and \\(\\int^\\infty_{-\\infty} x K(x)dx =0\\). For example, \\[\\begin{eqnarray*} \\text{the boxcar kernel:} &amp;&amp; K(x) = \\frac{1}{2}I(|x| \\leq 1)\\\\ \\text{the Gaussian kernel:} &amp;&amp; K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\\\ \\text{the Epanechnikov kernel:} &amp;&amp; K(x) = \\frac{3}{4}(1-x^2)I(|x| \\leq 1) \\\\ \\text{the tricube kernel:} &amp;&amp; K(x) = \\frac{70}{81}(1-|x|^3)^3I(|x| \\leq 1), \\end{eqnarray*}\\] where \\(I(|x| \\leq 1) = 1\\) if \\(|x| \\leq 1\\) and equals \\(0\\) otherwise. Since the kernel is symmetric around \\(0\\), the magnitude \\((x-x_i)/h\\) is the distance from \\(0\\). For the above kernels, the value of the kernels is smaller when we evaluate at a point further from \\(0\\). Therefore, data close to \\(x_0\\) will contribute larger weights in estimating \\(\\hat{f}(x_0)\\). The bandwidth will control the smoothness of the estimate: larger bandwidth will result in a smoother curve and smaller bandwidth will result in a noisy and rough curve. We can create a kernel density estimate of the distribution using geom_density(). ggplot(birthwt, aes(x = bwt)) + geom_density() + geom_density(adjust = 0.25, color = &quot;red&quot;) + # smaller bandwidth -&gt; noisy geom_density(adjust = 2, color = &quot;blue&quot;) # large bandwidth -&gt; smoother Overlaying a density curve with a histogram ggplot(birthwt, aes(x = bwt)) + geom_histogram(fill = &quot;cornsilk&quot;, aes(y = ..density..)) + geom_density() Displaying kernel density Estimates from grouped data To use geom_density() to display kernel density estimates from grouped data, the grouping variable must be a factor or a character vector. Recall that in birthwt_mod that we created earlier, the smoke variable is a character vector. With color: ggplot(birthwt_mod, aes(x = bwt, color = smoke)) + geom_density() With fill: ggplot(birthwt_mod, aes(x = bwt, fill = smoke)) + geom_density(alpha = 0.3) # to control the transparency With facet_grid(): ggplot(birthwt_mod, aes(x = bwt)) + geom_density() + facet_grid(smoke ~ .) 8.6 Saving your plots There are two types of image files: vector and raster (bitmap) Raster images are pixel-based. When you zoom in the image, you can see the individual pixels. Two examples are JPG and PNG files. JPG files quality is lower than that of the PNG files. Vector images are constructed using mathematical formulas. You can resize the image without a loss in image quality. When you zoom in the image, it is still smooth and clear. Two examples are AI and PDF files. 8.6.1 Outputting to pdf vector files Suppose you want to save the plot from the following code: ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() # first argument is the file name # width and height are in inches pdf(&quot;filename.pdf&quot;, width = 4, height = 4) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() dev.off() Outputting to a pdf file: usually the best option usually smaller than bitmap files such as PNG files. when you have overplotting (many points on the plot), a PDF file can be much larger than a PNG file. 8.6.2 Outputting to bitmap files # width and heights are in pixels png(&quot;png_plot.png&quot;, width = 600, height = 600) ggplot(mtcars, aes(x=wt,y=mpg)) + geom_point() dev.off() For high-quality print output, it is recommended to use at least 300 ppi (ppi = pixels per inch). Suppose you want to create a 4x4-inch PNG file with 300 ppi: ppi &lt;- 300 png(&quot;png_plot.png&quot;, width = 4*ppi, height = 4*ppi, res = ppi) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() dev.off() 8.7 Summary 8.7.1 Combining multiple operations with pipe %&gt;% 8.7.2 Bar charts examples of using pipe %&gt;% together with ggplot create bar charts of counts create bar charts of values change fill and outline of the bars create grouped bar charts create stacked bar charts convert a variable into factor in ggplot use different colour palette control the width of the bars 8.7.3 Line graphs create line graphs label the graph change the range of y-axis create line graphs with multiple lines use multiple geoms (geometric objects) (e.g. additing the points on top of the lines) change shape, size, fill, outline of points change line type 8.7.4 Scatter plot create scatter plots visualize an additional discrete variable visualize an additional continuous variable visualize two additional discrete variables overplotting (use smaller points, make points semitransparent, bin data into rectangels, jitter the points) label points in a scatter plot 8.7.5 Summarizing data distributions create histograms (frequency and density) compare two histograms (facet_grid(), fill()) create histograms with two additional discrete variables create kernel density estimates overlay a density curve with a histogram display kernel density estimates from grouped data (color, fill, facet_grid) 8.7.6 Saving your plots output to pdf vector files output to bitmap files "],["statistical-inference-in-r.html", "Chapter 9 Statistical Inference in R 9.1 Maximum Likelihood Estimation 9.2 Interval Estimation and Hypothesis Testing", " Chapter 9 Statistical Inference in R In this chapter, we discuss how to perform some estimations and hypothesis testings in R. You may have learned their theory in previous statistics courses. I do not intend to give a very comprehensive review to these methods due to time constraint. Optional Readings: You can find a few more statistical tests in Ch 9 of R Cookbook (https://rc2e.com/) You can review Ch 10-13 of John E. Freunds Mathematical Statistics with Applications by Irwin Miller and Marylees Miller (textbook for STAT 269) for some background and theory on statistical inference 9.1 Maximum Likelihood Estimation After you collect some data and formulate a statistical model, you have to estimate the parameters in your model. One of the most common methods is to use maximum likelihood estimation. Very often, there is no closed-form expression for your estimators. In general, suppose you have data \\(y_1,\\ldots,y_n\\). The likelihood function is a function of the parameter defined as \\[\\begin{equation*} L(\\theta|y_1,\\ldots,y_n) = f_{y_1,\\ldots,y_n}(y_1,\\ldots,y_n|\\theta), \\end{equation*}\\] where \\(f_{y_1,\\ldots,y_n}(\\cdot |\\theta)\\) is the joint pmf or pdf of \\(y_1,\\ldots,y_n\\) with parameter \\(\\theta\\). That is, the likelihood function evaluated at \\(\\theta\\) is simply the joint probability of observing \\(y_1,\\ldots,y_n\\) when the parameter value is \\(\\theta\\). Assuming \\(y_1,\\ldots,y_n\\) are i.i.d. with density \\(f(\\cdot|\\theta)\\), we have \\[\\begin{equation*} L(\\theta|y_1,\\ldots,y_n) = f_{y_1,\\ldots,y_n}(y_1,\\ldots,y_n|\\theta) = \\prod^n_{i=1} f(y_i|\\theta). \\end{equation*}\\] In maximum likelihood estimation, we estimate the parameter \\(\\theta\\) by maximizing \\(L\\). The maximizer is called the maximum likelihood estimator (MLE). Some theory Why do we want to maximize the likelihood? Informally, the likelihood is the chance of observing the data. Therefore, we want to find the parameters so that such a chance is maximized. MLE has good statistical properties. Under some regularity conditions, MLE is consistent: \\(\\hat{\\theta}_n\\) converges in probability to \\(\\theta_0\\) Asymptotically efficient: the estimator has the lowest variance asymptotically in some sense Asymptotically normality: can be used to find confidence intervals and perform hypothesis testings Example (Logistic Regression): Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) is a binary variable and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In logistic regression we assume that \\[\\begin{equation*} P(Y_i = 1|x_i, \\beta) = \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}} = \\frac{ e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}. \\end{equation*}\\] Since \\(Y_i\\) takes only two values, \\[\\begin{equation*} P(Y_i = 0|x_i, \\beta) = \\frac{1}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] We can use one single formula for \\(y = 0, 1\\): \\[\\begin{equation*} P(Y_i = y|x_i, \\beta) = \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The likelihood function (conditional on x) is \\[\\begin{equation*} L(\\beta|y_1,\\ldots,y_n, x_1,\\ldots,x_n) = \\prod^n_{i=1} P(Y_i = y_i|x_i, \\beta) = \\prod^n_{i=1} \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The MLE of \\(\\beta\\) is obtained by maximizing \\(L(\\beta|y,x)\\) with respect to \\(\\beta\\). We usually maximize the natural logarithm of the likelihood function instead of the likelihood function, which is easier. The log likelihood function is \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( (x^T_i \\beta) y_i - \\log(1+e^{x^T_i \\beta}) \\bigg). \\end{equation*}\\] In this case, there is no closed-form formula for finding the maximizer. Nevertheless, we can use numerical methods to find out the maximizer. Of course, there are existing functions to perform this task in R. However, we will illustrate how to perform an optimization using the function optim(). By default, optim() will find the minimum. Therefore, we will minimize the negative of the log likelihood function. Simulated Example This is also a good time to introduce how to perform simulation based on a model and check the validity of the estimation method and algorithm. We will first generate some covariates and binary variables based on the logistic regression model. # Setting set.seed(362) n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) Now, we will write a function to compute the negative of the log likelihood function (as a function of the parameter \\(\\beta\\) and the data \\(\\{y_i, x_{i1}, x_{i2}: i=1,\\ldots,n\\}\\)), which is the objective function to be minimized. neg_log_like &lt;- function(beta, y, x1, x2) { beta_X &lt;- beta[1] + beta[2] * x1 + beta[3] * x2 log_like &lt;- sum(beta_X * y) - sum(log(1 + exp(beta_X))) -log_like # return the negative log likelihood } After defining our objective function, we can now use optim() to perform the optimization. optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;) ## $par ## [1] 0.7830734 1.0067301 -1.4920231 ## ## $value ## [1] 632.7284 ## ## $counts ## function gradient ## 28 7 ## ## $convergence ## [1] 0 ## ## $message ## NULL par is the initial values for the optimization. We set some random numbers for the initial values by par = runif(3, 0, 1). Because the function neg_log_like have multiple arguments, we have to supply them inside optim(). method = \"BFGS\" is a quasi-Newton method. method = \"L-BFGS-B\" is also useful when you want to add box constraints to your variable. You can check ?optim to learn more about this. Output: par is the parameter values at which the minimum is obtained value is the minimum function value convergence = 0 indicates successful completion Compare with the built-in function glm() for estimating the parameters: # will discuss this in more detail later fit &lt;- glm(y ~ x1 + x2, family = &quot;binomial&quot;) fit ## ## Call: glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;) ## ## Coefficients: ## (Intercept) x1 x2 ## 0.7831 1.0067 -1.4920 ## ## Degrees of Freedom: 999 Total (i.e. Null); 997 Residual ## Null Deviance: 1324 ## Residual Deviance: 1265 AIC: 1271 You can see that both methods give the same estimates 0.783, 1.007, -1.492 for the regression coefficients. How do you know your method of estimation makes sense? How do you know if you simulate the data correctly? In the above example, the true parameters are 0.5, 1, -1. The estimates are not really that close to the true values. We do not know if the estimation method will give a good result in general. To tackle this problem, we could simulate many datasets, perform the estimation, and take a look at the distributions of the estimates. Perform the simulation and estimation \\(250\\) times: # Setting n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta no_iter &lt;- 250 beta_est &lt;- matrix(0, nrow = no_iter, ncol = length(beta_0)) for (i in 1:no_iter) { # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Estimation beta_est[i, ] &lt;- optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;)$par } Displaying the results: library(tidyverse) # create the dataframe for plotting data &lt;- tibble(est = c(beta_est[, 1], beta_est[, 2], beta_est[, 3]), beta = c(rep(&quot;Beta1&quot;, no_iter), rep(&quot;Beta2&quot;, no_iter), rep(&quot;Beta3&quot;, no_iter))) # dataframe for adding the vertical lines for the true parameters vline_data &lt;- tibble(beta = c(&quot;Beta1&quot;, &quot;Beta2&quot;, &quot;Beta3&quot;), mean = beta_0) ggplot(data = data, mapping = aes(x = est)) + geom_histogram(fill = &quot;lightblue&quot;) + facet_grid(~ beta, scales = &quot;free&quot;) + geom_vline(data = vline_data, aes(xintercept = mean), color = &quot;blue&quot;) From the above plots, you can see the distributions of your estimators. The true parameters lie in the middle of the distributions. You can also add lines to visualize the mean of the distributions. In this case, the lines actually overlap with lines for the true parameters. Thus, the estimators are essentially unbiased. You can also see that there are times that \\(\\hat{\\beta}_0\\) can be as large as \\(1\\) or as small as \\(0.1\\) while the true value is \\(0.5\\). You can also use a larger sample size. # setting set.seed(362) n &lt;- 100000 beta_0 &lt;- c(0.5, 1, -1) # true beta # simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Estimation (est &lt;- optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;)$par) ## [1] 0.4803615 1.0275395 -0.9871783 The estimates are now 0.48, 1.03, -0.99, which are close to the true values 0.5, 1, -1. We will see some applications of the logistic regression later. 9.1.1 Exercises on MLE Exercise 1 (Gamma distribution) You observe a random sample \\(y_1,\\ldots,y_n\\) from a Gamma distribution with unknown parameters \\(\\alpha, \\beta\\). The likelihood function is \\[\\begin{equation*} L(\\alpha, \\beta |y_1,\\ldots,y_n) = \\prod^n_{i=1} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{\\alpha-1}_i e^{-\\beta y_i}. \\end{equation*}\\] The log likelihood, after simpliciation, is \\[\\begin{equation*} \\log L(\\alpha, \\beta) = n \\alpha \\log \\beta - n \\log \\Gamma(\\alpha) + (\\alpha - 1) \\sum^n_{i=1} \\log y_i - \\beta \\sum^n_{i=1} y_i. \\end{equation*}\\] # Setting set.seed(1) alpha &lt;- 1.5 beta &lt;- 2 n &lt;- 10000 # Simulation x &lt;- rgamma(n, alpha ,beta) # Optimization (Estimation) # Assignment 4 Exercise 2 (Poisson Regression) Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) only takes nonnegative integer values (count data) and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In Poisson regression, we assume that \\(Y_i\\) has a Poisson distribution and \\[\\begin{equation*} \\log (E(Y_i|x_i)) = \\beta^T x_i = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}, \\end{equation*}\\] where \\(\\beta \\in \\mathbb{R}^{p+1}\\). Alternatively, condition on \\(x_i\\), \\(Y_i|x_i \\sim \\text{Pois}(e^{\\beta^T x_i})\\). The likelihood is \\[\\begin{equation*} L(\\beta|y,x) = \\prod^n_{i=1} \\frac{e^{-e^{\\beta^T x_i}} e^{(\\beta^T x_i)y_i}}{y_i!}. \\end{equation*}\\] The log likelihood is \\[\\begin{equation*} \\log L(\\beta|y, x) = \\sum^n_{i=1} \\bigg( -e^{\\beta^T x_i} + (\\beta^T x_i) y_i - \\log (y_i!) \\bigg). \\end{equation*}\\] Clearly, the term \\(\\log (y_i !)\\) does not depend on \\(\\beta\\) and hence we do not need to consider it during our optimization. Therefore, it suffices to maximize (or minimize the negative of) the following objective function \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( -e^{\\beta^T x_i} + (\\beta^T x_i) y_i \\bigg). \\end{equation*}\\] # Setting set.seed(1) beta &lt;- c(0.3, 0.5, -0.5) n &lt;- 10000 p &lt;- length(beta) - 1 # Simulation X &lt;- cbind(1, matrix(runif(n * p), nrow = n, ncol = p)) y &lt;- rpois(n, exp(X %*% beta)) # Optimization (Estimation) # Assignment 4 9.1.2 Summary Review of the MLE Use optim() to minimize an objective function Simplify the objective function before you try to minimize it (take log and remove terms that do not depend on the parameters) how to simulate from a model how to check the validity of the simulation results 9.2 Interval Estimation and Hypothesis Testing Two types of estimation: point estimation (e.g. MLE) and interval estimation (e.g. confidence interval). 9.2.1 Examples of Hypothesis Testing Optional reading: Chapter 12 in John E. Freunds Mathematical Statistics with Applications. You have a die and you wonder if it is unbiased. If the die is unbiased, the (population) mean of the result from rolling the die is \\(3.5\\). Suppose you roll the die \\(10\\) times, the sample mean is \\(5\\). What is your decision on determining if the die is biased or not? How confident is your decision? What if you roll the die \\(100\\) times, and the sample mean is \\(5\\)? What is your decision now? Are you more confident in your decision? What if your roll the die \\(10\\) times but the sample mean is \\(4\\)? To answer these questions, we need to understand interval estimation and hypothesis testing. Some more examples: An engineer has to decide on the basis of sample data whether the true average lifetime of a certain kind of tire is at least \\(42,000\\) miles An agronomist has to decide on the basis of experiments whether one kind of fertilizer produces a higher yield of soybeans than another A manufacturer of pharmaceutical products has to decide on the basis of samples whether 90 percent of all patients given a new medication will recover from a certain disease These problems can all be translated into the language of statistical tests of hypotheses. the engineer has to test the hypothesis that \\(\\theta\\), the parameter of an exponential population, is at least \\(42,000\\) the agronomist has to decide whether \\(\\mu_1 &gt; \\mu_2\\), where \\(\\mu_1\\) and \\(\\mu_2\\) are the means of two normal populations the manufacturer has to decide whether \\(\\theta\\), the parameter of a binomial population, equals \\(0.90\\) In each case it must be assumed, of course, that the chosen distribution correctly describes the experimental conditions; that is, the distribution provides the correct statistical model. 9.2.2 Null Hypotheses, Alternative Hypotheses, and p-values An assertion or conjecture about the distribution of one or more random variables is called a statistical hypothesis. If a statistical hypothesis completely specifies the distribution, it is called a simple hypothesis; if not, it is referred to as a composite hypothesis. Null hypothesis In view of the assumptions of no difference, hypotheses such as these led to the term null hypothesis, but nowadays this term is applied to any hypothesis that we may want to test. p-value: the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct Steps in Hypothesis Testing Assume the null hypothesis is true Calculate a test statistic. E.g., sample mean Calculate a \\(p\\)-value (denoted by \\(p\\)) from the statistic and its distribution. For example, if the die is unbiased, what is the probability that we observe the sample mean to be larger than \\(5\\) after rolling it \\(100\\) times? small \\(p\\)-value small: we have strong evidence to reject the null hypothesis because it is unlikely to observe such a test statistic if the null hypothesis is true large \\(p\\)-value small: we do not have enough evidence to reject the null hypothesis Remark In this course, we will follow the common convention to reject the null hypothesis when \\(p &lt; 0.05\\) In real applications, how small is small depends on the problems. Being statistically significant (small \\(p\\)-value) does not mean the difference between the null and alternative hypotheses is large. One should also look at the confidence intervals or distributions of your estimates. 9.2.3 Type I error and Type II error Type I error: reject the null hypothesis when it is true. The probability of committing a type I error is denoted by \\(\\alpha\\) (level of significance of the test). Type II error: do not reject the null hypothesis when it is false. The probability of committing a type II error is denoted by \\(\\beta\\). \\(H_0\\) is true \\(H_1\\) is true Reject \\(H_0\\) Type I error No Error Do not reject \\(H_0\\) No Error Type II Error A good test procedure is one in which both \\(\\alpha\\) and \\(\\beta\\) are small, thereby giving us a good chance of making the correct decision. When the sample size \\(n\\) is held fixed, reducing \\(\\alpha\\) by changing the rejection region will increase \\(\\beta\\) and vice versa. The only way in which we can reduce the probabilities of both types of errors is to increase \\(n\\). As long as \\(n\\) is held fixed, this inverse relationship between the probabilities of type I and type II errors is typical of statistical decision procedures. Usually, we control \\(\\alpha\\) to be small (e.g. \\(0.05\\)). 9.2.4 Inference for Mean of One Sample Hypothesis Testing Problem You have a random sample \\(X_1,\\ldots,X_n\\) from a population. You want to know if the population mean \\(\\mu\\) is equal to \\(\\mu_0\\). That is, \\(H_0 : \\mu =\\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\). Solution You can use the \\(t\\)-test for this problem. It is appropriate when either Your data is normally distributed You have a large sample size \\(n\\). A rule of thumb is \\(n &gt; 30\\). The test statistic is \\[\\begin{equation*} \\frac{\\overline{X}_n - \\mu_0}{s/\\sqrt{n}}, \\end{equation*}\\] where \\(\\overline{X}_n\\) is the sample mean and \\(s\\) is the sample standard deviation. In R, use t.test() to perform the t-test. # Simulate the data set.seed(362) # so that you can replicate the result x &lt;- rnorm(75, mean = 100, sd = 15) # Perform t-test t.test(x, mu = 95) ## ## One Sample t-test ## ## data: x ## t = 4.7246, df = 74, p-value = 1.071e-05 ## alternative hypothesis: true mean is not equal to 95 ## 95 percent confidence interval: ## 99.36832 105.74018 ## sample estimates: ## mean of x ## 102.5543 The \\(p\\)-value is small, so it is unlikely that the mean of the population is \\(95\\). The \\(p\\)-value in this case is \\(2 \\times P_{H_0}(T &gt; |\\text{obs. T.S.|})\\), where \\(T\\) has a \\(t\\)-distribution with degrees of freedom \\(n-1\\) if the data from are from a normal distribution and obs. T.S. stands for the observed test statistic. The subscript \\(H_0\\) is to stress that the probability measure is under \\(H_0\\). [Optional] How are the test statistic and \\(p\\)-value calculated? # test statistic (obs_ts &lt;- (mean(x) - 95) / (sd(x) / sqrt(length(x)))) ## [1] 4.724574 # p-value 2 * (1 - pt(abs(obs_ts), df = length(x) - 1)) ## [1] 1.071253e-05 What do you expect when \\(x\\) is from a distribution with a much larger SD? set.seed(362) # so that you can replicate the result x2 &lt;- rnorm(75, mean = 100, sd = 200) (test_x2 &lt;- t.test(x2, mu = 95)) ## ## One Sample t-test ## ## data: x2 ## t = 1.832, df = 74, p-value = 0.07097 ## alternative hypothesis: true mean is not equal to 95 ## 95 percent confidence interval: ## 91.57758 176.53580 ## sample estimates: ## mean of x ## 134.0567 Even if the estimate of the population mean is 134, the test does not reject the null hypothesis that the mean is \\(95\\). This is because the sample has a very large variance. Interval Estimation The \\(100(1-\\alpha)\\%\\) confidence interval of \\(\\mu\\) is given by \\[\\begin{equation*} \\bigg[ \\overline{X}_n - t_{n-1; \\alpha/2} \\frac{s}{\\sqrt{n}}, \\overline{X}_n + t_{n-1; \\alpha/2} \\frac{s}{\\sqrt{n}} \\bigg], \\end{equation*}\\] where \\(P(t(n-1) &gt; t_{n-1;\\alpha/2}) = \\alpha/2\\) and \\(t(n-1)\\) is a \\(t\\)-distributed random variable with degrees of freedom \\(n-1\\). To find the confidence interval of \\(\\mu\\) in R, use t.test(). For example, the \\(99\\%\\) confidence interval of \\(\\mu\\) is t.test(x, conf.level = 0.99) ## ## One Sample t-test ## ## data: x ## t = 64.139, df = 74, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 99 percent confidence interval: ## 98.32683 106.78168 ## sample estimates: ## mean of x ## 102.5543 Remark By omitting mu = 0.95, the default value is mu = 0. Since we are interested in finding the confidence intervals, we do not need to care about the value of \\(\\mu_0\\). [Optional] Without using t.test(): # just an illustration of how CI can be computed alpha &lt;- 0.01 n &lt;- length(x) half_width &lt;- qt(1 - alpha / 2, n - 1) * sd(x) / sqrt(n) c(mean(x) - half_width, mean(x) + half_width) ## [1] 98.32683 106.78168 Interpretation If you can repeat the experiment many times, then about \\(95\\%\\) of the confidence intervals computed in those many times will contain the true mean. no_sim &lt;- 10000 set.seed(362) # so that you can replicate the result true_mean &lt;- 100 CI &lt;- matrix(0, nrow = no_sim, ncol = 2) for (i in 1:no_sim) { CI[i, ] &lt;- t.test(rnorm(75, mean = true_mean, sd = 200))$conf.int } # find out the proportion of CIs that contain 0 mean(CI[, 1] &lt; 100 &amp; 100 &lt; CI[, 2]) ## [1] 0.9503 Example Recall that we talked about how to use simulation to estimate \\(P(X &gt; Y)\\), where \\(X\\) and \\(Y\\) are some random variables. For example, if \\(X \\sim N(0, 1)\\), \\(Y \\sim \\text{Exp}(2)\\), and they are independent. R code to estimate \\(P(X &gt; Y)\\): set.seed(1) n &lt;- 10000 X &lt;- rnorm(n, 0, 1) Y &lt;- rexp(n, 2) mean(X &gt; Y) ## [1] 0.3299 We know the true value is not exactly 0.3299 because the law of large numbers only ensure that the sample mean converges to the true mean. We can use t.test() to find an confidence interval for \\(P(X &gt; Y)\\). t.test(X &gt; Y) ## ## One Sample t-test ## ## data: X &gt; Y ## t = 70.162, df = 9999, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3206831 0.3391169 ## sample estimates: ## mean of x ## 0.3299 We see that the \\(95\\%\\) CI is (0.321, 0.339). To make the CI narrower, we can increase the number of simulation. set.seed(1) n &lt;- 1000000 X &lt;- rnorm(n, 0, 1) Y &lt;- rexp(n, 2) t.test(X &gt; Y) ## ## One Sample t-test ## ## data: X &gt; Y ## t = 704.63, df = 1e+06, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3308521 0.3326979 ## sample estimates: ## mean of x ## 0.331775 The \\(95\\%\\) CI becomes (0.331, 0.333). The width of the CI is equal to \\[\\begin{equation*} 2 \\times t_{n-1; \\alpha/2} \\frac{s}{\\sqrt{n}}. \\end{equation*}\\] From the above formula, you could determine the minimum number of simulations required to achieve a certain degree of accuracy, 9.2.5 Comparing the means of two samples Suppose you have one sample each from two populations. You want to test if the two populations have the same mean. There are two different \\(t\\)-tests for this task (assuming data are normally distributed or you have large samples): the observations are not paired the observations are paired To explain the meaning of paired data, consider two experiments to see if drinking coffee in the morning improves your test scores: Unpaired observations: Randomly select two groups of people. People in one group have a cup of morning coffee and take the test. The other group just takes the test. For each person, we have one test score. All the scores are independent. Paired observations: Randomly select one group of people. Give them the test twice, once with morning coffee and once without morning coffee. For each person, we have two test scores. Clearly, the two scores are not statistically independent. Example We illustrate the paired \\(t\\)-test using the dataset sleep (no package is required). The data contains the increase in hours of sleep when the subject took two soporific drugs compared to control on \\(10\\) subjects. Since each subject received two drugs, the observations are paired. Take a look at sleep: sleep ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 The sleep data is in a long format. Lets turn it into a wide format using spread, a function in the package tidyr, which is contained in tidyverse. (sleep_wide &lt;- spread(sleep, group, extra)) ## ID 1 2 ## 1 1 0.7 1.9 ## 2 2 -1.6 0.8 ## 3 3 -0.2 1.1 ## 4 4 -1.2 0.1 ## 5 5 -0.1 -0.1 ## 6 6 3.4 4.4 ## 7 7 3.7 5.5 ## 8 8 0.8 1.6 ## 9 9 0.0 4.6 ## 10 10 2.0 3.4 Paired \\(t\\)-test: t.test(sleep_wide[, 2], sleep_wide[, 3], paired = TRUE) ## ## Paired t-test ## ## data: sleep_wide[, 2] and sleep_wide[, 3] ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean of the differences ## -1.58 The \\(p\\)-value is \\(&lt; 0.05\\). We conclude that the effect of the two drugs are different. The \\(95\\%\\) CI of the difference between the two means is (-2.46, -0.70). Example Are the means of the birth weights in the smoking group and non-smoking group different? library(MASS) t.test(birthwt$bwt[birthwt$smoke == 1], birthwt$bwt[birthwt$smoke == 0]) ## ## Welch Two Sample t-test ## ## data: birthwt$bwt[birthwt$smoke == 1] and birthwt$bwt[birthwt$smoke == 0] ## t = -2.7299, df = 170.1, p-value = 0.007003 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -488.97860 -78.57486 ## sample estimates: ## mean of x mean of y ## 2771.919 3055.696 The \\(p\\)-value is \\(&lt; 0.05\\). We conclude that the means are different. The \\(95\\%\\) CI of the difference between the two means is (-489.0, -78.6). 9.2.6 Inference of a Sample Proportion You have a sample of values from a population consisting of successes and failures. The null hypothesis is the true proportion of success \\(p\\) is equal to some particular number \\(p_0\\). The alternative hypothesis is the \\(p \\neq p_0\\). Example You flip a coin \\(100\\) times independently. You want to test if the coin is fair \\((p_0 = 0.5)\\). # Simulate the coin flips set.seed(1) heads &lt;- rbinom(1, size = 100, prob = .4) # Test, p_0 = 0.5 (result &lt;- prop.test(heads, 100, p = 0.5)) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.1336 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.3233236 0.5228954 ## sample estimates: ## p ## 0.42 The point estimate is 0.42. Although the true probability of success used in the simulation is \\(0.4\\), for this particular data, we do not reject to null hypothesis that the true probability of success is \\(0.5\\) as the \\(p\\)-value equals 0.134, which is larger than \\(0.05\\). The 95% confidence interval is equal to (0.323, 0.523). You can change the alternative hypothesis to \\(p &gt; p_0\\) or \\(p &lt; p_0\\): prop.test(heads, 100, p = 0.5, alternative = &quot;greater&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.9332 ## alternative hypothesis: true p is greater than 0.5 ## 95 percent confidence interval: ## 0.3372368 1.0000000 ## sample estimates: ## p ## 0.42 prop.test(heads, 100, p = 0.5, alternative = &quot;less&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.06681 ## alternative hypothesis: true p is less than 0.5 ## 95 percent confidence interval: ## 0.0000000 0.5072341 ## sample estimates: ## p ## 0.42 9.2.7 Testing groups for equal proportions You have samples from two or more groups. The data from each group are binary-valued: either success or failure. You want to test if the groups have equal proportions of success. Example # 3 groups no_success &lt;- c(48, 60, 50) # no. of successes in the 3 groups no_trial &lt;- c(100, 100, 100) # corresponding no. of trails in the 3 groups prop.test(no_success, no_trial) ## ## 3-sample test for equality of proportions without continuity ## correction ## ## data: no_success out of no_trial ## X-squared = 3.3161, df = 2, p-value = 0.1905 ## alternative hypothesis: two.sided ## sample estimates: ## prop 1 prop 2 prop 3 ## 0.48 0.60 0.50 \\(p\\)-value is \\(&gt; 0.05\\). We do not reject the null hypothesis that the three groups have the same proportion of success. Example In a class of \\(38\\) students, \\(14\\) of them got \\(A\\). In another class of \\(40\\) students, only \\(10\\) got \\(A\\). We want to know if the difference between the two proportions is statistically significant. prop.test(c(14, 10), c(38, 40)) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: c(14, 10) out of c(38, 40) ## X-squared = 0.7872, df = 1, p-value = 0.3749 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.1110245 0.3478666 ## sample estimates: ## prop 1 prop 2 ## 0.3684211 0.2500000 The \\(p\\)-value is \\(&gt; 0.05\\). We do not reject the null hypothesis that the students in the two groups have the same proportion of getting an A. 9.2.8 Testing if two samples have the same underlying distribution Problem You have two random samples \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\). Let \\(F\\) and \\(G\\) be the distribution functions of \\(X_i\\)s and \\(Y_i\\)s respectively. You want to know if \\(F \\equiv G\\). Solution You may use the Kolmogorov-Smirnov test. \\(H_0: F = G\\) vs \\(H_1: F \\neq G\\). It does not require any assumptions. The test statistic is \\[ D := \\sup_x|F_n(x) - G_m(x)|,\\] where \\(F_n\\) and \\(G_m\\) are the empirical distribution functions of \\(X_i\\)s and \\(Y_i\\)s respectively. That is, \\[ F_n(x) := \\frac{1}{n} \\sum^n_{i=1} I(X_i \\leq x)\\] and \\[ G_m(x) := \\frac{1}{m} \\sum^m_{i=1} I(Y_i \\leq x).\\] Example set.seed(362) x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) ks.test(x, y) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: x and y ## D = 0.07, p-value = 0.9671 ## alternative hypothesis: two-sided The \\(p\\)-value is not small. We do not have enough evidence to reject the null hypothesis that the two distributions are the same. Example z &lt;- rnorm(100, 2, 1) ks.test(y, z) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: y and z ## D = 0.77, p-value &lt; 2.2e-16 ## alternative hypothesis: two-sided The \\(p\\)-value is very small. We will reject the null hypothesis that the two distributions are the same. Example Recall the dataset birthwt from the package MASS. We created the following histograms to visualize the distributions of birth weights for the two groups (smoke and no smoke). We may want to ask if the two distributions are different ks.test(birthwt$bwt[birthwt$smoke == 0], birthwt$bwt[birthwt$smoke == 1]) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: birthwt$bwt[birthwt$smoke == 0] and birthwt$bwt[birthwt$smoke == 1] ## D = 0.21962, p-value = 0.02598 ## alternative hypothesis: two-sided The \\(p\\)-value is smaller than \\(0.05\\). Therefore, we conclude that the difference of the two distributions is statistically significant. "],["k-nearest-neighbors.html", "Chapter 10 k-nearest neighbors 10.1 Introduction 10.2 Feature Scaling 10.3 Example: Classifying Breast Cancers", " Chapter 10 k-nearest neighbors Optional Reading: Chapter 3 in Machine Learning with R by Brett Lantz 10.1 Introduction Setting: We have data \\(\\{(x_i, y_i) : i=1,\\ldots,n \\}\\), where \\(x_i\\) and \\(y_i\\) are the vector of the features the class label for the \\(i\\)th observation respectively. For example, in breast cancer diagnosis, \\(x_i\\) could be some summary statistics of the radius, texture, perimeter, area of the cell nuclei from a digitized image of a fine needle aspirate of a breast mass; \\(y_i\\) is the diagnosis (malignant or benign). We now have a new data point \\(x^*\\) (of course we do not have the class label \\(y^*\\)) and we want to assign a class label to it. For example, after a subject has a breast fine needle aspiration, can we use existing data to predict if the subject has breast cancer? In this chapter, we will study the \\(k\\)-nearest neighbors (\\(k\\)-NN) algorithm for classification. \\(k\\)-NN algorithm uses information about an examples \\(k\\) nearest neighbors to classify unlabeled examples. To measure how close the examples are, we need a distance measure. Traditionally, the \\(k\\)-NN algorithm uses Euclidean distance. Given two points \\(u = (u_1,\\ldots,u_p)\\) and \\(w = (w_1,\\ldots,w_p)\\), the Euclidean distance between them is \\[\\begin{equation*} d(u, w) = \\sqrt{ \\sum^p_{i=1}(u_i - w_i)^2}. \\end{equation*}\\] Algorithm: Compute the Euclidean distance \\(d(x_i, x^*)\\) for \\(i=1,\\ldots,n\\) Find the \\(k\\) training data with the smallest \\(d(x_i, x^*)\\) The predicted class for \\(x^*\\) is determined by the majority vote of the \\(k\\) training data in Step 2. The idea before the algorithm is simple. We expect that observations with similar features should have the same class label. In the following figure, we have some labeled data. Suppose the black dot is our new data without label, which group will you assign this data to? A natural choice is to assign the black dot to group A because the \\(5\\) nearest neighbors are all in group A. This is the idea of \\(k\\)-nearest neighbors algorithm. Remark: We can use the \\(k\\)-NN algorithm for regression. In that case, \\(y_i\\) is the numeric response. The corresponding algorithm replaces the majority vote by the mean of the responses in the \\(k\\) nearest training data. Other distance measures could be used. There are different methods to break ties. There is no learning phase. Applications: Computer vision applications, including optical character recognition and facial recognition in both still images and video Recommendation systems that predict whether a person will enjoy a movie or song Identifying patterns in genetic data to detect specific proteins or diseases Strengths of \\(k\\)-NN: Simple and effective Makes no assumptions about the underlying data distribution Weaknesses of \\(k\\)-NN: Does not produce a model, limiting the ability to understand how the features are related to the class Requires selection of an appropriate \\(k\\) Slow classification phase Nominal features and missing data require additional processing Choosing an appropriate \\(k\\): Large \\(k\\): reduce the impact or variance caused by noisy data, may underfit the traning data Small \\(k\\): may overfit the data Extreme case: \\(k = n\\), the most common class will always be the prediction Some people suggest using \\(\\sqrt{n}\\) as \\(k\\). One may also use a validation set or cross-validation to choose the appropriate \\(k\\) (will discuss these later). 10.2 Feature Scaling Clearly, if the features are in different scales, the distance measure computed will be dominated by some of the features and will not take into account of the importance of other features. Lets assume we have \\(n\\) data and \\(p\\) features. Two common methods for feature scaling: min-max normalization For each \\(i=1,\\ldots,p\\), \\(j=1,\\ldots,n\\), \\[\\begin{equation*} x^*_{ij} = \\frac{x_{ij} - \\min x_i}{\\max x_i -\\min x_i}, \\end{equation*}\\] where \\(\\min x_i = \\min_{j=1,\\ldots,n} x_{ij}\\) and \\(\\max x_i = \\max_{j=1,\\ldots,n} x_{ij}\\). The normalized features will have values between \\(0\\) and \\(1\\). \\(z\\)-score standardization For each \\(i=1,\\ldots,p\\), \\(j=1,\\ldots,n\\), \\[\\begin{equation*} x^*_{ij} = \\frac{x_{ij} - \\overline{x}_i}{s_i}, \\end{equation*}\\] where \\(\\overline{x}_i\\) and \\(s_i\\) are the sample mean and standard deviation of \\(\\{x_{i1},\\ldots,x_{in}\\}\\). For nominal features, we can convert them into a numeric feature using dummy coding. For example, if a nominal feature called temperature takes three values: hot, medium and cold. You can define two additional binary indicator variables: \\[\\begin{equation*} \\text{hot} = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if temperature = hot} \\\\ 0 &amp; \\text{otherwise} \\end{array} \\right. \\end{equation*}\\] and \\[\\begin{equation*} \\text{medium} = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if temperature = medium} \\\\ 0 &amp; \\text{otherwise.} \\end{array} \\right. \\end{equation*}\\] When both hot and medium are \\(0\\), we know the temperature is cold. In general, if we have \\(n\\) categories, we only need to create \\(n-1\\) additional binary indicators. 10.3 Example: Classifying Breast Cancers We will use the breast cancecr from UC Irvine Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29 Download the dataset from onQ. Read the data (change the path to where you save your data): wbcd &lt;- read.csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/wisc_bc_data.csv&quot;) Variables: names(wbcd) ## [1] &quot;id&quot; &quot;diagnosis&quot; ## [3] &quot;radius_mean&quot; &quot;texture_mean&quot; ## [5] &quot;perimeter_mean&quot; &quot;area_mean&quot; ## [7] &quot;smoothness_mean&quot; &quot;compactness_mean&quot; ## [9] &quot;concavity_mean&quot; &quot;concave.points_mean&quot; ## [11] &quot;symmetry_mean&quot; &quot;fractal_dimension_mean&quot; ## [13] &quot;radius_se&quot; &quot;texture_se&quot; ## [15] &quot;perimeter_se&quot; &quot;area_se&quot; ## [17] &quot;smoothness_se&quot; &quot;compactness_se&quot; ## [19] &quot;concavity_se&quot; &quot;concave.points_se&quot; ## [21] &quot;symmetry_se&quot; &quot;fractal_dimension_se&quot; ## [23] &quot;radius_worst&quot; &quot;texture_worst&quot; ## [25] &quot;perimeter_worst&quot; &quot;area_worst&quot; ## [27] &quot;smoothness_worst&quot; &quot;compactness_worst&quot; ## [29] &quot;concavity_worst&quot; &quot;concave.points_worst&quot; ## [31] &quot;symmetry_worst&quot; &quot;fractal_dimension_worst&quot; Creating Training and Testing Datasets The first column is id, which should not be included in the classification. The second column is diagnosis. B means benign and M means malignant. In short, the meaning of malignant is cancerous and the meaning of benign is non-cancerous. We will separate the labels from the features. To evaluate the model performance, we always split our full dataset into a training dataset and a testing daatset. set.seed(6) # reproduce the result # create the random numbers for selecting the rows in the dataset random_index &lt;- sample(nrow(wbcd), 469) # our &quot;x&quot; wbcd_train &lt;- wbcd[random_index, -(1:2)] wbcd_test &lt;- wbcd[-random_index, -(1:2)] # our &quot;y&quot; wbcd_train_labels &lt;- wbcd[random_index, ]$diagnosis wbcd_test_labels &lt;- wbcd[-random_index, ]$diagnosis Note: In forming a training dataset and a testing dataset, you should not select the first \\(469\\) rows (unless you know the data have been randomly organized). Normalizing the data We have to normalize the features in both the training datasets and testing datasets We have to use the same normalizing methods for these two datasets Compute the min and max (or mean and sd) using only the training datasets wbcd_train_n &lt;- wbcd_train wbcd_test_n &lt;- wbcd_test train_min &lt;- apply(wbcd_train, 2, min) train_max &lt;- apply(wbcd_train, 2, max) for (i in 1:ncol(wbcd_train)) { wbcd_train_n[, i] &lt;- (wbcd_train[, i] - train_min[i]) / (train_max[i] - train_min[i]) # use the min and max from training data to normalize the testing data wbcd_test_n[, i] &lt;- (wbcd_test[, i] - train_min[i]) / (train_max[i] - train_min[i]) } We will use knn() in the package class to perform \\(k\\)-NN classification. knn() will return a factor of classifications of testing dataset. library(class) # install it if you haven&#39;t done so knn_predicted &lt;- knn(train = wbcd_train_n, test = wbcd_test_n, cl = wbcd_train_labels, k = 21) train : training dataset test: testing dataset cl: training labels k: \\(k\\)-nearest neighbors will be used 10.3.1 Evaluating Model Performance table(wbcd_test_labels, knn_predicted) ## knn_predicted ## wbcd_test_labels B M ## B 57 2 ## M 4 37 False positive: an error where the test result incorrectly indicates the presence of a condition such as a disease when the disease is not present. In this example, we have \\(2\\) false positives. False Negative: an error where the test result incorrectly fails to indicate the presence of a condition when it is present. In this example, we have \\(4\\) false negatives. Here the test result means our prediction. The accuracy is \\[\\begin{equation*} \\text{accuracy} = \\frac{57+37}{57+2+4+37} = 0.94. \\end{equation*}\\] The error rate is \\[\\begin{equation*} \\text{error rate} = \\frac{2 + 4}{57+2+4+37} = 0.06 = 1- \\text{accuracy}. \\end{equation*}\\] 10.3.2 Using \\(z\\)-score standardization Lets try the \\(z\\)-score standardization. wbcd_train_s &lt;- wbcd_train wbcd_test_s &lt;- wbcd_test train_mean &lt;- apply(wbcd_train, 2, mean) train_sd &lt;- apply(wbcd_train, 2, sd) for (i in 1:ncol(wbcd_train)) { wbcd_train_s[, i] &lt;- (wbcd_train[, i] - train_mean[i]) / train_sd[i] # use the mean and sd from training data to normalize the testing data wbcd_test_s[, i] &lt;- (wbcd_test[, i] - train_mean[i]) / train_sd[i] } Perform \\(k\\)-NN: knn_predicted &lt;- knn(train = wbcd_train_s, test = wbcd_test_s, cl = wbcd_train_labels, k = 21) Evaluate the performance: table(wbcd_test_labels, knn_predicted) ## knn_predicted ## wbcd_test_labels B M ## B 58 1 ## M 5 36 The performance using \\(z\\)-score standardization and min-max normalization is similar. 10.3.3 Testing alternative values of \\(k\\) k &lt;- c(1, 5, 11, 15, 21, 27) result &lt;- matrix(0, nrow = length(k), ncol = 4) result[, 1] &lt;- k colnames(result) = c(&quot;k value&quot;, &quot;False Negatives&quot;, &quot;False Positives&quot;, &quot;Percent Classified Correctly&quot;) for (i in 1:length(k)) { knn_predicted &lt;- knn(train = wbcd_train_n, test = wbcd_test_n, cl = wbcd_train_labels, k = k[i]) confusion_matrix &lt;- table(wbcd_test_labels, knn_predicted) result[i, 2] &lt;- confusion_matrix[2, 1] result[i, 3] &lt;- confusion_matrix[1, 2] result[i, 4] &lt;- sum(diag(confusion_matrix)) / length(wbcd_test_labels) } result ## k value False Negatives False Positives Percent Classified Correctly ## [1,] 1 4 3 0.93 ## [2,] 5 4 1 0.95 ## [3,] 11 3 1 0.96 ## [4,] 15 3 2 0.95 ## [5,] 21 4 2 0.94 ## [6,] 27 3 2 0.95 Important remark: while the accuracy is the highest when \\(k = 11\\), it does not mean the model is the best for this data. Note that We split the dataset into a training dataset and a testing dataset using one particular random partition. With another random partition, the results would be different. The more appropriate way to assess the accuracy is to use repeated \\(k\\)-fold cross validation (the \\(k\\) here is different from the \\(k\\) in our \\(k\\)-NN algorithm). We shall discuss this later. Having false Negatives could be a more severe problem than having false positives in this example. Although in this example, when \\(k=11\\), it also produces the lowest number of false negatives. "]]
