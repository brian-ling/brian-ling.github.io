[["index.html", "STAT 362 R for Data Science Syllabus", " STAT 362 R for Data Science Brian Ling 2023-02-11 Syllabus STAT 362 R for Data Science Department of Mathematics and Statistics, Queen’s University Course Description: Introduction to R, data creation and manipulation, data import and export, scripts and functions, control flow, debugging and profiling, data visualization, statistical inference, Monte Carlo methods, decision trees, support vector machines, neural network, numerical methods. For details, see onQ "],["introduction.html", "Chapter 1 Introduction 1.1 What is R and RStudio? 1.2 Why R? 1.3 What will you learn in this course? 1.4 Let’s Get Started 1.5 R Data Structures 1.6 Operators 1.7 Built-in Functions 1.8 Some Useful RStudio Shortcuts 1.9 Exercises 1.10 Comments to Exercises", " Chapter 1 Introduction 1.1 What is R and RStudio? R R is a programming language and environment for statistical computing, analysis, and graphics. R is an interpreted language (individual language expressions are read and then executed immediately as soon as the command is entered) To download R, go to https://cloud.r-project.org/ RStudio is an integrated development environment (IDE) for R programming Install R first, then go to https://rstudio.com/products/rstudio/download/ and download RStudio While you can work in R directly, it is recommended to work in RStudio. 1.2 Why R? It’s free, open source, and available on every major platform. As a result, if you do your analysis in R, anyone can easily reproduce it. A massive set of packages for statistical modelling, maching learning, visualization, and importing and manipulating data. Powerful tools for communicating your results. RMarkdown makes it easy to turn your results into HTML files, PDFs, Word documents, PowerPoint presentations, and more. Shiny allows you to make beautiful interactive apps without any knowledge of HTML or javascript. RStudio provides an integrated development environment. Cutting edge tools. Researchers in statistics and machine learning will often publish an R package to accompany their articles. This means you have access to the latest statistical techniques and implementations. The ease with which R can connect to high-performance programming languages like C, Fortran, and C++. BUT, R is not perfect. One of the challenges is that most R users are not programmers. This means, for example, that much of the R code you will see in the wild is written in haste to solve a pressing problem. As a result, code is not very elegant, fast, or easy to understand. Most users do not revise their code to address these shortcomings. R is also not a particularly fast programming languages, and poorly written R code can be terribly slow. 1.3 What will you learn in this course? Note: we do not assume you know R or any programming language before. 1.3.1 R and R as a programming language operators control flow (if..else.., for loop) defining a function 1.3.2 Data Wrangling Data wrangling = the process of tidying and transforming the data 1.3.3 Data Visualization Graphs are powerful to illustrate features of the data. You will learn how to create some basic plots as well as using the package ggplot2 to create more elegant plots. Consider a dataset about cars. library(ggplot2) mpg ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compact ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 audi a4 quattro 1.8 1999 4 manual(m5) 4 18 26 p compact ## 9 audi a4 quattro 1.8 1999 4 auto(l5) 4 16 25 p compact ## 10 audi a4 quattro 2 2008 4 manual(m6) 4 20 28 p compact ## # … with 224 more rows Among the variables in mpg are: displ, a car’s engine size, in litres. hwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. Scatterplot Scatterplot, points are labeled with colors according to the class variable Scatterplots Line Chart Bar chart Another Bar Chart Boxplot Histogram 1.3.4 Statistical Inference Many problems in different domains can be formulated into hypothesis testing problems. Are university graduates more likely to vote for Candidate A? Is a treatment effective in reducing weights? Is a drug effective in reducing mortality rate? We want to answer these questions that take into account of the intrinsic variability. Formally, we can perform hypothesis testing and compute the confidence intervals. These are what you learned in STAT 269. It is ok if you haven’t taken the STAT 269. The topics will be briefly reviewed. We will focus on the applications using R. 1.3.5 Machine Learning We will illustrate some machine learning methods using real datasets. For example, Diagnoising breast cancer with the k-NN algorithm Employ Naive Bayes to build an SMS junk message filter (text data) A wordcloud of text data Use neural network to predict the compressive strength of concrete 1.3.6 Some Numerical Methods Monte Carlo simulation (estimate probabilities, expectations, integrals) numerical optimizaiton methods (e.g. maximizing a multi-parameter likelihood function using optim) 1.3.7 Lastly It is important to communicate your results to other after performing the data analysis. Therefore, you will do a project with presentation and report. 1.4 Let’s Get Started The best way to learn R is to get started immediately and try the code by yourselves. We will not discuss every topic in detail at the beginning, which is not interesting and unnecessary. We shall revisit the topics when we need additional knowledge. Simple arithmetic expression # can be used a simple calculator 3+5 ## [1] 8 4*2 ## [1] 8 10/2 ## [1] 5 Comment a code: use the hash mark # # this is a comment, R will not run the code behine # Function for ‘combining’ c(4, 2, 3) # &quot;c&quot; is to &quot;combine&quot; the numbers ## [1] 4 2 3 Assignment (&lt;- is the assignment operator like = in many other programming languages) y &lt;- c(4, 2, 3) # create a vector called y with elements 4, 2, 3 c(1, 3, 5) -&gt; v # c(1,3,5) is assigned to v Output y ## [1] 4 2 3 v ## [1] 1 3 5 R is case-sensitive. When you type Y, you will see an error message: object ‘Y’ not found Y ## Error in eval(expr, envir, enclos): object &#39;Y&#39; not found 1.5 R Data Structures Reading: ML with R Ch2 Most frequently used data structures in R: vectors, factors, lists, arrays, matrices, data frames 1.5.1 Vectors Vector fundamental R data structure stores an ordered set of values called elements elements must be of the same type Type: integer, double, character, logical Integer, double, logical, character vectors x &lt;- 1:2 # integer vector, we use a:b to form the sequence of integers from a to b typeof(x) # type of the vector ## [1] &quot;integer&quot; x &lt;- c(1.1, 1.2) # double vector typeof(x) ## [1] &quot;double&quot; length(x) # length of the vector x ## [1] 2 x &lt; 2 # logical (TRUE/FALSE) ## [1] TRUE TRUE p &lt;- c(TRUE, FALSE) subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) # character vector Combine two vectors y &lt;- c(2, 4, 6) c(x, y) # note that we created x above ## [1] 1.1 1.2 2.0 4.0 6.0 c(y, subject_name) # 2, 4, 6 become characters &quot;2&quot;, &quot;4&quot;, &quot;6&quot; ## [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; Assessing elements in the vectors y &lt;- c(2, 4, 6) y[2] # second element ## [1] 4 y[3] # third element ## [1] 6 1.5.2 Factors A factor is a special type of vector that is solely used for representing categorical (male, female/group 1, group 2, group 3) or ordinal (cold, warm, hot/ low, medium, high) variables. Reasons for using factor the category labels are stored only once. E.g., rather than storing MALE, MALE, MALE, FEMALE, the computer may store 1,1,1,2(save memory) many machine learning algorithms treat categorical/ordinal and numeric features differently and may require the input as a factor Create a factor gender &lt;- factor(c(&quot;MALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;)) gender ## [1] MALE MALE FEMALE MALE ## Levels: FEMALE MALE # compared with c(&quot;MALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;) ## [1] &quot;MALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; 1.5.3 Matrix Matrix a collection of numbers in a rectangular form A matrix with dimension n by m means the matrix has n rows and m columns. Create Matrix: To create a \\(3\\times 4\\) matrix with elements 1:12 filled in column-wise A &lt;- matrix(1:12, nrow = 3, ncol = 4) # note that we use = instead of &lt;- A ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Dimension, number of rows, number of columns of a matrix # again R is case-sensitive, a and A are different dim(A) # to find the dimension of A ## [1] 3 4 nrow(A) # to find the number of row in A ## [1] 3 ncol(A) # to find the number of column in A ## [1] 4 By default, the matrix is filled column-wise. You can change to row-wise by adding byrow = TRUE B &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE) B ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 Select rows, columns, submatrix, element A[1, 2] # select the element in the 1st row and 2nd column ## [1] 4 A[2, ] # select 2nd row ## [1] 2 5 8 11 A[, 3] # select 3rd column ## [1] 7 8 9 A[1:2, 3:4] # select a submatrix ## [,1] [,2] ## [1,] 7 10 ## [2,] 8 11 Try: A[c(1, 2), c(1, 3, 4)] A[-1, ] A[, -2] Combine Two Matrices cbind(A, B) # combine column-wise ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1 4 7 10 1 2 3 4 ## [2,] 2 5 8 11 5 6 7 8 ## [3,] 3 6 9 12 9 10 11 12 rbind(A, B) # combine row-wise ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [4,] 1 2 3 4 ## [5,] 5 6 7 8 ## [6,] 9 10 11 12 Try: rbind(B, A) Transpose x &lt;- c(1, 2, 3) t(x) # transpose ## [,1] [,2] [,3] ## [1,] 1 2 3 Q &lt;- matrix(1:4, 2, 2) Q ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 t(Q) # transpose ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 Matrix Addition A &lt;- matrix(1:6, nrow = 2, ncol = 3) B &lt;- matrix(2:7, nrow = 2, ncol = 3) A + B ## [,1] [,2] [,3] ## [1,] 3 7 11 ## [2,] 5 9 13 A - B ## [,1] [,2] [,3] ## [1,] -1 -1 -1 ## [2,] -1 -1 -1 A + 2 ## [,1] [,2] [,3] ## [1,] 3 5 7 ## [2,] 4 6 8 c &lt;- c(1, 2) A + c ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 4 6 8 Elementwise Product A &lt;- matrix(1:6, nrow = 2, ncol = 3) B &lt;- matrix(1:2, nrow = 2, ncol = 3) A * B ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 4 8 12 c &lt;- 2 A * c ## [,1] [,2] [,3] ## [1,] 2 6 10 ## [2,] 4 8 12 c &lt;- c(10, 100) A * c ## [,1] [,2] [,3] ## [1,] 10 30 50 ## [2,] 200 400 600 c &lt;- c(10, 100, 1000) A * c # do you notice the pattern? ## [,1] [,2] [,3] ## [1,] 10 3000 500 ## [2,] 200 40 6000 Matrix Multiplication A &lt;- matrix(1:12, nrow = 3, ncol = 4) # 3x4 matrix t(A) # 4x3 matrix ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 t(A) %*% A #3x3 matrix, %*% = matrix multiplication ## [,1] [,2] [,3] [,4] ## [1,] 14 32 50 68 ## [2,] 32 77 122 167 ## [3,] 50 122 194 266 ## [4,] 68 167 266 365 B &lt;- matrix(1:9, nrow = 3, ncol = 3) B %*% A ## [,1] [,2] [,3] [,4] ## [1,] 30 66 102 138 ## [2,] 36 81 126 171 ## [3,] 42 96 150 204 A %*% B # error, non-conformable arguments ## Error in A %*% B: non-conformable arguments Diagonal Matrix diag(1:4) # diagonal matrix with diagonal elements being 1:4 ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 2 0 0 ## [3,] 0 0 3 0 ## [4,] 0 0 0 4 A &lt;- matrix(1:9, 3, 3) A ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 diag(A) # find the diagonal of A ## [1] 1 5 9 How to create an identity matrix in R? Inverse The inverse of a \\(n \\times n\\) matrix \\(A\\), denoted by \\(A^{-1}\\), is a \\(n \\times n\\) matrix such that \\(AA^{-1} = A^{-1} A = I_n\\), where \\(I_n\\) is the \\(n\\times n\\) identity matrix. To find the inverse of \\(A\\) in R: solve A &lt;- matrix(c(1, 0, 0, 3), 2, 2) solve(A) ## [,1] [,2] ## [1,] 1 0.0000000 ## [2,] 0 0.3333333 Some Statistical Applications I will mention a few connections of matrices with statistics. A dataset is naturally a matrix. Suppose that you have \\(n\\) people. You collected their health information: blood pressure, height, weight, age, whether they smoke (1 if yes, 0 if no), whether they drink (1/0), etc. Linear regression: we observe \\((x,y)\\), where \\(x\\) is a vector of covariates and \\(y\\) is your response. For example, \\(y\\) is the blood pressure, \\(x\\) is the collection of other health information. The linear regression model assumes that \\[y = \\beta_0 + x^T \\beta_1 + \\varepsilon,\\] where \\(\\varepsilon\\) is the error term. Our goal is to estimate \\(\\beta:=(\\beta_0, \\beta_1)\\). Let \\(X\\) be the design matrix. That is \\(X\\) is a \\(n \\times p\\) matrix where \\(n\\) is the number of observation, \\(p-1\\) is the number of covariates. The least squares solution for \\(\\beta\\) is \\[\\hat{\\beta} = (X^T X)^{-1}X^TY.\\] We will revisit linear regression later (I know some of you may not know linear regression). Correlation matrix, Covariance matrix. Let \\(X\\) be a random vector (column vector). The covariance matrix is defined as \\[\\Sigma := E[(X-E(X))(X-E(X))^T].\\] 1.5.4 Lists store an ordered set of elements like a vector can store different R data types (unlike a vector) # let&#39;s create some vectors (of different types) subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) # at this point, you should notice that meaningful names should be used for the variables temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) # notice how we use _ to separate two words # this is one of the styles in coding, you should be consistent with your style data &lt;- list(fullname = subject_name, temperature = temperature, flu_status = flu_status) # you may wonder what is the meaning of temperature = temperature # in &quot;fullname = subject_name&quot; # on the left of = is the name of the 1st element of your list # on the right of = is the name of the variable that you want to # assign the value to the 1st element data ## $fullname ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; ## ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE To assess the element of a list: data$flu_status ## [1] FALSE FALSE TRUE data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE data[2:3] # if you don&#39;t have the names ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE 1.5.5 Data frames Data frame can be understood as a list of vectors, each having exactly the same number of values, arranged in a structure like a spreadsheet or database gender &lt;- c(&quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;) blood &lt;- c(&quot;O&quot;, &quot;AB&quot;, &quot;A&quot;) pt_data &lt;- data.frame(subject_name, temperature, flu_status, gender, blood) pt_data ## subject_name temperature flu_status gender blood ## 1 John 98.1 FALSE MALE O ## 2 Jane 98.6 FALSE FEMALE AB ## 3 Steve 101.4 TRUE MALE A colnames(pt_data) ## [1] &quot;subject_name&quot; &quot;temperature&quot; &quot;flu_status&quot; &quot;gender&quot; &quot;blood&quot; pt_data$subject_name ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; pt_data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE pt_data[1, ] # like a matrix ## subject_name temperature flu_status gender blood ## 1 John 98.1 FALSE MALE O pt_data[, 2:3] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE Create a new column pt_data$temp_c &lt;- (pt_data$temperature - 32) * 5 / 9 pt_data ## subject_name temperature flu_status gender blood temp_c ## 1 John 98.1 FALSE MALE O 36.72222 ## 2 Jane 98.6 FALSE FEMALE AB 37.00000 ## 3 Steve 101.4 TRUE MALE A 38.55556 pt_data$fever &lt;- (pt_data$temp_c &gt; 37.6) pt_data ## subject_name temperature flu_status gender blood temp_c fever ## 1 John 98.1 FALSE MALE O 36.72222 FALSE ## 2 Jane 98.6 FALSE FEMALE AB 37.00000 FALSE ## 3 Steve 101.4 TRUE MALE A 38.55556 TRUE 1.6 Operators Priority Operator Meaning 1 $ component selection 2 [ [[ subscripts, elements 3 ^ (caret) exponentiation 4 - unary minus 5 : sequence operator 6 %% %/% %*% modulus, integer divide, matrix multiply 7 * / multiply, divide 8 + - add, subtract 9 &lt; &gt; &lt;= &gt;= == != comparison 10 ! not 11 &amp; | &amp;&amp; || logical and, logical or 12 &lt;- -&gt; = assignments # $ for list, data frame, etc subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) data &lt;- list(fullname = subject_name, temperature = temperature, flu_status = flu_status) data$fullname ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; pt_data &lt;- data.frame(subject_name, temperature, flu_status) pt_data$temperature ## [1] 98.1 98.6 101.4 # [ ], [[]] x &lt;- c(1, 5, 7) x[2] ## [1] 5 data[[1]] ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; # x^r = x to the power of r x &lt;- 2 x^4 # 16 ## [1] 16 # modulus 7 %% 2 # 7 divided by 2 equals 3 but it remains 1, modulus = reminder ## [1] 1 10 %% 3 ## [1] 1 20 %% 2 ## [1] 0 # integer division 7 %/% 2 ## [1] 3 20 %/% 3 ## [1] 6 Comparison # &lt;, &gt;, &lt;=, &gt;=, ==, != x &lt;- 2 x &gt; 3 ## [1] FALSE x &lt; 4 ## [1] TRUE x &lt;- c(1, 5, 7) x &lt; 3 # compare each element with 3 ## [1] TRUE FALSE FALSE x &gt;= 5 ## [1] FALSE TRUE TRUE x == 5 # if x is equal to 5, not x = 5 ## [1] FALSE TRUE FALSE x != 5 # if x is not equal to 5 ## [1] TRUE FALSE TRUE x &lt;- TRUE !x # not x ## [1] FALSE x &lt;- 2 x &lt;= 2 ## [1] TRUE !(x &lt;= 2) ## [1] FALSE &amp; and &amp;&amp; indicate logical AND. The shorter form performs elementwise comparisons. The longer form examine only the first element of each vector. x &lt;- c(TRUE, FALSE, TRUE) y &lt;- c(FALSE, FALSE, TRUE) x &amp; y ## [1] FALSE FALSE TRUE x &amp;&amp; y ## Warning in x &amp;&amp; y: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in x &amp;&amp; y: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## [1] FALSE z &lt;- c(TRUE) x &amp;&amp; z ## Warning in x &amp;&amp; z: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## [1] TRUE | and || indicate logical OR. The shorter form performs elementwise comparisons. The longer form examine only the first element of each vector. x &lt;- c(TRUE, FALSE, TRUE) y &lt;- c(FALSE, FALSE, TRUE) x | y ## [1] TRUE FALSE TRUE x || y ## Warning in x || y: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## [1] TRUE z &lt;- c(TRUE) x || z ## Warning in x || z: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## [1] TRUE Assignment # these assignments are the same, it is recommended to use &lt;- a &lt;- 2 a ## [1] 2 2 -&gt; b b ## [1] 2 c = 2 c ## [1] 2 Do !(x &gt; 1) &amp; (x &lt; 4) and !((x &gt; 1) &amp; (x &lt; 4)) give different results? Example Let v be a vector of integers. Write a one-line R code to compute the product of all the even integers in v. To illustrate how to solve the question step by step: v &lt;- -10:10 # begin writing your code by setting some integers v %% 2 # find the remainder, if the remainder is 0, it is an even number ## [1] 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 v %% 2 == 0 # check which elements is 0 ## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE v[v %% 2 == 0] # select the elements which are &quot;TRUE&quot; ## [1] -10 -8 -6 -4 -2 0 2 4 6 8 10 prod(v[v %% 2 == 0]) # find the product ## [1] 0 # the result is 0. v may not be a good example to check if the code is correct # let&#39;s change to some other vector v &lt;- 2:8 prod(v[v %% 2 == 0]) ## [1] 384 2 * 4 * 6 * 8 #check ## [1] 384 # now, the final answer is prod(v[v %% 2 == 0]) 1.6.1 Vectorized Operators An important property of many of the operators is that they are “vectorized”. This means that the operation will be performed elementwise. x &lt;- c(1, 2, 3) y &lt;- c(5, 6, 7) x + y ## [1] 6 8 10 x * y ## [1] 5 12 21 2 * x # you do not need to use c(2,2,2)*x ## [1] 2 4 6 y / 2 # you do not need to use y/c(2,2,2) ## [1] 2.5 3.0 3.5 A &lt;- matrix(1:9, nrow = 3, ncol = 3) B &lt;- matrix(1:9, nrow = 3, ncol = 3) A + B ## [,1] [,2] [,3] ## [1,] 2 8 14 ## [2,] 4 10 16 ## [3,] 6 12 18 A * B # this is not matrix multiplication, but elementiwse multiplication ## [,1] [,2] [,3] ## [1,] 1 16 49 ## [2,] 4 25 64 ## [3,] 9 36 81 x &lt;- c(1, 3, 5) y &lt;- c(2, 2, 9) x &lt; y ## [1] TRUE FALSE TRUE Another example: X &lt;- c(1, 1, 5) Y &lt;- c(5, 5, 1) X &gt; Y # FALSE FALSE TRUE ## [1] FALSE FALSE TRUE sum(X &gt; Y) # TRUE = 1, FALSE = 0 ## [1] 1 mean(X &gt; Y) # 1/3 ## [1] 0.3333333 The code mean(X &gt; Y) is computing \\[\\begin{equation*} \\frac{1}{3}( I(X_1 &gt; Y_1) + I(X_2 &gt; Y_2) + I(X_3 &gt; Y_3)), \\end{equation*}\\] where \\(I(\\cdot)\\) is the indicator function, that is, \\(I(X_1 &gt; Y_1) = 1\\) if \\(X_1 &gt; Y_1\\) and \\(I(X_1 &gt; Y_1) = 0\\) if \\(X_1 \\leq Y_1\\). Example: if A is matrix and x is a vector, you can find the sums of the elements in A and x by sum(A) and sum(x) respectively. x &lt;- 1:10 sum(x) ## [1] 55 A &lt;- matrix(1:10, 5, 2) sum(A) ## [1] 55 Example: Find \\(\\sum^{5}_{x=1}\\sum^{4}_{y=1} \\frac{x}{x+y}\\) without any loops. (x &lt;- matrix(1:5, nrow = 4, ncol = 5, byrow = TRUE)) # define and display at the same time using (...) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 1 2 3 4 5 ## [3,] 1 2 3 4 5 ## [4,] 1 2 3 4 5 (y &lt;- matrix(1:4, nrow = 4, ncol = 5, byrow = FALSE)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1 ## [2,] 2 2 2 2 2 ## [3,] 3 3 3 3 3 ## [4,] 4 4 4 4 4 x / (x + y) # vectorized operation ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.5000000 0.6666667 0.7500000 0.8000000 0.8333333 ## [2,] 0.3333333 0.5000000 0.6000000 0.6666667 0.7142857 ## [3,] 0.2500000 0.4000000 0.5000000 0.5714286 0.6250000 ## [4,] 0.2000000 0.3333333 0.4285714 0.5000000 0.5555556 sum(x / (x + y)) ## [1] 10.72817 1.7 Built-in Functions Common mathematical functions sqrt, abs, sin, cos, log, exp. To get help on the usage of a function. Use ?. For example, if you want to know more about log. Type ?log in the console. You will then see that by default, log computes the natrual logarithms. Other useful functions Name Operations ceiling smallest integer greater than or equal to element floor largest integer less than or equal to element trunc ignore the decimal part round round up for positive and round down for negative sort sort the vector in ascending or descending order sum, prod sum and produce of a vector cumsum, cumprod cumulative sum and product min, max return the smallest and largest values range return a vector of length 2 containing the min and max mean return the sample mean of a vector var return the sample variance of a vector sd return the sample standard deviation of a vector seq generate a sequence of number rep replicate elements in a vector Note: If you have data \\(x_1,\\ldots,x_n\\), the sample variance is defined as \\[ S^2_n := \\frac{1}{n-1} \\sum^n_{i=1}(x_i-\\overline{x}_n)^2. \\] Note that we divide the sum by \\(n-1\\) but not \\(n\\). The sample standard deviation is the square root of the sample variance. x &lt;- 1:5 y &lt;- sqrt(x) y ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 ceiling(y) ## [1] 1 2 2 2 3 sum(x) ## [1] 15 prod(x) ## [1] 120 cumsum(x) ## [1] 1 3 6 10 15 cumprod(x) ## [1] 1 2 6 24 120 min(x) ## [1] 1 max(x) ## [1] 5 range(x) ## [1] 1 5 mean(x) ## [1] 3 var(x) ## [1] 2.5 rep(0, 10) # create a vector of length 10 with all elements being 0 ## [1] 0 0 0 0 0 0 0 0 0 0 rep(1, 10) # create a vector of length 10 with all elements being 1 ## [1] 1 1 1 1 1 1 1 1 1 1 1.7.1 sort() x &lt;- c(1, 5, 3, 10) sort(x) # default = ascending order ## [1] 1 3 5 10 sort(x, decreasing = TRUE) # descending order ## [1] 10 5 3 1 1.7.2 seq() This is an example of function with more than one argument. # seq(from, to) seq(1:5) ## [1] 1 2 3 4 5 seq(from = 1, to = 5) ## [1] 1 2 3 4 5 # seq(from, to, by) seq(1, 5, 2) ## [1] 1 3 5 seq(from = 1, to =5, by = 2) ## [1] 1 3 5 # seq(from, to, length) seq(0, 10, length = 21) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 seq(from = 0, to = 10, length = 21) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 1.7.3 rep() # rep(data, times), try ?rep rep(0, 10) ## [1] 0 0 0 0 0 0 0 0 0 0 rep(c(1, 2, 3), 3) ## [1] 1 2 3 1 2 3 1 2 3 1.7.4 pmax, pmin x &lt;- c(1, 3, 5) y &lt;- c(2, 4, 4) max(x, y) # maximum of x and y ## [1] 5 min(x, y) # minimum of x and y ## [1] 1 pmax(x, y) # elementwise comparison ## [1] 2 4 5 pmin(x, y) # elementwise comparison ## [1] 1 3 4 1.8 Some Useful RStudio Shortcuts See also https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts Ctrl + 1: Move focus to the Source Editor (when you are in the Console) Ctrl + 2: Move focus to the Console (when you are in the source window) \\(\\uparrow\\) (the up arrow key on the keyboard): go to the previous command (in the console) \\(\\downarrow\\) (the down arrow key on the keyboard): go to the next command (in the console) Esc: Delete the current command/ Interrupt currently executing command Ctrl + Tab: go to the next tab 1.9 Exercises To test your understanding, try to evaluate the following code by hand and then check with the output from R. x &lt;- (10:1)[c(-1, -4)] x &lt;- x^2 x[5] # what do you expect to see? a&lt;-c(1:3, 6, 3 + 3, &quot;sta&quot;, 3 &gt; 5) a #? typeof(a) #? length(a) #? x &lt;- rep(1:6, rep(1:3, 2)) &gt; x %% 2 == 0 #? &gt; x[x %% 2 == 0] #? &gt; round(-3.7) #? &gt; trunc(-3.7) #? &gt; floor(-3.7) #? &gt; ceiling(-3.7) #? &gt; round(3.8) #? &gt; trunc(3.8) #? &gt; floor(3.8) #? &gt; ceiling(3.8) #? &gt; x&lt;-c(4, 3, 8, 7, 5, 6, 2, 1) &gt; sort(x) #? &gt; order(x) #? &gt; sum(x) + prod(x) #? &gt; cumsum(x) + cumprod(x) #? &gt; max(x) + min(x) #? 1.10 Comments to Exercises These are comments but not answers but you can get the answers immediately by running the code. x &lt;- (10:1)[c(-1, -4)] # 10:1 will give you a vector with elements 10, 9, 8,...,1 10:1 ## [1] 10 9 8 7 6 5 4 3 2 1 # [c(-1, -4)] will remove the 1st and 4th elements in the vector # therefore 10 and 7 will be removed from 10:1 given x ## [1] 9 8 6 5 4 3 2 1 x &lt;- x^2 # ^2 will square each of the elements in the vector x[5] # 36 ## [1] 16 a&lt;-c(1:3, 6, 3 + 3, &quot;sta&quot;, 3 &gt; 5) a # when you combine numeric, characters and logical values, the results become character ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;6&quot; &quot;6&quot; &quot;sta&quot; &quot;FALSE&quot; typeof(a) # character ## [1] &quot;character&quot; length(a) # ## [1] 7 x &lt;- rep(1:6, rep(1:3, 2)) # we first evaluate rep(1:3, 2), which is # 1 2 3 1 2 3 # rep then replicates the elements by the corresponding number of times # 1 is repeated 1 time # 2 is repeated 2 times # 3 is repeated 3 times # 4 is repeated 1 time # 5 is repeated 2 times # 6 is repeated 3 times x%%2 == 0 # check if the modulus is 0 when x is divided by 2 ## [1] FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE # this is equivalent to ask if the elements are even x[x%%2 == 0] # find out all the even elements ## [1] 2 2 4 6 6 6 # if you need to use these functions # you may try with a positive number and a negative number # to see if the results are what you want round(-3.7) # try round(-3.5), round(-3.4) ## [1] -4 trunc(-3.7) ## [1] -3 floor(-3.7) ## [1] -4 ceiling(-3.7) ## [1] -3 round(3.8) ## [1] 4 trunc(3.8) ## [1] 3 floor(3.8) ## [1] 3 ceiling(3.8) ## [1] 4 x&lt;-c(4, 3, 8, 7, 5, 6, 2, 1) sort(x) # asecending order ## [1] 1 2 3 4 5 6 7 8 order(x) # from the result, can you guess what it does? ## [1] 8 7 2 1 5 6 4 3 # order(x) returns a permutation which rearranges its first argument into ascending or descending order # that is, x[order(x)] = sort(x) sum(x) + prod(x) ## [1] 40356 cumsum(x) + cumprod(x) ## [1] 8 19 111 694 3387 20193 40355 40356 max(x) + min(x) ## [1] 9 "],["probability-and-simulation.html", "Chapter 2 Probability and Simulation 2.1 Probability Distributions 2.2 Simulation 2.3 Additional Exercises:", " Chapter 2 Probability and Simulation Optional Reading: R Cookbook Ch8 2.1 Probability Distributions Using the normal distribution as an example: Function Purpose dnorm Normal density pnorm Normal CDF qnorm Normal quantile function rnorm Normal random variables Examples Density of \\(N(2, 3^2)\\) at \\(5\\). dnorm(5, mean = 2, sd = 3) ## [1] 0.08065691 \\(P(X \\leq 3)\\), where \\(X \\sim N(2, 3^2)\\) pnorm(3, mean = 2, sd = 3) ## [1] 0.6305587 # &quot;mean =&quot; and &quot;sd =&quot; are optional pnorm(3, 2, 3) ## [1] 0.6305587 Generate 10 random variables, each follows \\(N(3, 4^2)\\). rnorm(10, 3, 4) ## [1] 3.9482377 0.1151053 2.0070930 3.2651827 11.0344095 -2.2832325 4.4176193 -2.6013656 5.8579585 -6.1388674 95th percenttile of \\(N(0, 1)\\). Find \\(q\\) such that \\(P(Z \\leq q) = 0.95\\) qnorm(0.95, 0, 1) ## [1] 1.644854 Plotting the normal density x &lt;- seq(-4, 4, by = 0.1) plot(x, dnorm(x), type = &quot;l&quot;, main = &quot;Density of N(0,1)&quot;) # &quot;l&quot; for lines 2.1.1 Common Distributions Common discrete distributions Discrete distribution R name Parameters Binomial binom n = number of trials; p = probability of success for one trial Geometric geom p = probability of success for one trial Negative binomial (NegBinomial) nbinom size = number of successful trials; either prob = probability of successful trial or mu = mean Poisson pois lambda = mean Common continuous distributions Continuous distribution R name Parameters Beta beta shape1; shape2 Cauchy cauchy location; scale Chi-squared (Chisquare) chisq df = degrees of freedom Exponential exp rate F f df1 and df2 = degrees of freedom Gamma gamma rate; either rate or scale Log-normal (Lognormal) lnorm meanlog = mean on logarithmic scale; sdlog = standard deviation on logarithmic scale Logistic logis location; scale Normal norm mean; sd = standard deviation Student’s t (TDist) t df = degrees of freedom Uniform unif min = lower limit; max = upper limit To get help on the distributions: ?dnorm ?dbeta ?dcauchy # the following distributions need to use different code ?TDist ?Chisquare ?Lognormal Examples (Using Binomial as an Example) dbinom(2, 10, 0.6) # p_X(2), p_X is the pmf of X, X ~ Bin(n=10, p=0.6) ## [1] 0.01061683 pbinom(2, 10, 0.6) # F_X(2), F_X is the CDF of X, X ~ Bin(n=10, p=0.6) ## [1] 0.01229455 qbinom(0.5, 10, 0.6) # 50th percentile of X ## [1] 6 rbinom(4, 10, 0.6) # generate 4 random variables from Bin(n=10, p=0.6) ## [1] 5 7 7 6 x &lt;- 0:10 plot(x, dbinom(x, 10, 0.6), type = &quot;h&quot;) # &quot;h&quot; for histogram like vertical lines 2.1.2 Exercises The average number of trucks arriving on any one day at a truck depot in a certain city is known to be 12. Assuming the number of trucks arriving on any one day has a Poisson distribution, what is the probability that on a given day fewer than 9 (strictly less than 9) trucks will arrive at this depot? ppois(8, 12) ## [1] 0.1550278 Let \\(Z \\sim N(0, 1)\\). Find \\(c\\) such that \\(P(Z \\leq c) = 0.1151\\) qnorm(0.1151) ## [1] -1.199844 \\(P(1\\leq Z \\leq c) = 0.1525\\) c &lt;- qnorm(pnorm(1) + 0.1525) # draw a graph # test the answer pnorm(c) - pnorm(1) ## [1] 0.1525 \\(P(-c \\leq Z \\leq c) = 0.8164\\). # P(0 &lt;= Z &lt;= c) = 0.8164/2 # P(Z &lt;= c) = 0.8164/2 + 0.5 c &lt;- qnorm(0.8164 / 2 + 0.5) # test our answer pnorm(c)- pnorm(-c) ## [1] 0.8164 Plot the density of a chi-squared distribution with degrees of freedom \\(4\\), from \\(x=0\\) to \\(x=10\\). Find the 95th percentile of this distribution. # note that a chi-squared r.v. is nonnegative x &lt;- seq(0, 10, by = 0.1) plot(x, dchisq(x, df = 4), type = &quot;l&quot;) qchisq(0.95, df = 4) ## [1] 9.487729 Simulate \\(10\\) Bernoulli random variables with parameter \\(0.6\\). # Bernoulli(p) = Bin(1, p) rbinom(10, size = 1, prob = 0.6) ## [1] 1 1 1 1 1 1 1 1 0 1 Plot the Poisson pmf with parameter \\(2\\) from \\(x = 0\\) to \\(x = 10\\). x &lt;- 0:10 plot(x, dpois(x, 2), type = &quot;h&quot;) Draw a plot to illustrate that the 97.5th percentile of the t distribution will be getting closer to that of the standard normal distribution when the degrees of freedom increases. x &lt;- 10:200 plot(x, qt(0.975, df = x), type = &quot;l&quot;, ylim = c(1.9,2.3)) # add a horizontal line with value at qnorm(0.975) # lty = 2 for dashed line, check ?par abline(h = qnorm(0.975), lty = 2) # Therefore, for a large sample, t-test and z-test will give you similar result. 2.2 Simulation We have already seen how to use functions like runif, rnorm, rbinom to generate random variables. R actually generates pseudo-random number sequence (deterministic sequence of numbers that approximates the properties of random numbers) The pseduo-random number sequence will be the same if it is initialized by the same seed (can be used to reproduce the same simulation results or used to debug). # every time you run the first two lines, you get the same result set.seed(1) runif(5) ## [1] 0.2655087 0.3721239 0.5728534 0.9082078 0.2016819 # every time you run the following code, you get a different result runif(5) ## [1] 0.89838968 0.94467527 0.66079779 0.62911404 0.06178627 Sampling from discrete distributions Usage of sample: sample(x, size, replace = FALSE, prob = NULL) See also ?sample. sample(10) # random permutation of integers from 1 to 10 ## [1] 3 1 5 8 2 6 10 9 4 7 sample(10, replace = T) # sample with replacement ## [1] 5 9 9 5 5 2 10 9 1 4 sample(c(1, 3, 5), 5, replace = T) ## [1] 5 3 3 3 3 # simulate 20 random variables from a discrete distribution sample(c(-1,0,1), size = 20, prob = c(0.25, 0.5, 0.25), replace = T) ## [1] -1 1 1 -1 0 0 1 1 0 -1 0 0 0 0 0 1 1 0 -1 0 Example: Suppose we have a fair coin and we play a game. We flip the coin. We win $1 if the result is head and lose $1 if the result is tail. You play the game 100 times. You are interested in the cumulative profit. set.seed(1) # R actually generates pseudo random numbers # setting the seed ensure that each time you will get the same result # for illustration, code debugging, reproducibility profit &lt;- sample(c(-1, 1), size = 100, replace = T) plot(cumsum(profit), type = &quot;l&quot;) set.seed(2) profit &lt;- sample(c(-1, 1), size = 100, replace = T) plot(cumsum(profit), type = &quot;l&quot;) Example: You have two dice \\(A\\) and \\(B\\). For die \\(A\\), there are \\(6\\) sides with numbers \\(1,2,3,4,5,6\\) and the corresponding probability of getting these values are \\(0.1,0.1,0.1,0.1,0.1,0.5\\). For die \\(B\\), there are \\(4\\) sides with numbers \\(1,2,3,7\\) and the corresponding probability of getting these values are \\(0.3,0.2,0.3,0.2\\). You roll the two dice independently. Estimate \\(P(X &gt; Y)\\) using simulation, where \\(X\\) is the result from die \\(A\\) and \\(Y\\) is the result from die \\(B\\). n &lt;- 10000 # number of simulations X &lt;- sample(1:6, size = n, replace = TRUE, prob = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.5)) Y &lt;- sample(c(1, 2, 3, 7), size = n, replace = TRUE, prob = c(0.3, 0.2, 0.3, 0.2)) mean(X &gt; Y) ## [1] 0.6408 Why the sample mean approximates the required probability? Recall the strong law of large numbers (SLLN). Let \\(X_1,\\ldots,X_n\\) be independent and identically distributed random variables with mean \\(\\mu:=E(X)\\). Let \\(\\overline{X}_n:= \\frac{1}{n} \\sum^n_{i=1}X_i\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\rightarrow} \\mu.\\] Note: a.s. means almost surely. The above convergence means \\(P(\\lim_{n\\rightarrow \\infty} \\overline{X}_n = \\mu) = 1\\). Note that (the expectation of an indicator random variable is the probability that the corresponding event will happen) \\[ P(X&gt;Y) = E(I(X&gt;Y)).\\] To apply SLLN, we just need to recognize the underlying random variable is \\(I(X&gt;Y)\\). Then, with probability \\(1\\), the sample mean \\[ \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i) \\rightarrow P(X&gt;Y).\\] The quantity on the LHS is what we compute in mean(X&gt;Y) (note that we are using vectorized comparison). Another Simulation Example A simple model on the stock return assumes that (i) \\[r_{t+1} := \\log \\frac{P_{t+1}}{P_t} \\sim N(\\mu,\\sigma^2), \\] where \\(r_{t+1}\\) is the log-return at Day \\(t+1\\), \\(P_t\\) is the stock price at the end of Day \\(t\\); (ii) \\(r_1,r_2,\\ldots\\) are iid. Simple algebra shows that \\[P_{t+1} = P_t e^{r_{t+1}}.\\] Applying the above equation repeatedly, we have \\[P_{t+1} = P_{t-1}e^{r_{t+1}} e^{r_t} = \\ldots = P_0 e^{r_{t+1}} e^{r_t} \\cdots e^{r_1} = P_0 e^{ \\sum^{t+1}_{i=1} r_i}.\\] Suppose that the current price of a certain stock \\(P_0\\) is \\(100\\), \\(\\mu = 0.0002\\) and \\(\\sigma = 0.015\\). Using simulation, estimate the probability that the price is below $95 at the close of at least one of the next 30 trading days. no_sim &lt;- 10000 # number of simulation below &lt;- rep(0, no_sim) for (i in 1:no_sim) { price &lt;- 100 * exp(cumsum(rnorm(30, mean = 0.0002, sd = 0.015))) below[i] &lt;- min(price) &lt; 95 } mean(below) ## [1] 0.4472 2.3 Additional Exercises: Ex 1 Let \\(X \\sim N(mean = 2, sd = 1)\\), \\(Y \\sim Exp(rate = 2)\\), \\(Z \\sim Unif(0, 4)\\) (continuous uniform distribution on [0,4]). Suppose \\(X, Y, Z\\) are independent. Estimate \\(P(\\max(X,Y)&gt;Z)\\). Recall the difference between pmax and max: # always try with simple examples when you test the usage of functions x &lt;- c(1, 2, 3) y &lt;- c(0, 5, 10) max(x, y) ## [1] 10 pmax(x, y) ## [1] 1 5 10 Answer: n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(pmax(x, y) &gt; z) ## [1] 0.51278 Ex 2 Let \\(X \\sim N(mean = 2, sd = 1)\\), \\(Y \\sim Exp(rate = 2)\\), \\(Z \\sim Unif(0, 4)\\) (continuous uniform distribution on [0,4]). Suppose that \\(X, Y, Z\\) are independent. Estimate \\(P(\\min(X,Y)&gt;Z)\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(pmin(x, y) &gt; z) # what is the difference between pmin and min? ## [1] 0.11386 Ex 3 Let \\(X \\sim N(mean = 2, sd = 1)\\), \\(Y \\sim Exp(rate = 2)\\), \\(Z \\sim Unif(0, 4)\\) (continuous uniform distribution on [0,4]). Suppose that \\(X, Y, Z\\) are independent. Estimate \\(P(X^2 Y&gt;Z)\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(x ^ 2 * y &gt; z) ## [1] 0.40801 Ex 4 Person \\(A\\) generates a random variable \\(X \\sim N(2, 1)\\) and Person \\(B\\) generates a random variable \\(Z \\sim Unif(0, 4)\\). If \\(X &lt; Z\\), Person \\(A\\) will discard \\(X\\) and generate another random variable \\(Y \\sim Exp(0.5)\\). Find the probability that the number generated by \\(A\\) is greater than that by \\(B\\). n &lt;- 100000 greater &lt;- rep(0, n) for (i in 1:n) { X &lt;- rnorm(1, 2, 1) Z &lt;- runif(1, 0, 4) if (X&lt; Z) { Y &lt;- rexp(1, rate = 0.5) greater[i] &lt;- Y &gt; Z } else { greater[i] &lt;- 1 # 1 means A&#39;s no &gt; B&#39;s no } } mean(greater) ## [1] 0.63775 Remark: you may find that the following code gives you almost the same answer. Why? n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 0.5) z &lt;- runif(n, 0, 4) mean(pmax(x, y) &gt; z) ## [1] 0.63977 We will see additional simulation examples after we talk about some programming in R There are many important topics that we will not discuss algorithms for simulating random variables inverse transform acceptance rejection Markov Chain Monte Carlo methods to reduce variance in simulation Control variates Antithetic variates Importance sampling "],["programming-in-r.html", "Chapter 3 Programming in R 3.1 Writing functions in R 3.2 Control Flow 3.3 Loop functions 3.4 Automatically Reindent Code 3.5 Speed Consideration 3.6 Additional Exercises", " Chapter 3 Programming in R Optional reading: R Cookbook Ch 15 3.1 Writing functions in R When you have to copy and paste some code more than 2 times, you should consider writing a function Writing a function can simplify your code and isolate the main part of your program General format of a function function_name &lt;- function(argument1, argument2) { statements } Example: Let’s try to write a function to compute the sample variance. my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2) / (n - 1)) } y &lt;- 1:9 my_var(y) ## [1] 7.5 var(y) # compared with the bulit-in function ## [1] 7.5 x &lt;- rnorm(1000, mean = 0, sd = 2) my_var(x) ## [1] 3.698982 var(x) # why the result is not equal to 4? ## [1] 3.698982 We can also write my_var2 &lt;- function(x) {sum((x - mean(x))^2) / (length(x) - 1)} The variable x is the argument to be passed into the function. The variables mean_x and n are local variables whose scope is within this function. y &lt;- 2 f &lt;- function(x) { y &lt;- x x &lt;- 4 y } y ## [1] 2 f(3) # output the value f(3) ## [1] 3 y # y is unchanged, y is defined in the global environment ## [1] 2 x ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found We shall write code using proper indentation (easier to read and debug) # with indentation (use this one) my_var &lt;- function(x) { mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2) / (n - 1)) } # no indentation my_var &lt;- function(x) { mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2) / (n - 1)) } The number of arguments passed to a function can be more than one Example: Write a function to compute the pooled sample standard deviation of two independent samples \\(x_1,\\ldots,x_n\\) and \\(y_1,\\ldots,y_m\\) of sizes \\(n\\) and \\(m\\). Recall that the pooled sample standard deviation is defined as: \\[ S_p := \\sqrt{\\frac{(n-1)S^2_X + (m-1)S^2_Y}{m+n-2}}, \\] where \\(S^2_X\\) and \\(S^2_Y\\) are the sample variances of \\(x_1,\\ldots,x_n\\) and \\(y_1,\\ldots,y_m\\), respectively. pooled_sd &lt;- function(x, y) { n &lt;- length(x) m &lt;- length(y) return(sqrt(((n - 1) * var(x) + (m - 1) * var(y)) / (m + n - 2))) } Remark: if the final statement will output something, it will be the output of the function. You can also use return() as above. That is, pooled_sd and pooled_sd2 are exactly the same. pooled_sd2 &lt;- function(x, y) { n &lt;- length(x) m &lt;- length(y) sqrt(((n - 1) * var(x) + (m - 1) * var(y)) / (m + n - 2)) } You can return more than one value in a function my_var_sd &lt;- function(x) { mean_x &lt;- mean(x) n &lt;- length(x) my_var &lt;- sum((x - mean_x)^2) / (n - 1) return(c(my_var, sqrt(my_var))) } You may also return a list my_var_sd &lt;- function(x) { mean_x &lt;- mean(x) n &lt;- length(x) my_var &lt;- sum((x - mean_x)^2) / (n - 1) output &lt;- list(var = my_var, sd = sqrt(my_var)) return(output) } Example: write a function called my_summary that will output a list with elements being equal to the mean, sd, median, min and max of a given vector. my_summary &lt;- function(x) { output &lt;- list(mean = mean(x), sd = sd(x), median = median(x), min = min(x), max = max(x)) return(output) } my_summary(1:10) ## $mean ## [1] 5.5 ## ## $sd ## [1] 3.02765 ## ## $median ## [1] 5.5 ## ## $min ## [1] 1 ## ## $max ## [1] 10 Define a function with default value my_power &lt;- function(x, p = 2) { return(x^p) } my_power(3) # by default, p = 2, will compute 3^2 ## [1] 9 my_power(3, 3) # will compute 3^3 ## [1] 27 Examples with matrix input Write a function called matrix_times_vector to compute \\(X Y\\), where \\(X\\) is a matrix and \\(Y\\) is a vector. The output should be a vector. matrix_times_vector = function(X, Y) { as.vector(X %*% Y) } # e.g. X &lt;- matrix(1:12, 3, 4) Y &lt;- 1:4 X ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Y ## [1] 1 2 3 4 matrix_times_vector(X, Y) ## [1] 70 80 90 Note: X %*% Y will return a matrix. We can use as.vector to change it into a vector. It is common to see the error non-conformable arguments. This is because the dimensions of your matrices/vectors do not match. If you have a \\(n\\times p\\) matrix \\(A\\) and \\(m \\times q\\) matrix \\(B\\), you can do the matrix multiplication \\(AB\\) only if \\(p = m\\). In R, if this is not the case, there will be an error. Similarly, if you have a vector \\(d\\) of length \\(m\\). You can do the matrix multiplication \\(A d\\) only if \\(p = m\\). Write a function called matrix_times_vector2 to compute \\(X Y\\), where \\(X\\) is a matrix and \\(Y\\) is a vector. The output should be a vector. However, you should check if the dimensions of the inputs are appropriate before you perform the calculation. Display an error message The dimensions do not match if this is not the case. matrix_times_vector2 = function(X, Y) { p &lt;- ncol(X) m &lt;- length(Y) if (p == m) { return(as.vector(X %*% Y)) } else { cat(&quot;The dimensions do not match&quot;) } } # e.g. X &lt;- matrix(1:12, 4, 3) Y &lt;- 1:4 matrix_times_vector2(X, Y) ## The dimensions do not match X &lt;- matrix(1:12, 3, 4) Y &lt;- 1:4 matrix_times_vector2(X, Y) ## [1] 70 80 90 3.1.1 Argument Matching Two ways of calling R function with arguments: by position of the argument by name of the argument Example: set.seed(1) rnorm(5, 1, 2) # generate 5 rv from N(0, 2^2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 1.6590155 set.seed(1) rnorm(n = 5, mean = 1, sd = 2) # this is the same as above ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 1.6590155 When you call the function by specifiying the name of the argument, the order does not matter: set.seed(1) rnorm(mean = 1, n = 5, sd = 2) ## [1] -0.2529076 1.3672866 -0.6712572 4.1905616 1.6590155 But of course, one should not change the order in generally. 3.2 Control Flow 3.2.1 for loop You can use a for loop when you know how many times you will loop. Syntax: for (var in sequence) { statement # do this statement for each value of i } Examples: for (i in 1:5) { # note: you do not have to define i beforehand print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 for (i in c(1, 3, 6)) { print(i) } ## [1] 1 ## [1] 3 ## [1] 6 Example: write a function with a for loop to produce a conversion table of temperature from Fahrenheit (from \\(0\\) to \\(200\\) with increment \\(20\\)) to their Celsius equivalent. conv_table &lt;- function(low, up, step) { f_temp &lt;- seq(low, up, step) n &lt;- length(f_temp) c_temp &lt;- rep(0, n) for (i in 1:n) { c_temp[i] &lt;- (5 / 9) * (f_temp[i] - 32) } # alternatively, we can use the vectorized operation # c_temp &lt;- (5 / 9) * (f_temp - 32) cbind(f_temp, c_temp) } # test your function conv_table(0, 200, 20) ## f_temp c_temp ## [1,] 0 -17.777778 ## [2,] 20 -6.666667 ## [3,] 40 4.444444 ## [4,] 60 15.555556 ## [5,] 80 26.666667 ## [6,] 100 37.777778 ## [7,] 120 48.888889 ## [8,] 140 60.000000 ## [9,] 160 71.111111 ## [10,] 180 82.222222 ## [11,] 200 93.333333 3.2.2 nested for loop for loops can be nested inside of each other Examples Write R code to find \\(\\sum^{10}_{i=1} \\sum^4_{j=1} \\frac{i^2}{(i+j)^2}\\). sum &lt;- 0 for (i in 1:10) { for (j in 1:4) { sum &lt;- sum + i^2 / (i + j)^2 } } sum ## [1] 18.26491 Write R code to find \\(\\sum^{10}_{i=1} \\sum^i_{j=1} \\frac{i^2}{(i+j)^3}\\). sum &lt;- 0 for (i in 1:10) { for (j in 1:i) { sum &lt;- sum + i^2 / (i + j)^3 } } sum ## [1] 2.779252 3.2.3 while loop You can use a while loop if you want to loop until a specific condition is met. For example, when you minimize a function numerically using some iterative algorithm, you may want to stop when the objective value does not change much. You may not know how many loops are required in advance so that a while loop may be better than a for loop in this application. Syntax: while (condition) { statement # while the condition is TRUE, do this } A simple example: i &lt;- 1 while (i &lt; 6) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 # What happen if you change &quot;i &lt; 6&quot; to &quot;i &lt;= 6&quot;? Ans: will print 1 to 6 # What happen if you change &quot;i &lt; 6&quot; to &quot;i &lt;= 5&quot;? Ans: outputs are the same Another example: # find the smallest n such that 1^2+ 2^2+ ... + n^2 &gt; 65 sum &lt;- 0 i &lt;- 0 while (sum &lt; 65) { i &lt;- i + 1 sum &lt;- sum + i^2 print(c(i, sum)) } ## [1] 1 1 ## [1] 2 5 ## [1] 3 14 ## [1] 4 30 ## [1] 5 55 ## [1] 6 91 i # 6 ## [1] 6 3.2.4 if (cond) Syntax: if (condition) { statement # do this if the condition is TRUE } Example: write a function that outputs “positive” if a positive number is entered. # check if a number if positive my_pos &lt;- function(x) { if (x &gt; 0) { print(&quot;positive&quot;) } } my_pos(-2) my_pos(2) ## [1] &quot;positive&quot; 3.2.5 if (cond) else expr Syntax if (condition) { statement1 # do this if condition is TRUE } else { statement2 # do this if condition is FALSE } Example: # write my own absolute value function my_abs &lt;- function(x) { if (x&gt;=0) { return(x) } else { return(-x) } } my_abs(-2) ## [1] 2 my_abs(3) ## [1] 3 my_abs(0) ## [1] 0 Error-handling in a function: my_sqrt = function(x) { if (x &gt;= 0) { print(sqrt(x)) # do this if x &gt;= 0 } else { cat(&quot;Error: this is a negative number!&quot;) # do this otherwise } } my_sqrt(-2) ## Error: this is a negative number! 3.2.6 If else ladder Syntax # Example if (condition1) { statement1 } else if (condition2) { statement2 } else if (condition1) { statement3 } Example: score_to_grade = function(x) { if (x&gt;=90) { cat(&quot;A+&quot;) } else if (x &gt;= 85) { cat(&quot;A&quot;) } else if (x &gt;= 80) { cat(&quot;A-&quot;) } else { cat(&quot;B+ or below&quot;) } } # after you write the function, you should check each case carefully score_to_grade(92) ## A+ score_to_grade(88) ## A score_to_grade(83) ## A- score_to_grade(78) ## B+ or below 3.2.7 switch Suppose you wish to write a function to generate random variables with two options: standard normal and uniform. If we use if else, then rdist &lt;- function(n, dist) { if (dist == &quot;norm&quot;) { rnorm(n) } else if (dist == &quot;unif&quot;) { runif(n) } } Using switch: rdist_switch &lt;- function(n, dist) { switch(dist, norm = rnorm(n), unif = runif(n)) } 3.2.8 next, break break breaks out of a for, while or repeat loop; control is transferred to the first statement outside the inner-most loop. next halts the processing of the current iteration and advances the looping index. Both break and next apply only to the innermost of nested loops. Example of break: for (j in 1:3) { for (i in 1:5) { if (i &lt;= 3) { next } print(c(i, j)) } } ## [1] 4 1 ## [1] 5 1 ## [1] 4 2 ## [1] 5 2 ## [1] 4 3 ## [1] 5 3 Example of next: for (j in 1:3) { for (i in 1:5) { if (i &gt;= 3) { break } } print(c(i, j)) } ## [1] 3 1 ## [1] 3 2 ## [1] 3 3 3.3 Loop functions 3.3.1 apply() Apply a function over the margins of an array/ matrix Basic usage: apply(X, MARGIN, FUN) Margin: For a matrix, 1 indicates rows, 2 indicates columns FUN: function to be applied Examples X &lt;- matrix(runif(20), nrow = 4, ncol = 5) # row sum apply(X, 1, sum) ## [1] 2.020915 1.959644 3.476258 2.314615 # column sum apply(X, 2, sum) ## [1] 1.453658 2.977065 2.304328 1.430564 1.605818 # row mean apply(X, 1, mean) ## [1] 0.4041831 0.3919289 0.6952516 0.4629231 # column mean apply(X, 2, mean) ## [1] 0.3634145 0.7442663 0.5760820 0.3576409 0.4014545 For the special cases of finding row/column sums and means of matrices, there are specific functions: X &lt;- matrix(runif(20), nrow = 4, ncol = 5) rowSums(X) ## [1] 2.839428 2.709573 3.370761 2.423309 rowMeans(X) ## [1] 0.5678857 0.5419147 0.6741522 0.4846618 colSums(X) ## [1] 1.761405 2.398024 2.602992 2.655045 1.925607 colMeans(X) ## [1] 0.4403512 0.5995059 0.6507480 0.6637612 0.4814016 They are faster and their names are easier to understand when reading the code. 3.3.2 lapply() lapply(): Apply a function over a list of vector It will return a list of the same length as your input. Examples: set.seed(1) n &lt;- 1:5 lapply(n, rnorm) # the result is a list of 5 elements ## [[1]] ## [1] -0.6264538 ## ## [[2]] ## [1] 0.1836433 -0.8356286 ## ## [[3]] ## [1] 1.5952808 0.3295078 -0.8204684 ## ## [[4]] ## [1] 0.4874291 0.7383247 0.5757814 -0.3053884 ## ## [[5]] ## [1] 1.5117812 0.3898432 -0.6212406 -2.2146999 1.1249309 The following for loop will give the same result set.seed(1) output &lt;- list() for (i in 1:5) { output[[i]] &lt;- rnorm(i) } output ## [[1]] ## [1] -0.6264538 ## ## [[2]] ## [1] 0.1836433 -0.8356286 ## ## [[3]] ## [1] 1.5952808 0.3295078 -0.8204684 ## ## [[4]] ## [1] 0.4874291 0.7383247 0.5757814 -0.3053884 ## ## [[5]] ## [1] 1.5117812 0.3898432 -0.6212406 -2.2146999 1.1249309 3.4 Automatically Reindent Code To indent a block of code, highlight the text in RStudio, then press Ctrl+i (Windows or Linux) or press Cmd+i (Mac). Poor indentation, difficult to read for (i in 1:5) { if (i &gt;= 3) { print(i * 2) } else { print(i * 3) } } ## [1] 3 ## [1] 6 ## [1] 6 ## [1] 8 ## [1] 10 Highlight the block of code, press Ctrl+i or Cmd+i for (i in 1:5) { if (i &gt;= 3) { print(i * 2) } else { print(i * 3) } } ## [1] 3 ## [1] 6 ## [1] 6 ## [1] 8 ## [1] 10 3.5 Speed Consideration While the computing power is getting stronger and stronger, we should still write code that runs efficiently. # suppose we want to simulate 200,000 normal random variables n &lt;- 200000 x &lt;- rep(0, n) # create a vector for storage of the values initial_time &lt;- proc.time() for (i in 1:n) { x[i] &lt;- rnorm(1) } proc.time() - initial_time ## user system elapsed ## 0.08 0.00 0.24 n &lt;- 200000 x &lt;- rep(0, n) # create a vector for storage of the values # Alternatively system.time({ for (i in 1:n) { x[i] &lt;- rnorm(1) } }) ## user system elapsed ## 0.00 0.05 0.22 The ‘user time’ is the CPU time charged for the execution of user instructions of the calling process. The ‘system time’ is the CPU time charged for execution by the system on behalf of the calling process. A much more efficient way for the same task is to use system.time({ n &lt;- 200000 x &lt;- rnorm(n) }) ## user system elapsed ## 0 0 0 Another example: set.seed(1) x &lt;- rnorm(2e6) y &lt;- rnorm(2e6) v &lt;- rep(0, 2e6) system.time({ for (i in 1:length(x)){ v[i] &lt;- x[i] + y[i] } }) ## user system elapsed ## 0.06 0.00 0.07 system.time(v &lt;- x + y) ## user system elapsed ## 0 0 0 The general rule is to use vectorized operations whenever possible and to avoid using for loops. We use a for loop when the code is not time-consuming or when the code is hard to write without using a for loop. A more advanced option is to combine C++ with R using the package rcpp. That is, we can write the most time-consuming part of the R code in C++, which could run many times faster (will not be discussed in this course). See also http://www.noamross.net/archives/2014-04-16-vectorization-in-r-why/ 3.6 Additional Exercises Write a function that takes two numbers as arguments and returns the sum of the numbers. my_sum &lt;- function(x, y) { return(x + y) } my_sum(2, 3) ## [1] 5 Write a function that takes a vector as an argument and returns the mean of the elements of the vector. my_mean &lt;- function(x) { # let&#39;s use a for loop to compute the mean n &lt;- length(x) output &lt;- 0 for (i in 1:n) { output &lt;- output + x[i] } return(output / n) } my_mean(c(1, 3, 11)) ## [1] 5 Write a function that takes a matrix as an argument and returns the sum of the diagonal elements. sum_diag &lt;- function(A) { return(sum(diag(A))) } C &lt;- matrix(1:81, nrow = 9, ncol = 9) sum_diag(C) ## [1] 369 Write a for loop that iterates from 1 to 10 and prints the square of each number. for (i in 1:10) { print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 ## [1] 100 Write a for loop that iterates through a vector of names and prints each name. v_names &lt;- c(&quot;A&quot;, &quot;BC&quot;, &quot;EO&quot;, &quot;QP&quot;) for (i in 1:length(v_names)) { print(v_names[i]) } ## [1] &quot;A&quot; ## [1] &quot;BC&quot; ## [1] &quot;EO&quot; ## [1] &quot;QP&quot; # alternatively for (i in v_names) { print(i) } ## [1] &quot;A&quot; ## [1] &quot;BC&quot; ## [1] &quot;EO&quot; ## [1] &quot;QP&quot; Write a for loop that iterates through a vector of numbers and prints only the even numbers. x &lt;- c(1, 5, 3, 4, 2, 10) for (i in 1:length(x)) { if (x[i]%%2 == 0) { print(x[i]) } } ## [1] 4 ## [1] 2 ## [1] 10 Sum the even numbers in a vector x &lt;- c(1, 5, 3, 4, 2, 10) x %% 2 == 0 # vectorized operation ## [1] FALSE FALSE FALSE TRUE TRUE TRUE sum(x[x %% 2 == 0]) ## [1] 16 # alternative way of doing the same task output &lt;- 0 for (i in 1:length(x)) { if (x[i] %% 2 == 0) { output &lt;- output + x[i] } } output ## [1] 16 Generate some random integers from 1 to 1 million sample(1:1e6, size = 10) ## [1] 561439 127167 780154 220509 719167 764255 396230 217832 690367 751599 Write an if statement that checks if a variable x is equal to 5, and if so print “x is equal to 5”. x &lt;- 7 if (x == 5) { print(&quot;x is equal to 5&quot;) } Write an if-else statement that checks if a variable y is greater than 10 and if so, print “y is greater than 10”; otherwise, print “y is less than or equal to 10”. y &lt;- 10 if (y &gt; 10) { print(&quot;y is greater than 10&quot;) } else { print(&quot;y is less than or equal to 10&quot;) } ## [1] &quot;y is less than or equal to 10&quot; Write a nested if-else statement that checks if a variable z is positive, and if so, check if it is even or odd and prints “z is positive and even”. Print “z is not positive” if z is not positive. z &lt;- -2 if (z &gt; 0) { if (z %% 2 == 0) { print(&quot;z is positive and even&quot;) } } else { print(&quot;z is not positive&quot;) } ## [1] &quot;z is not positive&quot; Another example check_pos_even &lt;- function(x) { if (x &gt; 0) { pos_neg &lt;- &quot;+ve&quot; if (x %% 2 == 0) { even_odd &lt;- &quot;even&quot; } else { even_odd &lt;- &quot;odd&quot; } } else { pos_neg &lt;- &quot;-ve&quot; if (x %% 2 == 0) { even_odd &lt;- &quot;even&quot; } else { even_odd &lt;- &quot;odd&quot; } } print(paste0(pos_neg, &quot; and &quot;, even_odd)) } check_pos_even(1) ## [1] &quot;+ve and odd&quot; "],["managing-data-with-r.html", "Chapter 4 Managing Data with R 4.1 Missing Values 4.2 Saving, loading, and removing R data structures 4.3 Importing and saving data from CSV files 4.4 Data Transformation with dplyr 4.5 arrange() 4.6 filter() 4.7 select() 4.8 mutate() 4.9 summarize(), group_by() 4.10 Combining Multiple Operations with Pipe %&gt;% 4.11 Summary", " Chapter 4 Managing Data with R Reference: see https://r4ds.had.co.nz/transform.html Optional Reading: ML with R Ch2 4.1 Missing Values Missing values are common in real datasets. NA is used to denote missing values. (x &lt;- c(1, 2, 3, NA, 4, NA, 4)) # we can use (x&lt;-1) to assign 1 to x and display x at the same time ## [1] 1 2 3 NA 4 NA 4 mean(x) # mean cannot be computed when missing values exist ## [1] NA mean(x, na.rm = TRUE) # NA values will be removed before computing mean ## [1] 2.8 sd(x) # sd cannot be computed when missing values exist ## [1] NA sd(x, na.rm = TRUE) # NA values will be removed before computing SD ## [1] 1.30384 is.na(x) # logical vector ## [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE x[!is.na(x)] #select the elements with non-missing valuess ## [1] 1 2 3 4 4 na.omit(x) # select the elements with non-missing valuess ## [1] 1 2 3 4 4 ## attr(,&quot;na.action&quot;) ## [1] 4 6 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; 4.2 Saving, loading, and removing R data structures Removing all objects in R: rm(list = ls()) ls() returns a vector of all data structures currently in memory x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) ls() ## [1] &quot;x&quot; &quot;y&quot; To remove x from the memory rm(x) x # because we have deleted x, an error message occurs ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found Saving objects to a file (regardless of whether they are vectors, factors, lists, etc) A &lt;- matrix(1:9, 3, 3) f &lt;- function(x){ return(1) } save(A, f, file = &quot;my_data.RData&quot;) Loading objects from a .RData file. rm(list = ls()) # remove everything load(&quot;my_data.RData&quot;) A ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 f ## function(x){ ## return(1) ## } 4.3 Importing and saving data from CSV files Finding current directory getwd() ## [1] &quot;C:/Queens Teaching/Teaching/STAT 362 W23 R for data science/01c_published_webiste&quot; If you use mac, you will probably see \"/Users/..../\". Setting working directory setwd(&quot;C:/Queens Teaching/Teaching/STAT 362 W23 R for data science/01_lect_notes/Ch4_Managing_Data_and_dplyr&quot;) # use /, not \\ If you use mac, change the above code accordingly. Writing to a file my_data &lt;- data.frame(x = c(5, 10, 3), y = c(4, 5, 6)) my_data ## x y ## 1 5 4 ## 2 10 5 ## 3 3 6 write.csv(my_data, &quot;C:/Queens Teaching/Teaching/STAT 362 W23 R for data science/01_lect_notes/Ch4_Managing_Data_and_dplyr/my_data.csv&quot;, row.names = FALSE) Reading a csv file A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. If we use read_csv from the package tidyverse, the resulting object is a tibble. If we use read.csv from base R, the resulting object is a data frame. See R for data science (https://r4ds.had.co.nz/data-import.html) for a discussion on the differences between read.csv and read_csv. my_data &lt;- read.csv(&quot;C:/Queens Teaching/Teaching/STAT 362 W23 R for data science/01_lect_notes/Ch4_Managing_Data_and_dplyr/my_data.csv&quot;) 4.4 Data Transformation with dplyr Preparation We will use a dataset in the package nycflights13. To install it: install.packages(&quot;nycflights13&quot;) To use the dataset or functions in the package, we first load the library: library(nycflights13) In Chapter 4, we have installed tidyverse, which contains the package dplyr. Now, load the package library(tidyverse) nycflights13 The dataset flights in the package nyclfights13 contains all \\(336,776\\) flights that departed from New York City in 2013. Check ?flights for details. # let&#39;s view the dataset flights ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_de…¹ arr_t…² sched…³ arr_d…⁴ carrier flight tailnum origin dest air_t…⁵ dista…⁶ hour minute ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6 2013 1 1 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## 7 2013 1 1 555 600 -5 913 854 19 B6 507 N516JB EWR FLL 158 1065 6 0 ## 8 2013 1 1 557 600 -3 709 723 -14 EV 5708 N829AS LGA IAD 53 229 6 0 ## 9 2013 1 1 557 600 -3 838 846 -8 B6 79 N593JB JFK MCO 140 944 6 0 ## 10 2013 1 1 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD 138 733 6 0 ## # … with 336,766 more rows, 1 more variable: time_hour &lt;dttm&gt;, and abbreviated variable names ¹​dep_delay, ²​arr_time, ³​sched_arr_time, ## # ⁴​arr_delay, ⁵​air_time, ⁶​distance flights is a tibble. Tibbles are data frames with better properties. Optional: If you are interested in the differences between a data frame and a tibble, you can go to https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html To view the complete dataset, use View(flights). Five key dplyr functions arrange(): reorder the rows fliter(): pick observations by their values select(): pick variables by their names mutate(): create new variables with functions of existing variables summarize(): collapse many values down to a single summary All functions work similarly: The first argument is a data frame/ tibble The subsequent argument describe what to do with the data frame, using the variable names (without quotes). The result is a new data frame. Of course, it is also possible to perform the same tasks without using dplyr functions. We will also discuss briefly how to use the base subsetting with [] to select the data. In general, the functions in dplyr are designed to transform the data more easily. 4.5 arrange() arrange() orders your dataset. If more than one column name is provided, each additional column will be used to break ties in the values of preceding columns. To reorder by a column in ascending order: arrange(flights, year, month, day) Let’s create a simple dataset to illustrate this because flights is already sorted in year, month and day. (data &lt;- tibble(x = c(2, 2, 1, 4, 5), y = c(2, 3, 10, 10, 10))) ## # A tibble: 5 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 ## 2 2 3 ## 3 1 10 ## 4 4 10 ## 5 5 10 arrange(data, x) ## # A tibble: 5 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 ## 2 2 2 ## 3 2 3 ## 4 4 10 ## 5 5 10 # first sort in ascending order of x, use y to break any ties and sort in descending order arrange(data, x, desc(y)) ## # A tibble: 5 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 ## 2 2 3 ## 3 2 2 ## 4 4 10 ## 5 5 10 To reorder by a column in descending order, use desc(): arrange(flights, desc(arr_delay)) Missing values are always sorted at the end # create a tibble with one column called x with values 5,2,NA df &lt;- tibble(x = c(5, 2, NA)) arrange(df, x) ## # A tibble: 3 × 1 ## x ## &lt;dbl&gt; ## 1 2 ## 2 5 ## 3 NA arrange(df, desc(x)) ## # A tibble: 3 × 1 ## x ## &lt;dbl&gt; ## 1 5 ## 2 2 ## 3 NA 4.5.1 Exercises Sort flights to find the most delayed flights. Find the flights that left earliest. arrange(flights, desc(dep_delay)) arrange(flights, dep_delay) Which flights traveled the longest? Which traveled the shortest? arrange(flights, desc(distance)) arrange(flights, distance) 4.6 filter() filter() only includes rows where the condition is TRUE Select all flights on Jan 1st: # flights is the name of your data frame # month == 1, day == 1 is the condition (jan1 &lt;- filter(flights, month == 1, day == 1)) ## # A tibble: 842 × 19 ## year month day dep_time sched_dep_time dep_de…¹ arr_t…² sched…³ arr_d…⁴ carrier flight tailnum origin dest air_t…⁵ dista…⁶ hour minute ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6 2013 1 1 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## 7 2013 1 1 555 600 -5 913 854 19 B6 507 N516JB EWR FLL 158 1065 6 0 ## 8 2013 1 1 557 600 -3 709 723 -14 EV 5708 N829AS LGA IAD 53 229 6 0 ## 9 2013 1 1 557 600 -3 838 846 -8 B6 79 N593JB JFK MCO 140 944 6 0 ## 10 2013 1 1 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD 138 733 6 0 ## # … with 832 more rows, 1 more variable: time_hour &lt;dttm&gt;, and abbreviated variable names ¹​dep_delay, ²​arr_time, ³​sched_arr_time, ⁴​arr_delay, ## # ⁵​air_time, ⁶​distance Let’s use a simple dataset to see how to perform the same task without filter: # just a simple dataset (data &lt;- tibble(x = c(1, 3, 5, 5, 3), y = 1:5)) ## # A tibble: 5 × 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 1 1 ## 2 3 2 ## 3 5 3 ## 4 5 4 ## 5 3 5 data$x == 5 # logical vector ## [1] FALSE FALSE TRUE TRUE FALSE data[data$x == 5, ] # select the rows with value &quot;TRUE&quot; ## # A tibble: 2 × 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 5 3 ## 2 5 4 # returning to the flights dataset base_jan1 &lt;- flights[(flights$month == 1 &amp; flights$day == 1), ] (flights$month == 1 &amp; flights$day == 1) is a logical vector indicating if the corresponding flight was on Jan 1 (TRUE if yes). Now, let’s check if jan1 and base_jan1 are the same using identical: identical(jan1, base_jan1) # TURE means they are the same, FALSE means they are not the same ## [1] TRUE More Examples Select flights that departed in Nov or Dec filter(flights, month == 11 | month == 12) # alternatively, simpler code filter(flights, month %in% c(11, 12)) How to use the operator %in%? y &lt;- c(1,3,5) x &lt;- 1 x %in% y #whether 1 is in {1,3,5} ## [1] TRUE x &lt;- c(1,3,2,4,1) x %in% y # check whether each element in x is in {1,3,5} ## [1] TRUE TRUE FALSE FALSE TRUE %in% also works with characters c(&quot;a&quot;, &quot;b&quot;) %in% c(&quot;a&quot;, &quot;c&quot;, &quot;d&quot;) ## [1] TRUE FALSE The result is TRUE FALSE because \"a\" is in c(\"a\", \"c\", \"d\") and \"b\" is not in c(\"a\", \"c\", \"d\"). Perform the same task without filter: flights[flights$month == 11 | flights$month == 12,] # or flights[flights$month %in% c(11, 12), ] Flights that were not delayed (on arrival or departure) by more than two hours: delay1 &lt;- filter(flights, arr_delay &lt;= 120, dep_delay &lt;= 120) Without using filter delay2 &lt;- flights[flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120, ] Let’s check if delay1 and delay2 are the same. identical(delay1, delay2) ## [1] FALSE The result is FALSE, meaning delay1 and delay2 are not the same. Why? Because some elements in flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120 are NA. # to find out the number of NA values sum(is.na(flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120)) ## [1] 9304 As a result, with the base subsetting method, a row with all NA values will be selected. On the other hand, for filter, when a condition evaluates to NA, the row will be dropped. Let’s create a small dataset to illustrate this. From now on, let’s try to use tibble instead of data.frame. data &lt;- tibble(x = 1:4, y = c(1, 2, NA, 4)) data[data$y &lt;= 3, ] ## # A tibble: 3 × 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 NA NA # to avoid the above problem # which() returns which elements are TRUE which(data$y &lt;= 3) # the result is 1, 2 ## [1] 1 2 data[which(data$y &lt;= 3), ] # select row 1, row 2 ## # A tibble: 2 × 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 # using filter filter(data, y &lt;= 3) ## # A tibble: 2 × 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 # if we want to include the row where the value of y is NA # recall that | means &quot;or&quot; data[which(data$y &lt;= 3 | is.na(data$y)), ] ## # A tibble: 3 × 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA # using filter filter(data, y &lt;= 3 | is.na(y)) ## # A tibble: 3 × 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA If you want to drop the NA values with base subsetting[], you may use delay3 &lt;- flights[which((flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120) == TRUE), ] To see if delay1 and delay3 are exactly the same: identical(delay1, delay3) # TRUE means exactly the same ## [1] TRUE At this point, you should see that filter could perform the same tasks with simpler code. 4.6.1 Exercises 1a. Find all flights that had an arrival delay of two or more hours (drop the rows with NA in arr_delay). # Using &quot;filter&quot; filter(flights, arr_delay &gt;= 120) # Without using &quot;filter&quot; flights[which(flights$arr_delay &gt;= 120), ] 1b. Find all flights that flew to Houston (IAH or HOU). # Using &quot;filter&quot; filter(flights, dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)) # Without using &quot;filter&quot; flights[which(flights$dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)), ] 1c. Find all flights that were operated by United, American, or Delta # find all the sorted carrier codes in the dataset sort(unique(flights$carrier)) ## [1] &quot;9E&quot; &quot;AA&quot; &quot;AS&quot; &quot;B6&quot; &quot;DL&quot; &quot;EV&quot; &quot;F9&quot; &quot;FL&quot; &quot;HA&quot; &quot;MQ&quot; &quot;OO&quot; &quot;UA&quot; &quot;US&quot; &quot;VX&quot; &quot;WN&quot; &quot;YV&quot; # look up airline names from their carrier codes airlines ## # A tibble: 16 × 2 ## carrier name ## &lt;chr&gt; &lt;chr&gt; ## 1 9E Endeavor Air Inc. ## 2 AA American Airlines Inc. ## 3 AS Alaska Airlines Inc. ## 4 B6 JetBlue Airways ## 5 DL Delta Air Lines Inc. ## 6 EV ExpressJet Airlines Inc. ## 7 F9 Frontier Airlines Inc. ## 8 FL AirTran Airways Corporation ## 9 HA Hawaiian Airlines Inc. ## 10 MQ Envoy Air ## 11 OO SkyWest Airlines Inc. ## 12 UA United Air Lines Inc. ## 13 US US Airways Inc. ## 14 VX Virgin America ## 15 WN Southwest Airlines Co. ## 16 YV Mesa Airlines Inc. # after looking up the names, we know UA = United, AA = American, DL = Delta filter(flights, carrier %in% c(&quot;UA&quot;, &quot;AA&quot;, &quot;DL&quot;)) ## # A tibble: 139,504 × 19 ## year month day dep_time sched_dep_time dep_de…¹ arr_t…² sched…³ arr_d…⁴ carrier flight tailnum origin dest air_t…⁵ dista…⁶ hour minute ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 5 2013 1 1 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## 6 2013 1 1 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD 138 733 6 0 ## 7 2013 1 1 558 600 -2 924 917 7 UA 194 N29129 JFK LAX 345 2475 6 0 ## 8 2013 1 1 558 600 -2 923 937 -14 UA 1124 N53441 EWR SFO 361 2565 6 0 ## 9 2013 1 1 559 600 -1 941 910 31 AA 707 N3DUAA LGA DFW 257 1389 6 0 ## 10 2013 1 1 559 600 -1 854 902 -8 UA 1187 N76515 EWR LAS 337 2227 6 0 ## # … with 139,494 more rows, 1 more variable: time_hour &lt;dttm&gt;, and abbreviated variable names ¹​dep_delay, ²​arr_time, ³​sched_arr_time, ## # ⁴​arr_delay, ⁵​air_time, ⁶​distance 1d. Find all flights that departed in summer (July, August, and September) filter(flights, month %in% c(7, 8, 9)) # Alternative Method filter(flights, between(month, 7, 9)) 1e. Find all flights that arrived more than two hours late, but didn’t leave late filter(flights, arr_delay &gt; 120, dep_delay &lt;= 0) The next two exercises are trickier. 1f. Find all flights that were delayed by at least an hour, but made up over 30 minutes in flight. First, the flight was delayed by at least an hour is the same as dep_delay &gt;=60. Second, if a flight made up over 30 minutes in flight, the arrival delay must be at least 30 minutes less than the departure delay, which is the same as dep_delay - arr_delay &gt; 30. filter(flights, dep_delay &gt;= 60, dep_delay - arr_delay &gt; 30) 1g. Find all flights that departed between midnight and 6 a.m. (inclusive). The first question that should come to your mind is how is midnight represented in the dataset? Let’s take a look at summary(flights$dep_time). summary(flights$dep_time) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1 907 1401 1349 1744 2400 8255 The minimum is 1 and the maximum is 2400. Therefore, you know midnight is represented by 2400 instead of 0 in this dataset. The answer to the question would be filter(flights, dep_time &lt;= 600 | dep_time == 2400) 4.7 select() Very often, we are only interested in some variables in a dataset. In that case, we can focus on the variables by creating a new dataset with those variables only. select() is to select the columns in a dataset by the name of the columns Selecting Variables Suppose you want to select the following \\(3\\) columns in flights: year, month, day: select(flights, year, month, day) ## # A tibble: 336,776 × 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # … with 336,766 more rows The usual way without using select() is flights[c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] # or flights[, c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] ## # A tibble: 336,776 × 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # … with 336,766 more rows Select all columns between year and day (inclusive) select(flights, year:day) ## # A tibble: 336,776 × 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # … with 336,766 more rows Excluding Variables Select all columns except those from year to day (inclusive) select(flights, -(year:day)) select(flights, -year, -month, -day) Without using select() flights[, !(colnames(flights) %in% c(&quot;year&quot;, &quot;day&quot;, &quot;month&quot;))] 4.7.1 Exercises You can also use starts_with(\"abc\") matches names that begin with \"abc\" ends_with(\"xyz\") matches names that end with \"xyz\" contains(\"ijk\") mathces names that contain \"ijk\" Ex: select all columns that end with \"delay\". select(flights, ends_with(&quot;delay&quot;)) ## # A tibble: 336,776 × 2 ## dep_delay arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 ## 2 4 20 ## 3 2 33 ## 4 -1 -18 ## 5 -6 -25 ## 6 -4 12 ## 7 -5 19 ## 8 -3 -14 ## 9 -3 -8 ## 10 -2 8 ## # … with 336,766 more rows Ex: select all columns that start with \"a\". select(flights, starts_with(&quot;a&quot;)) ## # A tibble: 336,776 × 3 ## arr_time arr_delay air_time ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 830 11 227 ## 2 850 20 227 ## 3 923 33 160 ## 4 1004 -18 183 ## 5 812 -25 116 ## 6 740 12 150 ## 7 913 19 158 ## 8 709 -14 53 ## 9 838 -8 140 ## 10 753 8 138 ## # … with 336,766 more rows Does the result of running the following code surprise you? select(flights, contains(&quot;TIME&quot;)) ## # A tibble: 336,776 × 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time time_hour ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 517 515 830 819 227 2013-01-01 05:00:00 ## 2 533 529 850 830 227 2013-01-01 05:00:00 ## 3 542 540 923 850 160 2013-01-01 05:00:00 ## 4 544 545 1004 1022 183 2013-01-01 05:00:00 ## 5 554 600 812 837 116 2013-01-01 06:00:00 ## 6 554 558 740 728 150 2013-01-01 05:00:00 ## 7 555 600 913 854 158 2013-01-01 06:00:00 ## 8 557 600 709 723 53 2013-01-01 06:00:00 ## 9 557 600 838 846 140 2013-01-01 06:00:00 ## 10 558 600 753 745 138 2013-01-01 06:00:00 ## # … with 336,766 more rows Yes, because we used “TIME” but not “time” and still get the selected columns. If you check ?contains, you can see that the default is to ignore the case. To change the default: select(flights, contains(&quot;TIME&quot;, ignore.case = FALSE)) ## # A tibble: 336,776 × 0 Ex: without using select(), select all the columns that contain “time”. flights[grep(&quot;time&quot;, names(flights))] ## # A tibble: 336,776 × 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time time_hour ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 517 515 830 819 227 2013-01-01 05:00:00 ## 2 533 529 850 830 227 2013-01-01 05:00:00 ## 3 542 540 923 850 160 2013-01-01 05:00:00 ## 4 544 545 1004 1022 183 2013-01-01 05:00:00 ## 5 554 600 812 837 116 2013-01-01 06:00:00 ## 6 554 558 740 728 150 2013-01-01 05:00:00 ## 7 555 600 913 854 158 2013-01-01 06:00:00 ## 8 557 600 709 723 53 2013-01-01 06:00:00 ## 9 557 600 838 846 140 2013-01-01 06:00:00 ## 10 558 600 753 745 138 2013-01-01 06:00:00 ## # … with 336,766 more rows Basic Usage of grep: grep(pattern, x). pattern: character string. e.g., “time”, “delay”, “air” x: a character vector where matches are sought. e.g., the colnames of a dataframe. Output: a vector of the indices of the elements of x that yielded a match. some_names &lt;- c(&quot;ab&quot;, &quot;bc&quot;, &quot;cd&quot;) grep(&quot;b&quot;, some_names) ## [1] 1 2 grep(&quot;d&quot;, some_names) ## [1] 3 grep(&quot;e&quot;, some_names) ## integer(0) 4.8 mutate() Add new variables with mutate() Very often, we want to create a new variable based on existing variables. For example, if we have distance and time, we can compute the speed by distance/time. mutate() always adds new columns at the end of the dataset. Let’s create a narrower dataset so that we can see the result of mutate without use View(). # create a smaller dataset flights_sml &lt;- select(flights, year:day, arr_delay, dep_delay, distance, air_time) Now, let’s use mutate() to create a variable called gain (how much time we gain in flight) defined as arr_delay - dep_delay and a variable called speed (miles/hour) defined as distance/air_time * 60. mutate(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 × 9 ## year month day arr_delay dep_delay distance air_time gain speed ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 11 2 1400 227 9 370. ## 2 2013 1 1 20 4 1416 227 16 374. ## 3 2013 1 1 33 2 1089 160 31 408. ## 4 2013 1 1 -18 -1 1576 183 -17 517. ## 5 2013 1 1 -25 -6 762 116 -19 394. ## 6 2013 1 1 12 -4 719 150 16 288. ## 7 2013 1 1 19 -5 1065 158 24 404. ## 8 2013 1 1 -14 -3 229 53 -11 259. ## 9 2013 1 1 -8 -3 944 140 -5 405. ## 10 2013 1 1 8 -2 733 138 10 319. ## # … with 336,766 more rows To keep the new variables only, use transmute(): transmute(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 × 2 ## gain speed ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9 370. ## 2 16 374. ## 3 31 408. ## 4 -17 517. ## 5 -19 394. ## 6 16 288. ## 7 24 404. ## 8 -11 259. ## 9 -5 405. ## 10 10 319. ## # … with 336,766 more rows Without using select and mutate(), one may use flights_sml2 &lt;- flights[c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;arr_delay&quot;, &quot;dep_delay&quot;, &quot;distance&quot;, &quot;air_time&quot;)] flights_sml2$gain &lt;- flights_sml2$arr_delay - flights_sml2$dep_delay flights_sml2$speed &lt;- flights_sml2$distance / flights_sml2$air_time * 60 4.8.1 Exercises Ex: Currently, dep_time and sched_dep_time are convenient to look at, but hard to compute with because they are not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Recall that dep_time and sched_dep_time are in HHMM format. For example, 1304 means 1:04pm. The number of minutes since midnight is \\(13\\times 60 + 4 = 784\\). In general, we can use the integer division to find the number of hours since midnight, then multiply by \\(60\\), and finally add the remainder for the minutes. For example, 1304 %/% 100 # get the number of hours since midnight ## [1] 13 1304 %% 100 # get the remainder ## [1] 4 1304 %/% 100 * 60 + 1304 %% 100 # number of minutes since midnight ## [1] 784 Recall that midnight is represented as 2400 in the dataset and the number of minutes since midnight should be 0. However, if we use the above method for midnight, we get 2400 %/% 100 * 60 + 2400 %% 100 # this is not correct for midnight ## [1] 1440 Therefore, we also have to deal with this case. One possible solution is to do another integer division by 1440 (24x60 = 1440): (1304 %/% 100 * 60 + 1304 %% 100) %% 1440 # will not change the result if the time is not midnight ## [1] 784 (2400 %/% 100 * 60 + 2400 %% 100) %% 1440 # this is correct for midnight ## [1] 0 Go back to flights: # let&#39;s keep only the dep_time and sched_dep_time flights_time &lt;- select(flights, dep_time, sched_dep_time) mutate(flights_time, min_dep_time = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, min_sched_dep_time = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440) ## # A tibble: 336,776 × 4 ## dep_time sched_dep_time min_dep_time min_sched_dep_time ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 515 317 315 ## 2 533 529 333 329 ## 3 542 540 342 340 ## 4 544 545 344 345 ## 5 554 600 354 360 ## 6 554 558 354 358 ## 7 555 600 355 360 ## 8 557 600 357 360 ## 9 557 600 357 360 ## 10 558 600 358 360 ## # … with 336,766 more rows The above code doesn’t look good because we have written the formula to complete the same task twice. We can define a function to do this: time_to_minutes &lt;- function(x) { (x %/% 100 * 60 + x %% 100) %% 1440 } With the function time_to_minutes, we have: mutate(flights, min_dep_time = time_to_minutes(dep_time), min_sched_dep_time = time_to_minutes(sched_dep_time)) If you cannot think of using %% 1440 to deal with the midnight cases, we can write: # ind for indicator approach time_to_minutes_ind &lt;- function(x) { (x %/% 100 * 60 + x %% 100) * (x != 2400) } # Explanation # if x is 2400, (x!=2400) is FALSE, FALSE times a number y is 0 # if x is not 2400, (x!=2400) is TRUE, TRUE times a number y is y # because &quot;FALSE=0, TRUE=1&quot; mutate(flights, min_dep_time = time_to_minutes_ind(dep_time), min_sched_dep_time = time_to_minutes_ind(sched_dep_time)) An alternative way is to use ifelse (which is a vectorized function). Usage of ifelse: ifelse(test, yes, no) time_to_minutes_ifelse &lt;- function(x) { ifelse(x != 2400, x %/% 100 * 60 + x %% 100, 0) } mutate(flights, min_dep_time = time_to_minutes_ifelse(dep_time), min_sched_dep_time = time_to_minutes_ifelse(sched_dep_time)) ## # A tibble: 336,776 × 21 ## year month day dep_time sched_dep_time dep_de…¹ arr_t…² sched…³ arr_d…⁴ carrier flight tailnum origin dest air_t…⁵ dista…⁶ hour minute ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6 2013 1 1 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## 7 2013 1 1 555 600 -5 913 854 19 B6 507 N516JB EWR FLL 158 1065 6 0 ## 8 2013 1 1 557 600 -3 709 723 -14 EV 5708 N829AS LGA IAD 53 229 6 0 ## 9 2013 1 1 557 600 -3 838 846 -8 B6 79 N593JB JFK MCO 140 944 6 0 ## 10 2013 1 1 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD 138 733 6 0 ## # … with 336,766 more rows, 3 more variables: time_hour &lt;dttm&gt;, min_dep_time &lt;dbl&gt;, min_sched_dep_time &lt;dbl&gt;, and abbreviated variable names ## # ¹​dep_delay, ²​arr_time, ³​sched_arr_time, ⁴​arr_delay, ⁵​air_time, ⁶​distance A less efficient way with for loop and if-else: # if for if-else approach time_to_minutes_if &lt;- function(x) { n &lt;- length(x) output &lt;- rep(0, n) for (i in 1:n) { if (is.na(x[i])){ # check for NA output[i] &lt;- NA } else if (x[i] != 2400) { # if not equal to 2400 output[i] &lt;- x[i] %/% 100 * 60 + x[i] %% 100 } else { # if equal to 2400 output[i] &lt;- 0 } } return(output) } mutate(flights, min_dep_time = time_to_minutes_if(dep_time), min_sched_dep_time = time_to_minutes_if(sched_dep_time)) Compare the efficiency: system.time(mutate(flights, min_dep_time = time_to_minutes(dep_time), min_sched_dep_time = time_to_minutes(sched_dep_time))) ## user system elapsed ## 0.03 0.00 0.04 system.time(mutate(flights, min_dep_time = time_to_minutes_if(dep_time), min_sched_dep_time = time_to_minutes_if(sched_dep_time))) ## user system elapsed ## 0.08 0.00 0.19 4.9 summarize(), group_by() Average delay over the year (not useful): summarize(flights, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 × 1 ## delay ## &lt;dbl&gt; ## 1 12.6 Using summarize with group_by and ungroup can result in more useful statistics. Use group_by. First argument is your dataset. Subsequent arguments indicate how you want to group the data. In summarize, the dataset becomes the dataset from group_by. After performing the calculation using summarize, use ungroup. If you do not use ungroup after group_by, the grouping structure is still retained in the object that you create and the subsequent calculation may yield something that you do not expect. E.g.: Average departure delay per date (more useful): by_day &lt;- group_by(flights, year, month, day) mean_delay &lt;- summarize(by_day, delay = mean(dep_delay, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;year&#39;, &#39;month&#39;. You can override using the `.groups` argument. (mean_delay &lt;- ungroup(mean_delay)) # use ungroup() after gropu_by() ## # A tibble: 365 × 4 ## year month day delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.5 ## 2 2013 1 2 13.9 ## 3 2013 1 3 11.0 ## 4 2013 1 4 8.95 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.55 ## 9 2013 1 9 2.28 ## 10 2013 1 10 2.84 ## # … with 355 more rows 4.10 Combining Multiple Operations with Pipe %&gt;% You can use the shortcut Ctrl/Cmd + Shift + M to insert the pipe operator %&gt;%. x %&gt;% f(y) turns into f(x,y) For example, flights %&gt;% filter(month == 1, day == 1) is the same as filter(flights, month == 1, day == 1) x %&gt;% f(y) %&gt;% g(z) turns into g(f(x,y),z). For example, flights %&gt;% group_by(year, month) %&gt;% summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() is the same as ungroup(summarize(group_by(flights, year, month), mean_dep_delay = mean(dep_delay, na.rm = TRUE))) Using the pipe can avoid creating and naming intermediate objects that we don’t need. Instead, the pipe focuses on the sequence of actions, not the object that the actions being performed on. It also tends to make the code easier to read. For the above example, you can read it as: for the dataset flights, we first group the data by year and month, then summarize the data by finding the mean. We can think of the pipe %&gt;% as “then”. Pipe %&gt;% can also be used with ggplot: flights %&gt;% group_by(year, month) %&gt;% summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(month), y = mean_dep_delay)) + geom_col() # notice the spacing for good indentation Notice the difference between %&gt;% and + in the above code. E.g. Create new columns for the average departure delay by date and average departure delay by destination using mutate group_date &lt;- group_by(select(flights, year:day, dep_delay, dest), month, day) flights2 &lt;- mutate(group_date, avg_date_dep_delay = mean(dep_delay, na.rm = TRUE)) group_dest &lt;- group_by(flights2, dest) (flights3 &lt;- ungroup(mutate(group_dest, avg_dest_dep_delay = mean(dep_delay, na.rm = TRUE)))) ## # A tibble: 336,776 × 7 ## year month day dep_delay dest avg_date_dep_delay avg_dest_dep_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 IAH 11.5 10.8 ## 2 2013 1 1 4 IAH 11.5 10.8 ## 3 2013 1 1 2 MIA 11.5 8.88 ## 4 2013 1 1 -1 BQN 11.5 12.4 ## 5 2013 1 1 -6 ATL 11.5 12.5 ## 6 2013 1 1 -4 ORD 11.5 13.6 ## 7 2013 1 1 -5 FLL 11.5 12.7 ## 8 2013 1 1 -3 IAD 11.5 17.0 ## 9 2013 1 1 -3 MCO 11.5 11.3 ## 10 2013 1 1 -2 ORD 11.5 13.6 ## # … with 336,766 more rows flights %&gt;% select(year:day, dep_delay, dest) %&gt;% group_by(month, day) %&gt;% mutate(avg_date_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% group_by(dest) %&gt;% # By default, group_by() overrides existing grouping mutate(avg_dest_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() ## # A tibble: 336,776 × 7 ## year month day dep_delay dest avg_date_dep_delay avg_dest_dep_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 IAH 11.5 10.8 ## 2 2013 1 1 4 IAH 11.5 10.8 ## 3 2013 1 1 2 MIA 11.5 8.88 ## 4 2013 1 1 -1 BQN 11.5 12.4 ## 5 2013 1 1 -6 ATL 11.5 12.5 ## 6 2013 1 1 -4 ORD 11.5 13.6 ## 7 2013 1 1 -5 FLL 11.5 12.7 ## 8 2013 1 1 -3 IAD 11.5 17.0 ## 9 2013 1 1 -3 MCO 11.5 11.3 ## 10 2013 1 1 -2 ORD 11.5 13.6 ## # … with 336,766 more rows Note: if you have to manipulate some intermediate objects, it may make sense not to use the pipe operator in that situation. More examples: Average delay per month: by_month &lt;- group_by(flights, month) ungroup(summarize(by_month, delay = mean(dep_delay, na.rm = TRUE))) ## # A tibble: 12 × 2 ## month delay ## &lt;int&gt; &lt;dbl&gt; ## 1 1 10.0 ## 2 2 10.8 ## 3 3 13.2 ## 4 4 13.9 ## 5 5 13.0 ## 6 6 20.8 ## 7 7 21.7 ## 8 8 12.6 ## 9 9 6.72 ## 10 10 6.24 ## 11 11 5.44 ## 12 12 16.6 flights %&gt;% group_by(month) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() # a good habit is to use ungroup after the calculations ## # A tibble: 12 × 2 ## month delay ## &lt;int&gt; &lt;dbl&gt; ## 1 1 10.0 ## 2 2 10.8 ## 3 3 13.2 ## 4 4 13.9 ## 5 5 13.0 ## 6 6 20.8 ## 7 7 21.7 ## 8 8 12.6 ## 9 9 6.72 ## 10 10 6.24 ## 11 11 5.44 ## 12 12 16.6 Ex: Find the average weights of the cars grouped by the number of gears. mtcars_gear &lt;- group_by(mtcars, gear) ungroup(summarize(mtcars_gear, avg_wt = mean(wt, na.rm = TRUE))) ## # A tibble: 3 × 2 ## gear avg_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3.89 ## 2 4 2.62 ## 3 5 2.63 mtcars %&gt;% group_by(gear) %&gt;% summarize(avg_wt = mean(wt, na.rm = TRUE)) %&gt;% ungroup() ## # A tibble: 3 × 2 ## gear avg_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3.89 ## 2 4 2.62 ## 3 5 2.63 Ex: Find the sample standard deviation of the weights of the cars grouped by the number of gears. mtcars %&gt;% group_by(gear) %&gt;% summarize(sd_wt = sd(wt, na.rm = TRUE)) %&gt;% ungroup() ## # A tibble: 3 × 2 ## gear sd_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 0.833 ## 2 4 0.633 ## 3 5 0.819 4.11 Summary Functions and operators learned in this chapter: arrange, filter, identical, %in%, tibble, ifelse, mutate, transmute, select, grep, group_by, summarize, ungroup, %&gt;% "],["creating-some-basic-plots.html", "Chapter 5 Creating Some Basic Plots 5.1 Scatter Plot 5.2 Line Graph 5.3 Bar Chart 5.4 Histogram 5.5 Box Plot 5.6 Plotting a function curve 5.7 More on plots with base R", " Chapter 5 Creating Some Basic Plots Reference: R graphics cookbook (https://r-graphics.org/) The base R contains many basic methods for producing graphics. We will learn some of them in this chapter. These plotting functions are good for very quick exploration of data. For more elegant plots, we will use the package ggplot2 (next chapter). We will use some simple datasets in base R to illustrate how to create some basic plots in this chapter. 5.1 Scatter Plot Let’s take a look at the mtcars dataset. This dataset comes with base R. head(mtcars) # this is a data frame ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) # display the structure of the data frame ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... mpg: miles/gallon wt: weight (1000lbs) Scatter plot with base graphics # x-axis: mtcars$wt # y-axis: mtcars$mpg plot(x = mtcars$wt, y = mtcars$mpg) # &quot;x =&quot;, &quot;y =&quot; are optional You can produce the same plot with plot(mtcars$wt, mtcars$mpg) Scatter plot with base graphics (when you only have \\(x\\) and \\(y\\) but not a dataset) set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 2 * x + rnorm(100, 0, 1) plot(x, y) 5.2 Line Graph The dataset pressure (also in base R) contains the relation between temperature in degrees Celsius and vapor pressure of mercury in millimeters (of mercury). Line graph with base graphics # the only difference from a scatter plot is that we add type=&quot;l&quot; plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) #l = line Line graph with base graphics with points plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) points(pressure$temperature, pressure$pressure) # add some points Line graph with base graphics with another line and points (with color) plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) points(pressure$temperature, pressure$pressure) # the additional line may not have a physical meaningful # just an illustration on how to add a line with base graphics lines(pressure$temperature, pressure$pressure / 2, col = &quot;red&quot;) points(pressure$temperature, pressure$pressure / 2, col = &quot;red&quot;) Colors in R You can go to https://www.r-graph-gallery.com/ggplot2-color.html and read more about colors in R. For example, you can specify the color by name, rgb, number and hex code. plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;, col = rgb(0.1, 0.2, 0.5, 1)) lines(pressure$temperature, pressure$pressure / 2, type = &quot;l&quot;, col = 2) lines(pressure$temperature, pressure$pressure * 2, col = &quot;#8B2813&quot;) lines(pressure$temperature, pressure$pressure * 3, col = &quot;cornflowerblue&quot;) 5.3 Bar Chart Two types of bar charts: Bar chart of values. x-axis: discrete variable, y-axis: numeric data (not necessarily count data) Bar chart of count. x-axis: discrete variable, y-axis: count of cases in the discrete variable Remark: for histogram: x-axis = continuous variable, y-axis = count of cases in the interval. The BOD data set has 6 rows and 2 columns giving the biochemical oxygen demand versus time in an evaluation of water quality. str(BOD) ## &#39;data.frame&#39;: 6 obs. of 2 variables: ## $ Time : num 1 2 3 4 5 7 ## $ demand: num 8.3 10.3 19 16 15.6 19.8 ## - attr(*, &quot;reference&quot;)= chr &quot;A1.4, p. 270&quot; Bar chart of values with base graphics # names.arg = a vector of names to be plotted below each bar or group of bars. barplot(BOD$demand, names.arg = BOD$Time) Bar chart of counts with base graphics In the dateset mtcars, cylis the number of cylinders in the car. The possible values are \\(4, 6\\), and \\(8\\). We first find the count of each unique value in mtcars$cyl: table(mtcars$cyl) ## ## 4 6 8 ## 11 7 14 To plot the bar chart, we use barplot(table(mtcars$cyl)) 5.4 Histogram mpg in mtcars is the miles/gallon of the car. It is a continuous variable. Histogram with base graphics hist(mtcars$mpg) Histogram with base graphics # Specify approximate number of bins with &quot;breaks&quot; hist(mtcars$mpg, breaks = 10) Remark: different bin widths will give you histograms with different looks. 5.5 Box Plot A box plot (or box-and-whisker plot) display the dateset based on the five-number summary (minimum, maximum, sample median, first (\\(Q_1\\)) and third (\\(Q_3\\)) quartiles). IQR = Interquartile range = \\(Q_3 - Q_1\\) A box plot usually includes two parts, a box and a set of whiskers. The box is drawn from the first quartile to third quartile with a horizontal line drawn in the middle denoting the median. The boundaries of the whiskers is often based on the 1.5 IQR value. From above \\(Q_3\\), a distance of \\(1.5\\) times the IQR is measured out and a whisker is drawn up to the largest observed data point from the dataset that falls within this distance. Similar for the lower one. All other data points outside the buondary of the whiskers are plotted as outliers. Let’s take a look at another dataset ToothGrowth. In particular, supp is a factor. str(ToothGrowth) ## &#39;data.frame&#39;: 60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: num 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ... Box plot with basic graphics (using plot) # if x is a factor, use the following code plot(x = ToothGrowth$supp, y = ToothGrowth$len) Box plot with basic graphics (using boxplot) # len ~ supp is an example of a &quot;formula&quot; (y ~ x) boxplot(len ~ supp, data = ToothGrowth) Box plot with basic graphics + interaction of two variables on x-axis (using boxplot) boxplot(len ~ supp + dose, data = ToothGrowth) 5.6 Plotting a function curve curve(x^3 - 5 * x, from = -4, to = 4) Alternatively: x &lt;- seq(-4, 4, len = 1000) plot(x, x^3 - 5 * x, type = &quot;l&quot;) Plotting a built-in function curve(dnorm(x), from = -4, to = 4) Plotting a self-defined function my_function &lt;- function(x) { 1 / (1 + exp(-x + 10)) } curve(my_function, from = 0, to = 20) Plotting a function with additional arguments curve(dnorm(x, mean = 2, sd = 3), from = -4, to = 4) 5.7 More on plots with base R 5.7.1 Multi-frame plot To create a 3x2 multi-frame plot. Use par(mfrow = c(3, 2)). set.seed(1) x &lt;- rnorm(100, 50, 5) y &lt;- x + rnorm(100, 2, 2) # create a 2x2 multi-frame plot par(mfrow=c(2, 2)) hist(x) hist(y,breaks = 10) plot(x, y) boxplot(x, y) 5.7.2 Type of Plot Option Type type = \"p\" Points (default) type = \"l\" Lines connecting the data points type = \"b\" Points and non-overlapping lines type = \"h\" Height lines type = \"o\" Points and overlapping lines par(mfrow=c(3, 2)) x &lt;- -5:5 y &lt;- x^2 plot(x, y) plot(x, y, type = &quot;p&quot;) plot(x, y, type = &quot;l&quot;) plot(x, y, type = &quot;b&quot;) plot(x, y, type = &quot;h&quot;) plot(x, y, type = &quot;o&quot;) 5.7.3 Parameters of a plot Parameter Meaning type See Type of Plot main Title sub Subtitle xlab x-axis label ylab y-axis label xlim x-axis range ylim y-axis range pch Symbol of data points col Color of data points lty Type of the line To illustrate some of the components: set.seed(1) x &lt;- rnorm(100, 50, 15) y &lt;- x + rnorm(100, 2, 13) plot(x, y, col = &quot;red&quot;, pch = 15, main = &quot;This is the title&quot;, xlim = c(0, 100), ylim = c(0,100), xlab = &quot;name of x-axis&quot;, ylab = &quot;name of y-axis&quot;) 5.7.4 Elements on plot Function Description abline(c, m) plot the line y = mx +c abline(h = a) plot the line y = a abline(v = b) plot the line x = b lines(x, y) line joining points with coordinates (x,y) set.seed(1) x &lt;- rnorm(100, 50, 15) y &lt;- x + rnorm(100, 2, 13) plot(x, y) # connect the points (20, 20), (30, 80), (40, 40) with a line lines(x = c(20, 30, 40),y = c(20, 80, 40), col = &quot;red&quot;) abline(v = 60, col = &quot;blue&quot;) "],["data-visualization-with-ggplot2.html", "Chapter 6 Data Visualization with ggplot2 6.1 Bar charts 6.2 Line Graph 6.3 Scatter Plots 6.4 Summarizing Data Distributions 6.5 Saving your plots 6.6 Axes, appearance 6.7 Summary", " Chapter 6 Data Visualization with ggplot2 Main reference for this chapter: R graphics cookbook (https://r-graphics.org/) In the previous chapter, we learned how to create some basic plots with base R. In this chapter, we will see how to use ggplot for data visualization. We will use some of the datasets from the the package gcookbook. Therefore, we will install it now. install.packages(&quot;gcookbook&quot;) Load the packages gcookbook, tidyverse and nycflights13. library(gcookbook) # contains some datasets for illustration library(tidyverse) # contains ggplot2 and dplyr library(nycflights13) # contains the dataset &quot;flights&quot; 6.1 Bar charts We will start with bar charts. Many of the usages discussed in this section can also be transferable to create other plots. Recall that there are two types of bar charts: Bar chart of values. x-axis: discrete variable, y-axis: numeric data (not necessarily count data) Bar chart of counts. x-axis: discrete variable, y-axis: count of cases in the discrete variable Using ggplot: For bar chart of values, we use geom_col(), which is the same as using geom_bar(stat = \"identity\"). For bar chart of counts, we use geom_bar(), which is the same as using geom_bar(stat = \"count\"). That is, the default for geom_bar() is to use stat = \"count\". Bar chart of values: pg_mean is a simple dataset with groupwise means of some plant growth data. pg_mean ## group weight ## 1 ctrl 5.032 ## 2 trt1 4.661 ## 3 trt2 5.526 ggplot(data = pg_mean, mapping = aes(x = group, y = weight)) + geom_col() Recall the mtcars dataset. Let’s create a bar chart of values for the mean weights grouped by the number of gears. First, we summarize the data using summarize. by_gear &lt;- group_by(mtcars, gear) mtcars_wt &lt;- summarize(by_gear, mean_wt_by_gear = mean(wt)) # Alternatively, using %&gt;% mtcars_wt &lt;- mtcars %&gt;% group_by(gear) %&gt;% summarize(mean_wt_by_gear = mean(wt)) Create the bar chart: ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col() To change the colour of the bars, use fill. ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(fill = &quot;lightblue&quot;) By default, there is no outline around the fill. To add an outline, use colour (or color). ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(color = &quot;red&quot;) Of course, you can combine the two settings: ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(fill = &quot;lightblue&quot;, color = &quot;red&quot;) Graph with grouped bars The most basic bar chart of values have one categorical variable on the x-axis and one continuous variable on the y-axis. If you want to include another categorical variable to divide up the data, you can use a graph with grouped bars. In mtcars, vs represents the engine of the car with 0 = V-shaped and 1 = straight. We can use vc to divide up the data in addition to gear using fill. To create a grouped bar chart, set position = \"dodge\" in geom_col(); otherwise, you will get a stacked bar chart. # prepare the data by_gear_vs &lt;- group_by(mtcars, gear, vs) mtcars_wt2 &lt;- summarize(by_gear_vs, mean_wt = mean(wt)) # convert to factor in the data mtcars_wt2$vs &lt;- as.factor(mtcars_wt2$vs) # Alternatively, using %&gt;% mtcars_wt2 &lt;- mtcars %&gt;% group_by(gear, vs) %&gt;% summarize(mean_wt = mean(wt)) %&gt;% ungroup() %&gt;% mutate(vs = as.factor(vs)) # plot ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) Without position = \"dodge\", we get a stacked bar chart: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col() You can also convert vs to factor in call to ggplot(): # prepare the data by_gear_vs &lt;- group_by(mtcars, gear, vs) mtcars_wt3 &lt;- summarize(by_gear_vs, mean_wt = mean(wt)) # plot ggplot(mtcars_wt3, aes(x = gear, y = mean_wt, fill = factor(vs))) + geom_col(position = &quot;dodge&quot;) To change the colours of the bars: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Pastel2&quot;) You can try with different palettes: library(RColorBrewer) display.brewer.all() Using palette = \"Oranges\": ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Oranges&quot;) Using a manually defined palette: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_manual(values = c(&quot;#cc6666&quot;, &quot;#66cccc&quot;)) Bar Charts of Counts Creating a bar chart of counts is very similar to creating a bar chart of values. Bar chart of the number of cars by gear in mtcars: ggplot(mtcars, aes(x = gear)) + geom_bar() Bar chart of the number of flights by each month in nycflights13: flights$month &lt;- as.factor(flights$month) ggplot(flights, aes(x = month)) + geom_bar(fill = &quot;lightblue&quot;) Controlling the width (by default, width = 0.9): ggplot(flights, aes(x = month)) + geom_bar(fill = &quot;lightblue&quot;, width = 0.5) Bar chart of the number of flights by origin and month: ggplot(flights, aes(x = origin, fill = month)) + geom_bar(position = &quot;dodge&quot;, color = &quot;black&quot;) 6.2 Line Graph Suppose you want to make a line graph of the daily average departure delay in flights. From now on, we will use %&gt;% whenever it is appropriate. avg_delay &lt;- flights %&gt;% group_by(month, day) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(Time = 1:365) ggplot(avg_delay, aes(x = Time, y = delay)) + geom_line() Labeling the graph: # notice how we put each argument on its own line when the arguments # do not all fit on one line ggplot(avg_delay, aes(x=Time, y=delay)) + geom_line() + labs( y = &quot;Average Delay&quot;, title = &quot;Daily Average Departure Delay of Flights from NYC in 2013&quot; ) By default, the range of the y-axis of a line graph is just enough to include all the y values in the data. Sometimes, you may want to change the range manually. For example, the range of the y-axis in the following graph does not include 0. ggplot(BOD, aes(x = Time, y = demand)) + geom_line() If you want to include 0 in the y range, you can use ylim: ggplot(BOD, aes(x = Time, y = demand)) + geom_line() + ylim(0, max(BOD$demand)) Line Graph with multiple lines Suppose we want to create a line graph showing the daily average departure delay from the 3 airports in flights. # prepare the data flights_delay &lt;- flights %&gt;% group_by(month, origin) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() Line Graph: ggplot(flights_delay, aes(x = month, y = delay, color = origin)) + geom_line() With different line types: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin)) + geom_line() Add the points on top of the lines: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin)) + geom_line() + geom_point() Change the point shapes according to origin: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin, shape = origin)) + geom_line() + geom_point() To use one single shape for the points, we can specify the shape in geom_point(). The default shape is shape = 16. The default size is size = 2. fill is only applicable for shape = 21 to 25. ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin)) + geom_line() + geom_point(shape = 22, size = 3, fill = &quot;white&quot;, color = &quot;darkred&quot;) Using another colour palette and changing the size of the lines: ggplot(flights_delay, aes(x = month, y = delay, color = origin)) + geom_line(size = 2) + geom_point(shape = 22, size = 3, fill = &quot;white&quot;, color = &quot;darkred&quot;)+ scale_colour_brewer(palette = &quot;Set2&quot;) 6.3 Scatter Plots Scatter plots are often used to visualize the relationship between two continuous variables. It is also possible to use a scatter plot when either or both variables are discrete. The dataset heightweight contains sex, age, height and weight of some schoolchildren. head(heightweight) ## sex ageYear ageMonth heightIn weightLb ## 1 f 11.92 143 56.3 85.0 ## 2 f 12.92 155 62.3 105.0 ## 3 f 12.75 153 63.3 108.0 ## 4 f 13.42 161 59.0 92.0 ## 5 f 15.92 191 62.5 112.5 ## 6 f 14.25 171 62.5 112.0 To create a basic scatter plot, use geom_point(): ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point() You can control the shape, size, and color of the points as illustrated in the last section. ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point(size = 1.5, shape = 4, color = &quot;blue&quot;) If shape = 21-25, you can control the color in the points and outline of the points using fill and color, respectively. ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point(size = 1.5, shape = 22, fill = &quot;red&quot;, color = &quot;blue&quot;) Visualizing an additional discrete variable Suppose you want to use different colours for the points according to different categories of sex. ggplot(heightweight, aes(x = ageYear, y = heightIn, color = sex)) + geom_point() Suppose you want to use different shapes for the points according to different categories of sex. ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex)) + geom_point() You can use colours and shapes at the same time: ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, color = sex)) + geom_point() You can change the shapes or colours manually: ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, color = sex)) + geom_point() + scale_shape_manual(values = c(21,22)) + scale_colour_brewer(palette = &quot;Set2&quot;) Visualizing an additional continuous variable You may map an additional continuous variable to color. ggplot(heightweight, aes(x = ageYear, y = heightIn, color = weightLb)) + geom_point() Visualizing two additional discrete variables Let’s create a new column to indicate if the child weights &lt; 100 or &gt;= 100 pounds (this is a discrete variable). heightweight2 &lt;- heightweight %&gt;% mutate(weightgroup = ifelse(weightLb &lt; 100, &quot;&lt; 100&quot;, &quot;&gt;= 100&quot;)) Now, we can add both sex and weightgroup in the plot in the following way: ggplot(heightweight2, aes(x = ageYear, y = heightIn, shape = sex, fill = weightgroup)) + geom_point() + scale_shape_manual(values = c(21, 24)) + scale_fill_manual( values = c(&quot;red&quot;, &quot;black&quot;), guide = guide_legend(override.aes = list(shape = 21)) # to change the legend ) Changing the mark ticks, limits and labels of the x-axis and y-axis: ggplot(heightweight2, aes(x = ageYear, y = heightIn, shape = sex, fill = weightgroup)) + geom_point() + scale_shape_manual(values = c(21, 24)) + scale_fill_manual( values = c(&quot;red&quot;, &quot;black&quot;), guide = guide_legend(override.aes = list(shape = 21)) # to change the legend ) + scale_x_continuous(name = &quot;Age (Year)&quot;, breaks = 11:18, limits = c(11, 18)) + scale_y_continuous(name = &quot;Height (In)&quot;, breaks = seq(50, 70, 5), limits = c(50, 73)) 6.3.1 Overplotting Overplotting refers to the situation when you have a large dataset so that the points in a scatter plot overlap and obscure each other. # We can create a variable to store the &quot;ggplot&quot; diamonds_ggplot &lt;- ggplot(diamonds, aes(x = carat, y = price)) diamonds_ggplot + geom_point() Possible solutions for overplotting: Use smaller points (size) # with diamonds_ggplot, we do not have to type # ggplot(diamonds, aes(x = carat, y = price)) diamonds_ggplot + geom_point(size = 0.1) Make the points semitransparent (alpha) diamonds_ggplot + geom_point(alpha = 0.05, size = 0.1) # 0.05 = 95% transparent We can see some vertical bands at some values of carats, meaning that diamonds tend to be cut to those sizes. Bin the data into rectangles (stat_bin2d) bins controls the number of bins in the x and y directions. The color of the rectangle indicates how many data points there are in the region. # by default, bins = 30 diamonds_ggplot + stat_bin2d(bins = 3) With bins = 50: diamonds_ggplot + stat_bin2d(bins = 50) + scale_fill_gradient(low = &quot;lightblue&quot;, high = &quot;red&quot;) Overplotting can also occur when the data is discrete on one or both axes. In the following example, we use the dataset ChickWeight, where Time is a discrete variable. head(ChickWeight) ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 # create a base plot cw_ggplot &lt;- ggplot(ChickWeight, aes(x = Time, y = weight)) cw_ggplot + geom_point() You may randomly jitter the points: cw_ggplot + geom_point(position = &quot;jitter&quot;) Jittering the points means a small amount of random variation is added to the location of each point. If you only want to jitter in the x-direction: cw_ggplot + geom_point(position = position_jitter(width = 0.5, height = 0)) 6.3.2 Labelling points in a scatter plot We can use annotate() or geom_text_repel() to label points in a scatter plot. For the latter, we have to install the package ggrepel. We will use the countries dataset in the package gcookbook and visualize the relationship between health expenditures and infant mortality rate. We will consider a subset of data by focusing the data from 2009 and countries with more than \\(2,000\\) USD health expenditures per capita: countries_subset &lt;- countries %&gt;% filter(Year == 2009, healthexp &gt; 2000) Using annotate: # find out the x and y coordinates for the point corresponding to Canada canada_x &lt;- filter(countries_subset, Name == &quot;Canada&quot;)$healthexp canada_y &lt;- filter(countries_subset, Name == &quot;Canada&quot;)$infmortality ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + annotate(&quot;text&quot;, x = canada_x, y = canada_y + 0.2, label = &quot;Canada&quot;) # + 0.2 is to avoid the label placing on top of the point Label all the points with geom_text_repel: # to use geom_text_repel, load the package ggrepel library(ggrepel) ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + geom_text_repel(aes(label = Name), size = 3) Label all the points with geom_label_repel (with a box around the label): # geom_label_repel also depends on the package ggrepel ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + geom_label_repel(aes(label = Name), size = 3) 6.4 Summarizing Data Distributions 6.4.1 Histogram Histogram can be used to visualize the distribution of a variable. We will illustrate how to create histograms using the dataset birthwt from the package MASS. library(MASS) birthwt contains data of 189 birth weights with some covariates of the mothers. Take a look at the dataset: head(birthwt) ## low age lwt race smoke ptl ht ui ftv bwt ## 85 0 19 182 2 0 0 0 1 0 2523 ## 86 0 33 155 3 0 0 0 0 3 2551 ## 87 0 20 105 1 1 0 0 0 1 2557 ## 88 0 21 108 1 1 0 0 1 2 2594 ## 89 0 18 107 1 1 0 0 1 0 2600 ## 91 0 21 124 3 0 0 0 0 0 2622 Basic histogram: ggplot(birthwt, aes(x=bwt)) + geom_histogram() Plot a histogram with density (not frequency): ggplot(birthwt, aes(x=bwt)) + geom_histogram(aes(y = ..density..)) To compare two histograms Use facet_grid() to display two histograms in the same plot. Suppose we group the data according to the smoking status during pregnancy and we want to display the two histograms of the birth weight: ggplot(birthwt, aes(x = bwt)) + geom_histogram() + facet_grid(smoke ~ .) To change the label, we can change the content of the variable: # create another dataset birthwt_mod &lt;- birthwt birthwt_mod$smoke &lt;- ifelse(birthwt_mod$smoke == 1, &quot;Smoke&quot;, &quot;No Smoke&quot;) ggplot(birthwt_mod, aes(x = bwt)) + geom_histogram() + facet_grid(smoke ~ .) Alternatively, we can use recode_factor: birthwt_mod$smoke &lt;- recode_factor(birthwt_mod$smoke, &quot;0&quot; = &quot;No Smoke&quot;, &quot;1&quot; = &quot;Smoke&quot;) Use fill() to put two groups in the same plot with different colors. We need to set position = \"identity\"; otherwise, the bars will be stacked on top of each other vertically which is not what we want. ggplot(birthwt_mod, aes(x=bwt, fill=smoke)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) #+ # facet_grid(race~., scales=&quot;free&quot;) It is also possible to use both facet_grid and fill when we have want to group the data with two discrete variables. We will illustrate this with grouping according to the smoking status and the race. We also add scales = \"free\" so that the ranges of the y-axes will be adjusted according to the data in each histogram. # change the name so that the labels can be understood easily birthwt_mod$race[which(birthwt_mod$race==1)] = &quot;White&quot; birthwt_mod$race[which(birthwt_mod$race==2)] = &quot;Black&quot; birthwt_mod$race[which(birthwt_mod$race==3)] = &quot;Other&quot; ggplot(birthwt_mod, aes(x = bwt, fill = smoke)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) + facet_grid(race ~ ., scales = &quot;free&quot;) Note: we do not have a large dataset in this example so that grouping by two variables may not give us a very good understanding of the data. 6.4.2 Kernel Density Estimate Kernel density estimation is a nonparametric method to estimate the density of the samples. Nonparametric method means we do not impose a parametric model. A parametric model has a finite dimensional parameter \\(\\theta \\in \\mathbb{R}^d\\) for some finite \\(d\\). Let \\(X_1,\\ldots,X_n\\) be i.i.d. random variables from some distribution with density \\(f\\). The histogram for \\(f\\) at point \\(x_0\\) is \\[\\begin{equation*} \\hat{f}(x_0) = \\frac{\\text{number of $x_i$ in the bin containing $x_0$}}{n h}, \\end{equation*}\\] where the bin width is \\(h\\). As we already know, the histogram will not give a smooth estimate of the density. One may use another method called kernel density estimator, which could produce smooth estimate of the density. The kernel density estimator is \\[\\begin{equation*} \\hat{f}_n(x_0) = \\frac{1}{nh}\\sum^n_{i=1} K \\bigg( \\frac{x_0 - x_i}{h} \\bigg), \\end{equation*}\\] where \\(K\\) is a kernel and \\(h\\) is the bandwidth. For our purposes, a kernel is a non-negative symmetric function such that \\(\\int^\\infty_{-\\infty}K(x)dx = 1\\) and \\(\\int^\\infty_{-\\infty} x K(x)dx =0\\). For example, \\[\\begin{eqnarray*} \\text{the boxcar kernel:} &amp;&amp; K(x) = \\frac{1}{2}I(|x| \\leq 1)\\\\ \\text{the Gaussian kernel:} &amp;&amp; K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\\\ \\text{the Epanechnikov kernel:} &amp;&amp; K(x) = \\frac{3}{4}(1-x^2)I(|x| \\leq 1) \\\\ \\text{the tricube kernel:} &amp;&amp; K(x) = \\frac{70}{81}(1-|x|^3)^3I(|x| \\leq 1), \\end{eqnarray*}\\] where \\(I(|x| \\leq 1) = 1\\) if \\(|x| \\leq 1\\) and equals \\(0\\) otherwise. Since the kernel is symmetric around \\(0\\), the magnitude \\((x-x_i)/h\\) is the distance from \\(0\\). For the above kernels, the value of the kernels is smaller when we evaluate at a point further from \\(0\\). Therefore, data close to \\(x_0\\) will contribute larger weights in estimating \\(\\hat{f}(x_0)\\). The bandwidth will control the smoothness of the estimate: larger bandwidth will result in a smoother curve and smaller bandwidth will result in a noisy and rough curve. We can create a kernel density estimate of the distribution using geom_density(). ggplot(birthwt, aes(x = bwt)) + geom_density() + geom_density(adjust = 0.25, color = &quot;red&quot;) + # smaller bandwidth -&gt; noisy geom_density(adjust = 2, color = &quot;blue&quot;) # large bandwidth -&gt; smoother Overlaying a density curve with a histogram ggplot(birthwt, aes(x = bwt)) + geom_histogram(fill = &quot;cornsilk&quot;, aes(y = ..density..)) + geom_density() Displaying kernel density Estimates from grouped data To use geom_density() to display kernel density estimates from grouped data, the grouping variable must be a factor or a character vector. Recall that in birthwt_mod that we created earlier, the smoke variable is a character vector. With color: ggplot(birthwt_mod, aes(x = bwt, color = smoke)) + geom_density() With fill: ggplot(birthwt_mod, aes(x = bwt, fill = smoke)) + geom_density(alpha = 0.3) # to control the transparency With facet_grid(): ggplot(birthwt_mod, aes(x = bwt)) + geom_density() + facet_grid(smoke ~ .) 6.5 Saving your plots There are two types of image files: vector and raster (bitmap) Raster images are pixel-based. When you zoom in the image, you can see the individual pixels. Two examples are JPG and PNG files. JPG files’ quality is lower than that of the PNG files. Vector images are constructed using mathematical formulas. You can resize the image without a loss in image quality. When you zoom in the image, it is still smooth and clear. Two examples are AI and PDF files. 6.5.1 Outputting to pdf vector files Suppose you want to save the plot from the following code: ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() # first argument is the file name # width and height are in inches pdf(&quot;filename.pdf&quot;, width = 4, height = 4) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() dev.off() Outputting to a pdf file: usually the best option usually smaller than bitmap files such as PNG files. when you have overplotting (many points on the plot), a PDF file can be much larger than a PNG file. 6.5.2 Outputting to bitmap files # width and heights are in pixels png(&quot;png_plot.png&quot;, width = 600, height = 600) ggplot(mtcars, aes(x=wt,y=mpg)) + geom_point() dev.off() For high-quality print output, it is recommended to use at least 300 ppi (ppi = pixels per inch). Suppose you want to create a 4x4-inch PNG file with 300 ppi: ppi &lt;- 300 png(&quot;png_plot.png&quot;, width = 4*ppi, height = 4*ppi, res = ppi) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() dev.off() 6.6 Axes, appearance 6.6.1 Swapping X- and Y-axes plot1 &lt;- ggplot(PlantGrowth, aes(x = group, y = weight)) + geom_boxplot() plot2 &lt;- plot1 + coord_flip() ggarrange(plot1, plot2) 6.6.2 Setting the range of a continuous axis m_plot &lt;- ggplot(marathon, aes(x = Half, y = Full)) + geom_point() m_plot2 &lt;- m_plot + xlim(0, max(marathon$Half)) + ylim(0, max(marathon$Full)) ggarrange(m_plot, m_plot2) 6.6.3 Changing the text of axis labels hw_plot &lt;- ggplot(heightweight, aes(x = ageYear, y = heightIn, colour = sex)) + geom_point() hw_plot + xlab(&quot;Age\\n(years)&quot;) + ylab(&quot;Height in Inches&quot;) 6.6.4 Adding Title library(ggpubr) # to use ggarrange hw_plot &lt;- ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point() hw_plot1 &lt;- hw_plot + ggtitle(&quot;Age and Height of Schoolchildren&quot;) hw_plot2 &lt;- hw_plot + ggtitle(&quot;Age and Height \\nof Schoolchildren&quot;) ggarrange(hw_plot1, hw_plot2) 6.6.5 Adding Subtitle You can add a subtitle by providing a string as the second argument of ggtitle(). It will display with slightly smaller text than the main title. hw_plot + ggtitle(&quot;Age and Height of Schoolchildren&quot;, &quot;11.5 to 17.5 years old&quot;) 6.6.6 Using Themes # Grey theme (default theme) hw_plot_grey &lt;- hw_plot + theme_grey() hw_plot_classic &lt;- hw_plot + theme_classic() hw_plot_bw &lt;- hw_plot + theme_bw() hw_plot_minimal &lt;- hw_plot + theme_minimal() ggarrange(hw_plot_grey, hw_plot_classic, hw_plot_bw, hw_plot_minimal) 6.7 Summary 6.7.1 Bar charts examples of using pipe %&gt;% together with ggplot create bar charts of counts create bar charts of values change “fill” and “outline” of the bars create grouped bar charts create stacked bar charts convert a variable into factor in ggplot use different colour palette control the width of the bars 6.7.2 Line graphs create line graphs label the graph change the range of y-axis create line graphs with multiple lines use multiple geoms (geometric objects) (e.g. additing the points on top of the lines) change shape, size, fill, outline of points change line type 6.7.3 Scatter plot create scatter plots visualize an additional discrete variable visualize an additional continuous variable visualize two additional discrete variables overplotting (use smaller points, make points semitransparent, bin data into rectangels, jitter the points) label points in a scatter plot 6.7.4 Summarizing data distributions create histograms (frequency and density) compare two histograms (facet_grid(), fill()) create histograms with two additional discrete variables create kernel density estimates overlay a density curve with a histogram display kernel density estimates from grouped data (color, fill, facet_grid) 6.7.5 Saving your plots output to pdf vector files output to bitmap files 6.7.6 Axes, appearance swapping x- and y-axes setting the range of a continuous axis change the text of axis labels adding title adding subtitle using themes "],["statistical-inference-in-r.html", "Chapter 7 Statistical Inference in R 7.1 Maximum Likelihood Estimation 7.2 Interval Estimation and Hypothesis Testing", " Chapter 7 Statistical Inference in R In this chapter, we discuss how to perform some parameter estimations and hypothesis testings in R. You may have learned their theory in previous statistics courses. I do not intend to give a very comprehensive review to these methods due to time constraint. Optional Readings: You can find a few more statistical tests in Ch 9 of R Cookbook (https://rc2e.com/) You can review Ch 10-13 of John E. Freund’s Mathematical Statistics with Applications by Irwin Miller and Marylees Miller (textbook for STAT 269) for some background and theory on statistical inference 7.1 Maximum Likelihood Estimation After you collect some data and formulate a statistical model, you have to estimate the parameters in your model. One of the most common methods is to use maximum likelihood estimation. Very often, there is no closed-form expression for your estimators. In general, suppose you have data \\(y_1,\\ldots,y_n\\). The likelihood function is a function of the parameter defined as \\[\\begin{equation*} L(\\theta|y_1,\\ldots,y_n) = f_{y_1,\\ldots,y_n}(y_1,\\ldots,y_n|\\theta), \\end{equation*}\\] where \\(f_{y_1,\\ldots,y_n}(\\cdot |\\theta)\\) is the joint pmf or pdf of \\(y_1,\\ldots,y_n\\) with parameter \\(\\theta\\). That is, the likelihood function evaluated at \\(\\theta\\) is simply the joint “probability” of observing \\(y_1,\\ldots,y_n\\) when the parameter value is \\(\\theta\\). Assuming \\(y_1,\\ldots,y_n\\) are i.i.d. with density \\(f(\\cdot|\\theta)\\), we have \\[\\begin{equation*} L(\\theta|y_1,\\ldots,y_n) = f_{y_1,\\ldots,y_n}(y_1,\\ldots,y_n|\\theta) = \\prod^n_{i=1} f(y_i|\\theta). \\end{equation*}\\] In maximum likelihood estimation, we estimate the parameter \\(\\theta\\) by maximizing \\(L\\). The maximizer is called the maximum likelihood estimator (MLE). Some theory Why do we want to maximize the likelihood? Informally, the likelihood is the chance of observing the data. Therefore, we want to find the parameters so that such a chance is maximized. MLE has good statistical properties. Under some regularity conditions, MLE is consistent: \\(\\hat{\\theta}_n\\) converges in probability to \\(\\theta_0\\) Asymptotically efficient: the estimator has the lowest variance asymptotically in some sense Asymptotically normality: can be used to find confidence intervals and perform hypothesis testings Example (Logistic Regression): Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) is a binary variable and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In logistic regression we assume that \\[\\begin{equation*} P(Y_i = 1|x_i, \\beta) = \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}} = \\frac{ e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}. \\end{equation*}\\] Since \\(Y_i\\) takes only two values, \\[\\begin{equation*} P(Y_i = 0|x_i, \\beta) = \\frac{1}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] We can use one single formula for \\(y = 0, 1\\): \\[\\begin{equation*} P(Y_i = y|x_i, \\beta) = \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The likelihood function (conditional on x) is \\[\\begin{equation*} L(\\beta|y_1,\\ldots,y_n, x_1,\\ldots,x_n) = \\prod^n_{i=1} P(Y_i = y_i|x_i, \\beta) = \\prod^n_{i=1} \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The MLE of \\(\\beta\\) is obtained by maximizing \\(L(\\beta|y,x)\\) with respect to \\(\\beta\\). We usually maximize the natural logarithm of the likelihood function instead of the likelihood function, which is easier. The log likelihood function is \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( (x^T_i \\beta) y_i - \\log(1+e^{x^T_i \\beta}) \\bigg). \\end{equation*}\\] In this case, there is no closed-form formula for finding the maximizer. Nevertheless, we can use numerical methods to find out the maximizer. Of course, there are existing functions to perform this task in R. However, we will illustrate how to perform an optimization using the function optim(). By default, optim() will find the minimum. Therefore, we will minimize the negative of the log likelihood function. Simulated Example This is also a good time to introduce how to perform simulation based on a model and check the validity of the estimation method and algorithm. We will first generate some covariates and binary variables based on the logistic regression model. # Setting set.seed(362) n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) Now, we will write a function to compute the negative of the log likelihood function (as a function of the parameter \\(\\beta\\) and the data \\(\\{y_i, x_{i1}, x_{i2}: i=1,\\ldots,n\\}\\)), which is the objective function to be minimized. neg_log_like &lt;- function(beta, y, x1, x2) { beta_X &lt;- beta[1] + beta[2] * x1 + beta[3] * x2 log_like &lt;- sum(beta_X * y) - sum(log(1 + exp(beta_X))) -log_like # return the negative log likelihood } After defining our objective function, we can now use optim() to perform the optimization. optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;) ## $par ## [1] 0.7830734 1.0067301 -1.4920231 ## ## $value ## [1] 632.7284 ## ## $counts ## function gradient ## 28 7 ## ## $convergence ## [1] 0 ## ## $message ## NULL par is the initial values for the optimization. We set some random numbers for the initial values by par = runif(3, 0, 1). Because the function neg_log_like have multiple arguments, we have to supply them inside optim(). method = \"BFGS\" is a quasi-Newton method. method = \"L-BFGS-B\" is also useful when you want to add box constraints to your variable. You can check ?optim to learn more about this. Output: par is the parameter values at which the minimum is obtained value is the minimum function value convergence = 0 indicates successful completion Compare with the built-in function glm() for estimating the parameters: # will discuss this in more detail later fit &lt;- glm(y ~ x1 + x2, family = &quot;binomial&quot;) fit ## ## Call: glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;) ## ## Coefficients: ## (Intercept) x1 x2 ## 0.7831 1.0067 -1.4920 ## ## Degrees of Freedom: 999 Total (i.e. Null); 997 Residual ## Null Deviance: 1324 ## Residual Deviance: 1265 AIC: 1271 You can see that both methods give the same estimates 0.783, 1.007, -1.492 for the regression coefficients. How do you know your method of estimation makes sense? How do you know if you have simulated the data correctly? In the above example, the true parameters are 0.5, 1, -1. The estimates are not really that close to the true values. We do not know if the estimation method will give a good result in general. To tackle this problem, we could simulate many datasets, perform the estimation, and take a look at the distributions of the estimates. Perform the simulation and estimation \\(250\\) times: # Setting n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta no_iter &lt;- 250 beta_est &lt;- matrix(0, nrow = no_iter, ncol = length(beta_0)) for (i in 1:no_iter) { # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Estimation beta_est[i, ] &lt;- optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;)$par } Displaying the results: library(tidyverse) # create the dataframe for plotting data &lt;- tibble(est = c(beta_est[, 1], beta_est[, 2], beta_est[, 3]), beta = c(rep(&quot;Beta1&quot;, no_iter), rep(&quot;Beta2&quot;, no_iter), rep(&quot;Beta3&quot;, no_iter))) # dataframe for adding the vertical lines for the true parameters vline_data &lt;- tibble(beta = c(&quot;Beta1&quot;, &quot;Beta2&quot;, &quot;Beta3&quot;), mean = beta_0) ggplot(data = data, mapping = aes(x = est)) + geom_histogram(fill = &quot;lightblue&quot;) + facet_grid(~ beta, scales = &quot;free&quot;) + geom_vline(data = vline_data, aes(xintercept = mean), color = &quot;blue&quot;) From the above plots, you can see the distributions of your estimators. The true parameters lie in the middle of the distributions. You can also add lines to visualize the mean of the distributions. In this case, the lines actually overlap with lines for the true parameters. Thus, the estimators are essentially unbiased. You can also see that there are times that \\(\\hat{\\beta}_0\\) can be as large as \\(1\\) or as small as \\(0.1\\) while the true value is \\(0.5\\). You can also use a larger sample size. # setting set.seed(362) n &lt;- 100000 beta_0 &lt;- c(0.5, 1, -1) # true beta # simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Estimation (est &lt;- optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;)$par) ## [1] 0.4803615 1.0275395 -0.9871783 The estimates are now 0.48, 1.03, -0.99, which are close to the true values 0.5, 1, -1. We will see some applications of the logistic regression later. 7.1.1 Exercises on MLE Exercise 1 (Gamma distribution) You observe a random sample \\(y_1,\\ldots,y_n\\) from a Gamma distribution with unknown parameters \\(\\alpha, \\beta\\). The likelihood function is \\[\\begin{equation*} L(\\alpha, \\beta |y_1,\\ldots,y_n) = \\prod^n_{i=1} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{\\alpha-1}_i e^{-\\beta y_i}. \\end{equation*}\\] The log likelihood, after simpliciation, is \\[\\begin{equation*} \\log L(\\alpha, \\beta) = n \\alpha \\log \\beta - n \\log \\Gamma(\\alpha) + (\\alpha - 1) \\sum^n_{i=1} \\log y_i - \\beta \\sum^n_{i=1} y_i. \\end{equation*}\\] # Setting set.seed(1) alpha &lt;- 1.5 beta &lt;- 2 n &lt;- 10000 # Simulation x &lt;- rgamma(n, alpha ,beta) # Optimization (Estimation) # Assignment 4 Exercise 2 (Poisson Regression) Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) only takes nonnegative integer values (count data) and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In Poisson regression, we assume that \\(Y_i\\) has a Poisson distribution and \\[\\begin{equation*} \\log (E(Y_i|x_i)) = \\beta^T x_i = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}, \\end{equation*}\\] where \\(\\beta \\in \\mathbb{R}^{p+1}\\). Alternatively, condition on \\(x_i\\), \\(Y_i|x_i \\sim \\text{Pois}(e^{\\beta^T x_i})\\). The likelihood is \\[\\begin{equation*} L(\\beta|y,x) = \\prod^n_{i=1} \\frac{e^{-e^{\\beta^T x_i}} e^{(\\beta^T x_i)y_i}}{y_i!}. \\end{equation*}\\] The log likelihood is \\[\\begin{equation*} \\log L(\\beta|y, x) = \\sum^n_{i=1} \\bigg( -e^{\\beta^T x_i} + (\\beta^T x_i) y_i - \\log (y_i!) \\bigg). \\end{equation*}\\] Clearly, the term \\(\\log (y_i !)\\) does not depend on \\(\\beta\\) and hence we do not need to consider it during our optimization. Therefore, it suffices to maximize (or minimize the negative of) the following objective function \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( -e^{\\beta^T x_i} + (\\beta^T x_i) y_i \\bigg). \\end{equation*}\\] # Setting set.seed(1) beta &lt;- c(0.3, 0.5, -0.5) n &lt;- 10000 p &lt;- length(beta) - 1 # Simulation X &lt;- cbind(1, matrix(runif(n * p), nrow = n, ncol = p)) y &lt;- rpois(n, exp(X %*% beta)) # Optimization (Estimation) # Assignment 4 7.1.2 Summary Review of the MLE Use optim() to minimize an objective function Simplify the objective function before you try to minimize it (take log and remove terms that do not depend on the parameters) how to simulate from a model how to check the validity of the simulation results 7.2 Interval Estimation and Hypothesis Testing Two types of estimation: point estimation (e.g. MLE) and interval estimation (e.g. confidence interval). 7.2.1 Examples of Hypothesis Testing Optional reading: Chapter 12 in John E. Freund’s Mathematical Statistics with Applications. You have a die and you wonder if it is unbiased. If the die is unbiased, the (population) mean of the result from rolling the die is \\(3.5\\). Suppose you roll the die \\(10\\) times, the sample mean is \\(5\\). What is your decision on determining if the die is biased or not? How confident is your decision? What if you roll the die \\(100\\) times, and the sample mean is \\(5\\)? What is your decision now? Are you more confident in your decision? What if your roll the die \\(10\\) times but the sample mean is \\(4\\)? To answer these questions, we need to understand interval estimation and hypothesis testing. Some more examples: An engineer has to decide on the basis of sample data whether the true average lifetime of a certain kind of tire is at least \\(42,000\\) miles An agronomist has to decide on the basis of experiments whether one kind of fertilizer produces a higher yield of soybeans than another A manufacturer of pharmaceutical products has to decide on the basis of samples whether 90 percent of all patients given a new medication will recover from a certain disease These problems can all be translated into the language of statistical tests of hypotheses. the engineer has to test the hypothesis that \\(\\theta\\), the parameter of an exponential population, is at least \\(42,000\\) the agronomist has to decide whether \\(\\mu_1 &gt; \\mu_2\\), where \\(\\mu_1\\) and \\(\\mu_2\\) are the means of two normal populations the manufacturer has to decide whether \\(\\theta\\), the parameter of a binomial population, equals \\(0.90\\) In each case it must be assumed, of course, that the chosen distribution correctly describes the experimental conditions; that is, the distribution provides the correct statistical model. 7.2.2 Null Hypotheses, Alternative Hypotheses, and p-values An assertion or conjecture about the distribution of one or more random variables is called a statistical hypothesis. If a statistical hypothesis completely specifies the distribution, it is called a simple hypothesis; if not, it is referred to as a composite hypothesis. Null hypothesis: In view of the assumptions of “no difference,” hypotheses such as these led to the term null hypothesis, but nowadays this term is applied to any hypothesis that we may want to test. p-value: the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct Steps in Hypothesis Testing Assume the null hypothesis is true Calculate a test statistic. E.g., sample mean Calculate a \\(p\\)-value (denoted by \\(p\\)) from the statistic and its distribution. For example, if the die is unbiased, what is the probability that we observe the sample mean to be larger than \\(5\\) after rolling it \\(100\\) times? small \\(p\\)-value small: we have strong evidence to reject the null hypothesis because it is unlikely to observe such a test statistic if the null hypothesis is true large \\(p\\)-value small: we do not have enough evidence to reject the null hypothesis Remark In this course, we will follow the “common convention” to reject the null hypothesis when \\(p &lt; 0.05\\) In real applications, how small is small depends on the problems. Being statistically significant (small \\(p\\)-value) does not mean the difference between the null and alternative hypotheses is large. One should also look at the confidence intervals or distributions of your estimates. 7.2.3 Type I error and Type II error Type I error: reject the null hypothesis when it is true. The probability of committing a type I error is denoted by \\(\\alpha\\) (level of significance of the test). Type II error: do not reject the null hypothesis when it is false. The probability of committing a type II error is denoted by \\(\\beta\\). \\(H_0\\) is true \\(H_1\\) is true Reject \\(H_0\\) Type I error No Error Do not reject \\(H_0\\) No Error Type II Error A good test procedure is one in which both \\(\\alpha\\) and \\(\\beta\\) are small, thereby giving us a good chance of making the correct decision. When the sample size \\(n\\) is held fixed, reducing \\(\\alpha\\) by changing the rejection region will increase \\(\\beta\\) and vice versa. The only way in which we can reduce the probabilities of both types of errors is to increase \\(n\\). As long as \\(n\\) is held fixed, this inverse relationship between the probabilities of type I and type II errors is typical of statistical decision procedures. Usually, we control \\(\\alpha\\) to be small (e.g. \\(0.05\\)). 7.2.4 Inference for Mean of One Sample Hypothesis Testing Problem You have a random sample \\(X_1,\\ldots,X_n\\) from a population. You want to know if the population mean \\(\\mu\\) is equal to \\(\\mu_0\\). That is, \\(H_0 : \\mu =\\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\). Solution You can use the \\(t\\)-test for this problem. It is appropriate when either Your data is normally distributed You have a large sample size \\(n\\). A rule of thumb is \\(n &gt; 30\\). The test statistic is \\[\\begin{equation*} \\frac{\\overline{X}_n - \\mu_0}{s/\\sqrt{n}}, \\end{equation*}\\] where \\(\\overline{X}_n\\) is the sample mean and \\(s\\) is the sample standard deviation. In R, use t.test() to perform the t-test. # Simulate the data set.seed(362) # so that you can replicate the result x &lt;- rnorm(75, mean = 100, sd = 15) # Perform t-test t.test(x, mu = 95) ## ## One Sample t-test ## ## data: x ## t = 4.7246, df = 74, p-value = 1.071e-05 ## alternative hypothesis: true mean is not equal to 95 ## 95 percent confidence interval: ## 99.36832 105.74018 ## sample estimates: ## mean of x ## 102.5543 The \\(p\\)-value is small, so it is unlikely that the mean of the population is \\(95\\). The \\(p\\)-value in this case is \\(2 \\times P_{H_0}(T &gt; |\\text{obs. T.S.|})\\), where \\(T\\) has a \\(t\\)-distribution with degrees of freedom \\(n-1\\) if the data from are from a normal distribution and obs. T.S. stands for the observed test statistic. The subscript \\(H_0\\) is to stress that the probability measure is under \\(H_0\\). [Optional] How are the test statistic and \\(p\\)-value calculated? # test statistic (obs_ts &lt;- (mean(x) - 95) / (sd(x) / sqrt(length(x)))) ## [1] 4.724574 # p-value 2 * (1 - pt(abs(obs_ts), df = length(x) - 1)) ## [1] 1.071253e-05 What do you expect when \\(x\\) is from a distribution with a much larger SD? set.seed(362) # so that you can replicate the result x2 &lt;- rnorm(75, mean = 100, sd = 200) (test_x2 &lt;- t.test(x2, mu = 95)) ## ## One Sample t-test ## ## data: x2 ## t = 1.832, df = 74, p-value = 0.07097 ## alternative hypothesis: true mean is not equal to 95 ## 95 percent confidence interval: ## 91.57758 176.53580 ## sample estimates: ## mean of x ## 134.0567 Even if the estimate of the population mean is 134, the test does not reject the null hypothesis that the mean is \\(95\\). This is because the sample has a very large variance. Interval Estimation Denote \\(\\overline{x}_n\\) and \\(s_n\\) be the sample mean and sample standard deviation, respectively. The \\(100(1-\\alpha)\\%\\) confidence interval of \\(\\mu\\) is given by \\[\\begin{equation*} \\bigg[ \\overline{x}_n - t_{n-1; \\alpha/2} \\frac{s_n}{\\sqrt{n}}, \\overline{x}_n + t_{n-1; \\alpha/2} \\frac{s_n}{\\sqrt{n}} \\bigg], \\end{equation*}\\] where \\(t_{n-1;\\alpha/2}\\) satisfies \\(P(T &gt; t_{n-1;\\alpha/2}) = \\alpha/2\\) and \\(T \\sim t(n-1)\\). To find the confidence interval of \\(\\mu\\) in R, use t.test(). For example, the \\(99\\%\\) confidence interval of \\(\\mu\\) is t.test(x, conf.level = 0.99) ## ## One Sample t-test ## ## data: x ## t = 64.139, df = 74, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 99 percent confidence interval: ## 98.32683 106.78168 ## sample estimates: ## mean of x ## 102.5543 Remark By omitting mu = 0.95, the default value is mu = 0. Since we are interested in finding the confidence intervals, we do not need to care about the value of \\(\\mu_0\\). [Optional] Without using t.test(): # just an illustration of how CI can be computed alpha &lt;- 0.01 n &lt;- length(x) half_width &lt;- qt(1 - alpha / 2, n - 1) * sd(x) / sqrt(n) c(mean(x) - half_width, mean(x) + half_width) ## [1] 98.32683 106.78168 Interpretation If you can repeat the experiment many times, then about \\(95\\%\\) of the confidence intervals computed in those many times will contain the true mean. no_sim &lt;- 10000 set.seed(362) # so that you can replicate the result true_mean &lt;- 100 CI &lt;- matrix(0, nrow = no_sim, ncol = 2) for (i in 1:no_sim) { CI[i, ] &lt;- t.test(rnorm(75, mean = true_mean, sd = 200))$conf.int } # find out the proportion of CIs that contain 0 mean(CI[, 1] &lt; 100 &amp; 100 &lt; CI[, 2]) ## [1] 0.9503 Example Recall that we talked about how to use simulation to estimate \\(P(X &gt; Y)\\), where \\(X\\) and \\(Y\\) are some random variables. For example, if \\(X \\sim N(0, 1)\\), \\(Y \\sim \\text{Exp}(2)\\), and they are independent. R code to estimate \\(P(X &gt; Y)\\): set.seed(1) n &lt;- 10000 X &lt;- rnorm(n, 0, 1) Y &lt;- rexp(n, 2) mean(X &gt; Y) ## [1] 0.3299 We know the true value is not exactly 0.3299 because the law of large numbers only ensure that the sample mean converges to the true mean. We can use t.test() to find an confidence interval for \\(P(X &gt; Y)\\). t.test(X &gt; Y) ## ## One Sample t-test ## ## data: X &gt; Y ## t = 70.162, df = 9999, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3206831 0.3391169 ## sample estimates: ## mean of x ## 0.3299 We see that the \\(95\\%\\) CI is (0.321, 0.339). To make the CI narrower, we can increase the number of simulation. set.seed(1) n &lt;- 1000000 X &lt;- rnorm(n, 0, 1) Y &lt;- rexp(n, 2) t.test(X &gt; Y) ## ## One Sample t-test ## ## data: X &gt; Y ## t = 704.63, df = 1e+06, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3308521 0.3326979 ## sample estimates: ## mean of x ## 0.331775 The \\(95\\%\\) CI becomes (0.331, 0.333). The width of the CI is equal to \\[\\begin{equation*} 2 \\times t_{n-1; \\alpha/2} \\frac{s}{\\sqrt{n}}. \\end{equation*}\\] From the above formula, you could determine the minimum number of simulations required to achieve a certain degree of accuracy, 7.2.5 Comparing the means of two samples Suppose you have one sample each from two populations. You want to test if the two populations have the same mean. There are two different \\(t\\)-tests for this task (assuming data are normally distributed or you have large samples): the observations are not paired the observations are paired To explain the meaning of paired data, consider two experiments to see if drinking coffee in the morning improves your test scores: Unpaired observations: Randomly select two groups of people. People in one group have a cup of morning coffee and take the test. The other group just takes the test. For each person, we have one test score. All the scores are independent. Paired observations: Randomly select one group of people. Give them the test twice, once with morning coffee and once without morning coffee. For each person, we have two test scores. Clearly, the two scores are not statistically independent. Example We illustrate the paired \\(t\\)-test using the dataset sleep (no package is required). The data contains the increase in hours of sleep when the subject took two soporific drugs compared to control on \\(10\\) subjects. Since each subject received two drugs, the observations are paired. Take a look at sleep: sleep ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 The sleep data is in a long format. Let’s turn it into a wide format using spread, a function in the package tidyr, which is contained in tidyverse. (sleep_wide &lt;- spread(sleep, group, extra)) ## ID 1 2 ## 1 1 0.7 1.9 ## 2 2 -1.6 0.8 ## 3 3 -0.2 1.1 ## 4 4 -1.2 0.1 ## 5 5 -0.1 -0.1 ## 6 6 3.4 4.4 ## 7 7 3.7 5.5 ## 8 8 0.8 1.6 ## 9 9 0.0 4.6 ## 10 10 2.0 3.4 Paired \\(t\\)-test: t.test(sleep_wide[, 2], sleep_wide[, 3], paired = TRUE) ## ## Paired t-test ## ## data: sleep_wide[, 2] and sleep_wide[, 3] ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean difference ## -1.58 The \\(p\\)-value is \\(&lt; 0.05\\). We conclude that the effect of the two drugs are different. The \\(95\\%\\) CI of the difference between the two means is (-2.46, -0.70). Example Are the means of the birth weights in the smoking group and non-smoking group different? library(MASS) t.test(birthwt$bwt[birthwt$smoke == 1], birthwt$bwt[birthwt$smoke == 0]) ## ## Welch Two Sample t-test ## ## data: birthwt$bwt[birthwt$smoke == 1] and birthwt$bwt[birthwt$smoke == 0] ## t = -2.7299, df = 170.1, p-value = 0.007003 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -488.97860 -78.57486 ## sample estimates: ## mean of x mean of y ## 2771.919 3055.696 The \\(p\\)-value is \\(&lt; 0.05\\). We conclude that the means are different. The \\(95\\%\\) CI of the difference between the two means is (-489.0, -78.6). 7.2.6 Inference of a Sample Proportion You have a sample of values from a population consisting of successes and failures. The null hypothesis is the true proportion of success \\(p\\) is equal to some particular number \\(p_0\\). The alternative hypothesis is the \\(p \\neq p_0\\). Example You flip a coin \\(100\\) times independently. You want to test if the coin is fair \\((p_0 = 0.5)\\). # Simulate the coin flips set.seed(1) heads &lt;- rbinom(1, size = 100, prob = .4) # Test, p_0 = 0.5 (result &lt;- prop.test(heads, 100, p = 0.5)) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.1336 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.3233236 0.5228954 ## sample estimates: ## p ## 0.42 The point estimate is 0.42. Although the true probability of success used in the simulation is \\(0.4\\), for this particular data, we do not reject to null hypothesis that the true probability of success is \\(0.5\\) as the \\(p\\)-value equals 0.134, which is larger than \\(0.05\\). The 95% confidence interval is equal to (0.323, 0.523). You can change the alternative hypothesis to \\(p &gt; p_0\\) or \\(p &lt; p_0\\): prop.test(heads, 100, p = 0.5, alternative = &quot;greater&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.9332 ## alternative hypothesis: true p is greater than 0.5 ## 95 percent confidence interval: ## 0.3372368 1.0000000 ## sample estimates: ## p ## 0.42 prop.test(heads, 100, p = 0.5, alternative = &quot;less&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.06681 ## alternative hypothesis: true p is less than 0.5 ## 95 percent confidence interval: ## 0.0000000 0.5072341 ## sample estimates: ## p ## 0.42 7.2.7 Testing groups for equal proportions You have samples from two or more groups. The data from each group are binary-valued: either “success” or “failure”. You want to test if the groups have equal proportions of “success”. Example # 3 groups no_success &lt;- c(48, 60, 50) # no. of successes in the 3 groups no_trial &lt;- c(100, 100, 100) # corresponding no. of trails in the 3 groups prop.test(no_success, no_trial) ## ## 3-sample test for equality of proportions without continuity correction ## ## data: no_success out of no_trial ## X-squared = 3.3161, df = 2, p-value = 0.1905 ## alternative hypothesis: two.sided ## sample estimates: ## prop 1 prop 2 prop 3 ## 0.48 0.60 0.50 \\(p\\)-value is \\(&gt; 0.05\\). We do not reject the null hypothesis that the three groups have the same proportion of success. Example In a class of \\(38\\) students, \\(14\\) of them got \\(A\\). In another class of \\(40\\) students, only \\(10\\) got \\(A\\). We want to know if the difference between the two proportions is statistically significant. prop.test(c(14, 10), c(38, 40)) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(14, 10) out of c(38, 40) ## X-squared = 0.7872, df = 1, p-value = 0.3749 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.1110245 0.3478666 ## sample estimates: ## prop 1 prop 2 ## 0.3684211 0.2500000 The \\(p\\)-value is \\(&gt; 0.05\\). We do not reject the null hypothesis that the students in the two groups have the same proportion of getting an A. 7.2.8 Testing if two samples have the same underlying distribution Problem You have two random samples \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\). Let \\(F\\) and \\(G\\) be the distribution functions of \\(X_i\\)’s and \\(Y_i\\)’s respectively. You want to know if \\(F \\equiv G\\). Solution You may use the Kolmogorov-Smirnov test. \\(H_0: F = G\\) vs \\(H_1: F \\neq G\\). It does not require any assumptions. The test statistic is \\[ D := \\sup_{x \\in \\mathbb{R}}|F_n(x) - G_m(x)|,\\] where \\(F_n\\) and \\(G_m\\) are the empirical distribution functions of \\(X_i\\)’s and \\(Y_i\\)’s respectively. That is, \\[ F_n(x) := \\frac{1}{n} \\sum^n_{i=1} I(X_i \\leq x)\\] and \\[ G_m(x) := \\frac{1}{m} \\sum^m_{i=1} I(Y_i \\leq x).\\] Example set.seed(362) x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) ks.test(x, y) ## ## Asymptotic two-sample Kolmogorov-Smirnov test ## ## data: x and y ## D = 0.07, p-value = 0.9671 ## alternative hypothesis: two-sided The \\(p\\)-value is not small. We do not have enough evidence to reject the null hypothesis that the two distributions are the same. Example z &lt;- rnorm(100, 2, 1) ks.test(y, z) ## ## Asymptotic two-sample Kolmogorov-Smirnov test ## ## data: y and z ## D = 0.77, p-value &lt; 2.2e-16 ## alternative hypothesis: two-sided The \\(p\\)-value is very small. We will reject the null hypothesis that the two distributions are the same. Example Recall the dataset birthwt from the package MASS. We created the following histograms to visualize the distributions of birth weights for the two groups (smoke and no smoke). We may want to ask if the two distributions are different ks.test(birthwt$bwt[birthwt$smoke == 0], birthwt$bwt[birthwt$smoke == 1]) ## ## Exact two-sample Kolmogorov-Smirnov test ## ## data: birthwt$bwt[birthwt$smoke == 0] and birthwt$bwt[birthwt$smoke == 1] ## D = 0.21962, p-value = 0.01915 ## alternative hypothesis: two-sided The \\(p\\)-value is smaller than \\(0.05\\). Therefore, we conclude that the difference of the two distributions is statistically significant. "],["root-finding-and-optimization.html", "Chapter 8 Root finding and optimization 8.1 Root Finding 8.2 Newton-Raphson Method 8.3 Minimization and Maximization 8.4 optim", " Chapter 8 Root finding and optimization Reference: Computational Methods for Numerical Analysis with R by James P. Howard, II Packages used: library(Deriv) # for symbolic differentiation library(tidyverse) In this chapter, some basic algorithms for root finding and optimization are introduced. A root of a function means the \\(x\\) such that \\(f(x) = 0\\). We will also discuss how to implement them in R to practise your code writing ability. 8.1 Root Finding 8.1.1 Bisection Method Game: Before we introduce the bisection method, consider a game where Player \\(A\\) picks an integer from \\(1\\) to \\(100\\), and Player \\(B\\) has to guess this number. Player \\(A\\) will tell Player \\(B\\) if the guess is too low or too high. For example, Player \\(A\\) picks \\(10\\). Player \\(B\\) guesses \\(40\\). Player \\(A\\) tells Player \\(B\\) the number is too high. Player \\(B\\) will then pick a number from \\(1\\) to \\(40\\), say, \\(28\\). Play \\(A\\) tells Player \\(B\\) the number is also too high. Eventually, Player \\(B\\) can guess the number correctly. You may realize that the best approach is to halve the range with each turn. Idea of Bisection Method: Recall that for any continuous function \\(f\\), if it has values of opposite sign in an interval, then it has a root in the interval. For example, if \\(f(a) &lt; 0\\) and \\(f(b) &gt; 0\\), then there exists \\(c \\in (a, b)\\) such that \\(f(c) = 0\\). Now, suppose that \\(f(a) f(b) &lt; 0\\) (so that \\(f\\) has values of opposite sign at \\(a\\) and \\(b\\)). Let \\(\\varepsilon\\) denote a tolerance level. Algorithm: Let \\(c = \\frac{a + b}{2}\\). If \\(f(c) = 0\\), stop and return \\(c\\). If \\(\\text{sign}(f(a)) \\neq \\text{sign}(f(c))\\); set \\(b = c\\); else set \\(a = c\\). Repeat Step 1 to 3 until \\(|b-a| &lt; \\varepsilon\\). Implementation in R: This is a good example to see when a while loop is useful. bisection &lt;- function(f, a, b, tol = 1e-5, max_iter = 100) { iter &lt;- 0 # Step 4 while (abs(b - a) &gt; tol) { # Step 1 c &lt;- (a + b) / 2 # Step 2: if f(c) = 0, stop and return c if (f(c) == 0) { return(c) } iter &lt;- iter + 1 if (iter &gt; max_iter) { warning(&quot;Maximum number of iterations reached&quot;) return(c) } # Step 3 if (f(a) * f(c) &lt; 0) { b &lt;- c } else { a &lt;- c } # print(round(c(c, f(c)), digits = 3)) } return((a + b) / 2) } Remark: 1e-3 = 0.001. We set the default value of \\(\\varepsilon\\) to \\(0.001\\) and the maximum number of iterations to \\(100\\). Example f &lt;- function(x) { x^2 - 1 } Without providing the values of tol and max_iter, the function will use tol = 1e-3 and max_iter = 100. bisection(f, 0.5, 1.25) ## [1] 1.000001 To change tol: bisection(f, 0.5, 1.25, tol = 0.1) ## [1] 1.015625 bisection(f, 0.5, 1.25, tol = 0.0000001) ## [1] 1 To change max_iter: bisection(f, 0.5, 1.25, tol = 0.000000000001, max_iter = 10) ## Warning in bisection(f, 0.5, 1.25, tol = 1e-12, max_iter = 10): Maximum number of iterations reached ## [1] 0.9998779 8.2 Newton-Raphson Method Newton-Raphson method can be used to find a root of a function. Idea of Netwon-Raphson Method (or Netwon’s Method): Given an initial estimate of the root \\(x_0\\), approximate your function by its tangent line at \\(x_0\\) and find the root (you can find the root of a line easily). To find the root, we equate the slope at \\(x_0\\) found by \\(f&#39;(x_0)\\) and using the two points \\((x_1, 0)\\) and \\((x_0, f(x_0))\\), where \\(x_1\\) denotes the root of the tangent line at \\(x_0\\). That is, \\[\\begin{equation*} \\frac{f(x_0) - 0}{x_0 - x_1} = f&#39;(x_0). \\end{equation*}\\] Rearranging the terms give \\[\\begin{equation*} x_1 = x_0 - \\frac{f(x_0)}{f&#39;(x_0)}. \\end{equation*}\\] Iterate the above procedure until convergence. Here is an gif animination from wikipedia: https://en.wikipedia.org/wiki/Newton%27s_method#/media/File:NewtonIteration_Ani.gif Algorithm of Netwon-Raphson Method: Given an initial estimate of the root \\(x_0\\), set \\[\\begin{equation*} x_1 = x_0 - \\frac{f(x_0)}{f&#39;(x_0)}. \\end{equation*}\\] Iterate the following equation until convergence \\[\\begin{equation*} x_n = x_{n-1} - \\frac{f(x_{n-1})}{f&#39;(x_{n-1})}. \\end{equation*}\\] Implementation in R: f is the function you want to find the root fp is the first derivative of f x0 is the initial value. Since we cannot iterate indefinitely, we have to set a tolerance (tol) and a maximum number of iteration (max_iter). NR &lt;- function(f, fp, x0, tol = 1e-3, max_iter = 100) { iter &lt;- 0 old_x &lt;- x0 x &lt;- old_x + 10 * tol # any number such that abs(x - old_x) &gt; tol while(abs(x - old_x) &gt; tol) { iter &lt;- iter + 1 if (iter &gt; max_iter) { print(&quot;Maximum number of iterations reached&quot;) return(x) } old_x &lt;- x x &lt;- x - f(x) / fp(x) } return(x) } Symbolic differentiation To perform symbolic differentiation in R (instead of numerical differentiation), we can use Deriv() in the package Deriv. f &lt;- function(x) { x^2 - 2 * x + 1 } # Symbolic differentiation fp &lt;- Deriv(f) fp # we know it is 2x - 2 ## function (x) ## 2 * x - 2 Apply our NR function Let’s plot the function first. Using our function: NR(f, fp, 1.25, tol = 1e-3) ## [1] 1.000508 8.3 Minimization and Maximization 8.3.1 Newton-Raphson Method Recall that at local extrema, \\(f&#39;(x) = 0\\), therefore we can use the Netwon-Raphson method for minimization and maximization. Algorithm of Netwon-Raphson Method: Given an initial estimate of the root \\(x_0\\), set \\[\\begin{equation*} x_1 = x_0 - \\frac{f&#39;(x_0)}{f&#39;&#39;(x_0)}. \\end{equation*}\\] Iterate the following equation until convergence \\[\\begin{equation*} x_n = x_{n-1} - \\frac{f&#39;(x_{n-1})}{f&#39;&#39;(x_{n-1})}. \\end{equation*}\\] Multivariate Version Iterate the following equation until convergence: \\[\\begin{equation*} x_n = x_{n-1} - [f&#39;&#39;(x_{n-1})]^{-1} f&#39;(x_{n-1}), \\end{equation*}\\] where \\(f&#39;(x)\\) is the gradient and \\(f&#39;&#39;(x)\\) is the Hessian matrix. Example (Logistic Regression) Recall that \\(x_i = (x_{i1},\\ldots,x_{ip})^T\\) and \\(y = (y_1,\\ldots,y_n)^T\\). Log-likelihood of the logistic regresison model: \\[\\begin{equation*} l(\\beta) = \\sum^n_{i=1} \\{(x^T_i \\beta)y_i - \\log (1 + e^{x^T_i \\beta}) \\}. \\end{equation*}\\] To apply the Netwon’s method, we have to find the gradient and the Hessian matrix. The gradient is \\[\\begin{align*} \\frac{\\partial l(\\beta)}{\\partial \\beta} &amp;= \\sum^n_{i=1} \\bigg( x_i y_i - \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}} x_i \\bigg) \\\\ &amp;= X^T y - X^T p_\\beta \\\\ &amp;= X^T(y - p_\\beta), \\end{align*}\\] where \\(p_\\beta = (p(x_1;\\beta),\\ldots,p(x_n;\\beta))^T\\) and \\(p(x_i;\\beta) = P(Y=1|\\beta, x_i) = \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}}\\). To find the Hessian matrix, we first find \\(\\frac{\\partial l(\\beta)}{\\partial \\beta_j \\partial \\beta_k}\\): \\[\\begin{align*} \\frac{\\partial l(\\beta)}{\\partial \\beta_j \\partial \\beta_k} &amp;= - \\sum^n_{i=1} x_{ij} \\frac{ (1+e^{x^T_i \\beta}) e^{x_i^T \\beta} x_{ik} - (e^{x^T_i \\beta})^2 x_{ik}}{(1+e^{x^T_i \\beta})^2 } \\\\ &amp;= - \\sum^n_{i=1} x_{ij} x_{ik} \\frac{ e^{x^T_i \\beta} }{(1+e^{x^T_i \\beta})^2} \\\\ &amp;= - \\sum^n_{i=1} x_{ij} x_{ik} p(x_i; \\beta) (1- p(x_i;\\beta)). \\end{align*}\\] Thus, \\[\\begin{align*} \\frac{\\partial l(\\beta)}{\\partial \\beta \\partial \\beta^T} &amp;= -\\sum^n_{i=1} x_i x_i^T p(x_i;\\beta)(1-p(x_i;\\beta)) \\\\ &amp; = - X^T W_\\beta X, \\end{align*}\\] where \\(W\\) is a \\(n \\times n\\) diagonal matrix with elements \\(p(x_i;\\beta)(1-p(x_i;\\beta))\\). Update using Newton’s method: \\[\\begin{equation*} \\hat{\\beta}_{\\text{new}} = \\hat{\\beta}_{\\text{old}} - (-X^T W_{\\hat{\\beta}_{\\text{old}}} X)^{-1} X^T(y- p_{\\hat{\\beta}_{\\text{old}}} ). \\end{equation*}\\] set.seed(362) n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Design matrix X &lt;- cbind(1, x1, x2) beta &lt;- runif(3, 0, 1) for (i in 1:7) { num &lt;- as.vector(exp(X %*% beta)) p_beta &lt;- num / (1 + num) grad &lt;- t(X) %*% (y - p_beta) W &lt;- diag(p_beta * (1 - p_beta)) Hess &lt;- - t(X) %*% W %*% X beta &lt;- beta - solve(Hess) %*% grad print(c(paste0(&quot;Iteration: &quot;, i), round(as.vector(beta), 4))) } ## [1] &quot;Iteration: 1&quot; &quot;0.8283&quot; &quot;0.9995&quot; &quot;-1.7345&quot; ## [1] &quot;Iteration: 2&quot; &quot;0.7835&quot; &quot;1.0039&quot; &quot;-1.4903&quot; ## [1] &quot;Iteration: 3&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; ## [1] &quot;Iteration: 4&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; ## [1] &quot;Iteration: 5&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; ## [1] &quot;Iteration: 6&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; ## [1] &quot;Iteration: 7&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; # compared with the glm function glm(y ~ x1 + x2, family = &quot;binomial&quot;)$coef ## (Intercept) x1 x2 ## 0.7830729 1.0067405 -1.4920333 8.3.2 Gradient Descent Gradient Descent Iterative method for finding a local minimum Requires an initial value and a step size \\(h\\) Idea of Gradient Descent: The gradient descent method uses the derivative at \\(x\\) and takes a step down, of size \\(h\\), in the direction of the slope. The process is repeated using the new point as \\(x\\). As the function slides down a slope, the derivative will start shrinking resulting in smaller changes in \\(x\\). As the change in \\(x\\) decreases below the tolerance value, we have reached a local minimum. Algorithm of Gradient Descent: Set an initial point \\(x_0\\) and a step size \\(h &gt; 0\\) Iterate until convergence: \\[\\begin{equation*} x_{n+1} = x_{n} - h f&#39;(x_{n}). \\end{equation*}\\] Implementation in R: grad_des &lt;- function(fp, x0, h = 1e-3, tol = 1e-4, max_iter = 1000) { iter &lt;- 0 old_x &lt;- x0 x &lt;- x0 + 2 * tol while(abs(x - old_x) &gt; tol) { iter &lt;- iter + 1 if (iter &gt; max_iter) { stop(&quot;Maximum number of iterations reached&quot;) } old_x &lt;- x x &lt;- x - h * fp(x) } return(x) } Example: Find the local minima of \\(f(x) = \\frac{1}{4} x^4 + x^3 - x - 1\\). Plot: Gradient descent: f &lt;- function(x) {1/4 * x^4 + x^3 - x - 1} fp &lt;- Deriv(f) grad_des(fp, x0 = -2, h = 0.01) ## [1] -2.878224 grad_des(fp, x0 = 2, h = 0.01) ## [1] 0.5344022 Displaying error message: grad_des(fp, x0 = -2, h = 0.01, max_iter = 2) ## Error in grad_des(fp, x0 = -2, h = 0.01, max_iter = 2): Maximum number of iterations reached Remark: to find a local maximum, use gradient ascent. Algorithm: Set an initial point \\(x_0\\) and a step size \\(h &gt; 0\\) Iterate until convergence: \\[\\begin{equation*} x_{n+1} = x_{n} + h f&#39;(x_{n}). \\end{equation*}\\] Implementation: grad_asc &lt;- function(fp, x0, h = 1e-3, tol = 1e-4, max_iter = 1000) { grad_des(fp, x0, -h, tol, max_iter) } Example: grad_asc(fp, x0 = 0, h = 0.01) ## [1] -0.6490877 8.4 optim Going back to optim(), the par plays the same role as the x_0 in the Netwon-Raphson method and the gradient descent method. Since we have some tolerance level and the iteration will stop when the tolerance level is reached, if you start at different initial values, you may get slightly different results I used some random numbers for the initial values in optim() because we do not know a good estimate of the minimizer. In general, you will perform the optimization with several set of different initial values and see which one results in a smaller function value. Sometimes, the algorithm will get stuck in a local minimum and using several set of random initial values can alleviate this problem. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
