[["index.html", "STAT 362 R for Data Science Syllabus", " STAT 362 R for Data Science Brian Ling 2022-03-30 Syllabus STAT 362 R for Data Science Department of Mathematics and Statistics, Queens University Course Description: Introduction to R, data creation and manipulation, data import and export, scripts and functions, control flow, debugging and profiling, data visualization, statistical inference, Monte Carlo methods, decision trees, support vector machines, neural network, numerical methods. For details, see onQ "],["introduction.html", "Chapter 1 Introduction 1.1 What is R and RStudio? 1.2 What will you learn in this course? 1.3 Lets Get Started 1.4 R Data Structures 1.5 Operators 1.6 Built-in Functions 1.7 Some Useful RStudio Shortcuts 1.8 Exercises 1.9 Comments to Exercises", " Chapter 1 Introduction 1.1 What is R and RStudio? R R is a language and environment for statistical computing and graphics. R is an interpreted language (individual language expressions are read and then executed immediately as soon as the command is entered) To download R, go to https://cloud.r-project.org/ RStudio is an integrated development environment (IDE) for R programming Install R first, then go to https://rstudio.com/products/rstudio/download/ and download RStudio While you can work in R directly, it is recommended to work in RStudio. 1.2 What will you learn in this course? Note: we do not assume you know R or any programming language before. 1.2.1 R and R as a programming language operators control flow (if..else.., for loop) defining a function 1.2.2 Data Wrangling Data wrangling = the process of tidying and transforming the data 1.2.3 Data Visualization Graphs are powerful to illustrate features of the data. You will learn how to create some basic plots as well as using the package ggplot2 to create more elegant plots. Consider a dataset about cars. library(ggplot2) mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l~ f 18 29 p comp~ ## 2 audi a4 1.8 1999 4 manual~ f 21 29 p comp~ ## 3 audi a4 2 2008 4 manual~ f 20 31 p comp~ ## 4 audi a4 2 2008 4 auto(a~ f 21 30 p comp~ ## 5 audi a4 2.8 1999 6 auto(l~ f 16 26 p comp~ ## 6 audi a4 2.8 1999 6 manual~ f 18 26 p comp~ ## 7 audi a4 3.1 2008 6 auto(a~ f 18 27 p comp~ ## 8 audi a4 quattro 1.8 1999 4 manual~ 4 18 26 p comp~ ## 9 audi a4 quattro 1.8 1999 4 auto(l~ 4 16 25 p comp~ ## 10 audi a4 quattro 2 2008 4 manual~ 4 20 28 p comp~ ## # ... with 224 more rows Among the variables in mpg are: displ, a cars engine size, in litres. hwy, a cars fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. Scatterplot Scatterplot, points are labeled with colors according to the class variable Scatterplots Line Chart Bar chart Another Bar Chart Boxplot Histogram 1.2.4 Statistical Inference Many problems in different domains can be formulated into hypothesis testing problems. Are university graduates more likely to vote for Candidate A? Is a treatment effective in reducing weights? Is a drug effective in reducing mortality rate? We want to answer these questions that take into account of the intrinsic variability. Formally, we can perform hypothesis testing and compute the confidence intervals. These are what you learned in STAT 269. It is ok if you havent taken the STAT 269. The topics will be briefly reviewed. We will focus on the applications using R. 1.2.5 Machine Learning We will illustrate some machine learning methods using real datasets. For example, Diagnoising breast cancer with the k-NN algorithm Employ Naive Bayes to build an SMS junk message filter (text data) A wordcloud of text data Use neural network to predict the compressive strength of concrete 1.2.6 Some Numerical Methods Monte Carlo simulation (estimate probabilities, expectations, integrals) numerical optimizaiton methods (e.g.Â maximizing a multi-parameter likelihood function using optim) 1.2.7 Lastly It is important to communicate your results to other after performing the data analysis. Therefore, you will do a project with presentation and report. 1.3 Lets Get Started The best way to learn R is to get started immediately and try the code by yourselves. We will not discuss every topic in detail at the beginning, which is not interesting and unnecessary. We shall revisit the topics when we need additional knowledge. Simple arithmetic expression # can be used a simple calculator 3 + 5 ## [1] 8 4 * 2 ## [1] 8 10 / 2 ## [1] 5 Comment a code: use the hash mark # # this is a comment, R will not run the code behine # Function for combining c(4, 2, 3) # &quot;c&quot; is to &quot;combine&quot; the numbers ## [1] 4 2 3 Assignment (&lt;- is the assignment operator like = in many other programming languages) y &lt;- c(4, 2, 3) # create a vector called y with elements 4, 2, 3 c(1, 3, 5) -&gt; v # c(1,3,5) is assigned to v Output y ## [1] 4 2 3 v ## [1] 1 3 5 R is case-sensitive. When you type Y, you will see an error message: object Y not found Y ## Error in eval(expr, envir, enclos): object &#39;Y&#39; not found 1.4 R Data Structures Reading: ML with R Ch2 Most frequently used data structures in R: vectors, factors, lists, arrays, matrices, data frames 1.4.1 Vectors Vector fundamental R data structure stores an ordered set of values called elements elements must be of the same type Type: integer, double, character, logical Integer, double, logical, character vectors x &lt;- 1:2 # integer vector, we use a:b to form the sequence of integers from a to b typeof(x) # type of the vector ## [1] &quot;integer&quot; x &lt;- c(1.1, 1.2) # double vector typeof(x) ## [1] &quot;double&quot; length(x) # length of the vector x ## [1] 2 x &lt; 2 # logical (TRUE/FALSE) ## [1] TRUE TRUE p &lt;- c(TRUE, FALSE) subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) # character vector Combine two vectors y &lt;- c(2, 4, 6) c(x, y) # note that we created x above ## [1] 1.1 1.2 2.0 4.0 6.0 c(y, subject_name) # 2, 4, 6 become characters &quot;2&quot;, &quot;4&quot;, &quot;6&quot; ## [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; Assessing elements in the vectors y &lt;- c(2, 4, 6) y[2] # second element ## [1] 4 y[3] # third element ## [1] 6 1.4.2 Factors A factor is a special type of vector that is solely used for representing categorical (male, female/group 1, group 2, group 3) or ordinal (cold, warm, hot/ low, medium, high) variables. Reasons for using factor the category labels are stored only once. E.g., rather than storing MALE, MALE, MALE, FEMALE, the computer may store 1,1,1,2(save memory) many machine learning algorithms treat categorical/ordinal and numeric features differently and may require the input as a factor Create a factor gender &lt;- factor(c(&quot;MALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;)) gender ## [1] MALE MALE FEMALE MALE ## Levels: FEMALE MALE # compared with c(&quot;MALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;) ## [1] &quot;MALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; 1.4.3 Matrix Matrix a collection of numbers in a rectangular form A matrix with dimension n by m means the matrix has n rows and m columns. Create Matrix: To create a \\(3\\times 4\\) matrix with elements 1:12 filled in column-wise A &lt;- matrix(1:12, nrow = 3, ncol = 4) # note that we use = instead of &lt;- A ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Dimension, number of rows, number of columns of a matrix # again R is case-sensitive, a and A are different dim(A) # to find the dimension of A ## [1] 3 4 nrow(A) # to find the number of row in A ## [1] 3 ncol(A) # to find the number of column in A ## [1] 4 By default, the matrix is filled column-wise. You can change to row-wise by adding byrow = TRUE B &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE) B ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 Select rows, columns, submatrix, element A[1, 2] # select the element in the 1st row and 2nd column ## [1] 4 A[2, ] # select 2nd row ## [1] 2 5 8 11 A[, 3] # select 3rd column ## [1] 7 8 9 A[1:2, 3:4] # select a submatrix ## [,1] [,2] ## [1,] 7 10 ## [2,] 8 11 Try: A[c(1, 2), c(1, 3, 4)] A[-1, ] A[, -2] Combine Two Matrices cbind(A, B) # combine column-wise ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1 4 7 10 1 2 3 4 ## [2,] 2 5 8 11 5 6 7 8 ## [3,] 3 6 9 12 9 10 11 12 rbind(A, B) # combine row-wise ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [4,] 1 2 3 4 ## [5,] 5 6 7 8 ## [6,] 9 10 11 12 Try: rbind(B, A) Transpose x &lt;- c(1, 2, 3) t(x) # transpose ## [,1] [,2] [,3] ## [1,] 1 2 3 Q &lt;- matrix(1:4, 2, 2) Q ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 t(Q) # transpose ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 Matrix Addition A &lt;- matrix(1:6, nrow = 2, ncol = 3) B &lt;- matrix(2:7, nrow = 2, ncol = 3) A + B ## [,1] [,2] [,3] ## [1,] 3 7 11 ## [2,] 5 9 13 A - B ## [,1] [,2] [,3] ## [1,] -1 -1 -1 ## [2,] -1 -1 -1 A + 2 ## [,1] [,2] [,3] ## [1,] 3 5 7 ## [2,] 4 6 8 c &lt;- c(1, 2) A + c ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 4 6 8 Elementwise Product A &lt;- matrix(1:6, nrow = 2, ncol = 3) B &lt;- matrix(1:2, nrow = 2, ncol = 3) A * B ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 4 8 12 c &lt;- 2 A * c ## [,1] [,2] [,3] ## [1,] 2 6 10 ## [2,] 4 8 12 c &lt;- c(10, 100) A * c ## [,1] [,2] [,3] ## [1,] 10 30 50 ## [2,] 200 400 600 c &lt;- c(10, 100, 1000) A * c # do you notice the pattern? ## [,1] [,2] [,3] ## [1,] 10 3000 500 ## [2,] 200 40 6000 Matrix Multiplication A &lt;- matrix(1:12, nrow = 3, ncol = 4) # 3x4 matrix t(A) # 4x3 matrix ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 t(A) %*% A #3x3 matrix, %*% = matrix multiplication ## [,1] [,2] [,3] [,4] ## [1,] 14 32 50 68 ## [2,] 32 77 122 167 ## [3,] 50 122 194 266 ## [4,] 68 167 266 365 B &lt;- matrix(1:9, nrow = 3, ncol = 3) B %*% A ## [,1] [,2] [,3] [,4] ## [1,] 30 66 102 138 ## [2,] 36 81 126 171 ## [3,] 42 96 150 204 A %*% B # error, non-conformable arguments ## Error in A %*% B: non-conformable arguments Diagonal Matrix diag(1:4) # diagonal matrix with diagonal elements being 1:4 ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 2 0 0 ## [3,] 0 0 3 0 ## [4,] 0 0 0 4 A &lt;- matrix(1:9, 3, 3) A ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 diag(A) # find the diagonal of A ## [1] 1 5 9 How to create an identity matrix in R? Inverse The inverse of a \\(n \\times n\\) matrix \\(A\\), denoted by \\(A^{-1}\\), is a \\(n \\times n\\) matrix such that \\(AA^{-1} = A^{-1} A = I_n\\), where \\(I_n\\) is the \\(n\\times n\\) identity matrix. To find the inverse of \\(A\\) in R: solve A &lt;- matrix(c(1, 0, 0, 3), 2, 2) solve(A) ## [,1] [,2] ## [1,] 1 0.0000000 ## [2,] 0 0.3333333 Some Statistical Applications I will mention a few connections of matrices with statistics. A dataset is naturally a matrix. Suppose that you have \\(n\\) people. You collected their health information: blood pressure, height, weight, age, whether they smoke (1 if yes, 0 if no), whether they drink (1/0), etc. Linear regression: we observe \\((x,y)\\), where \\(x\\) is a vector of covariates and \\(y\\) is your response. For example, \\(y\\) is the blood pressure, \\(x\\) is the collection of other health information. The linear regression model assumes that \\[y = \\beta_0 + x^T \\beta_1 + \\varepsilon,\\] where \\(\\varepsilon\\) is the error term. Our goal is to estimate \\(\\beta:=(\\beta_0, \\beta_1)\\). Let \\(X\\) be the design matrix. That is \\(X\\) is a \\(n \\times p\\) matrix where \\(n\\) is the number of observation, \\(p-1\\) is the number of covariates. The least squares solution for \\(\\beta\\) is \\[\\hat{\\beta} = (X^T X)^{-1}X^TY.\\] We will revisit linear regression later (I know some of you may not know linear regression). Correlation matrix, Covariance matrix. Let \\(X\\) be a random vector (column vector). The covariance matrix is defined as \\[\\Sigma := E[(X-E(X))(X-E(X))^T].\\] 1.4.4 Lists store an ordered set of elements like a vector can store different R data types (unlike a vector) # let&#39;s create some vectors (of different types) subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) # at this point, you should notice that meaningful names should be used for the variables temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) # notice how we use _ to separate two words # this is one of the styles in coding, you should be consistent with your style data &lt;- list(fullname = subject_name, temperature = temperature, flu_status = flu_status) # you may wonder what is the meaning of temperature = temperature # in &quot;fullname = subject_name&quot; # on the left of = is the name of the 1st element of your list # on the right of = is the name of the variable that you want to # assign the value to the 1st element data ## $fullname ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; ## ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE To assess the element of a list: data$flu_status ## [1] FALSE FALSE TRUE data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE data[2:3] # if you don&#39;t have the names ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE 1.4.5 Data frames Data frame can be understood as a list of vectors, each having exactly the same number of values, arranged in a structure like a spreadsheet or database gender &lt;- c(&quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;) blood &lt;- c(&quot;O&quot;, &quot;AB&quot;, &quot;A&quot;) pt_data &lt;- data.frame(subject_name, temperature, flu_status, gender, blood) pt_data ## subject_name temperature flu_status gender blood ## 1 John 98.1 FALSE MALE O ## 2 Jane 98.6 FALSE FEMALE AB ## 3 Steve 101.4 TRUE MALE A colnames(pt_data) ## [1] &quot;subject_name&quot; &quot;temperature&quot; &quot;flu_status&quot; &quot;gender&quot; &quot;blood&quot; pt_data$subject_name ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; pt_data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE pt_data[1, ] # like a matrix ## subject_name temperature flu_status gender blood ## 1 John 98.1 FALSE MALE O pt_data[, 2:3] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE Create a new column pt_data$temp_c &lt;- (pt_data$temperature - 32) * 5 / 9 pt_data ## subject_name temperature flu_status gender blood temp_c ## 1 John 98.1 FALSE MALE O 36.72222 ## 2 Jane 98.6 FALSE FEMALE AB 37.00000 ## 3 Steve 101.4 TRUE MALE A 38.55556 pt_data$fever &lt;- (pt_data$temp_c &gt; 37.6) pt_data ## subject_name temperature flu_status gender blood temp_c fever ## 1 John 98.1 FALSE MALE O 36.72222 FALSE ## 2 Jane 98.6 FALSE FEMALE AB 37.00000 FALSE ## 3 Steve 101.4 TRUE MALE A 38.55556 TRUE 1.5 Operators Priority Operator Meaning 1 $ component selection 2 [ [[ subscripts, elements 3 ^ (caret) exponentiation 4 - unary minus 5 : sequence operator 6 %% %/% %*% modulus, integer divide, matrix multiply 7 * / multiply, divide 8 + - add, subtract 9 &lt; &gt; &lt;= &gt;= == != comparison 10 ! not 11 &amp; | &amp;&amp; || logical and, logical or 12 &lt;- -&gt; = assignments # $ for list, data frame, etc subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) data &lt;- list(fullname = subject_name, temperature = temperature, flu_status = flu_status) data$fullname ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; pt_data &lt;- data.frame(subject_name, temperature, flu_status) pt_data$temperature ## [1] 98.1 98.6 101.4 # [ ], [[]] x &lt;- c(1, 5, 7) x[2] ## [1] 5 data[[1]] ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; # x^r = x to the power of r x &lt;- 2 x^4 # 16 ## [1] 16 # modulus 7 %% 2 # 7 divided by 2 equals 3 but it remains 1, modulus = reminder ## [1] 1 10 %% 3 ## [1] 1 20 %% 2 ## [1] 0 # integer division 7 %/% 2 ## [1] 3 20 %/% 3 ## [1] 6 Comparison # &lt;, &gt;, &lt;=, &gt;=, ==, != x &lt;- 2 x &gt; 3 ## [1] FALSE x &lt; 4 ## [1] TRUE x &lt;- c(1, 5, 7) x &lt; 3 # compare each element with 3 ## [1] TRUE FALSE FALSE x &gt;= 5 ## [1] FALSE TRUE TRUE x == 5 # if x is equal to 5, not x = 5 ## [1] FALSE TRUE FALSE x != 5 # if x is not equal to 5 ## [1] TRUE FALSE TRUE x &lt;- TRUE !x # not x ## [1] FALSE x &lt;- 2 x &lt;= 2 ## [1] TRUE !(x &lt;= 2) ## [1] FALSE &amp; and &amp;&amp; indicate logical AND. The shorter form performs elementwise comparisons. The longer form examine only the first element of each vector. x &lt;- c(TRUE, FALSE, TRUE) y &lt;- c(FALSE, FALSE, TRUE) x &amp; y ## [1] FALSE FALSE TRUE x &amp;&amp; y ## [1] FALSE z &lt;- c(TRUE) x &amp;&amp; z ## [1] TRUE | and || indicate logical OR. The shorter form performs elementwise comparisons. The longer form examine only the first element of each vector. x &lt;- c(TRUE, FALSE, TRUE) y &lt;- c(FALSE, FALSE, TRUE) x | y ## [1] TRUE FALSE TRUE x || y ## [1] TRUE z &lt;- c(TRUE) x || z ## [1] TRUE Assignment # these assignments are the same, it is recommended to use &lt;- a &lt;- 2 a ## [1] 2 2 -&gt; b b ## [1] 2 c = 2 c ## [1] 2 Do !(x &gt; 1) &amp; (x &lt; 4) and !((x &gt; 1) &amp; (x &lt; 4)) give different results? 1.5.1 Vectorized Operators An important property of many of the operators is that they are vectorized. This means that the operation will be performed elementwise. x &lt;- c(1, 2, 3) y &lt;- c(5, 6, 7) x + y ## [1] 6 8 10 x * y ## [1] 5 12 21 2 * x # you do not need to use c(2,2,2)*x ## [1] 2 4 6 y / 2 # you do not need to use y/c(2,2,2) ## [1] 2.5 3.0 3.5 A &lt;- matrix(1:9, nrow = 3, ncol = 3) B &lt;- matrix(1:9, nrow = 3, ncol = 3) A + B ## [,1] [,2] [,3] ## [1,] 2 8 14 ## [2,] 4 10 16 ## [3,] 6 12 18 A * B # this is not matrix multiplication, but elementiwse multiplication ## [,1] [,2] [,3] ## [1,] 1 16 49 ## [2,] 4 25 64 ## [3,] 9 36 81 x &lt;- c(1, 3, 5) y &lt;- c(2, 2, 9) x &lt; y ## [1] TRUE FALSE TRUE 1.6 Built-in Functions Common mathematical functions sqrt, abs, sin, cos, log, exp. To get help on the usage of a function. Use ?. For example, if you want to know more about log. Type ?log in the console. You will then see that by default, log computes the natrual logarithms. Other useful functions Name Operations ceiling smallest integer greater than or equal to element floor largest integer less than or equal to element trunc ignore the decimal part round round up for positive and round down for negative sort sort the vector in ascending or descending order sum, prod sum and produce of a vector cumsum, cumprod cumulative sum and product min, max return the smallest and largest values range return a vector of length 2 containing the min and max mean return the sample mean of a vector var return the sample variance of a vector sd return the sample standard deviation of a vector seq generate a sequence of number rep replicate elements in a vector Note: If you have data \\(x_1,\\ldots,x_n\\), the sample variance is defined as \\[ S^2_n := \\frac{1}{n-1} \\sum^n_{i=1}(x_i-\\overline{x}_n)^2. \\] Note that we divide the sum by \\(n-1\\) but not \\(n\\). The sample standard deviation is the square root of the sample variance. x &lt;- 1:5 y &lt;- sqrt(x) y ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 ceiling(y) ## [1] 1 2 2 2 3 sum(x) ## [1] 15 prod(x) ## [1] 120 cumsum(x) ## [1] 1 3 6 10 15 cumprod(x) ## [1] 1 2 6 24 120 min(x) ## [1] 1 max(x) ## [1] 5 range(x) ## [1] 1 5 mean(x) ## [1] 3 var(x) ## [1] 2.5 rep(0, 10) # create a vector of length 10 with all elements being 0 ## [1] 0 0 0 0 0 0 0 0 0 0 rep(1, 10) # create a vector of length 10 with all elements being 1 ## [1] 1 1 1 1 1 1 1 1 1 1 1.6.1 sort() x &lt;- c(1, 5, 3, 10) sort(x) # default = ascending order ## [1] 1 3 5 10 sort(x, decreasing = TRUE) # descending order ## [1] 10 5 3 1 1.6.2 seq() This is an example of function with more than one argument. # seq(from, to) seq(1:5) ## [1] 1 2 3 4 5 seq(from = 1, to = 5) ## [1] 1 2 3 4 5 # seq(from, to, by) seq(1, 5, 2) ## [1] 1 3 5 seq(from = 1, to = 5, by = 2) ## [1] 1 3 5 # seq(from, to, length) seq(0, 10, length = 21) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 ## [16] 7.5 8.0 8.5 9.0 9.5 10.0 seq(from = 0, to = 10, length = 21) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 ## [16] 7.5 8.0 8.5 9.0 9.5 10.0 1.6.3 rep() # rep(data, times), try ?rep rep(0, 10) ## [1] 0 0 0 0 0 0 0 0 0 0 rep(c(1, 2, 3), 3) ## [1] 1 2 3 1 2 3 1 2 3 1.6.4 pmax, pmin x &lt;- c(1, 3, 5) y &lt;- c(2, 4, 4) max(x, y) # maximum of x and y ## [1] 5 min(x, y) # minimum of x and y ## [1] 1 pmax(x, y) # elementwise comparison ## [1] 2 4 5 pmin(x, y) # elementwise comparison ## [1] 1 3 4 1.7 Some Useful RStudio Shortcuts See also https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts Ctrl + 1: Move focus to the Source Editor (when you are in the Console) Ctrl + 2: Move focus to the Console (when you are in the source window) \\(\\uparrow\\) (the up arrow key on the keyboard): go to the previous command (in the console) \\(\\downarrow\\) (the down arrow key on the keyboard): go to the next command (in the console) Esc: Delete the current command/ Interrupt currently executing command Ctrl + Tab: go to the next tab 1.8 Exercises To test your understanding, try to evaluate the following code by hand and then check with the output from R. x &lt;- (10:1)[c(-1, -4)] x &lt;- x^2 x[5] # what do you expect to see? a&lt;-c(1:3, 6, 3 + 3, &quot;sta&quot;, 3 &gt; 5) a #? typeof(a) #? length(a) #? x&lt;-rep(1:6, rep(1:3, 2)) x %% 2 == 0 #? x[x %% 2 == 0] #? round(-3.7) #? trunc(-3.7) #? floor(-3.7) #? ceiling(-3.7) #? round(3.8) #? trunc(3.8) #? floor(3.8) #? ceiling(3.8) #? x &lt;- c(4, 3, 8, 7, 5, 6, 2, 1) sort(x) #? order(x) #? sum(x) + prod(x) #? cumsum(x) + cumprod(x) #? max(x) + min(x) #? 1.9 Comments to Exercises These are comments but not answers but you can get the answers immediately by running the code. x &lt;- (10:1)[c(-1, -4)] # 10:1 will give you a vector with elements 10, 9, 8,...,1 10:1 ## [1] 10 9 8 7 6 5 4 3 2 1 # [c(-1, -4)] will remove the 1st and 4th elements in the vector # therefore 10 and 7 will be removed from 10:1 given x ## [1] 9 8 6 5 4 3 2 1 x &lt;- x^2 # ^2 will square each of the elements in the vector x[5] # 36 ## [1] 16 a&lt;-c(1:3, 6, 3 + 3, &quot;sta&quot; , 3 &gt; 5) a # when you combine numeric, characters and logical values, the results become character ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;6&quot; &quot;6&quot; &quot;sta&quot; &quot;FALSE&quot; typeof(a) # character ## [1] &quot;character&quot; length(a) # ## [1] 7 x&lt;-rep(1:6, rep(1:3, 2)) # we first evaluate rep(1:3, 2), which is # 1 2 3 1 2 3 # rep then replicates the elements by the corresponding number of times # 1 is repeated 1 time # 2 is repeated 2 times # 3 is repeated 3 times # 4 is repeated 1 time # 5 is repeated 2 times # 6 is repeated 3 times x %% 2 == 0 # check if the modulus is 0 when x is divided by 2 ## [1] FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE # this is equivalent to ask if the elements are even x[x %% 2 == 0] # find out all the even elements ## [1] 2 2 4 6 6 6 # if you need to use these functions # you may try with a positive number and a negative number # to see if the results are what you want round(-3.7) # try round(-3.5), round(-3.4) ## [1] -4 trunc(-3.7) ## [1] -3 floor(-3.7) ## [1] -4 ceiling(-3.7) ## [1] -3 round(3.8) ## [1] 4 trunc(3.8) ## [1] 3 floor(3.8) ## [1] 3 ceiling(3.8) ## [1] 4 x&lt;-c(4, 3, 8, 7, 5, 6, 2, 1) sort(x) # asecending order ## [1] 1 2 3 4 5 6 7 8 order(x) # from the result, can you guess what it does? ## [1] 8 7 2 1 5 6 4 3 # order(x) returns a permutation which rearranges its first argument into ascending or descending order # that is, x[order(x)] = sort(x) sum(x) + prod(x) ## [1] 40356 cumsum(x) + cumprod(x) ## [1] 8 19 111 694 3387 20193 40355 40356 max(x) + min(x) ## [1] 9 "],["probability.html", "Chapter 2 Probability 2.1 Probability Distributions 2.2 Simulation", " Chapter 2 Probability Optional Reading: R Cookbook Ch8 2.1 Probability Distributions Using the normal distribution as an example: Function Purpose dnorm Normal density pnorm Normal CDF qnorm Normal quantile function rnorm Normal random variables Examples Density of \\(N(2, 3^2)\\) at \\(5\\). dnorm(5, mean = 2, sd = 3) ## [1] 0.08065691 \\(P(X \\leq 3)\\), where \\(X \\sim N(2, 3^2)\\) pnorm(3, mean = 2, sd = 3) ## [1] 0.6305587 # &quot;mean =&quot; and &quot;sd =&quot; are optional pnorm(3, 2, 3) ## [1] 0.6305587 Generate 10 random variables, each follows \\(N(3, 4^2)\\). rnorm(10, 3, 4) ## [1] -1.9158384 -0.9097009 1.4642193 0.2965204 2.3519916 7.4867494 3.5952755 ## [8] 4.9657566 2.0905154 9.2462552 95th percenttile of \\(N(0, 1)\\). Find \\(q\\) such that \\(P(Z \\leq q) = 0.95\\) qnorm(0.95, 0, 1) ## [1] 1.644854 Plotting the normal density x &lt;- seq(-4, 4, by = 0.1) plot(x, dnorm(x), type = &quot;l&quot;, main = &quot;Density of N(0,1)&quot;) # &quot;l&quot; for lines 2.1.1 Common Distributions Common discrete distributions Discrete distribution R name Parameters Binomial binom n = number of trials; p = probability of success for one trial Geometric geom p = probability of success for one trial Negative binomial (NegBinomial) nbinom size = number of successful trials; either prob = probability of successful trial or mu = mean Poisson pois lambda = mean Common continuous distributions Continuous distribution R name Parameters Beta beta shape1; shape2 Cauchy cauchy location; scale Chi-squared (Chisquare) chisq df = degrees of freedom Exponential exp rate F f df1 and df2 = degrees of freedom Gamma gamma rate; either rate or scale Log-normal (Lognormal) lnorm meanlog = mean on logarithmic scale; sdlog = standard deviation on logarithmic scale Logistic logis location; scale Normal norm mean; sd = standard deviation Students t (TDist) t df = degrees of freedom Uniform unif min = lower limit; max = upper limit To get help on the distributions: ?dnorm ?dbeta ?dcauchy # the following distributions need to use different code ?TDist ?Chisquare ?Lognormal Examples (Using Binomial as an Example) dbinom(2, 10, 0.6) # p_X(2), p_X is the pmf of X, X ~ Bin(n=10, p=0.6) ## [1] 0.01061683 pbinom(2, 10, 0.6) # F_X(2), F_X is the CDF of X, X ~ Bin(n=10, p=0.6) ## [1] 0.01229455 qbinom(0.5, 10, 0.6) # 50th percentile of X ## [1] 6 rbinom(4, 10, 0.6) # generate 4 random variables from Bin(n=10, p=0.6) ## [1] 5 7 6 7 x &lt;- 0:10 plot(x, dbinom(x, 10, 0.6), type = &quot;h&quot;) # &quot;h&quot; for histogram like vertical lines 2.1.2 Exercises The average number of trucks arriving on any one day at a truck depot in a certain city is known to be 12. Assuming the number of trucks arriving on any one day has a Poisson distribution, what is the probability that on a given day fewer than 9 (strictly less than 9) trucks will arrive at this depot? ppois(8, 12) ## [1] 0.1550278 Let \\(Z \\sim N(0, 1)\\). Find \\(c\\) such that \\(P(Z \\leq c) = 0.1151\\) qnorm(0.1151) ## [1] -1.199844 \\(P(1\\leq Z \\leq c) = 0.1525\\) c &lt;- qnorm(pnorm(1) + 0.1525) # draw a graph # test the answer pnorm(c) - pnorm(1) ## [1] 0.1525 \\(P(-c \\leq Z \\leq c) = 0.8164\\). # P(0 &lt;= Z &lt;= c) = 0.8164/2 # P(Z &lt;= c) = 0.8164/2 + 0.5 c &lt;- qnorm(0.8164 / 2 + 0.5) # test our answer pnorm(c)- pnorm(-c) ## [1] 0.8164 Plot the density of a chi-squared distribution with degrees of freedom \\(4\\), from \\(x=0\\) to \\(x=10\\). Find the 95th percentile of this distribution. # note that a chi-squared r.v. is nonnegative x &lt;- seq(0, 10, by = 0.1) plot(x, dchisq(x, df = 4), type = &quot;l&quot;) qchisq(0.95, df = 4) ## [1] 9.487729 Simulate \\(10\\) Bernoulli random variables with parameter \\(0.6\\). # Bernoulli(p) = Bin(1, p) rbinom(10, size = 1, prob = 0.6) ## [1] 1 1 1 1 1 1 1 1 1 1 Plot the Poisson pmf with parameter \\(2\\) from \\(x = 0\\) to \\(x = 10\\). x &lt;- 0:10 plot(x, dpois(x, 2), type = &quot;h&quot;) Draw a plot to illustrate that the 97.5th percentile of the t distribution will be getting closer to that of the standard normal distribution when the degrees of freedom increases. x &lt;- 10:200 plot(x, qt(0.975, df = x), type = &quot;l&quot;, ylim = c(1.9,2.3)) # add a horizontal line with value at qnorm(0.975) # lty = 2 for dashed line, check ?par abline(h = qnorm(0.975), lty = 2) # Therefore, for a large sample, t-test and z-test will give you similar result. 2.2 Simulation We have already seen how to use functions like runif, rnorm, rbinom to generate random variables. R actually generates pseudo-random number sequence (deterministic sequence of numbers that approximates the properties of random numbers) The pseduo-random number sequence will be the same if it is initialized by the same seed (can be used to reproduce the same simulation results or used to debug). # every time you run the first two lines, you get the same result set.seed(1) runif(5) ## [1] 0.2655087 0.3721239 0.5728534 0.9082078 0.2016819 # every time you run the following code, you get a different result runif(5) ## [1] 0.89838968 0.94467527 0.66079779 0.62911404 0.06178627 Sampling from discrete distributions Usage of sample: sample(x, size, replace = FALSE, prob = NULL) See also ?sample. sample(10) # random permutation of integers from 1 to 10 ## [1] 3 1 5 8 2 6 10 9 4 7 sample(10, replace = T) # sample with replacement ## [1] 5 9 9 5 5 2 10 9 1 4 sample(c(1, 3, 5), 5, replace = T) ## [1] 5 3 3 3 3 # simulate 20 random variables from a discrete distribution sample(c(-1,0,1), size = 20, prob = c(0.25, 0.5, 0.25), replace = T) ## [1] -1 1 1 -1 0 0 1 1 0 -1 0 0 0 0 0 1 1 0 -1 0 Example: Suppose we have a fair coin and we play a game. We flip the coin. We win $1 if the result is head and lose $1 if the result is tail. You play the game 100 times. You are interested in the cumulative profit. set.seed(1) # R actually generates pseudo random numbers # setting the seed ensure that each time you will get the same result # for illustration, code debugging, reproducibility profit &lt;- sample(c(-1, 1), size = 100, replace = T) plot(cumsum(profit), type = &quot;l&quot;) set.seed(2) profit &lt;- sample(c(-1, 1), size = 100, replace = T) plot(cumsum(profit), type = &quot;l&quot;) Example: You have two dice \\(A\\) and \\(B\\). For die \\(A\\), there are \\(6\\) sides with numbers \\(1,2,3,4,5,6\\) and the corresponding probability of getting these values are \\(0.1,0.1,0.1,0.1,0.1,0.5\\). For die \\(B\\), there are \\(4\\) sides with numbers \\(1,2,3,7\\) and the corresponding probability of getting these values are \\(0.3,0.2,0.3,0.2\\). You roll the two dice independently. Estimate \\(P(X &gt; Y)\\) using simulation, where \\(X\\) is the result from die \\(A\\) and \\(Y\\) is the result from die \\(B\\). n &lt;- 10000 # number of simulations X &lt;- sample(1:6, size = n, replace = TRUE, prob = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.5)) Y &lt;- sample(c(1, 2, 3, 7), size = n, replace = TRUE, prob = c(0.3, 0.2, 0.3, 0.2)) mean(X &gt; Y) ## [1] 0.6408 Why the sample mean approximates the required probability? Recall the strong law of large numbers (SLLN). Let \\(X_1,\\ldots,X_n\\) be independent and identically distributed random variables with mean \\(\\mu:=E(X)\\). Let \\(\\overline{X}_n:= \\frac{1}{n} \\sum^n_{i=1}X_i\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\rightarrow} \\mu.\\] Note: a.s. means almost surely. The above convergence means \\(P(\\lim_{n\\rightarrow \\infty} \\overline{X}_n = \\mu) = 1\\). Note that (the expectation of an indicator random variable is the probability that the corresponding event will happen) \\[ P(X&gt;Y) = E(I(X&gt;Y)).\\] To apply SLLN, we just need to recognize the underlying random variable is \\(I(X&gt;Y)\\). Then, with probability \\(1\\), the sample mean \\[ \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i) \\rightarrow P(X&gt;Y).\\] The quantity on the LHS is what we compute in mean(X&gt;Y) (note that we are using vectorized comparison). We will see additional simulation examples after we talk about some programming in R There are many important topics that we will not discuss algorithms for simulating random variables inverse transform acceptance rejection Markov Chain Monte Carlo methods to reduce variance in simulation Control variates Antithetic variates Importance sampling "],["programming-in-r.html", "Chapter 3 Programming in R 3.1 Writing functions in R 3.2 Control Flow 3.3 Automatically Reindent Code 3.4 Speed Consideration 3.5 Another Simulation Example", " Chapter 3 Programming in R Optional reading: R Cookbook Ch 15, R for data science https://r4ds.had.co.nz/program-intro.html Hands-on programming with R: https://rstudio-education.github.io/hopr/ 3.1 Writing functions in R When you have to copy and paste some code more than 2 times, you should consider writing a function Writing a function can simplify your code and isolate the main part of your program General format of a function function_name &lt;- function(argument1, argument2) { statements } Example: Lets try to write a function to compute the sample variance. my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2) / (n - 1)) } y &lt;- 1:9 my_var(y) ## [1] 7.5 var(y) # compared with the bulit-in function ## [1] 7.5 x &lt;- rnorm(1000, mean = 0, sd = 2) my_var(x) ## [1] 4.262595 var(x) # why the result is not equal to 4? ## [1] 4.262595 We can also write my_var2 &lt;- function(x) {sum((x - mean(x))^2) / (length(x) - 1)} The variable x is the argument to be passed into the function. The variables mean_x and n are local variables whose scope is within this function. y &lt;- 2 f &lt;- function(x) { y &lt;- x x &lt;- 4 y } y ## [1] 2 f(3) # output the value f(3) ## [1] 3 y # y is unchanged, y is defined in the global environment ## [1] 2 x ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found We shall write code using proper indentation (easier to read and debug) # with indentation (use this one) my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2) / (n - 1)) } # no indentation my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2) / (n - 1)) } The number of arguments passed to a function can be more than one Example: Write a function to compute the pooled sample standard deviation of two independent samples \\(x_1,\\ldots,x_n\\) and \\(y_1,\\ldots,y_m\\) of sizes \\(n\\) and \\(m\\). Recall that the pooled sample standard deviation is defined as: \\[ S_p := \\sqrt{\\frac{(n-1)S^2_X + (m-1)S^2_Y}{m+n-2}}, \\] where \\(S^2_X\\) and \\(S^2_Y\\) are the sample variances of \\(x_1,\\ldots,x_n\\) and \\(y_1,\\ldots,y_m\\), respectively. pooled_sd &lt;- function(x, y) { n &lt;- length(x) m &lt;- length(y) return(sqrt(((n - 1) * var(x) + (m - 1) * var(y)) / (m + n - 2))) } Remark: if the final statement will output something, it will be the output of the function. You can also use return() as above. That is, pooled_sd and pooled_sd2 are exactly the same. pooled_sd2 &lt;- function(x, y) { n &lt;- length(x) m &lt;- length(y) sqrt(((n - 1) * var(x) + (m - 1) * var(y)) / (m + n - 2)) } You can return more than one value in a function my_var_sd &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) my_var &lt;- sum((x - mean_x)^2) / (n - 1) return(c(my_var, sqrt(my_var))) } You may also return a list my_var_sd &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) my_var &lt;- sum((x - mean_x)^2) / (n - 1) output &lt;- list(var = my_var, sd = sqrt(my_var)) return(output) } Example: write a function called my_summary that will output a list with elements being equal to the mean, sd, median, min and max of a given vector. my_summary &lt;- function(x){ output &lt;- list(mean = mean(x), sd = sd(x), median = median(x), min = min(x), max = max(x)) return(output) } my_summary(1:10) ## $mean ## [1] 5.5 ## ## $sd ## [1] 3.02765 ## ## $median ## [1] 5.5 ## ## $min ## [1] 1 ## ## $max ## [1] 10 Define a function with default value my_power &lt;- function(x, p = 2) { return(x^p) } my_power(3) # by default, p = 2, will compute 3^2 ## [1] 9 my_power(3, 3) # will compute 3^3 ## [1] 27 3.2 Control Flow 3.2.1 for loop You can use a for loop when you know how many times you will loop. Syntax: for (var in sequence) { statement # do this statement for each value of i } Examples: for (i in 1:5) { # note: you do not have to define i beforehand print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 for (i in c(1, 3, 6)) { print(i) } ## [1] 1 ## [1] 3 ## [1] 6 Examples Write R code to find \\(\\sum^{10}_{i=1} \\sum^4_{j=1} \\frac{i^2}{(i+j)^2}\\). sum &lt;- 0 for (i in 1:10) { for (j in 1:4) { sum &lt;- sum + i^2 / (i + j)^2 } } sum ## [1] 18.26491 Write R code to find \\(\\sum^{10}_{i=1} \\sum^i_{j=1} \\frac{i^2}{(i+j)^3}\\). sum &lt;- 0 for (i in 1:10) { for (j in 1:i) { sum &lt;- sum + i^2 / (i + j)^3 } } sum ## [1] 2.779252 3.2.2 while loop You can use a while loop if you want to loop until a specific condition is met. For example, when you minimize a function numerically using some iterative algorithm, you may want to stop when the objective value does not change much. You may not know how many loops are required in advance so that a while loop may be better than a for loop in this application. Syntax: while (condition) { statement # while the condition is TRUE, do this } A simple example: i &lt;- 1 while (i &lt; 6) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 # What happen if you change &quot;i &lt; 6&quot; to &quot;i &lt;= 6&quot;? Ans: will print 1 to 6 # What happen if you change &quot;i &lt; 6&quot; to &quot;i &lt;= 5&quot;? Ans: outputs are the same Another example: # find the smallest n such that 1^2+ 2^2+ ... + n^2 &gt; 65 sum &lt;- 0 i &lt;- 0 while (sum &lt; 65) { i &lt;- i + 1 sum &lt;- sum + i^2 print(c(i, sum)) } ## [1] 1 1 ## [1] 2 5 ## [1] 3 14 ## [1] 4 30 ## [1] 5 55 ## [1] 6 91 i # 6 ## [1] 6 3.2.3 if (cond) Syntax: if (condition) { statement # do this if the condition is TRUE } Example: write a function that outputs positive if a positive number is entered. # check if a number if positive my_pos &lt;- function(x) { if (x &gt; 0) { print(&quot;positive&quot;) } } my_pos(-2) my_pos(2) ## [1] &quot;positive&quot; 3.2.4 if (cond) else expr Syntax if (condition) { statement1 # do this if condition is TRUE } else { statement2 # do this if condition is FALSE } Example: # write my own absolute value function my_abs &lt;- function(x) { if (x&gt;=0) { return(x) } else { return(-x) } } my_abs(-2) ## [1] 2 my_abs(3) ## [1] 3 my_abs(0) ## [1] 0 Error-handling in a function: my_sqrt = function(x) { if (x &gt;= 0) { print(sqrt(x)) # do this if x &gt;= 0 } else { cat(&quot;Error: this is a negative number!&quot;) # do this otherwise } } my_sqrt(-2) ## Error: this is a negative number! 3.2.5 If else ladder Syntax # Example if (condition1) { statement1 } else if (condition2) { statement2 } else if (condition1) { statement3 } Example: score_to_grade = function(x) { if (x&gt;=90) { cat(&quot;A+&quot;) } else if (x &gt;= 85) { cat(&quot;A&quot;) } else if (x &gt;= 80) { cat(&quot;A-&quot;) } else { cat(&quot;B+ or below&quot;) } } # after you write the function, you should check each case carefully score_to_grade(92) ## A+ score_to_grade(88) ## A score_to_grade(83) ## A- score_to_grade(78) ## B+ or below 3.3 Automatically Reindent Code In programming, indentation is used to format code to improve readability. Although R does not care how many whitespace is present in each line, you should still maintain a consistent indentation style. To indent a block of code automatically, highlight the text in RStudio, then press Ctrl+i (Windows or Linux) or press Cmd+i (Mac). Poor indentation, difficult to read for (i in 1:5) { if (i &gt;= 3) { print(i * 2) } else { print(i * 3) } } ## [1] 3 ## [1] 6 ## [1] 6 ## [1] 8 ## [1] 10 Highlight the block of code, press Ctrl+i or Cmd+i for (i in 1:5) { if (i &gt;= 3) { print(i * 2) } else { print(i * 3) } } ## [1] 3 ## [1] 6 ## [1] 6 ## [1] 8 ## [1] 10 3.4 Speed Consideration While the computing power is getting stronger and stronger, we should still write code that runs efficiently. # suppose we want to simulate 200,000 normal random variables n &lt;- 200000 x &lt;- rep(0, n) # create a vector for storage of the values initial_time &lt;- proc.time() for (i in 1:n) { x[i] &lt;- rnorm(1) } proc.time() - initial_time ## user system elapsed ## 1.23 0.27 1.59 n &lt;- 200000 x &lt;- rep(0, n) # create a vector for storage of the values # Alternatively system.time({ for (i in 1:n) { x[i] &lt;- rnorm(1) } }) ## user system elapsed ## 1.19 0.34 1.55 The user time is the CPU time charged for the execution of user instructions of the calling process. The system time is the CPU time charged for execution by the system on behalf of the calling process. A much more efficient way for the same task is to use system.time({ n &lt;- 200000 x &lt;- rnorm(n) }) ## user system elapsed ## 0.05 0.00 0.04 Another example: set.seed(1) x &lt;- rnorm(2e6) y &lt;- rnorm(2e6) v &lt;- rep(0, 2e6) system.time({ for (i in 1:length(x)){ v[i] &lt;- x[i] + y[i] } }) ## user system elapsed ## 0.39 0.01 0.40 system.time(v &lt;- x + y) ## user system elapsed ## 0 0 0 The general rule is to use vectorized operations whenever possible and to avoid using for loops. We use a for loop when the code is not time-consuming or when the code is hard to write without using a for loop. A more advanced option is to combine C++ with R using the package rcpp. That is, we can write the most time-consuming part of the R code in C++, which could run many times faster (will not be discussed in this course). See also http://www.noamross.net/archives/2014-04-16-vectorization-in-r-why/ 3.5 Another Simulation Example A simple model on the stock return assumes that (i) \\[r_{t+1} := \\log \\frac{P_{t+1}}{P_t} \\sim N(\\mu,\\sigma^2), \\] where \\(r_{t+1}\\) is the log-return at Day \\(t+1\\), \\(P_t\\) is the stock price at the end of Day \\(t\\); (ii) \\(r_1,r_2,\\ldots\\) are iid. Simple algebra shows that \\[P_{t+1} = P_t e^{r_{t+1}}\\] and \\[P_{t+1} = P_0 e^{ \\sum^{t+1}_{i=1} r_i}.\\] Suppose that the current price of a certain stock \\(P_0\\) is \\(100\\), \\(\\mu = 0.0002\\) and \\(\\sigma = 0.015\\). Using simulation, estimate the probability that the price is below $95 at the close of at least one of the next 30 trading days. no_sim &lt;- 10000 # number of simulation below &lt;- rep(0, no_sim) for (i in 1:no_sim) { price &lt;- 100 * exp(cumsum(rnorm(30, mean = 0.0002, sd = 0.015))) below[i] &lt;- min(price) &lt; 95 } mean(below) ## [1] 0.4384 "],["creating-some-basic-plots.html", "Chapter 4 Creating Some Basic Plots 4.1 Scatter Plot 4.2 Line Graph 4.3 Bar Chart 4.4 Histogram 4.5 Box Plot 4.6 Plotting a function curve 4.7 More on plots with base R 4.8 Summary of ggplot", " Chapter 4 Creating Some Basic Plots The base R contains many basic methods for producing graphics. We will learn some of them in this chapter. For more elegant plots, we will use the package ggplot2. We will use some simple datasets in base R to illustrate how to create some basic plots in this chapter. In this next chapter, we will discuss how to input our own data and transform them. Reference: R graphics cookbook, R for data science, R Cookbook. We will load the package tidyverse. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. You have to install the package before you can use it. install.packages(&quot;tidyverse&quot;) Load the library library(tidyverse) # load the tidyverse package (which contains ggplot2) if we want to make it clear what package an object/ function comes from, use package name followed by two colons, like dplyr::mutate(). 4.1 Scatter Plot Lets take a look at the mtcars dataset. This dataset comes with base R. head(mtcars) # this is a data frame ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) # display the structure of the data frame ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... mpg: miles/gallon wt: weight (1000lbs) Scatter plot with base graphics # x-axis: mtcars$wt # y-axis: mtcars$mpg plot(x = mtcars$wt, y = mtcars$mpg) # &quot;x =&quot;, &quot;y =&quot; are optional You can produce the same plot with plot(mtcars$wt, mtcars$mpg) Scatter plot with ggplot2 ggplot(data = mtcars, aes(x = wt, y = mpg)) + geom_point() Scatter plot with base graphics (when you only have \\(x\\) and \\(y\\) but not a dataset) set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 2 * x + rnorm(100, 0, 1) plot(x, y) Scatter plot with ggplot2 (when you only have \\(x\\) and \\(y\\) but not a dataset) set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 2 * x + rnorm(100, 0, 1) ggplot(mapping = aes(x = x, y = y)) + # by default, data = NULL geom_point() 4.2 Line Graph The dataset pressure (also in base R) contains the relation between temperature in degrees Celsius and vapor pressure of mercury in millimeters (of mercury). Line graph with base graphics # the only difference from a scatter plot is that we add type=&quot;l&quot; plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) #l = line Line graph with base graphics with points plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) points(pressure$temperature, pressure$pressure) # add some points Line graph with base graphics with another line and points (with color) plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) points(pressure$temperature, pressure$pressure) # the additional line may not have a physical meaningful # just an illustration on how to add a line with base graphics lines(pressure$temperature, pressure$pressure / 2, col = &quot;red&quot;) points(pressure$temperature, pressure$pressure / 2, col = &quot;red&quot;) Colors in R You can go to https://www.r-graph-gallery.com/ggplot2-color.html and read more about colors in R. For example, you can specify the color by name, rgb, number and hex code. plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;, col = rgb(0.1, 0.2, 0.5, 1)) lines(pressure$temperature, pressure$pressure / 2, type = &quot;l&quot;, col = 2) lines(pressure$temperature, pressure$pressure * 2, col = &quot;#8B2813&quot;) lines(pressure$temperature, pressure$pressure * 3, col = &quot;cornflowerblue&quot;) Line graph with ggplot2 ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() Line graph with ggplot2 with points ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() + geom_point() Line graph with ggplot2 with another line and points (with color) ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() + geom_point() + geom_line(aes(x = temperature, y = pressure / 2), color = &quot;red&quot;) + geom_point(aes(x = temperature, y = pressure / 2), color = &quot;#8B2813&quot;) Remark: it is common with ggplot() to split the command on multiple lines, ending each line with a + so that R knows that the command will continue on the next line. 4.3 Bar Chart Two types of bar charts: Bar chart of values. x-axis: discrete variable, y-axis: numeric data (not necessarily count data) Bar chart of count. x-axis: discrete variable, y-axis: count of cases in the discrete variable Remark: for histogram: x-axis = continuous variable, y-axis = count of cases in the interval. The BOD data set has 6 rows and 2 columns giving the biochemical oxygen demand versus time in an evaluation of water quality. # instead of using head(BOD) and str(BOD), we can also change it to a &quot;tibble&quot; # we can view the first few lines and the data structure as_tibble(BOD) ## # A tibble: 6 x 2 ## Time demand ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 8.3 ## 2 2 10.3 ## 3 3 19 ## 4 4 16 ## 5 5 15.6 ## 6 7 19.8 Bar chart of values with base graphics # names.arg = a vector of names to be plotted below each bar or group of bars. barplot(BOD$demand, names.arg = BOD$Time) Bar chart of counts with base graphics In the dateset mtcars, cylis the number of cylinders in the car. The possible values are \\(4, 6\\), and \\(8\\). We first find the count of each unique value in mtcars$cyl: table(mtcars$cyl) ## ## 4 6 8 ## 11 7 14 To plot the bar chart, we use barplot(table(mtcars$cyl)) Bar chart with values with ggplot2 We first make the Time variable to a factor: BOD2 &lt;- BOD BOD2$Time &lt;- as.factor(BOD2$Time) ggplot(BOD2, aes(x = Time, y = demand)) + geom_col() What will happen if we do not make Time as a factor: ggplot(BOD, aes(x = Time, y = demand)) + geom_col() Bar chart of counts with ggplot2 # the y position is calculated by counting the number of rows for each value of cyl ggplot(mtcars, aes(x = cyl)) + geom_bar() 4.4 Histogram mpg in mtcars is the miles/gallon of the car. It is a continuous variable. Histogram with base graphics hist(mtcars$mpg) Histogram with base graphics # Specify approximate number of bins with &quot;breaks&quot; hist(mtcars$mpg, breaks = 10) Histogram with ggplo2 ggplot(mtcars, aes(x = mpg)) + geom_histogram() ggplot(mtcars, aes(x = mpg)) + geom_histogram(binwidth = 2.5) Remark: different bin widths will give you histograms with different looks. 4.5 Box Plot Lets take a look at another dataset ToothGrowth. In particular, supp is a factor. as_tibble(ToothGrowth) ## # A tibble: 60 x 3 ## len supp dose ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 ## 6 10 VC 0.5 ## 7 11.2 VC 0.5 ## 8 11.2 VC 0.5 ## 9 5.2 VC 0.5 ## 10 7 VC 0.5 ## # ... with 50 more rows Box plot with basic graphics (using plot) # if x is a factor, use the following code plot(x = ToothGrowth$supp, y = ToothGrowth$len) Box plot with basic graphics (using boxplot) # len ~ supp is an example of a &quot;formula&quot; (y ~ x) boxplot(len ~ supp, data = ToothGrowth) Box plot with basic graphics + interaction of two variables on x-axis (using boxplot) boxplot(len ~ supp + dose, data = ToothGrowth) Box plot with ggplot2 ggplot(ToothGrowth, aes(x = supp, y = len)) + geom_boxplot() Box plot with ggplot2 + interaction of two variables on x-axis ggplot(ToothGrowth, aes(x = interaction(supp, dose), y = len)) + geom_boxplot() What will happen if dose take a lot more values? ggplot(ToothGrowth, aes(x = interaction(supp, dose + 1:5), y = len)) + geom_boxplot() 4.6 Plotting a function curve curve(x^3 - 5 * x, from = -4, to = 4) Alternatively: x &lt;- seq(-4, 4, len = 1000) plot(x, x^3 - 5 * x, type = &quot;l&quot;) Plotting a built-in function curve(dnorm(x), from = -4, to = 4) Plotting a self-defined function my_function &lt;- function(x) { 1 / (1 + exp(-x + 10)) } curve(my_function, from = 0, to = 20) Plotting a function with additional arguments curve(dnorm(x, mean = 2, sd = 3), from = -4, to = 4) 4.7 More on plots with base R 4.7.1 Multi-frame plot To create a 3x2 multi-frame plot. Use par(mfrow = c(3, 2)). set.seed(1) x &lt;- rnorm(100, 50, 5) y &lt;- x + rnorm(100, 2, 2) # create a 2x2 multi-frame plot par(mfrow=c(2, 2)) hist(x) hist(y,breaks = 10) plot(x, y) boxplot(x, y) 4.7.2 Type of Plot Option Type type = \"p\" Points (default) type = \"l\" Lines connecting the data points type = \"b\" Points and non-overlapping lines type = \"h\" Height lines type = \"o\" Points and overlapping lines par(mfrow=c(3, 2)) x &lt;- -5:5 y &lt;- x^2 plot(x, y) plot(x, y, type = &quot;p&quot;) plot(x, y, type = &quot;l&quot;) plot(x, y, type = &quot;b&quot;) plot(x, y, type = &quot;h&quot;) plot(x, y, type = &quot;o&quot;) 4.7.3 Parameters of a plot Parameter Meaning type See Type of Plot main Title sub Subtitle xlab x-axis label ylab y-axis label xlim x-axis range ylim y-axis range pch Symbol of data points col Color of data points lty Type of the line To illustrate some of the components: set.seed(1) x &lt;- rnorm(100, 50, 15) y &lt;- x + rnorm(100, 2, 13) plot(x, y, col = &quot;red&quot;, pch = 15, main = &quot;This is the title&quot;, xlim = c(0, 100), ylim = c(0,100), xlab = &quot;name of x-axis&quot;, ylab = &quot;name of y-axis&quot;) 4.7.4 Elements on plot Function Description abline(c,m) plot the line y = mx +c abline(h = a) plot the line y = a abline(v = b) plot the line x = b lines(x,y) line joining points with coordinates (x,y) set.seed(1) x &lt;- rnorm(100, 50, 15) y &lt;- x + rnorm(100, 2, 13) plot(x, y) # connect the points (20, 20), (30, 80), (40, 40) with a line lines(x = c(20, 30, 40),y = c(20, 80, 40), col = &quot;red&quot;) abline(v = 60, col = &quot;blue&quot;) 4.8 Summary of ggplot Plot geom scatter plot geom_point() line graph geom_line() bar chart of values geom_col() bar chart of counts geom_bar() histogram geom_histogram() box plot geom_boxplot() "],["managing-data-with-r.html", "Chapter 5 Managing Data with R 5.1 Missing Values 5.2 Saving, loading, and removing R data structures 5.3 Importing and saving data from CSV files", " Chapter 5 Managing Data with R Optional Reading: ML with R Ch2 5.1 Missing Values Missing values are common in real datasets. NA is used to denote missing values. (x &lt;- c(1, 2, 3, NA, 4, NA, 4)) # we can use (x&lt;-1) to assign 1 to x and display x at the same time ## [1] 1 2 3 NA 4 NA 4 mean(x) # mean cannot be computed when missing values exist ## [1] NA mean(x, na.rm = TRUE) # NA values will be removed before computing mean ## [1] 2.8 sd(x) # sd cannot be computed when missing values exist ## [1] NA sd(x, na.rm = TRUE) # NA values will be removed before computing SD ## [1] 1.30384 is.na(x) # logical vector ## [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE x[!is.na(x)] #select the elements with non-missing valuess ## [1] 1 2 3 4 4 na.omit(x) # select the elements with non-missing valuess ## [1] 1 2 3 4 4 ## attr(,&quot;na.action&quot;) ## [1] 4 6 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; 5.2 Saving, loading, and removing R data structures Removing all objects in R: rm(list = ls()) ls() returns a vector of all data structures currently in memory x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) ls() ## [1] &quot;x&quot; &quot;y&quot; To remove x from the memory rm(x) x # because we have deleted x, an error message occurs ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found Saving objects to a file (regardless of whether they are vectors, factors, lists, etc) A &lt;- matrix(1:9, 3, 3) f &lt;- function(x){ return(1) } save(A, f, file = &quot;my_data.RData&quot;) Loading objects from a .RData file. rm(list = ls()) # remove everything load(&quot;my_data.RData&quot;) A ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 f ## function(x){ ## return(1) ## } 5.3 Importing and saving data from CSV files Finding current directory getwd() ## [1] &quot;C:/Queens Teaching/Teaching/STAT 362 W22/01c_published_webiste&quot; If you use mac, you will probably see \"/Users/..../\". Setting working directory setwd(&quot;C:/Queens Teaching/Teaching/STAT 362/Notes&quot;) # use /, not \\ If you use mac, change the above code accordingly. Writing to a file my_data &lt;- data.frame(x = c(1, 2, 3), y = c(4, 5, 6)) my_data ## x y ## 1 1 4 ## 2 2 5 ## 3 3 6 library(tidyverse) write_csv(my_data, &quot;C:/Queens Teaching/Teaching/STAT 362/my_data.csv&quot;) Reading a csv file A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. If we use read_csv from the package tidyverse, the resulting object is a tibble. If we use read.csv from base R, the resulting object is a data frame. See R for data science (https://r4ds.had.co.nz/data-import.html) for a discussion on the differences between read.csv and read_csv. A tibble allows us to perform different types of transformation (next chapter). my_data &lt;- read_csv(&quot;C:/Queens Teaching/Teaching/STAT 362/my_data.csv&quot;) "],["review-chapter-1-5.html", "Chapter 6 Review (Chapter 1-5) 6.1 Simulation 6.2 Matrix 6.3 Basic Operation 6.4 Some basic plots in R", " Chapter 6 Review (Chapter 1-5) 6.1 Simulation Basic vectorized comparison X &lt;- c(1, 1, 5) Y &lt;- c(5, 5, 1) X &gt; Y # FALSE FALSE TRUE ## [1] FALSE FALSE TRUE sum(X &gt; Y) # TRUE = 1, FALSE = 0 ## [1] 1 mean(X &gt; Y) # 1/3 ## [1] 0.3333333 The code mean(X &gt; Y) is computing \\[\\begin{equation*} \\frac{1}{3}( I(X_1 &gt; Y_1) + I(X_2 &gt; Y_2) + I(X_3 &gt; Y_3)), \\end{equation*}\\] where \\(I(\\cdot)\\) is the indicator function, that is, \\(I(X_1 &gt; Y_1) = 1\\) if \\(X_1 &gt; Y_1\\) and \\(I(X_1 &gt; Y_1) = 0\\) if \\(X_1 \\leq Y_1\\). Example: Let \\(X \\sim N(mean = 0, sd = 2)\\) and \\(Y \\sim Exp(rate = 3)\\). Estimate \\(P(X &gt; Y)\\) using simulation. n &lt;- 10000 # number of simulations X &lt;- rnorm(n, 0, 2) Y &lt;- rexp(n, 3) mean(X &gt; Y) ## [1] 0.4309 The code mean(X &gt; Y) is computing \\[\\begin{equation*} \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i), \\end{equation*}\\] which is an approximation of \\(P(X&gt;Y)\\). Theory Why the sample mean \\(\\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i)\\) approximates the required probability \\(P(X&gt;Y)\\)? Recall the strong law of large numbers (SLLN): Let \\(X_1,\\ldots,X_n\\) be independent and identically distributed random variables with mean \\(\\mu:=E(X)\\). Let \\(\\overline{X}_n:= \\frac{1}{n} \\sum^n_{i=1}X_i\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\rightarrow} \\mu.\\] Note: a.s. means almost surely (probability = 1). The above convergence means \\(P(\\lim_{n\\rightarrow \\infty} \\overline{X}_n = \\mu) = 1\\). Note that (the expectation of an indicator random variable is the probability that the corresponding event will happen) \\[ P(X&gt;Y) = E(I(X&gt;Y)).\\] To apply SLLN, we just need to recognize the underlying random variable is \\(I(X&gt;Y)\\) (which follows a Bernoulli distribution with parameter \\(P(X&gt;Y)\\)). Then, with probability \\(1\\), the sample mean \\[ \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i) \\rightarrow P(X&gt;Y).\\] The quantity on the LHS is what we compute in mean(X &gt; Y) (note that we are using vectorized comparison). This is the reason why we can use mean(X &gt; Y) to estimate \\(P(X&gt;Y)\\). Ex 1: Let X ~ N(mean = 2, sd =1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(\\max(X,Y)&gt;Z)\\). Recall the difference between pmax and max: # always try with simple examples when you test the usage of functions x &lt;- c(1, 2, 3) y &lt;- c(0, 5, 10) max(x, y) ## [1] 10 pmax(x, y) ## [1] 1 5 10 n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(pmax(x, y) &gt; z) ## [1] 0.51502 Ex 2 Let X ~ N(mean = 2, sd = 1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(\\min(X,Y)&gt;Z)\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(pmin(x, y) &gt; z) # what is the difference between pmin and min? ## [1] 0.11361 Ex 3 Let X ~ N(mean = 2, sd = 1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(X^2 Y&gt;Z)\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(x ^ 2 * y &gt; z) ## [1] 0.41158 Ex 4 Person \\(A\\) generates a random variable \\(X \\sim N(2, 1)\\) and Person \\(B\\) generates a random variable \\(Z \\sim Unif(0, 4)\\). If \\(X &lt; Z\\), Person \\(A\\) will discard \\(X\\) and generate another random variable \\(Y \\sim Exp(0.5)\\). Find the probability that the number generated by \\(A\\) is greater than that by \\(B\\). n &lt;- 100000 greater &lt;- rep(0, n) for (i in 1:n) { X &lt;- rnorm(1, 2, 1) Z &lt;- runif(1, 0, 4) if (X&lt; Z) { Y &lt;- rexp(1, rate = 0.5) greater[i] &lt;- Y &gt; Z } else { greater[i] &lt;- 1 # 1 means A&#39;s no &gt; B&#39;s no } } mean(greater) ## [1] 0.63698 Remark: you may find that the following code gives you almost the same answer. Why? n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 0.5) z &lt;- runif(n, 0, 4) mean(pmax(x, y) &gt; z) ## [1] 0.63904 Ex 5 Let X ~ N(mean = 2, sd = 1) and Y ~ Exp(rate = 2). Estimate \\(E(min(X,Y))\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) mean(pmin(x, y)) ## [1] 0.445328 The code mean(pmin(x, y)) computes \\[\\begin{equation*} \\frac{1}{n} \\sum^n_{i=1} \\min(X_i, Y_i), \\end{equation*}\\] which approximates \\(E(\\min(X,Y))\\) by the SLLN. 6.2 Matrix Ex6: Write a function called matrix_times_vector to compute \\(X Y\\), where \\(X\\) is a matrix and \\(Y\\) is a vector. The output should be a vector. matrix_times_vector = function(X, Y) { as.vector(X %*% Y) } # e.g. X &lt;- matrix(1:12, 3, 4) Y &lt;- 1:4 X ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Y ## [1] 1 2 3 4 matrix_times_vector(X, Y) ## [1] 70 80 90 Note: X %*% Y will return a matrix. We can use as.vector to change it into a vector. It is common to see the error non-conformable arguments. This is because the dimensions of your matrices/vectors do not match. If you have a \\(n\\times p\\) matrix \\(A\\) and \\(m \\times q\\) matrix \\(B\\), you can do the matrix multiplication \\(AB\\) only if \\(p = m\\). In R, if this is not the case, there will be an error. Similarly, if you have a vector \\(d\\) of length \\(m\\). You can do the matrix multiplication \\(A d\\) only if \\(p = m\\). Ex7: Write a function called matrix_times_vector2 to compute \\(X Y\\), where \\(X\\) is a matrix and \\(Y\\) is a vector. The output should be a vector. However, you should check if the dimensions of the inputs are appropriate before you perform the calculation. Display an error message The dimensions do not match if this is not the case. matrix_times_vector2 = function(X, Y) { p &lt;- ncol(X) m &lt;- length(Y) if (p == m) { return(as.vector(X %*% Y)) } else { cat(&quot;The dimensions do not match&quot;) } } # e.g. X &lt;- matrix(1:12, 4, 3) Y &lt;- 1:4 matrix_times_vector2(X, Y) ## The dimensions do not match X &lt;- matrix(1:12, 3, 4) Y &lt;- 1:4 matrix_times_vector2(X, Y) ## [1] 70 80 90 Example: if A is matrix and x is a vector, you can find the sums of the elements in A and x by sum(A) and sum(x) respectively. x &lt;- 1:10 sum(x) ## [1] 55 A &lt;- matrix(1:10, 5, 2) sum(A) ## [1] 55 Ex 8: Find \\(\\sum^{5}_{x=1}\\sum^{4}_{y=1} \\frac{x}{x+y}\\) without any loops. (x &lt;- matrix(1:5, nrow = 4, ncol = 5, byrow = TRUE)) # define and display at the same time using (...) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 1 2 3 4 5 ## [3,] 1 2 3 4 5 ## [4,] 1 2 3 4 5 (y &lt;- matrix(1:4, nrow = 4, ncol = 5, byrow = FALSE)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1 ## [2,] 2 2 2 2 2 ## [3,] 3 3 3 3 3 ## [4,] 4 4 4 4 4 x / (x + y) # vectorized operation ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.5000000 0.6666667 0.7500000 0.8000000 0.8333333 ## [2,] 0.3333333 0.5000000 0.6000000 0.6666667 0.7142857 ## [3,] 0.2500000 0.4000000 0.5000000 0.5714286 0.6250000 ## [4,] 0.2000000 0.3333333 0.4285714 0.5000000 0.5555556 sum(x / (x + y)) ## [1] 10.72817 6.3 Basic Operation Example Let v be a vector of integers. Write a one-line R code to compute the product of all the even integers in v. To illustrate how to solve the question step by step: v &lt;- -10:10 # begin writing your code by setting some integers v %% 2 # find the remainder, if the remainder is 0, it is an even number ## [1] 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 v %% 2 == 0 # check which elements is 0 ## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE ## [14] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE v[v %% 2 == 0] # select the elements which are &quot;TRUE&quot; ## [1] -10 -8 -6 -4 -2 0 2 4 6 8 10 prod(v[v %% 2 == 0]) # find the product ## [1] 0 # the result is 0. v may not be a good example to check if the code is correct # let&#39;s change to some other vector v &lt;- 2:8 prod(v[v %% 2 == 0]) ## [1] 384 2 * 4 * 6 * 8 #check ## [1] 384 # now, the final answer is prod(v[v %% 2 == 0]) Ex 9a: Given that x = 1:100. Write a one-line R code to copmute \\[\\begin{equation*} S := 1^2 - 2^2 + 3^2 - \\ldots + 99^2 - 100^2. \\end{equation*}\\] x &lt;- 1:100 sum((x[x %% 2 == 1]) ^ 2) - sum((x[x %% 2 == 0]) ^ 2) ## [1] -5050 Ex 9b (optional) Can you do Ex 9a without any program or calculator? Ex 10: Write a function with inputs n and p to compute \\[\\begin{equation*} S(n, p) := 1^p - 2^p + 3^p -\\ldots + (-1)^{n} (n-1)^p + (-1)^{n+1} n^p. \\end{equation*}\\] snp &lt;- function(n, p) { x &lt;- 1:n return(sum((x[x %% 2 == 1]) ^ p) - sum((x[x %% 2 == 0]) ^ p)) } 6.4 Some basic plots in R Example Write a function called my_summary_plot with input being two numeric vectors x,y and outputs a \\(2 \\times 2\\) multi-frame plot with (i) histogram of x, (ii) histogram of y, (iii) scatter plot of y versu x, and (iv) boxplots of x and y. # when you write a function, you can begin with some sample x and y x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) par(mfrow = c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) # after that, you wrap the code into a function without defining x, y my_summary_plot &lt;- function(x, y){ par(mfrow = c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) } # after you write the function, you should try if it will work or not rm(list = ls()) # let&#39;s remove everything in the memory my_summary_plot &lt;- function(x, y){ par(mfrow = c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) } x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) my_summary_plot(x, y) # try another example x &lt;- rnorm(100, 0, 1) y &lt;- 5 * x + rnorm(100, 0, 1) my_summary_plot(x, y) "],["data-transformation-with-dplyr.html", "Chapter 7 Data Transformation with dplyr 7.1 Introduction 7.2 arrange() 7.3 filter() 7.4 select() 7.5 mutate() 7.6 summarize(), group_by() 7.7 Combining Multiple Operations with Pipe %&gt;% 7.8 Summary", " Chapter 7 Data Transformation with dplyr 7.1 Introduction Reference: see https://r4ds.had.co.nz/transform.html Preparation We will use a dataset in the package nycflights13. To install it: install.packages(&quot;nycflights13&quot;) To use the dataset or functions in the package, we first load the library: library(nycflights13) In Chapter 4, we have installed tidyverse, which contains the package dplyr. Now, load the package library(tidyverse) nycflights13 The dataset flights in the package nyclfights13 contains all \\(336,776\\) flights that departed from New York City in 2013. Check ?flights for details. # let&#39;s view the dataset flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ... with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; flights is a tibble. Tibbles are data frames with better properties. Optional: If you are interested in the differences between a data frame and a tibble, you can go to https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html To view the complete dataset, use View(flights). Five key dplyr functions arrange(): reorder the rows fliter(): pick observations by their values select(): pick variables by their names mutate(): create new variables with functions of existing variables summarize(): collapse many values down to a single summary All functions work similarly: The first argument is a data frame/ tibble The subsequent argument describe what to do with the data frame, using the variable names (without quotes). The result is a new data frame. Of course, it is also possible to perform the same tasks without using dplyr functions. We will also discuss briefly how to use the base subsetting with [] to select the data. In general, the functions in dplyr are designed to transform the data more easily. 7.2 arrange() arrange() orders your dataset. If more than one column name is provided, each additional column will be used to break ties in the values of preceding columns. To reorder by a column in ascending order: arrange(flights, year, month, day) Lets create a simple dataset to illustrate this because flights is already sorted in year, month and day. (data &lt;- tibble(x = c(2, 2, 1, 4, 5), y = c(2, 3, 10, 10, 10))) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 ## 2 2 3 ## 3 1 10 ## 4 4 10 ## 5 5 10 arrange(data, x) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 ## 2 2 2 ## 3 2 3 ## 4 4 10 ## 5 5 10 # first sort in ascending order of x, use y to break any ties and sort in descending order arrange(data, x, desc(y)) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 ## 2 2 3 ## 3 2 2 ## 4 4 10 ## 5 5 10 To reorder by a column in descending order, use desc(): arrange(flights, desc(arr_delay)) Missing values are always sorted at the end # create a tibble with one column called x with values 5,2,NA df &lt;- tibble(x = c(5, 2, NA)) arrange(df, x) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 2 ## 2 5 ## 3 NA arrange(df, desc(x)) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 5 ## 2 2 ## 3 NA 7.2.1 Exercises Sort flights to find the most delayed flights. Find the flights that left earliest. arrange(flights, desc(dep_delay)) arrange(flights, dep_delay) Which flights traveled the longest? Which traveled the shortest? arrange(flights, desc(distance)) arrange(flights, distance) 7.3 filter() filter() only includes rows where the condition is TRUE Select all flights on Jan 1st: # flights is the name of your data frame # month == 1, day == 1 is the condition (jan1 &lt;- filter(flights, month == 1, day == 1)) ## # A tibble: 842 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ... with 832 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, ## # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Lets use a simple dataset to see how to perform the same task without filter: # just a simple dataset (data &lt;- tibble(x = c(1, 3, 5, 5, 3), y = 1:5)) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 1 1 ## 2 3 2 ## 3 5 3 ## 4 5 4 ## 5 3 5 data$x == 5 # logical vector ## [1] FALSE FALSE TRUE TRUE FALSE data[data$x == 5, ] # select the rows with value &quot;TRUE&quot; ## # A tibble: 2 x 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 5 3 ## 2 5 4 # returning to the flights dataset base_jan1 &lt;- flights[(flights$month == 1 &amp; flights$day == 1), ] (flights$month == 1 &amp; flights$day == 1) is a logical vector indicating if the corresponding flight was on Jan 1 (TRUE if yes). Now, lets check if jan1 and base_jan1 are the same using identical: identical(jan1, base_jan1) # TURE means they are the same, FALSE means they are not the same ## [1] TRUE More Examples Select flights that departed in Nov or Dec filter(flights, month == 11 | month == 12) # alternatively, simpler code filter(flights, month %in% c(11, 12)) How to use the operator %in%? y &lt;- c(1,3,5) x &lt;- 1 x %in% y #whether 1 is in {1,3,5} ## [1] TRUE x &lt;- c(1,3,2,4,1) x %in% y # check whether each element in x is in {1,3,5} ## [1] TRUE TRUE FALSE FALSE TRUE %in% also works with characters c(&quot;a&quot;, &quot;b&quot;) %in% c(&quot;a&quot;, &quot;c&quot;, &quot;d&quot;) ## [1] TRUE FALSE The result is TRUE FALSE because \"a\" is in c(\"a\", \"c\", \"d\") and \"b\" is not in c(\"a\", \"c\", \"d\"). Perform the same task without filter: flights[flights$month == 11 | flights$month == 12,] # or flights[flights$month %in% c(11, 12), ] Flights that were not delayed (on arrival or departure) by more than two hours: delay1 &lt;- filter(flights, arr_delay &lt;= 120, dep_delay &lt;= 120) Without using filter delay2 &lt;- flights[flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120, ] Lets check if delay1 and delay2 are the same. identical(delay1, delay2) ## [1] FALSE The result is FALSE, meaning delay1 and delay2 are not the same. Why? Because some elements in flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120 are NA. # to find out the number of NA values sum(is.na(flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120)) ## [1] 9304 As a result, with the base subsetting method, a row with all NA values will be selected. On the other hand, for filter, when a condition evaluates to NA, the row will be dropped. Lets create a small dataset to illustrate this. From now on, lets try to use tibble instead of data.frame. data &lt;- tibble(x = 1:4, y = c(1, 2, NA, 4)) data[data$y &lt;= 3, ] ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 NA NA # to avoid the above problem # which() returns which elements are TRUE which(data$y &lt;= 3) # the result is 1, 2 ## [1] 1 2 data[which(data$y &lt;= 3), ] # select row 1, row 2 ## # A tibble: 2 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 # using filter filter(data, y &lt;= 3) ## # A tibble: 2 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 # if we want to include the row where the value of y is NA # recall that | means &quot;or&quot; data[which(data$y &lt;= 3 | is.na(data$y)), ] ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA # using filter filter(data, y &lt;= 3 | is.na(y)) ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA If you want to drop the NA values with base subsetting[], you may use delay3 &lt;- flights[which((flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120) == TRUE), ] To see if delay1 and delay3 are exactly the same: identical(delay1, delay3) # TRUE means exactly the same ## [1] TRUE At this point, you should see that filter could perform the same tasks with simpler code. 7.3.1 Exercises 1a. Find all flights that had an arrival delay of two or more hours (drop the rows with NA in arr_delay). # Using &quot;filter&quot; filter(flights, arr_delay &gt;= 120) # Without using &quot;filter&quot; flights[which(flights$arr_delay &gt;= 120), ] 1b. Find all flights that flew to Houston (IAH or HOU). # Using &quot;filter&quot; filter(flights, dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)) # Without using &quot;filter&quot; flights[which(flights$dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)), ] 1c. Find all flights that were operated by United, American, or Delta # find all the sorted carrier codes in the dataset sort(unique(flights$carrier)) ## [1] &quot;9E&quot; &quot;AA&quot; &quot;AS&quot; &quot;B6&quot; &quot;DL&quot; &quot;EV&quot; &quot;F9&quot; &quot;FL&quot; &quot;HA&quot; &quot;MQ&quot; &quot;OO&quot; &quot;UA&quot; &quot;US&quot; &quot;VX&quot; &quot;WN&quot; ## [16] &quot;YV&quot; # look up airline names from their carrier codes airlines ## # A tibble: 16 x 2 ## carrier name ## &lt;chr&gt; &lt;chr&gt; ## 1 9E Endeavor Air Inc. ## 2 AA American Airlines Inc. ## 3 AS Alaska Airlines Inc. ## 4 B6 JetBlue Airways ## 5 DL Delta Air Lines Inc. ## 6 EV ExpressJet Airlines Inc. ## 7 F9 Frontier Airlines Inc. ## 8 FL AirTran Airways Corporation ## 9 HA Hawaiian Airlines Inc. ## 10 MQ Envoy Air ## 11 OO SkyWest Airlines Inc. ## 12 UA United Air Lines Inc. ## 13 US US Airways Inc. ## 14 VX Virgin America ## 15 WN Southwest Airlines Co. ## 16 YV Mesa Airlines Inc. # after looking up the names, we know UA = United, AA = American, DL = Delta filter(flights, carrier %in% c(&quot;UA&quot;, &quot;AA&quot;, &quot;DL&quot;)) ## # A tibble: 139,504 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 554 600 -6 812 837 ## 5 2013 1 1 554 558 -4 740 728 ## 6 2013 1 1 558 600 -2 753 745 ## 7 2013 1 1 558 600 -2 924 917 ## 8 2013 1 1 558 600 -2 923 937 ## 9 2013 1 1 559 600 -1 941 910 ## 10 2013 1 1 559 600 -1 854 902 ## # ... with 139,494 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1d. Find all flights that departed in summer (July, August, and September) filter(flights, month %in% c(7, 8, 9)) # Alternative Method filter(flights, between(month, 7, 9)) 1e. Find all flights that arrived more than two hours late, but didnt leave late filter(flights, arr_delay &gt; 120, dep_delay &lt;= 0) The next two exercises are trickier. 1f. Find all flights that were delayed by at least an hour, but made up over 30 minutes in flight. First, the flight was delayed by at least an hour is the same as dep_delay &gt;=60. Second, if a flight made up over 30 minutes in flight, the arrival delay must be at least 30 minutes less than the departure delay, which is the same as dep_delay - arr_delay &gt; 30. filter(flights, dep_delay &gt;= 60, dep_delay - arr_delay &gt; 30) 1g. Find all flights that departed between midnight and 6 a.m. (inclusive). The first question that should come to your mind is how is midnight represented in the dataset? Lets take a look at summary(flights$dep_time). summary(flights$dep_time) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1 907 1401 1349 1744 2400 8255 The minimum is 1 and the maximum is 2400. Therefore, you know midnight is represented by 2400 instead of 0 in this dataset. The answer to the question would be filter(flights, dep_time &lt;= 600 | dep_time == 2400) 7.4 select() Very often, we are only interested in some variables in a dataset. In that case, we can focus on the variables by creating a new dataset with those variables only. select() is to select the columns in a dataset by the name of the columns Selecting Variables Suppose you want to select the following \\(3\\) columns in flights: year, month, day: select(flights, year, month, day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows The usual way without using select() is flights[c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] # or flights[, c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows Select all columns between year and day (inclusive) select(flights, year:day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows Excluding Variables Select all columns except those from year to day (inclusive) select(flights, -(year:day)) select(flights, -year, -month, -day) Without using select() flights[, !(colnames(flights) %in% c(&quot;year&quot;, &quot;day&quot;, &quot;month&quot;))] 7.4.1 Exercises You can also use starts_with(\"abc\") matches names that begin with \"abc\" ends_with(\"xyz\") matches names that end with \"xyz\" contains(\"ijk\") mathces names that contain \"ijk\" Ex: select all columns that end with \"delay\". select(flights, ends_with(&quot;delay&quot;)) ## # A tibble: 336,776 x 2 ## dep_delay arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 ## 2 4 20 ## 3 2 33 ## 4 -1 -18 ## 5 -6 -25 ## 6 -4 12 ## 7 -5 19 ## 8 -3 -14 ## 9 -3 -8 ## 10 -2 8 ## # ... with 336,766 more rows Ex: select all columns that start with \"a\". select(flights, starts_with(&quot;a&quot;)) ## # A tibble: 336,776 x 3 ## arr_time arr_delay air_time ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 830 11 227 ## 2 850 20 227 ## 3 923 33 160 ## 4 1004 -18 183 ## 5 812 -25 116 ## 6 740 12 150 ## 7 913 19 158 ## 8 709 -14 53 ## 9 838 -8 140 ## 10 753 8 138 ## # ... with 336,766 more rows Does the result of running the following code surprise you? select(flights, contains(&quot;TIME&quot;)) ## # A tibble: 336,776 x 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time time_hour ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 517 515 830 819 227 2013-01-01 05:00:00 ## 2 533 529 850 830 227 2013-01-01 05:00:00 ## 3 542 540 923 850 160 2013-01-01 05:00:00 ## 4 544 545 1004 1022 183 2013-01-01 05:00:00 ## 5 554 600 812 837 116 2013-01-01 06:00:00 ## 6 554 558 740 728 150 2013-01-01 05:00:00 ## 7 555 600 913 854 158 2013-01-01 06:00:00 ## 8 557 600 709 723 53 2013-01-01 06:00:00 ## 9 557 600 838 846 140 2013-01-01 06:00:00 ## 10 558 600 753 745 138 2013-01-01 06:00:00 ## # ... with 336,766 more rows Yes, because we used TIME but not time and still get the selected columns. If you check ?contains, you can see that the default is to ignore the case. To change the default: select(flights, contains(&quot;TIME&quot;, ignore.case = FALSE)) ## # A tibble: 336,776 x 0 Ex: without using select(), select all the columns that contain time. flights[grep(&quot;time&quot;, names(flights))] ## # A tibble: 336,776 x 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time time_hour ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 517 515 830 819 227 2013-01-01 05:00:00 ## 2 533 529 850 830 227 2013-01-01 05:00:00 ## 3 542 540 923 850 160 2013-01-01 05:00:00 ## 4 544 545 1004 1022 183 2013-01-01 05:00:00 ## 5 554 600 812 837 116 2013-01-01 06:00:00 ## 6 554 558 740 728 150 2013-01-01 05:00:00 ## 7 555 600 913 854 158 2013-01-01 06:00:00 ## 8 557 600 709 723 53 2013-01-01 06:00:00 ## 9 557 600 838 846 140 2013-01-01 06:00:00 ## 10 558 600 753 745 138 2013-01-01 06:00:00 ## # ... with 336,766 more rows Basic Usage of grep: grep(pattern, x). pattern: character string. e.g., time, delay, air x: a character vector where matches are sought. e.g., the colnames of a dataframe. Output: a vector of the indices of the elements of x that yielded a match. some_names &lt;- c(&quot;ab&quot;, &quot;bc&quot;, &quot;cd&quot;) grep(&quot;b&quot;, some_names) ## [1] 1 2 grep(&quot;d&quot;, some_names) ## [1] 3 grep(&quot;e&quot;, some_names) ## integer(0) 7.5 mutate() Add new variables with mutate() Very often, we want to create a new variable based on existing variables. For example, if we have distance and time, we can compute the speed by distance/time. mutate() always adds new columns at the end of the dataset. Lets create a narrower dataset so that we can see the result of mutate without use View(). # create a smaller dataset flights_sml &lt;- select(flights, year:day, arr_delay, dep_delay, distance, air_time) Now, lets use mutate() to create a variable called gain (how much time we gain in flight) defined as arr_delay - dep_delay and a variable called speed (miles/hour) defined as distance/air_time * 60. mutate(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 x 9 ## year month day arr_delay dep_delay distance air_time gain speed ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 11 2 1400 227 9 370. ## 2 2013 1 1 20 4 1416 227 16 374. ## 3 2013 1 1 33 2 1089 160 31 408. ## 4 2013 1 1 -18 -1 1576 183 -17 517. ## 5 2013 1 1 -25 -6 762 116 -19 394. ## 6 2013 1 1 12 -4 719 150 16 288. ## 7 2013 1 1 19 -5 1065 158 24 404. ## 8 2013 1 1 -14 -3 229 53 -11 259. ## 9 2013 1 1 -8 -3 944 140 -5 405. ## 10 2013 1 1 8 -2 733 138 10 319. ## # ... with 336,766 more rows To keep the new variables only, use transmute(): transmute(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 x 2 ## gain speed ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9 370. ## 2 16 374. ## 3 31 408. ## 4 -17 517. ## 5 -19 394. ## 6 16 288. ## 7 24 404. ## 8 -11 259. ## 9 -5 405. ## 10 10 319. ## # ... with 336,766 more rows Without using select and mutate(), one may use flights_sml2 &lt;- flights[c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;arr_delay&quot;, &quot;dep_delay&quot;, &quot;distance&quot;, &quot;air_time&quot;)] flights_sml2$gain &lt;- flights_sml2$arr_delay - flights_sml2$dep_delay flights_sml2$speed &lt;- flights_sml2$distance / flights_sml2$air_time * 60 7.5.1 Exercises Ex: Currently, dep_time and sched_dep_time are convenient to look at, but hard to compute with because they are not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Recall that dep_time and sched_dep_time are in HHMM format. For example, 1304 means 1:04pm. The number of minutes since midnight is \\(13\\times 60 + 4 = 784\\). In general, we can use the integer division to find the number of hours since midnight, then multiply by \\(60\\), and finally add the remainder for the minutes. For example, 1304 %/% 100 # get the number of hours since midnight ## [1] 13 1304 %% 100 # get the remainder ## [1] 4 1304 %/% 100 * 60 + 1304 %% 100 # number of minutes since midnight ## [1] 784 Recall that midnight is represented as 2400 in the dataset and the number of minutes since midnight should be 0. However, if we use the above method for midnight, we get 2400 %/% 100 * 60 + 2400 %% 100 # this is not correct for midnight ## [1] 1440 Therefore, we also have to deal with this case. One possible solution is to do another integer division by 1440 (24x60 = 1440): (1304 %/% 100 * 60 + 1304 %% 100) %% 1440 # will not change the result if the time is not midnight ## [1] 784 (2400 %/% 100 * 60 + 2400 %% 100) %% 1440 # this is correct for midnight ## [1] 0 Go back to flights: # let&#39;s keep only the dep_time and sched_dep_time flights_time &lt;- select(flights, dep_time, sched_dep_time) mutate(flights_time, min_dep_time = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, min_sched_dep_time = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440) ## # A tibble: 336,776 x 4 ## dep_time sched_dep_time min_dep_time min_sched_dep_time ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 515 317 315 ## 2 533 529 333 329 ## 3 542 540 342 340 ## 4 544 545 344 345 ## 5 554 600 354 360 ## 6 554 558 354 358 ## 7 555 600 355 360 ## 8 557 600 357 360 ## 9 557 600 357 360 ## 10 558 600 358 360 ## # ... with 336,766 more rows The above code doesnt look good because we have written the formula to complete the same task twice. We can define a function to do this: time_to_minutes &lt;- function(x) { (x %/% 100 * 60 + x %% 100) %% 1440 } With the function time_to_minutes, we have: mutate(flights, min_dep_time = time_to_minutes(dep_time), min_sched_dep_time = time_to_minutes(sched_dep_time)) If you cannot think of using %% 1440 to deal with the midnight cases, we can write: # ind for indicator approach time_to_minutes_ind &lt;- function(x) { (x %/% 100 * 60 + x %% 100) * (x != 2400) } # Explanation # if x is 2400, (x!=2400) is FALSE, FALSE times a number y is 0 # if x is not 2400, (x!=2400) is TRUE, TRUE times a number y is y # because &quot;FALSE=0, TRUE=1&quot; mutate(flights, min_dep_time = time_to_minutes_ind(dep_time), min_sched_dep_time = time_to_minutes_ind(sched_dep_time)) An alternative way is to use ifelse (which is a vectorized function). Usage of ifelse: ifelse(test, yes, no) time_to_minutes_ifelse &lt;- function(x) { ifelse(x != 2400, x %/% 100 * 60 + x %% 100, 0) } mutate(flights, min_dep_time = time_to_minutes_ifelse(dep_time), min_sched_dep_time = time_to_minutes_ifelse(sched_dep_time)) ## # A tibble: 336,776 x 21 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ... with 336,766 more rows, and 13 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ## # min_dep_time &lt;dbl&gt;, min_sched_dep_time &lt;dbl&gt; A less efficient way with for loop and if-else: # if for if-else approach time_to_minutes_if &lt;- function(x) { n &lt;- length(x) output &lt;- rep(0, n) for (i in 1:n) { if (is.na(x[i])){ # check for NA output[i] &lt;- NA } else if (x[i] != 2400) { # if not equal to 2400 output[i] &lt;- x[i] %/% 100 * 60 + x[i] %% 100 } else { # if equal to 2400 output[i] &lt;- 0 } } return(output) } mutate(flights, min_dep_time = time_to_minutes_if(dep_time), min_sched_dep_time = time_to_minutes_if(sched_dep_time)) Compare the efficiency: system.time(mutate(flights, min_dep_time = time_to_minutes(dep_time), min_sched_dep_time = time_to_minutes(sched_dep_time))) ## user system elapsed ## 0.17 0.02 0.19 system.time(mutate(flights, min_dep_time = time_to_minutes_if(dep_time), min_sched_dep_time = time_to_minutes_if(sched_dep_time))) ## user system elapsed ## 1.34 0.00 1.38 7.6 summarize(), group_by() Average delay over the year (not useful): summarize(flights, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 x 1 ## delay ## &lt;dbl&gt; ## 1 12.6 Using summarize with group_by and ungroup can result in more useful statistics. Use group_by. First argument is your dataset. Subsequent arguments indicate how you want to group the data. In summarize, the dataset becomes the dataset from group_by. After performing the calculation using summarize, use ungroup. If you do not use ungroup after group_by, the grouping structure is still retained in the object that you create and the subsequent calculation may yield something that you do not expect. E.g.: Average departure delay per date (more useful): by_day &lt;- group_by(flights, year, month, day) mean_delay &lt;- summarize(by_day, delay = mean(dep_delay, na.rm = TRUE)) (mean_delay &lt;- ungroup(mean_delay)) # use ungroup() after gropu_by() ## # A tibble: 365 x 4 ## year month day delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.5 ## 2 2013 1 2 13.9 ## 3 2013 1 3 11.0 ## 4 2013 1 4 8.95 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.55 ## 9 2013 1 9 2.28 ## 10 2013 1 10 2.84 ## # ... with 355 more rows 7.7 Combining Multiple Operations with Pipe %&gt;% You can use the shortcut Ctrl/Cmd + Shift + M to insert the pipe operator %&gt;%. x %&gt;% f(y) turns into f(x, y) For example, flights %&gt;% filter(month == 1, day == 1) is the same as filter(flights, month == 1, day == 1) x %&gt;% f(y) %&gt;% g(z) turns into g(f(x, y), z). For example, flights %&gt;% group_by(year, month) %&gt;% summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() is the same as ungroup(summarize(group_by(flights, year, month), mean_dep_delay = mean(dep_delay, na.rm = TRUE))) Using the pipe can avoid creating and naming intermediate objects that we dont need. Instead, the pipe focuses on the sequence of actions, not the object that the actions being performed on. It also tends to make the code easier to read. For the above example, you can read it as: for the dataset flights, we first group the data by year and month, then summarize the data by finding the mean. We can think of the pipe %&gt;% as then. Pipe %&gt;% can also be used with ggplot: flights %&gt;% group_by(year, month) %&gt;% summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(month), y = mean_dep_delay)) + geom_col() # notice the spacing for good indentation Notice the difference between %&gt;% and + in the above code. E.g. Create new columns for the average departure delay by date and average departure delay by destination using mutate group_date &lt;- group_by(select(flights, year:day, dep_delay, dest), month, day) flights2 &lt;- mutate(group_date, avg_date_dep_delay = mean(dep_delay, na.rm = TRUE)) group_dest &lt;- group_by(flights2, dest) (flights3 &lt;- ungroup(mutate(group_dest, avg_dest_dep_delay = mean(dep_delay, na.rm = TRUE)))) ## # A tibble: 336,776 x 7 ## year month day dep_delay dest avg_date_dep_delay avg_dest_dep_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 IAH 11.5 10.8 ## 2 2013 1 1 4 IAH 11.5 10.8 ## 3 2013 1 1 2 MIA 11.5 8.88 ## 4 2013 1 1 -1 BQN 11.5 12.4 ## 5 2013 1 1 -6 ATL 11.5 12.5 ## 6 2013 1 1 -4 ORD 11.5 13.6 ## 7 2013 1 1 -5 FLL 11.5 12.7 ## 8 2013 1 1 -3 IAD 11.5 17.0 ## 9 2013 1 1 -3 MCO 11.5 11.3 ## 10 2013 1 1 -2 ORD 11.5 13.6 ## # ... with 336,766 more rows flights %&gt;% select(year:day, dep_delay, dest) %&gt;% group_by(month, day) %&gt;% mutate(avg_date_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% group_by(dest) %&gt;% # By default, group_by() overrides existing grouping mutate(avg_dest_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() ## # A tibble: 336,776 x 7 ## year month day dep_delay dest avg_date_dep_delay avg_dest_dep_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 IAH 11.5 10.8 ## 2 2013 1 1 4 IAH 11.5 10.8 ## 3 2013 1 1 2 MIA 11.5 8.88 ## 4 2013 1 1 -1 BQN 11.5 12.4 ## 5 2013 1 1 -6 ATL 11.5 12.5 ## 6 2013 1 1 -4 ORD 11.5 13.6 ## 7 2013 1 1 -5 FLL 11.5 12.7 ## 8 2013 1 1 -3 IAD 11.5 17.0 ## 9 2013 1 1 -3 MCO 11.5 11.3 ## 10 2013 1 1 -2 ORD 11.5 13.6 ## # ... with 336,766 more rows Note: if you have to manipulate some intermediate objects, it may make sense not to use the pipe operator in that situation. More examples: Average delay per month: by_month &lt;- group_by(flights, month) ungroup(summarize(by_month, delay = mean(dep_delay, na.rm = TRUE))) ## # A tibble: 12 x 2 ## month delay ## &lt;int&gt; &lt;dbl&gt; ## 1 1 10.0 ## 2 2 10.8 ## 3 3 13.2 ## 4 4 13.9 ## 5 5 13.0 ## 6 6 20.8 ## 7 7 21.7 ## 8 8 12.6 ## 9 9 6.72 ## 10 10 6.24 ## 11 11 5.44 ## 12 12 16.6 flights %&gt;% group_by(month) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() # a good habit is to use ungroup after the calculations ## # A tibble: 12 x 2 ## month delay ## &lt;int&gt; &lt;dbl&gt; ## 1 1 10.0 ## 2 2 10.8 ## 3 3 13.2 ## 4 4 13.9 ## 5 5 13.0 ## 6 6 20.8 ## 7 7 21.7 ## 8 8 12.6 ## 9 9 6.72 ## 10 10 6.24 ## 11 11 5.44 ## 12 12 16.6 Ex: Find the average weights of the cars grouped by the number of gears. mtcars_gear &lt;- group_by(mtcars, gear) ungroup(summarize(mtcars_gear, avg_wt = mean(wt, na.rm = TRUE))) ## # A tibble: 3 x 2 ## gear avg_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3.89 ## 2 4 2.62 ## 3 5 2.63 mtcars %&gt;% group_by(gear) %&gt;% summarize(avg_wt = mean(wt, na.rm = TRUE)) %&gt;% ungroup() ## # A tibble: 3 x 2 ## gear avg_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3.89 ## 2 4 2.62 ## 3 5 2.63 Ex: Find the sample standard deviation of the weights of the cars grouped by the number of gears. mtcars %&gt;% group_by(gear) %&gt;% summarize(sd_wt = sd(wt, na.rm = TRUE)) %&gt;% ungroup() ## # A tibble: 3 x 2 ## gear sd_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 0.833 ## 2 4 0.633 ## 3 5 0.819 7.8 Summary Functions and operators learned in this chapter: arrange, filter, identical, %in%, tibble, ifelse, mutate, transmute, select, grep, group_by, summarize, ungroup, %&gt;% "],["data-visualization-with-ggplot2.html", "Chapter 8 Data Visualization with ggplot2 8.1 Bar charts 8.2 Line Graph 8.3 Scatter Plots 8.4 Summarizing Data Distributions 8.5 Saving your plots 8.6 Summary", " Chapter 8 Data Visualization with ggplot2 Main reference for this chapter: R graphics cookbook (https://r-graphics.org/) In Chapter 4, we learned how to create some basic plots with base R and the function ggplot (in the package ggplot2, which is also contained in the package tidyverse). In this chapter, we will study how to use ggplot for data visualization in more detail. We will use some of the datasets from the the package gcookbook. Therefore, we will install it now. install.packages(&quot;gcookbook&quot;) Load the packages gcookbook, tidyverse and nycflights13. library(gcookbook) # contains some datasets for illustration library(tidyverse) # contains ggplot2 and dplyr library(nycflights13) # contains the dataset &quot;flights&quot; 8.1 Bar charts We will start with bar charts. Many of the usages discussed in this section can also be transferable to create other plots. Recall that there are two types of bar charts: Bar chart of values. x-axis: discrete variable, y-axis: numeric data (not necessarily count data) Bar chart of counts. x-axis: discrete variable, y-axis: count of cases in the discrete variable Using ggplot: For bar chart of values, we use geom_col(), which is the same as using geom_bar(stat = \"identity\"). For bar chart of counts, we use geom_bar(), which is the same as using geom_bar(stat = \"count\"). That is, the default for geom_bar() is to use stat = \"count\". Bar chart of values: pg_mean is a simple dataset with groupwise means of some plant growth data. pg_mean ## group weight ## 1 ctrl 5.032 ## 2 trt1 4.661 ## 3 trt2 5.526 ggplot(data = pg_mean, mapping = aes(x = group, y = weight)) + geom_col() Recall the mtcars dataset. Lets create a bar chart of values for the mean weights grouped by the number of gears. First, we summarize the data using summarize. by_gear &lt;- group_by(mtcars, gear) mtcars_wt &lt;- summarize(by_gear, mean_wt_by_gear = mean(wt)) # Alternatively, using %&gt;% mtcars_wt &lt;- mtcars %&gt;% group_by(gear) %&gt;% summarize(mean_wt_by_gear = mean(wt)) Create the bar chart: ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col() To change the colour of the bars, use fill. ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(fill = &quot;lightblue&quot;) By default, there is no outline around the fill. To add an outline, use colour (or color). ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(color = &quot;red&quot;) Of course, you can combine the two settings: ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(fill = &quot;lightblue&quot;, color = &quot;red&quot;) Graph with grouped bars The most basic bar chart of values have one categorical variable on the x-axis and one continuous variable on the y-axis. If you want to include another categorical variable to divide up the data, you can use a graph with grouped bars. In mtcars, vs represents the engine of the car with 0 = V-shaped and 1 = straight. We can use vc to divide up the data in addition to gear using fill. To create a grouped bar chart, set position = \"dodge\" in geom_col(); otherwise, you will get a stacked bar chart. # prepare the data by_gear_vs &lt;- group_by(mtcars, gear, vs) mtcars_wt2 &lt;- summarize(by_gear_vs, mean_wt = mean(wt)) # convert to factor in the data mtcars_wt2$vs &lt;- as.factor(mtcars_wt2$vs) # using pipe %&gt;% to prepare the data mtcars_wt2 &lt;- mtcars %&gt;% group_by(gear, vs) %&gt;% summarize(mean_wt = mean(wt)) %&gt;% ungroup() %&gt;% mutate(vs = as.factor(vs)) # plot ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) Without position = \"dodge\", we get a stacked bar chart: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col() You can also convert vs to factor in call to ggplot(): # prepare the data by_gear_vs &lt;- group_by(mtcars, gear, vs) mtcars_wt3 &lt;- summarize(by_gear_vs, mean_wt = mean(wt)) # plot ggplot(mtcars_wt3, aes(x = gear, y = mean_wt, fill = factor(vs))) + geom_col(position = &quot;dodge&quot;) To change the colours of the bars: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Pastel2&quot;) You can try with different palettes: library(RColorBrewer) display.brewer.all() Using palette = \"Oranges\": ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Oranges&quot;) Using a manually defined palette: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_manual(values = c(&quot;#cc6666&quot;, &quot;#66cccc&quot;)) # also see Section 4.2 for the color Bar Charts of Counts Creating a bar chart of counts is very similar to creating a bar chart of values. Bar chart of the number of cars by gear in mtcars: ggplot(mtcars, aes(x = gear)) + geom_bar() Bar chart of the number of flights by each month in nycflights13: ggplot(flights, aes(x = month)) + geom_bar(fill = &quot;lightblue&quot;) + scale_x_continuous(breaks = 1:12) # change the labels Controlling the width (by default, width = 0.9): # if we use change the month to factor(month) # the labels are corrects ggplot(flights, aes(x = factor(month))) + geom_bar(fill = &quot;lightblue&quot;, width = 0.5) + labs(x = &quot;month&quot;) Bar chart of the number of flights by origin and month: # have to convert month to factor ggplot(flights, aes(x = origin, fill = factor(month))) + geom_bar(position = &quot;dodge&quot;, color = &quot;black&quot;) + scale_fill_discrete(name = &quot;Month&quot;) # change the title of the legend 8.2 Line Graph Suppose you want to make a line graph of the daily average departure delay in flights. From now on, we will use %&gt;% whenever it is appropriate. avg_delay &lt;- flights %&gt;% group_by(month, day) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(Time = 1:365) ggplot(avg_delay, aes(x = Time, y = delay)) + geom_line() Labeling the graph: # notice how we put each argument on its own line when the arguments # do not all fit on one line ggplot(avg_delay, aes(x=Time, y=delay)) + geom_line() + labs( y = &quot;Average Delay&quot;, title = &quot;Daily Average Departure Delay of Flights from NYC in 2013&quot; ) + scale_x_continuous(breaks = seq(1, 365, by = 28)) By default, the range of the y-axis of a line graph is just enough to include all the y values in the data. Sometimes, you may want to change the range manually. For example, the range of the y-axis in the following graph does not include 0. ggplot(BOD, aes(x = Time, y = demand)) + geom_line() + scale_x_continuous(breaks = 1:7) If you want to include 0 in the y range, you can use ylim: ggplot(BOD, aes(x = Time, y = demand)) + geom_line() + scale_x_continuous(breaks = 1:7) + ylim(0, max(BOD$demand)) Line Graph with multiple lines Suppose we want to create a line graph showing the daily average departure delay from the 3 airports in flights. # prepare the data flights_delay &lt;- flights %&gt;% group_by(month, origin) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() Line Graph: ggplot(flights_delay, aes(x = month, y = delay, color = origin)) + geom_line() + scale_x_continuous(breaks = 1:12) With different line types: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin)) + geom_line() + scale_x_continuous(breaks = 1:12) Add the points on top of the lines: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin)) + geom_line() + geom_point() + scale_x_continuous(breaks = 1:12) Change the point shapes according to origin: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin, shape = origin)) + geom_line() + geom_point() + scale_x_continuous(breaks = 1:12) To use one single shape for the points, we can specify the shape in geom_point(). The default shape is shape = 16. The default size is size = 2. fill is only applicable for shape = 21 to 25. ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin)) + geom_line() + geom_point(shape = 22, size = 3, fill = &quot;white&quot;, color = &quot;darkred&quot;) + scale_x_continuous(breaks = 1:12) Using another colour palette and changing the size of the lines: ggplot(flights_delay, aes(x = month, y = delay, color = origin)) + geom_line(size = 2) + geom_point(shape = 22, size = 3, fill = &quot;white&quot;, color = &quot;darkred&quot;)+ scale_colour_brewer(palette = &quot;Set2&quot;) + scale_x_continuous(breaks = 1:12) 8.3 Scatter Plots Scatter plots are often used to visualize the relationship between two continuous variables. It is also possible to use a scatter plot when either or both variables are discrete. The dataset heightweight contains sex, age, height and weight of some schoolchildren. head(heightweight) ## sex ageYear ageMonth heightIn weightLb ## 1 f 11.92 143 56.3 85.0 ## 2 f 12.92 155 62.3 105.0 ## 3 f 12.75 153 63.3 108.0 ## 4 f 13.42 161 59.0 92.0 ## 5 f 15.92 191 62.5 112.5 ## 6 f 14.25 171 62.5 112.0 To create a basic scatter plot, use geom_point(): ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point() You can control the shape, size, and color of the points as illustrated in the last section. ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point(size = 1.5, shape = 4, color = &quot;blue&quot;) If shape = 21-25, you can control the color in the points and outline of the points using fill and color, respectively. ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point(size = 1.5, shape = 22, fill = &quot;red&quot;, color = &quot;blue&quot;) Visualizing an additional discrete variable Suppose you want to use different colours for the points according to different categories of sex. ggplot(heightweight, aes(x = ageYear, y = heightIn, color = sex)) + geom_point() Suppose you want to use different shapes for the points according to different categories of sex. ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex)) + geom_point() You can use colours and shapes at the same time: ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, color = sex)) + geom_point() You can change the shapes or colours manually: ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, color = sex)) + geom_point() + scale_shape_manual(values = c(21,22)) + scale_colour_brewer(palette = &quot;Set2&quot;) Visualizing an additional continuous variable You may map an additional continuous variable to color. ggplot(heightweight, aes(x = ageYear, y = heightIn, color = weightLb)) + geom_point() Visualizing two additional discrete variables Lets create a new column to indicate if the child weights &lt; 100 or &gt;= 100 pounds (this is a discrete variable). heightweight2 &lt;- heightweight %&gt;% mutate(weightgroup = ifelse(weightLb &lt; 100, &quot;&lt; 100&quot;, &quot;&gt;= 100&quot;)) Now, we can add both sex and weightgroup in the plot in the following way: ggplot(heightweight2, aes(x = ageYear, y = heightIn, shape = sex, fill = weightgroup)) + geom_point() + scale_shape_manual(values = c(21, 24)) + scale_fill_manual( values = c(&quot;red&quot;, &quot;black&quot;), guide = guide_legend(override.aes = list(shape = 21)) # to change the legend ) Changing the mark ticks, limits and labels of the x-axis and y-axis: ggplot(heightweight2, aes(x = ageYear, y = heightIn, shape = sex, fill = weightgroup)) + geom_point() + scale_shape_manual(values = c(21, 24)) + scale_fill_manual( values = c(&quot;red&quot;, &quot;black&quot;), guide = guide_legend(override.aes = list(shape = 21)) # to change the legend ) + scale_x_continuous(name = &quot;Age (Year)&quot;, breaks = 11:18, limits = c(11, 18)) + scale_y_continuous(name = &quot;Height (In)&quot;, breaks = seq(50, 70, 5), limits = c(50, 73)) 8.3.1 Overplotting Overplotting refers to the situation when you have a large dataset so that the points in a scatter plot overlap and obscure each other. # We can create a variable to store the &quot;ggplot&quot; diamonds_ggplot &lt;- ggplot(diamonds, aes(x = carat, y = price)) diamonds_ggplot + geom_point() Possible solutions for overplotting: Use smaller points (size) # with diamonds_ggplot, we do not have to type # ggplot(diamonds, aes(x = carat, y = price)) diamonds_ggplot + geom_point(size = 0.1) Make the points semitransparent (alpha) diamonds_ggplot + geom_point(alpha = 0.05, size = 0.1) # 0.05 = 95% transparent We can see some vertical bands at some values of carats, meaning that diamonds tend to be cut to those sizes. Bin the data into rectangles (stat_bin2d) bins controls the number of bins in the x and y directions. The color of the rectangle indicates how many data points there are in the region. # by default, bins = 30 diamonds_ggplot + stat_bin2d(bins = 3) With bins = 50: diamonds_ggplot + stat_bin2d(bins = 50) + scale_fill_gradient(low = &quot;lightblue&quot;, high = &quot;red&quot;) Overplotting can also occur when the data is discrete on one or both axes. In the following example, we use the dataset ChickWeight, where Time is a discrete variable. head(ChickWeight) ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 # create a base plot cw_ggplot &lt;- ggplot(ChickWeight, aes(x = Time, y = weight)) cw_ggplot + geom_point() You may randomly jitter the points: cw_ggplot + geom_point(position = &quot;jitter&quot;) Jittering the points means a small amount of random variation is added to the location of each point. If you only want to jitter in the x-direction: cw_ggplot + geom_point(position = position_jitter(width = 0.5, height = 0)) 8.3.2 Labelling points in a scatter plot We can use annotate() or geom_text_repel() to label points in a scatter plot. For the latter, we have to install the package ggrepel. We will use the countries dataset in the package gcookbook and visualize the relationship between health expenditures and infant mortality rate. We will consider a subset of data by focusing the data from 2009 and countries with more than \\(2,000\\) USD health expenditures per capita: countries_subset &lt;- countries %&gt;% filter(Year == 2009, healthexp &gt; 2000) Using annotate: # find out the x and y coordinates for the point corresponding to Canada canada_x &lt;- filter(countries_subset, Name == &quot;Canada&quot;)$healthexp canada_y &lt;- filter(countries_subset, Name == &quot;Canada&quot;)$infmortality ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + annotate(&quot;text&quot;, x = canada_x, y = canada_y + 0.2, label = &quot;Canada&quot;) # + 0.2 is to avoid the label placing on top of the point Label all the points with geom_text_repel: # to use geom_text_repel, load the package ggrepel library(ggrepel) ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + geom_text_repel(aes(label = Name), size = 3) Label all the points with geom_label_repel (with a box around the label): # geom_label_repel also depends on the package ggrepel ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + geom_label_repel(aes(label = Name), size = 3) 8.4 Summarizing Data Distributions 8.4.1 Histogram Histogram can be used to visualize the distribution of a variable. We will illustrate how to create histograms using the dataset birthwt from the package MASS. library(MASS) birthwt contains data of 189 birth weights with some covariates of the mothers. Take a look at the dataset: head(birthwt) ## low age lwt race smoke ptl ht ui ftv bwt ## 85 0 19 182 2 0 0 0 1 0 2523 ## 86 0 33 155 3 0 0 0 0 3 2551 ## 87 0 20 105 1 1 0 0 0 1 2557 ## 88 0 21 108 1 1 0 0 1 2 2594 ## 89 0 18 107 1 1 0 0 1 0 2600 ## 91 0 21 124 3 0 0 0 0 0 2622 Basic histogram: ggplot(birthwt, aes(x=bwt)) + geom_histogram() Plot a histogram with density (not frequency): ggplot(birthwt, aes(x=bwt)) + geom_histogram(aes(y = ..density..)) To compare two histograms Use facet_grid() to display two histograms in the same plot. Suppose we group the data according to the smoking status during pregnancy and we want to display the two histograms of the birth weight: ggplot(birthwt, aes(x = bwt)) + geom_histogram() + facet_grid(smoke ~ .) To change the label, we can change the content of the variable: # create another dataset birthwt_mod &lt;- birthwt birthwt_mod$smoke &lt;- ifelse(birthwt_mod$smoke == 1, &quot;Smoke&quot;, &quot;No Smoke&quot;) ggplot(birthwt_mod, aes(x = bwt)) + geom_histogram() + facet_grid(smoke ~ .) Alternatively, we can use recode_factor: birthwt_mod$smoke &lt;- recode_factor(birthwt_mod$smoke, &quot;0&quot; = &quot;No Smoke&quot;, &quot;1&quot; = &quot;Smoke&quot;) Use fill() to put two groups in the same plot with different colors. We need to set position = \"identity\"; otherwise, the bars will be stacked on top of each other vertically which is not what we want. ggplot(birthwt_mod, aes(x=bwt, fill=smoke)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) #+ # facet_grid(race~., scales=&quot;free&quot;) It is also possible to use both facet_grid and fill when we have want to group the data with two discrete variables. We will illustrate this with grouping according to the smoking status and the race. We also add scales = \"free\" so that the ranges of the y-axes will be adjusted according to the data in each histogram. # change the name so that the labels can be understood easily birthwt_mod$race[which(birthwt_mod$race==1)] = &quot;White&quot; birthwt_mod$race[which(birthwt_mod$race==2)] = &quot;Black&quot; birthwt_mod$race[which(birthwt_mod$race==3)] = &quot;Other&quot; ggplot(birthwt_mod, aes(x = bwt, fill = smoke)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) + facet_grid(race ~ ., scales = &quot;free&quot;) Note: we do not have a large dataset in this example so that grouping by two variables may not give us a very good understanding of the data. 8.4.2 Kernel Density Estimate Kernel density estimation is a nonparametric method to estimate the density of the samples. Nonparametric method means we do not impose a parametric model. A parametric model has a finite dimensional parameter \\(\\theta \\in \\mathbb{R}^d\\) for some finite \\(d\\). Let \\(X_1,\\ldots,X_n\\) be i.i.d. random variables from some distribution with density \\(f\\). The histogram for \\(f\\) at point \\(x_0\\) is \\[\\begin{equation*} \\hat{f}(x_0) = \\frac{\\text{number of $x_i$ in the bin containing $x_0$}}{n h}, \\end{equation*}\\] where the bin width is \\(h\\). As we already know, the histogram will not give a smooth estimate of the density. One may use another method called kernel density estimator, which could produce smooth estimate of the density. The kernel density estimator is \\[\\begin{equation*} \\hat{f}_n(x_0) = \\frac{1}{nh}\\sum^n_{i=1} K \\bigg( \\frac{x_0 - x_i}{h} \\bigg), \\end{equation*}\\] where \\(K\\) is a kernel and \\(h\\) is the bandwidth. For our purposes, a kernel is a non-negative symmetric function such that \\(\\int^\\infty_{-\\infty}K(x)dx = 1\\) and \\(\\int^\\infty_{-\\infty} x K(x)dx =0\\). For example, \\[\\begin{eqnarray*} \\text{the boxcar kernel:} &amp;&amp; K(x) = \\frac{1}{2}I(|x| \\leq 1)\\\\ \\text{the Gaussian kernel:} &amp;&amp; K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\\\ \\text{the Epanechnikov kernel:} &amp;&amp; K(x) = \\frac{3}{4}(1-x^2)I(|x| \\leq 1) \\\\ \\text{the tricube kernel:} &amp;&amp; K(x) = \\frac{70}{81}(1-|x|^3)^3I(|x| \\leq 1), \\end{eqnarray*}\\] where \\(I(|x| \\leq 1) = 1\\) if \\(|x| \\leq 1\\) and equals \\(0\\) otherwise. Since the kernel is symmetric around \\(0\\), the magnitude \\((x-x_i)/h\\) is the distance from \\(0\\). For the above kernels, the value of the kernels is smaller when we evaluate at a point further from \\(0\\). Therefore, data close to \\(x_0\\) will contribute larger weights in estimating \\(\\hat{f}(x_0)\\). The bandwidth will control the smoothness of the estimate: larger bandwidth will result in a smoother curve and smaller bandwidth will result in a noisy and rough curve. We can create a kernel density estimate of the distribution using geom_density(). ggplot(birthwt, aes(x = bwt)) + geom_density() + geom_density(adjust = 0.25, color = &quot;red&quot;) + # smaller bandwidth -&gt; noisy geom_density(adjust = 2, color = &quot;blue&quot;) # large bandwidth -&gt; smoother Overlaying a density curve with a histogram ggplot(birthwt, aes(x = bwt)) + geom_histogram(fill = &quot;cornsilk&quot;, aes(y = ..density..)) + geom_density() Displaying kernel density Estimates from grouped data To use geom_density() to display kernel density estimates from grouped data, the grouping variable must be a factor or a character vector. Recall that in birthwt_mod that we created earlier, the smoke variable is a character vector. With color: ggplot(birthwt_mod, aes(x = bwt, color = smoke)) + geom_density() With fill: ggplot(birthwt_mod, aes(x = bwt, fill = smoke)) + geom_density(alpha = 0.3) # to control the transparency With facet_grid(): ggplot(birthwt_mod, aes(x = bwt)) + geom_density() + facet_grid(smoke ~ .) 8.5 Saving your plots There are two types of image files: vector and raster (bitmap) Raster images are pixel-based. When you zoom in the image, you can see the individual pixels. Two examples are JPG and PNG files. JPG files quality is lower than that of the PNG files. Vector images are constructed using mathematical formulas. You can resize the image without a loss in image quality. When you zoom in the image, it is still smooth and clear. Two examples are AI and PDF files. 8.5.1 Outputting to pdf vector files Suppose you want to save the plot from the following code: ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() # first argument is the file name # width and height are in inches pdf(&quot;filename.pdf&quot;, width = 4, height = 4) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() dev.off() Outputting to a pdf file: usually the best option usually smaller than bitmap files such as PNG files. when you have overplotting (many points on the plot), a PDF file can be much larger than a PNG file. 8.5.2 Outputting to bitmap files # width and heights are in pixels png(&quot;png_plot.png&quot;, width = 600, height = 600) ggplot(mtcars, aes(x=wt,y=mpg)) + geom_point() dev.off() For high-quality print output, it is recommended to use at least 300 ppi (ppi = pixels per inch). Suppose you want to create a 4x4-inch PNG file with 300 ppi: ppi &lt;- 300 png(&quot;png_plot.png&quot;, width = 4*ppi, height = 4*ppi, res = ppi) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() dev.off() 8.6 Summary 8.6.1 Combining multiple operations with pipe %&gt;% 8.6.2 Bar charts examples of using pipe %&gt;% together with ggplot create bar charts of counts create bar charts of values change fill and outline of the bars create grouped bar charts create stacked bar charts convert a variable into factor in ggplot use different colour palette control the width of the bars 8.6.3 Line graphs create line graphs label the graph change the range of y-axis create line graphs with multiple lines use multiple geoms (geometric objects) (e.g.Â additing the points on top of the lines) change shape, size, fill, outline of points change line type 8.6.4 Scatter plot create scatter plots visualize an additional discrete variable visualize an additional continuous variable visualize two additional discrete variables overplotting (use smaller points, make points semitransparent, bin data into rectangels, jitter the points) label points in a scatter plot 8.6.5 Summarizing data distributions create histograms (frequency and density) compare two histograms (facet_grid(), fill()) create histograms with two additional discrete variables create kernel density estimates overlay a density curve with a histogram display kernel density estimates from grouped data (color, fill, facet_grid) 8.6.6 Saving your plots output to pdf vector files output to bitmap files "],["statistical-inference-in-r.html", "Chapter 9 Statistical Inference in R 9.1 Maximum Likelihood Estimation 9.2 Interval Estimation and Hypothesis Testing", " Chapter 9 Statistical Inference in R In this chapter, we discuss how to perform some parameter estimations and hypothesis testings in R. You may have learned their theory in previous statistics courses. I do not intend to give a very comprehensive review to these methods due to time constraint. Optional Readings: You can find a few more statistical tests in Ch 9 of R Cookbook (https://rc2e.com/) You can review Ch 10-13 of John E. Freunds Mathematical Statistics with Applications by Irwin Miller and Marylees Miller (textbook for STAT 269) for some background and theory on statistical inference 9.1 Maximum Likelihood Estimation After you collect some data and formulate a statistical model, you have to estimate the parameters in your model. One of the most common methods is to use maximum likelihood estimation. Very often, there is no closed-form expression for your estimators. In general, suppose you have data \\(y_1,\\ldots,y_n\\). The likelihood function is a function of the parameter defined as \\[\\begin{equation*} L(\\theta|y_1,\\ldots,y_n) = f_{y_1,\\ldots,y_n}(y_1,\\ldots,y_n|\\theta), \\end{equation*}\\] where \\(f_{y_1,\\ldots,y_n}(\\cdot |\\theta)\\) is the joint pmf or pdf of \\(y_1,\\ldots,y_n\\) with parameter \\(\\theta\\). That is, the likelihood function evaluated at \\(\\theta\\) is simply the joint probability of observing \\(y_1,\\ldots,y_n\\) when the parameter value is \\(\\theta\\). Assuming \\(y_1,\\ldots,y_n\\) are i.i.d. with density \\(f(\\cdot|\\theta)\\), we have \\[\\begin{equation*} L(\\theta|y_1,\\ldots,y_n) = f_{y_1,\\ldots,y_n}(y_1,\\ldots,y_n|\\theta) = \\prod^n_{i=1} f(y_i|\\theta). \\end{equation*}\\] In maximum likelihood estimation, we estimate the parameter \\(\\theta\\) by maximizing \\(L\\). The maximizer is called the maximum likelihood estimator (MLE). Some theory Why do we want to maximize the likelihood? Informally, the likelihood is the chance of observing the data. Therefore, we want to find the parameters so that such a chance is maximized. MLE has good statistical properties. Under some regularity conditions, MLE is consistent: \\(\\hat{\\theta}_n\\) converges in probability to \\(\\theta_0\\) Asymptotically efficient: the estimator has the lowest variance asymptotically in some sense Asymptotically normality: can be used to find confidence intervals and perform hypothesis testings Example (Logistic Regression): Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) is a binary variable and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In logistic regression we assume that \\[\\begin{equation*} P(Y_i = 1|x_i, \\beta) = \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}} = \\frac{ e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}. \\end{equation*}\\] Since \\(Y_i\\) takes only two values, \\[\\begin{equation*} P(Y_i = 0|x_i, \\beta) = \\frac{1}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] We can use one single formula for \\(y = 0, 1\\): \\[\\begin{equation*} P(Y_i = y|x_i, \\beta) = \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The likelihood function (conditional on x) is \\[\\begin{equation*} L(\\beta|y_1,\\ldots,y_n, x_1,\\ldots,x_n) = \\prod^n_{i=1} P(Y_i = y_i|x_i, \\beta) = \\prod^n_{i=1} \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The MLE of \\(\\beta\\) is obtained by maximizing \\(L(\\beta|y,x)\\) with respect to \\(\\beta\\). We usually maximize the natural logarithm of the likelihood function instead of the likelihood function, which is easier. The log likelihood function is \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( (x^T_i \\beta) y_i - \\log(1+e^{x^T_i \\beta}) \\bigg). \\end{equation*}\\] In this case, there is no closed-form formula for finding the maximizer. Nevertheless, we can use numerical methods to find out the maximizer. Of course, there are existing functions to perform this task in R. However, we will illustrate how to perform an optimization using the function optim(). By default, optim() will find the minimum. Therefore, we will minimize the negative of the log likelihood function. Simulated Example This is also a good time to introduce how to perform simulation based on a model and check the validity of the estimation method and algorithm. We will first generate some covariates and binary variables based on the logistic regression model. # Setting set.seed(362) n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) Now, we will write a function to compute the negative of the log likelihood function (as a function of the parameter \\(\\beta\\) and the data \\(\\{y_i, x_{i1}, x_{i2}: i=1,\\ldots,n\\}\\)), which is the objective function to be minimized. neg_log_like &lt;- function(beta, y, x1, x2) { beta_X &lt;- beta[1] + beta[2] * x1 + beta[3] * x2 log_like &lt;- sum(beta_X * y) - sum(log(1 + exp(beta_X))) -log_like # return the negative log likelihood } After defining our objective function, we can now use optim() to perform the optimization. optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;) ## $par ## [1] 0.7830734 1.0067301 -1.4920231 ## ## $value ## [1] 632.7284 ## ## $counts ## function gradient ## 28 7 ## ## $convergence ## [1] 0 ## ## $message ## NULL par is the initial values for the optimization. We set some random numbers for the initial values by par = runif(3, 0, 1). Because the function neg_log_like have multiple arguments, we have to supply them inside optim(). method = \"BFGS\" is a quasi-Newton method. method = \"L-BFGS-B\" is also useful when you want to add box constraints to your variable. You can check ?optim to learn more about this. Output: par is the parameter values at which the minimum is obtained value is the minimum function value convergence = 0 indicates successful completion Compare with the built-in function glm() for estimating the parameters: # will discuss this in more detail later fit &lt;- glm(y ~ x1 + x2, family = &quot;binomial&quot;) fit ## ## Call: glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;) ## ## Coefficients: ## (Intercept) x1 x2 ## 0.7831 1.0067 -1.4920 ## ## Degrees of Freedom: 999 Total (i.e. Null); 997 Residual ## Null Deviance: 1324 ## Residual Deviance: 1265 AIC: 1271 You can see that both methods give the same estimates 0.783, 1.007, -1.492 for the regression coefficients. How do you know your method of estimation makes sense? How do you know if you have simulated the data correctly? In the above example, the true parameters are 0.5, 1, -1. The estimates are not really that close to the true values. We do not know if the estimation method will give a good result in general. To tackle this problem, we could simulate many datasets, perform the estimation, and take a look at the distributions of the estimates. Perform the simulation and estimation \\(250\\) times: # Setting n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta no_iter &lt;- 250 beta_est &lt;- matrix(0, nrow = no_iter, ncol = length(beta_0)) for (i in 1:no_iter) { # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Estimation beta_est[i, ] &lt;- optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;)$par } Displaying the results: library(tidyverse) # create the dataframe for plotting data &lt;- tibble(est = c(beta_est[, 1], beta_est[, 2], beta_est[, 3]), beta = c(rep(&quot;Beta1&quot;, no_iter), rep(&quot;Beta2&quot;, no_iter), rep(&quot;Beta3&quot;, no_iter))) # dataframe for adding the vertical lines for the true parameters vline_data &lt;- tibble(beta = c(&quot;Beta1&quot;, &quot;Beta2&quot;, &quot;Beta3&quot;), mean = beta_0) ggplot(data = data, mapping = aes(x = est)) + geom_histogram(fill = &quot;lightblue&quot;) + facet_grid(~ beta, scales = &quot;free&quot;) + geom_vline(data = vline_data, aes(xintercept = mean), color = &quot;blue&quot;) From the above plots, you can see the distributions of your estimators. The true parameters lie in the middle of the distributions. You can also add lines to visualize the mean of the distributions. In this case, the lines actually overlap with lines for the true parameters. Thus, the estimators are essentially unbiased. You can also see that there are times that \\(\\hat{\\beta}_0\\) can be as large as \\(1\\) or as small as \\(0.1\\) while the true value is \\(0.5\\). You can also use a larger sample size. # setting set.seed(362) n &lt;- 100000 beta_0 &lt;- c(0.5, 1, -1) # true beta # simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Estimation (est &lt;- optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;)$par) ## [1] 0.4803615 1.0275395 -0.9871783 The estimates are now 0.48, 1.03, -0.99, which are close to the true values 0.5, 1, -1. We will see some applications of the logistic regression later. 9.1.1 Exercises on MLE Exercise 1 (Gamma distribution) You observe a random sample \\(y_1,\\ldots,y_n\\) from a Gamma distribution with unknown parameters \\(\\alpha, \\beta\\). The likelihood function is \\[\\begin{equation*} L(\\alpha, \\beta |y_1,\\ldots,y_n) = \\prod^n_{i=1} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{\\alpha-1}_i e^{-\\beta y_i}. \\end{equation*}\\] The log likelihood, after simpliciation, is \\[\\begin{equation*} \\log L(\\alpha, \\beta) = n \\alpha \\log \\beta - n \\log \\Gamma(\\alpha) + (\\alpha - 1) \\sum^n_{i=1} \\log y_i - \\beta \\sum^n_{i=1} y_i. \\end{equation*}\\] # Setting set.seed(1) alpha &lt;- 1.5 beta &lt;- 2 n &lt;- 10000 # Simulation x &lt;- rgamma(n, alpha ,beta) # Optimization (Estimation) # Assignment 4 Exercise 2 (Poisson Regression) Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) only takes nonnegative integer values (count data) and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In Poisson regression, we assume that \\(Y_i\\) has a Poisson distribution and \\[\\begin{equation*} \\log (E(Y_i|x_i)) = \\beta^T x_i = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}, \\end{equation*}\\] where \\(\\beta \\in \\mathbb{R}^{p+1}\\). Alternatively, condition on \\(x_i\\), \\(Y_i|x_i \\sim \\text{Pois}(e^{\\beta^T x_i})\\). The likelihood is \\[\\begin{equation*} L(\\beta|y,x) = \\prod^n_{i=1} \\frac{e^{-e^{\\beta^T x_i}} e^{(\\beta^T x_i)y_i}}{y_i!}. \\end{equation*}\\] The log likelihood is \\[\\begin{equation*} \\log L(\\beta|y, x) = \\sum^n_{i=1} \\bigg( -e^{\\beta^T x_i} + (\\beta^T x_i) y_i - \\log (y_i!) \\bigg). \\end{equation*}\\] Clearly, the term \\(\\log (y_i !)\\) does not depend on \\(\\beta\\) and hence we do not need to consider it during our optimization. Therefore, it suffices to maximize (or minimize the negative of) the following objective function \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( -e^{\\beta^T x_i} + (\\beta^T x_i) y_i \\bigg). \\end{equation*}\\] # Setting set.seed(1) beta &lt;- c(0.3, 0.5, -0.5) n &lt;- 10000 p &lt;- length(beta) - 1 # Simulation X &lt;- cbind(1, matrix(runif(n * p), nrow = n, ncol = p)) y &lt;- rpois(n, exp(X %*% beta)) # Optimization (Estimation) # Assignment 4 9.1.2 Summary Review of the MLE Use optim() to minimize an objective function Simplify the objective function before you try to minimize it (take log and remove terms that do not depend on the parameters) how to simulate from a model how to check the validity of the simulation results 9.2 Interval Estimation and Hypothesis Testing Two types of estimation: point estimation (e.g.Â MLE) and interval estimation (e.g.Â confidence interval). 9.2.1 Examples of Hypothesis Testing Optional reading: Chapter 12 in John E. Freunds Mathematical Statistics with Applications. You have a die and you wonder if it is unbiased. If the die is unbiased, the (population) mean of the result from rolling the die is \\(3.5\\). Suppose you roll the die \\(10\\) times, the sample mean is \\(5\\). What is your decision on determining if the die is biased or not? How confident is your decision? What if you roll the die \\(100\\) times, and the sample mean is \\(5\\)? What is your decision now? Are you more confident in your decision? What if your roll the die \\(10\\) times but the sample mean is \\(4\\)? To answer these questions, we need to understand interval estimation and hypothesis testing. Some more examples: An engineer has to decide on the basis of sample data whether the true average lifetime of a certain kind of tire is at least \\(42,000\\) miles An agronomist has to decide on the basis of experiments whether one kind of fertilizer produces a higher yield of soybeans than another A manufacturer of pharmaceutical products has to decide on the basis of samples whether 90 percent of all patients given a new medication will recover from a certain disease These problems can all be translated into the language of statistical tests of hypotheses. the engineer has to test the hypothesis that \\(\\theta\\), the parameter of an exponential population, is at least \\(42,000\\) the agronomist has to decide whether \\(\\mu_1 &gt; \\mu_2\\), where \\(\\mu_1\\) and \\(\\mu_2\\) are the means of two normal populations the manufacturer has to decide whether \\(\\theta\\), the parameter of a binomial population, equals \\(0.90\\) In each case it must be assumed, of course, that the chosen distribution correctly describes the experimental conditions; that is, the distribution provides the correct statistical model. 9.2.2 Null Hypotheses, Alternative Hypotheses, and p-values An assertion or conjecture about the distribution of one or more random variables is called a statistical hypothesis. If a statistical hypothesis completely specifies the distribution, it is called a simple hypothesis; if not, it is referred to as a composite hypothesis. Null hypothesis: In view of the assumptions of no difference, hypotheses such as these led to the term null hypothesis, but nowadays this term is applied to any hypothesis that we may want to test. p-value: the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct Steps in Hypothesis Testing Assume the null hypothesis is true Calculate a test statistic. E.g., sample mean Calculate a \\(p\\)-value (denoted by \\(p\\)) from the statistic and its distribution. For example, if the die is unbiased, what is the probability that we observe the sample mean to be larger than \\(5\\) after rolling it \\(100\\) times? small \\(p\\)-value small: we have strong evidence to reject the null hypothesis because it is unlikely to observe such a test statistic if the null hypothesis is true large \\(p\\)-value small: we do not have enough evidence to reject the null hypothesis Remark In this course, we will follow the common convention to reject the null hypothesis when \\(p &lt; 0.05\\) In real applications, how small is small depends on the problems. Being statistically significant (small \\(p\\)-value) does not mean the difference between the null and alternative hypotheses is large. One should also look at the confidence intervals or distributions of your estimates. 9.2.3 Type I error and Type II error Type I error: reject the null hypothesis when it is true. The probability of committing a type I error is denoted by \\(\\alpha\\) (level of significance of the test). Type II error: do not reject the null hypothesis when it is false. The probability of committing a type II error is denoted by \\(\\beta\\). \\(H_0\\) is true \\(H_1\\) is true Reject \\(H_0\\) Type I error No Error Do not reject \\(H_0\\) No Error Type II Error A good test procedure is one in which both \\(\\alpha\\) and \\(\\beta\\) are small, thereby giving us a good chance of making the correct decision. When the sample size \\(n\\) is held fixed, reducing \\(\\alpha\\) by changing the rejection region will increase \\(\\beta\\) and vice versa. The only way in which we can reduce the probabilities of both types of errors is to increase \\(n\\). As long as \\(n\\) is held fixed, this inverse relationship between the probabilities of type I and type II errors is typical of statistical decision procedures. Usually, we control \\(\\alpha\\) to be small (e.g.Â \\(0.05\\)). 9.2.4 Inference for Mean of One Sample Hypothesis Testing Problem You have a random sample \\(X_1,\\ldots,X_n\\) from a population. You want to know if the population mean \\(\\mu\\) is equal to \\(\\mu_0\\). That is, \\(H_0 : \\mu =\\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\). Solution You can use the \\(t\\)-test for this problem. It is appropriate when either Your data is normally distributed You have a large sample size \\(n\\). A rule of thumb is \\(n &gt; 30\\). The test statistic is \\[\\begin{equation*} \\frac{\\overline{X}_n - \\mu_0}{s/\\sqrt{n}}, \\end{equation*}\\] where \\(\\overline{X}_n\\) is the sample mean and \\(s\\) is the sample standard deviation. In R, use t.test() to perform the t-test. # Simulate the data set.seed(362) # so that you can replicate the result x &lt;- rnorm(75, mean = 100, sd = 15) # Perform t-test t.test(x, mu = 95) ## ## One Sample t-test ## ## data: x ## t = 4.7246, df = 74, p-value = 1.071e-05 ## alternative hypothesis: true mean is not equal to 95 ## 95 percent confidence interval: ## 99.36832 105.74018 ## sample estimates: ## mean of x ## 102.5543 The \\(p\\)-value is small, so it is unlikely that the mean of the population is \\(95\\). The \\(p\\)-value in this case is \\(2 \\times P_{H_0}(T &gt; |\\text{obs. T.S.|})\\), where \\(T\\) has a \\(t\\)-distribution with degrees of freedom \\(n-1\\) if the data from are from a normal distribution and obs. T.S. stands for the observed test statistic. The subscript \\(H_0\\) is to stress that the probability measure is under \\(H_0\\). [Optional] How are the test statistic and \\(p\\)-value calculated? # test statistic (obs_ts &lt;- (mean(x) - 95) / (sd(x) / sqrt(length(x)))) ## [1] 4.724574 # p-value 2 * (1 - pt(abs(obs_ts), df = length(x) - 1)) ## [1] 1.071253e-05 What do you expect when \\(x\\) is from a distribution with a much larger SD? set.seed(362) # so that you can replicate the result x2 &lt;- rnorm(75, mean = 100, sd = 200) (test_x2 &lt;- t.test(x2, mu = 95)) ## ## One Sample t-test ## ## data: x2 ## t = 1.832, df = 74, p-value = 0.07097 ## alternative hypothesis: true mean is not equal to 95 ## 95 percent confidence interval: ## 91.57758 176.53580 ## sample estimates: ## mean of x ## 134.0567 Even if the estimate of the population mean is 134, the test does not reject the null hypothesis that the mean is \\(95\\). This is because the sample has a very large variance. Interval Estimation Denote \\(\\overline{x}_n\\) and \\(s_n\\) be the sample mean and sample standard deviation, respectively. The \\(100(1-\\alpha)\\%\\) confidence interval of \\(\\mu\\) is given by \\[\\begin{equation*} \\bigg[ \\overline{x}_n - t_{n-1; \\alpha/2} \\frac{s_n}{\\sqrt{n}}, \\overline{x}_n + t_{n-1; \\alpha/2} \\frac{s_n}{\\sqrt{n}} \\bigg], \\end{equation*}\\] where \\(t_{n-1;\\alpha/2}\\) satisfies \\(P(T &gt; t_{n-1;\\alpha/2}) = \\alpha/2\\) and \\(T \\sim t(n-1)\\). To find the confidence interval of \\(\\mu\\) in R, use t.test(). For example, the \\(99\\%\\) confidence interval of \\(\\mu\\) is t.test(x, conf.level = 0.99) ## ## One Sample t-test ## ## data: x ## t = 64.139, df = 74, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 99 percent confidence interval: ## 98.32683 106.78168 ## sample estimates: ## mean of x ## 102.5543 Remark By omitting mu = 0.95, the default value is mu = 0. Since we are interested in finding the confidence intervals, we do not need to care about the value of \\(\\mu_0\\). [Optional] Without using t.test(): # just an illustration of how CI can be computed alpha &lt;- 0.01 n &lt;- length(x) half_width &lt;- qt(1 - alpha / 2, n - 1) * sd(x) / sqrt(n) c(mean(x) - half_width, mean(x) + half_width) ## [1] 98.32683 106.78168 Interpretation If you can repeat the experiment many times, then about \\(95\\%\\) of the confidence intervals computed in those many times will contain the true mean. no_sim &lt;- 10000 set.seed(362) # so that you can replicate the result true_mean &lt;- 100 CI &lt;- matrix(0, nrow = no_sim, ncol = 2) for (i in 1:no_sim) { CI[i, ] &lt;- t.test(rnorm(75, mean = true_mean, sd = 200))$conf.int } # find out the proportion of CIs that contain 0 mean(CI[, 1] &lt; 100 &amp; 100 &lt; CI[, 2]) ## [1] 0.9503 Example Recall that we talked about how to use simulation to estimate \\(P(X &gt; Y)\\), where \\(X\\) and \\(Y\\) are some random variables. For example, if \\(X \\sim N(0, 1)\\), \\(Y \\sim \\text{Exp}(2)\\), and they are independent. R code to estimate \\(P(X &gt; Y)\\): set.seed(1) n &lt;- 10000 X &lt;- rnorm(n, 0, 1) Y &lt;- rexp(n, 2) mean(X &gt; Y) ## [1] 0.3299 We know the true value is not exactly 0.3299 because the law of large numbers only ensure that the sample mean converges to the true mean. We can use t.test() to find an confidence interval for \\(P(X &gt; Y)\\). t.test(X &gt; Y) ## ## One Sample t-test ## ## data: X &gt; Y ## t = 70.162, df = 9999, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3206831 0.3391169 ## sample estimates: ## mean of x ## 0.3299 We see that the \\(95\\%\\) CI is (0.321, 0.339). To make the CI narrower, we can increase the number of simulation. set.seed(1) n &lt;- 1000000 X &lt;- rnorm(n, 0, 1) Y &lt;- rexp(n, 2) t.test(X &gt; Y) ## ## One Sample t-test ## ## data: X &gt; Y ## t = 704.63, df = 1e+06, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3308521 0.3326979 ## sample estimates: ## mean of x ## 0.331775 The \\(95\\%\\) CI becomes (0.331, 0.333). The width of the CI is equal to \\[\\begin{equation*} 2 \\times t_{n-1; \\alpha/2} \\frac{s}{\\sqrt{n}}. \\end{equation*}\\] From the above formula, you could determine the minimum number of simulations required to achieve a certain degree of accuracy, 9.2.5 Comparing the means of two samples Suppose you have one sample each from two populations. You want to test if the two populations have the same mean. There are two different \\(t\\)-tests for this task (assuming data are normally distributed or you have large samples): the observations are not paired the observations are paired To explain the meaning of paired data, consider two experiments to see if drinking coffee in the morning improves your test scores: Unpaired observations: Randomly select two groups of people. People in one group have a cup of morning coffee and take the test. The other group just takes the test. For each person, we have one test score. All the scores are independent. Paired observations: Randomly select one group of people. Give them the test twice, once with morning coffee and once without morning coffee. For each person, we have two test scores. Clearly, the two scores are not statistically independent. Example We illustrate the paired \\(t\\)-test using the dataset sleep (no package is required). The data contains the increase in hours of sleep when the subject took two soporific drugs compared to control on \\(10\\) subjects. Since each subject received two drugs, the observations are paired. Take a look at sleep: sleep ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 The sleep data is in a long format. Lets turn it into a wide format using spread, a function in the package tidyr, which is contained in tidyverse. (sleep_wide &lt;- spread(sleep, group, extra)) ## ID 1 2 ## 1 1 0.7 1.9 ## 2 2 -1.6 0.8 ## 3 3 -0.2 1.1 ## 4 4 -1.2 0.1 ## 5 5 -0.1 -0.1 ## 6 6 3.4 4.4 ## 7 7 3.7 5.5 ## 8 8 0.8 1.6 ## 9 9 0.0 4.6 ## 10 10 2.0 3.4 Paired \\(t\\)-test: t.test(sleep_wide[, 2], sleep_wide[, 3], paired = TRUE) ## ## Paired t-test ## ## data: sleep_wide[, 2] and sleep_wide[, 3] ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean of the differences ## -1.58 The \\(p\\)-value is \\(&lt; 0.05\\). We conclude that the effect of the two drugs are different. The \\(95\\%\\) CI of the difference between the two means is (-2.46, -0.70). Example Are the means of the birth weights in the smoking group and non-smoking group different? library(MASS) t.test(birthwt$bwt[birthwt$smoke == 1], birthwt$bwt[birthwt$smoke == 0]) ## ## Welch Two Sample t-test ## ## data: birthwt$bwt[birthwt$smoke == 1] and birthwt$bwt[birthwt$smoke == 0] ## t = -2.7299, df = 170.1, p-value = 0.007003 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -488.97860 -78.57486 ## sample estimates: ## mean of x mean of y ## 2771.919 3055.696 The \\(p\\)-value is \\(&lt; 0.05\\). We conclude that the means are different. The \\(95\\%\\) CI of the difference between the two means is (-489.0, -78.6). 9.2.6 Inference of a Sample Proportion You have a sample of values from a population consisting of successes and failures. The null hypothesis is the true proportion of success \\(p\\) is equal to some particular number \\(p_0\\). The alternative hypothesis is the \\(p \\neq p_0\\). Example You flip a coin \\(100\\) times independently. You want to test if the coin is fair \\((p_0 = 0.5)\\). # Simulate the coin flips set.seed(1) heads &lt;- rbinom(1, size = 100, prob = .4) # Test, p_0 = 0.5 (result &lt;- prop.test(heads, 100, p = 0.5)) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.1336 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.3233236 0.5228954 ## sample estimates: ## p ## 0.42 The point estimate is 0.42. Although the true probability of success used in the simulation is \\(0.4\\), for this particular data, we do not reject to null hypothesis that the true probability of success is \\(0.5\\) as the \\(p\\)-value equals 0.134, which is larger than \\(0.05\\). The 95% confidence interval is equal to (0.323, 0.523). You can change the alternative hypothesis to \\(p &gt; p_0\\) or \\(p &lt; p_0\\): prop.test(heads, 100, p = 0.5, alternative = &quot;greater&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.9332 ## alternative hypothesis: true p is greater than 0.5 ## 95 percent confidence interval: ## 0.3372368 1.0000000 ## sample estimates: ## p ## 0.42 prop.test(heads, 100, p = 0.5, alternative = &quot;less&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.06681 ## alternative hypothesis: true p is less than 0.5 ## 95 percent confidence interval: ## 0.0000000 0.5072341 ## sample estimates: ## p ## 0.42 9.2.7 Testing groups for equal proportions You have samples from two or more groups. The data from each group are binary-valued: either success or failure. You want to test if the groups have equal proportions of success. Example # 3 groups no_success &lt;- c(48, 60, 50) # no. of successes in the 3 groups no_trial &lt;- c(100, 100, 100) # corresponding no. of trails in the 3 groups prop.test(no_success, no_trial) ## ## 3-sample test for equality of proportions without continuity correction ## ## data: no_success out of no_trial ## X-squared = 3.3161, df = 2, p-value = 0.1905 ## alternative hypothesis: two.sided ## sample estimates: ## prop 1 prop 2 prop 3 ## 0.48 0.60 0.50 \\(p\\)-value is \\(&gt; 0.05\\). We do not reject the null hypothesis that the three groups have the same proportion of success. Example In a class of \\(38\\) students, \\(14\\) of them got \\(A\\). In another class of \\(40\\) students, only \\(10\\) got \\(A\\). We want to know if the difference between the two proportions is statistically significant. prop.test(c(14, 10), c(38, 40)) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(14, 10) out of c(38, 40) ## X-squared = 0.7872, df = 1, p-value = 0.3749 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.1110245 0.3478666 ## sample estimates: ## prop 1 prop 2 ## 0.3684211 0.2500000 The \\(p\\)-value is \\(&gt; 0.05\\). We do not reject the null hypothesis that the students in the two groups have the same proportion of getting an A. 9.2.8 Testing if two samples have the same underlying distribution Problem You have two random samples \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\). Let \\(F\\) and \\(G\\) be the distribution functions of \\(X_i\\)s and \\(Y_i\\)s respectively. You want to know if \\(F \\equiv G\\). Solution You may use the Kolmogorov-Smirnov test. \\(H_0: F = G\\) vs \\(H_1: F \\neq G\\). It does not require any assumptions. The test statistic is \\[ D := \\sup_{x \\in \\mathbb{R}}|F_n(x) - G_m(x)|,\\] where \\(F_n\\) and \\(G_m\\) are the empirical distribution functions of \\(X_i\\)s and \\(Y_i\\)s respectively. That is, \\[ F_n(x) := \\frac{1}{n} \\sum^n_{i=1} I(X_i \\leq x)\\] and \\[ G_m(x) := \\frac{1}{m} \\sum^m_{i=1} I(Y_i \\leq x).\\] Example set.seed(362) x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) ks.test(x, y) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: x and y ## D = 0.07, p-value = 0.9671 ## alternative hypothesis: two-sided The \\(p\\)-value is not small. We do not have enough evidence to reject the null hypothesis that the two distributions are the same. Example z &lt;- rnorm(100, 2, 1) ks.test(y, z) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: y and z ## D = 0.77, p-value &lt; 2.2e-16 ## alternative hypothesis: two-sided The \\(p\\)-value is very small. We will reject the null hypothesis that the two distributions are the same. Example Recall the dataset birthwt from the package MASS. We created the following histograms to visualize the distributions of birth weights for the two groups (smoke and no smoke). We may want to ask if the two distributions are different ks.test(birthwt$bwt[birthwt$smoke == 0], birthwt$bwt[birthwt$smoke == 1]) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: birthwt$bwt[birthwt$smoke == 0] and birthwt$bwt[birthwt$smoke == 1] ## D = 0.21962, p-value = 0.02598 ## alternative hypothesis: two-sided The \\(p\\)-value is smaller than \\(0.05\\). Therefore, we conclude that the difference of the two distributions is statistically significant. "],["root-finding-and-optimization.html", "Chapter 10 Root finding and optimization 10.1 Root Finding 10.2 Newton-Raphson Method 10.3 Minimization and Maximization 10.4 optim", " Chapter 10 Root finding and optimization Reference: Computational Methods for Numerical Analysis with R by James P. Howard, II Packages used: library(Deriv) # for symbolic differentiation library(tidyverse) In this chapter, some basic algorithms for root finding and optimization are introduced. A root of a function means the \\(x\\) such that \\(f(x) = 0\\). We will also discuss how to implement them in R to practise your code writing ability. 10.1 Root Finding 10.1.1 Bisection Method Game: Before we introduce the bisection method, consider a game where Player \\(A\\) picks an integer from \\(1\\) to \\(100\\), and Player \\(B\\) has to guess this number. Player \\(A\\) will tell Player \\(B\\) if the guess is too low or too high. For example, Player \\(A\\) picks \\(10\\). Player \\(B\\) guesses \\(40\\). Player \\(A\\) tells Player \\(B\\) the number is too high. Player \\(B\\) will then pick a number from \\(1\\) to \\(40\\), say, \\(28\\). Play \\(A\\) tells Player \\(B\\) the number is also too high. Eventually, Player \\(B\\) can guess the number correctly. You may realize that the best approach is to halve the range with each turn. Idea of Bisection Method: Recall that for any continuous function \\(f\\), if it has values of opposite sign in an interval, then it has a root in the interval. For example, if \\(f(a) &lt; 0\\) and \\(f(b) &gt; 0\\), then there exists \\(c \\in (a, b)\\) such that \\(f(c) = 0\\). Now, suppose that \\(f(a) f(b) &lt; 0\\) (so that \\(f\\) has values of opposite sign at \\(a\\) and \\(b\\)). Let \\(\\varepsilon\\) denote a tolerance level. Algorithm: Let \\(c = \\frac{a + b}{2}\\). If \\(f(c) = 0\\), stop and return \\(c\\). If \\(\\text{sign}(f(a)) \\neq \\text{sign}(f(c))\\); set \\(b = c\\); else set \\(a = c\\). Repeat Step 1 to 3 until \\(|b-a| &lt; \\varepsilon\\). Implementation in R: This is a good example to see when a while loop is useful. bisection &lt;- function(f, a, b, tol = 1e-5, max_iter = 100) { iter &lt;- 0 # Step 4 while (abs(b - a) &gt; tol) { # Step 1 c &lt;- (a + b) / 2 # Step 2: if f(c) = 0, stop and return c if (f(c) == 0) { return(c) } iter &lt;- iter + 1 if (iter &gt; max_iter) { warning(&quot;Maximum number of iterations reached&quot;) return(c) } # Step 3 if (f(a) * f(c) &lt; 0) { b &lt;- c } else { a &lt;- c } # print(round(c(c, f(c)), digits = 3)) } return((a + b) / 2) } Remark: 1e-3 = 0.001. We set the default value of \\(\\varepsilon\\) to \\(0.001\\) and the maximum number of iterations to \\(100\\). Example f &lt;- function(x) { x^2 - 1 } Without providing the values of tol and max_iter, the function will use tol = 1e-3 and max_iter = 100. bisection(f, 0.5, 1.25) ## [1] 1.000001 To change tol: bisection(f, 0.5, 1.25, tol = 0.1) ## [1] 1.015625 bisection(f, 0.5, 1.25, tol = 0.0000001) ## [1] 1 To change max_iter: bisection(f, 0.5, 1.25, tol = 0.000000000001, max_iter = 10) ## Warning in bisection(f, 0.5, 1.25, tol = 1e-12, max_iter = 10): Maximum number of ## iterations reached ## [1] 0.9998779 10.2 Newton-Raphson Method Newton-Raphson method can be used to find a root of a function. Idea of Netwon-Raphson Method (or Netwons Method): Given an initial estimate of the root \\(x_0\\), approximate your function by its tangent line at \\(x_0\\) and find the root (you can find the root of a line easily). To find the root, we equate the slope at \\(x_0\\) found by \\(f&#39;(x_0)\\) and using the two points \\((x_1, 0)\\) and \\((x_0, f(x_0))\\), where \\(x_1\\) denotes the root of the tangent line at \\(x_0\\). That is, \\[\\begin{equation*} \\frac{f(x_0) - 0}{x_0 - x_1} = f&#39;(x_0). \\end{equation*}\\] Rearranging the terms give \\[\\begin{equation*} x_1 = x_0 - \\frac{f(x_0)}{f&#39;(x_0)}. \\end{equation*}\\] Iterate the above procedure until convergence. Here is an gif animination from wikipedia: https://en.wikipedia.org/wiki/Newton%27s_method#/media/File:NewtonIteration_Ani.gif Algorithm of Netwon-Raphson Method: Given an initial estimate of the root \\(x_0\\), set \\[\\begin{equation*} x_1 = x_0 - \\frac{f(x_0)}{f&#39;(x_0)}. \\end{equation*}\\] Iterate the following equation until convergence \\[\\begin{equation*} x_n = x_{n-1} - \\frac{f(x_{n-1})}{f&#39;(x_{n-1})}. \\end{equation*}\\] Implementation in R: f is the function you want to find the root fp is the first derivative of f x0 is the initial value. Since we cannot iterate indefinitely, we have to set a tolerance (tol) and a maximum number of iteration (max_iter). NR &lt;- function(f, fp, x0, tol = 1e-3, max_iter = 100) { iter &lt;- 0 old_x &lt;- x0 x &lt;- old_x + 10 * tol # any number such that abs(x - old_x) &gt; tol while(abs(x - old_x) &gt; tol) { iter &lt;- iter + 1 if (iter &gt; max_iter) { print(&quot;Maximum number of iterations reached&quot;) return(x) } old_x &lt;- x x &lt;- x - f(x) / fp(x) } return(x) } Symbolic differentiation To perform symbolic differentiation in R (instead of numerical differentiation), we can use Deriv() in the package Deriv. f &lt;- function(x) { x^2 - 2 * x + 1 } # Symbolic differentiation fp &lt;- Deriv(f) fp # we know it is 2x - 2 ## function (x) ## 2 * x - 2 Apply our NR function Lets plot the function first. Using our function: NR(f, fp, 1.25, tol = 1e-3) ## [1] 1.000508 10.3 Minimization and Maximization 10.3.1 Newton-Raphson Method Recall that at local extrema, \\(f&#39;(x) = 0\\), therefore we can use the Netwon-Raphson method for minimization and maximization. Algorithm of Netwon-Raphson Method: Given an initial estimate of the root \\(x_0\\), set \\[\\begin{equation*} x_1 = x_0 - \\frac{f&#39;(x_0)}{f&#39;&#39;(x_0)}. \\end{equation*}\\] Iterate the following equation until convergence \\[\\begin{equation*} x_n = x_{n-1} - \\frac{f&#39;(x_{n-1})}{f&#39;&#39;(x_{n-1})}. \\end{equation*}\\] Multivariate Version Iterate the following equation until convergence: \\[\\begin{equation*} x_n = x_{n-1} - [f&#39;&#39;(x_{n-1})]^{-1} f&#39;(x_{n-1}), \\end{equation*}\\] where \\(f&#39;(x)\\) is the gradient and \\(f&#39;&#39;(x)\\) is the Hessian matrix. Example (Logistic Regression) Recall that \\(x_i = (x_{i1},\\ldots,x_{ip})^T\\) and \\(y = (y_1,\\ldots,y_n)^T\\). Log-likelihood of the logistic regresison model: \\[\\begin{equation*} l(\\beta) = \\sum^n_{i=1} \\{(x^T_i \\beta)y_i - \\log (1 + e^{x^T_i \\beta}) \\}. \\end{equation*}\\] To apply the Netwons method, we have to find the gradient and the Hessian matrix. The gradient is \\[\\begin{align*} \\frac{\\partial l(\\beta)}{\\partial \\beta} &amp;= \\sum^n_{i=1} \\bigg( x_i y_i - \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}} x_i \\bigg) \\\\ &amp;= X^T y - X^T p_\\beta \\\\ &amp;= X^T(y - p_\\beta), \\end{align*}\\] where \\(p_\\beta = (p(x_1;\\beta),\\ldots,p(x_n;\\beta))^T\\) and \\(p(x_i;\\beta) = P(Y=1|\\beta, x_i) = \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}}\\). To find the Hessian matrix, we first find \\(\\frac{\\partial l(\\beta)}{\\partial \\beta_j \\partial \\beta_k}\\): \\[\\begin{align*} \\frac{\\partial l(\\beta)}{\\partial \\beta_j \\partial \\beta_k} &amp;= - \\sum^n_{i=1} x_{ij} \\frac{ (1+e^{x^T_i \\beta}) e^{x_i^T \\beta} x_{ik} - (e^{x^T_i \\beta})^2 x_{ik}}{(1+e^{x^T_i \\beta})^2 } \\\\ &amp;= - \\sum^n_{i=1} x_{ij} x_{ik} \\frac{ e^{x^T_i \\beta} }{(1+e^{x^T_i \\beta})^2} \\\\ &amp;= - \\sum^n_{i=1} x_{ij} x_{ik} p(x_i; \\beta) (1- p(x_i;\\beta)). \\end{align*}\\] Thus, \\[\\begin{align*} \\frac{\\partial l(\\beta)}{\\partial \\beta \\partial \\beta^T} &amp;= -\\sum^n_{i=1} x_i x_i^T p(x_i;\\beta)(1-p(x_i;\\beta)) \\\\ &amp; = - X^T W_\\beta X, \\end{align*}\\] where \\(W\\) is a \\(n \\times n\\) diagonal matrix with elements \\(p(x_i;\\beta)(1-p(x_i;\\beta))\\). Update using Newtons method: \\[\\begin{equation*} \\hat{\\beta}_{\\text{new}} = \\hat{\\beta}_{\\text{old}} - (-X^T W_{\\hat{\\beta}_{\\text{old}}} X)^{-1} X^T(y- p_{\\hat{\\beta}_{\\text{old}}} ). \\end{equation*}\\] set.seed(362) n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Design matrix X &lt;- cbind(1, x1, x2) beta &lt;- runif(3, 0, 1) for (i in 1:7) { num &lt;- as.vector(exp(X %*% beta)) p_beta &lt;- num / (1 + num) grad &lt;- t(X) %*% (y - p_beta) W &lt;- diag(p_beta * (1 - p_beta)) Hess &lt;- - t(X) %*% W %*% X beta &lt;- beta - solve(Hess) %*% grad print(c(paste0(&quot;Iteration: &quot;, i), round(as.vector(beta), 4))) } ## [1] &quot;Iteration: 1&quot; &quot;0.8283&quot; &quot;0.9995&quot; &quot;-1.7345&quot; ## [1] &quot;Iteration: 2&quot; &quot;0.7835&quot; &quot;1.0039&quot; &quot;-1.4903&quot; ## [1] &quot;Iteration: 3&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; ## [1] &quot;Iteration: 4&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; ## [1] &quot;Iteration: 5&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; ## [1] &quot;Iteration: 6&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; ## [1] &quot;Iteration: 7&quot; &quot;0.7831&quot; &quot;1.0067&quot; &quot;-1.492&quot; # compared with the glm function glm(y ~ x1 + x2, family = &quot;binomial&quot;)$coef ## (Intercept) x1 x2 ## 0.7830729 1.0067405 -1.4920333 10.3.2 Gradient Descent Gradient Descent Iterative method for finding a local minimum Requires an initial value and a step size \\(h\\) Idea of Gradient Descent: The gradient descent method uses the derivative at \\(x\\) and takes a step down, of size \\(h\\), in the direction of the slope. The process is repeated using the new point as \\(x\\). As the function slides down a slope, the derivative will start shrinking resulting in smaller changes in \\(x\\). As the change in \\(x\\) decreases below the tolerance value, we have reached a local minimum. Algorithm of Gradient Descent: Set an initial point \\(x_0\\) and a step size \\(h &gt; 0\\) Iterate until convergence: \\[\\begin{equation*} x_{n+1} = x_{n} - h f&#39;(x_{n}). \\end{equation*}\\] Implementation in R: grad_des &lt;- function(fp, x0, h = 1e-3, tol = 1e-4, max_iter = 1000) { iter &lt;- 0 old_x &lt;- x0 x &lt;- x0 + 2 * tol while(abs(x - old_x) &gt; tol) { iter &lt;- iter + 1 if (iter &gt; max_iter) { stop(&quot;Maximum number of iterations reached&quot;) } old_x &lt;- x x &lt;- x - h * fp(x) } return(x) } Example: Find the local minima of \\(f(x) = \\frac{1}{4} x^4 + x^3 - x - 1\\). Plot: Gradient descent: f &lt;- function(x) {1/4 * x^4 + x^3 - x - 1} fp &lt;- Deriv(f) grad_des(fp, x0 = -2, h = 0.01) ## [1] -2.878224 grad_des(fp, x0 = 2, h = 0.01) ## [1] 0.5344022 Displaying error message: grad_des(fp, x0 = -2, h = 0.01, max_iter = 2) ## Error in grad_des(fp, x0 = -2, h = 0.01, max_iter = 2): Maximum number of iterations reached Remark: to find a local maximum, use gradient ascent. Algorithm: Set an initial point \\(x_0\\) and a step size \\(h &gt; 0\\) Iterate until convergence: \\[\\begin{equation*} x_{n+1} = x_{n} + h f&#39;(x_{n}). \\end{equation*}\\] Implementation: grad_asc &lt;- function(fp, x0, h = 1e-3, tol = 1e-4, max_iter = 1000) { grad_des(fp, x0, -h, tol, max_iter) } Example: grad_asc(fp, x0 = 0, h = 0.01) ## [1] -0.6490877 10.4 optim Going back to optim(), the par plays the same role as the x_0 in the Netwon-Raphson method and the gradient descent method. Since we have some tolerance level and the iteration will stop when the tolerance level is reached, if you start at different initial values, you may get slightly different results I used some random numbers for the initial values in optim() because we do not know a good estimate of the minimizer. In general, you will perform the optimization with several set of different initial values and see which one results in a smaller function value. Sometimes, the algorithm will get stuck in a local minimum and using several set of random initial values can alleviate this problem. "],["k-nearest-neighbors.html", "Chapter 11 k-nearest neighbors 11.1 Introduction 11.2 Feature Scaling 11.3 Example: Classifying Breast Cancers", " Chapter 11 k-nearest neighbors Optional Reading: Chapter 3 in Machine Learning with R by Brett Lantz 11.1 Introduction Setting: We have data \\(\\{(x_i, y_i) : i=1,\\ldots,n \\}\\), where \\(x_i\\) and \\(y_i\\) are the vector of the features the class label for the \\(i\\)th observation respectively. For example, in breast cancer diagnosis, \\(x_i\\) could be some summary statistics of the radius, texture, perimeter, area of the cell nuclei from a digitized image of a fine needle aspirate of a breast mass; \\(y_i\\) is the diagnosis (malignant or benign). We now have a new data point \\(x^*\\) (of course we do not have the class label \\(y^*\\)) and we want to assign a class label to it. For example, after a subject has a breast fine needle aspiration, can we use existing data to predict if the subject has breast cancer? In this chapter, we will study the \\(k\\)-nearest neighbors (\\(k\\)-NN) algorithm for classification. \\(k\\)-NN algorithm uses information about an examples \\(k\\) nearest neighbors to classify unlabeled examples. To measure how close the examples are, we need a distance measure. Traditionally, the \\(k\\)-NN algorithm uses Euclidean distance. Given two points \\(u = (u_1,\\ldots,u_p)\\) and \\(w = (w_1,\\ldots,w_p)\\), the Euclidean distance between them is \\[\\begin{equation*} d(u, w) = \\sqrt{ \\sum^p_{j=1}(u_j - w_j)^2}. \\end{equation*}\\] Algorithm: Compute the Euclidean distance \\(d(x_i, x^*)\\) for \\(i=1,\\ldots,n\\) Find the \\(k\\) training data with the smallest \\(d(x_i, x^*)\\) The predicted class for \\(x^*\\) is determined by the majority vote of the \\(k\\) training data in Step 2. The idea before the algorithm is simple. We expect that observations with similar features should have the same class label. In the following figure, we have some labeled data. Suppose the black dot is our new data without label, which group will you assign this data to? A natural choice is to assign the black dot to group A because the \\(5\\) nearest neighbors are all in group A. This is the idea of \\(k\\)-nearest neighbors algorithm. Remark: We can use the \\(k\\)-NN algorithm for regression. In that case, \\(y_i\\) is the numeric response. The corresponding algorithm replaces the majority vote by the mean of the responses in the \\(k\\) nearest training data. Other distance measures could be used. There are different methods to break ties. There is no learning phase. Applications: Computer vision applications, including optical character recognition and facial recognition in both still images and video Recommendation systems that predict whether a person will enjoy a movie or song Identifying patterns in genetic data to detect specific proteins or diseases Strengths of \\(k\\)-NN: Simple and effective Makes no assumptions about the underlying data distribution Weaknesses of \\(k\\)-NN: Does not produce a model, limiting the ability to understand how the features are related to the class Requires selection of an appropriate \\(k\\) Slow classification phase Nominal features and missing data require additional processing Choosing an appropriate \\(k\\): Large \\(k\\): reduce the impact or variance caused by noisy data, may underfit the traning data Small \\(k\\): may overfit the data Extreme case: \\(k = n\\), the most common class will always be the prediction Some people suggest using \\(\\sqrt{n}\\) as \\(k\\). One may also use a validation set or cross-validation to choose the appropriate \\(k\\) (will discuss these later). 11.2 Feature Scaling Clearly, if the features are in different scales, the distance measure computed will be dominated by some of the features and will not take into account of the importance of other features. Lets assume we have \\(n\\) data and \\(p\\) features. Two common methods for feature scaling: min-max normalization For each \\(i=1,\\ldots,n\\), \\(j=1,\\ldots,p\\), \\[\\begin{equation*} x^*_{ij} = \\frac{x_{ij} - \\min_{i=1,\\ldots,n} x_{ij}}{\\max_{i=1,\\ldots,n} x_{ij} -\\min_{i=1,\\ldots,n} x_{ij}}. \\end{equation*}\\] The normalized features will have values between \\(0\\) and \\(1\\). \\(z\\)-score standardization For each \\(i=1,\\ldots,n\\), \\(j=1,\\ldots,p\\), \\[\\begin{equation*} x^*_{ij} = \\frac{x_{ij} - \\overline{x}_j}{s_j}, \\end{equation*}\\] where \\(\\overline{x}_j\\) and \\(s_j\\) are the sample mean and standard deviation of \\(\\{x_{1j},\\ldots,x_{nj}\\}\\). For nominal features, we can convert them into a numeric feature using dummy coding (also known as one-hot encoding). For example, if a nominal feature called temperature takes three values: hot, medium and cold. You can define two additional binary indicator variables: \\[\\begin{equation*} \\text{hot} = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if temperature = hot} \\\\ 0 &amp; \\text{otherwise} \\end{array} \\right. \\end{equation*}\\] and \\[\\begin{equation*} \\text{medium} = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if temperature = medium} \\\\ 0 &amp; \\text{otherwise.} \\end{array} \\right. \\end{equation*}\\] When both hot and medium are \\(0\\), we know the temperature is cold. In general, if we have \\(n\\) categories, we only need to create \\(n-1\\) additional binary indicators. 11.3 Example: Classifying Breast Cancers We will use the breast cancecr from UC Irvine Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29 Download the dataset from onQ. Read the data (change the path to where you save your data): wbcd &lt;- read.csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/wisc_bc_data.csv&quot;) Variables: names(wbcd) ## [1] &quot;id&quot; &quot;diagnosis&quot; &quot;radius_mean&quot; ## [4] &quot;texture_mean&quot; &quot;perimeter_mean&quot; &quot;area_mean&quot; ## [7] &quot;smoothness_mean&quot; &quot;compactness_mean&quot; &quot;concavity_mean&quot; ## [10] &quot;concave.points_mean&quot; &quot;symmetry_mean&quot; &quot;fractal_dimension_mean&quot; ## [13] &quot;radius_se&quot; &quot;texture_se&quot; &quot;perimeter_se&quot; ## [16] &quot;area_se&quot; &quot;smoothness_se&quot; &quot;compactness_se&quot; ## [19] &quot;concavity_se&quot; &quot;concave.points_se&quot; &quot;symmetry_se&quot; ## [22] &quot;fractal_dimension_se&quot; &quot;radius_worst&quot; &quot;texture_worst&quot; ## [25] &quot;perimeter_worst&quot; &quot;area_worst&quot; &quot;smoothness_worst&quot; ## [28] &quot;compactness_worst&quot; &quot;concavity_worst&quot; &quot;concave.points_worst&quot; ## [31] &quot;symmetry_worst&quot; &quot;fractal_dimension_worst&quot; Creating Training and Testing Datasets The first column is id, which should not be included in the classification. The second column is diagnosis. B means benign and M means malignant. In short, the meaning of malignant is cancerous and the meaning of benign is non-cancerous. We will separate the labels from the features. To evaluate the model performance, we always split our full dataset into a training dataset and a testing daatset. set.seed(6) # reproduce the result # create the random numbers for selecting the rows in the dataset random_index &lt;- sample(nrow(wbcd), 469) # our &quot;x&quot; wbcd_train &lt;- wbcd[random_index, -(1:2)] wbcd_test &lt;- wbcd[-random_index, -(1:2)] # our &quot;y&quot; wbcd_train_labels &lt;- wbcd[random_index, ]$diagnosis wbcd_test_labels &lt;- wbcd[-random_index, ]$diagnosis Note: In forming a training dataset and a testing dataset, you should not select the first \\(469\\) rows (unless you know the data have been randomly organized). Normalizing the data We have to normalize the features in both the training datasets and testing datasets We have to use the same normalizing methods for these two datasets Compute the min and max (or mean and sd) using only the training datasets wbcd_train_n &lt;- wbcd_train wbcd_test_n &lt;- wbcd_test train_min &lt;- apply(wbcd_train, 2, min) train_max &lt;- apply(wbcd_train, 2, max) for (i in 1:ncol(wbcd_train)) { wbcd_train_n[, i] &lt;- (wbcd_train[, i] - train_min[i]) / (train_max[i] - train_min[i]) # use the min and max from training data to normalize the testing data wbcd_test_n[, i] &lt;- (wbcd_test[, i] - train_min[i]) / (train_max[i] - train_min[i]) } We will use knn() in the package class to perform \\(k\\)-NN classification. knn() will return a factor of classifications of testing dataset. library(class) # install it if you haven&#39;t done so knn_predicted &lt;- knn(train = wbcd_train_n, test = wbcd_test_n, cl = wbcd_train_labels, k = 21) train : training dataset test: testing dataset cl: training labels k: \\(k\\)-nearest neighbors will be used 11.3.1 Evaluating Model Performance table(wbcd_test_labels, knn_predicted) ## knn_predicted ## wbcd_test_labels B M ## B 57 2 ## M 4 37 False positive: an error where the test result incorrectly indicates the presence of a condition such as a disease when the disease is not present. In this example, we have \\(2\\) false positives. False Negative: an error where the test result incorrectly fails to indicate the presence of a condition when it is present. In this example, we have \\(4\\) false negatives. Here the test result means our prediction. The accuracy is \\[\\begin{equation*} \\text{accuracy} = \\frac{57+37}{57+2+4+37} = 0.94. \\end{equation*}\\] The error rate is \\[\\begin{equation*} \\text{error rate} = \\frac{2 + 4}{57+2+4+37} = 0.06 = 1- \\text{accuracy}. \\end{equation*}\\] 11.3.2 Using \\(z\\)-score standardization Lets try the \\(z\\)-score standardization. wbcd_train_s &lt;- wbcd_train wbcd_test_s &lt;- wbcd_test train_mean &lt;- apply(wbcd_train, 2, mean) train_sd &lt;- apply(wbcd_train, 2, sd) for (i in 1:ncol(wbcd_train)) { wbcd_train_s[, i] &lt;- (wbcd_train[, i] - train_mean[i]) / train_sd[i] # use the mean and sd from training data to normalize the testing data wbcd_test_s[, i] &lt;- (wbcd_test[, i] - train_mean[i]) / train_sd[i] } Perform \\(k\\)-NN: knn_predicted &lt;- knn(train = wbcd_train_s, test = wbcd_test_s, cl = wbcd_train_labels, k = 21) Evaluate the performance: table(wbcd_test_labels, knn_predicted) ## knn_predicted ## wbcd_test_labels B M ## B 58 1 ## M 5 36 The performance using \\(z\\)-score standardization and min-max normalization is similar. 11.3.3 Testing alternative values of \\(k\\) k &lt;- c(1, 5, 11, 15, 21, 27) result &lt;- matrix(0, nrow = length(k), ncol = 4) result[, 1] &lt;- k colnames(result) = c(&quot;k value&quot;, &quot;False Negatives&quot;, &quot;False Positives&quot;, &quot;Percent Classified Correctly&quot;) for (i in 1:length(k)) { knn_predicted &lt;- knn(train = wbcd_train_n, test = wbcd_test_n, cl = wbcd_train_labels, k = k[i]) confusion_matrix &lt;- table(wbcd_test_labels, knn_predicted) result[i, 2] &lt;- confusion_matrix[2, 1] result[i, 3] &lt;- confusion_matrix[1, 2] result[i, 4] &lt;- sum(diag(confusion_matrix)) / length(wbcd_test_labels) } result ## k value False Negatives False Positives Percent Classified Correctly ## [1,] 1 4 3 0.93 ## [2,] 5 4 1 0.95 ## [3,] 11 3 1 0.96 ## [4,] 15 3 2 0.95 ## [5,] 21 4 2 0.94 ## [6,] 27 3 2 0.95 Important remark: while the accuracy is the highest when \\(k = 11\\), it does not mean the model is the best for this data. Note that We split the dataset into a training dataset and a testing dataset using one particular random partition. With another random partition, the results would be different. The more appropriate way to assess the accuracy is to use repeated \\(k\\)-fold cross validation (the \\(k\\) here is different from the \\(k\\) in our \\(k\\)-NN algorithm). We shall discuss this later. Having false Negatives could be a more severe problem than having false positives in this example. Although in this example, when \\(k=11\\), it also produces the lowest number of false negatives. "],["linear-regression-models.html", "Chapter 12 Linear Regression Models 12.1 Simple Linear Regression 12.2 Smoothed Conditional Means 12.3 Multiple Linear Regression 12.4 Example: diamonds 12.5 Categorical Predictors 12.6 Compare models using ANOVA 12.7 Prediction 12.8 Interaction Terms 12.9 Variable Transformation 12.10 Polynomial Regression 12.11 Stepwise regression 12.12 Best subset", " Chapter 12 Linear Regression Models Reference: R cookbook https://rc2e.com/linearregressionandanova, any linear regression textbooks. Packages used in this chapter: library(tidyverse) # contains ggplot2 and dplyr library(nycflights13) # contains the dataset &quot;flights&quot; library(bestglm) # find the best subset model Regression analysis is the study of the relationship between the responses and the covariates. Linear regression is one of the most used statistical techniques when the response is continuous. It can be used for prediction and explain variation in the response variable. Two types of variables: response = dependent variable, usually denoted by \\(Y\\) explanatory variable = independent variable = covariate = predictor = feature, usually denoted by \\(X_1,\\ldots,X_p\\) 12.1 Simple Linear Regression Suppose we have data \\((x_1,y_1),\\ldots,(x_n,y_n)\\). A simple linear regression specifies that \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\end{equation*}\\] where \\(E(\\varepsilon_i) = 0\\), \\(Var(\\varepsilon_i) = \\sigma^2 &gt; 0\\), and \\(\\varepsilon_i\\)s are i.i.d. Taking expectation on both sides of the above equation, we see that \\[\\begin{equation*} E(Y|X=x) = \\beta_0 + \\beta_1 x. \\end{equation*}\\] Hence, the conditional mean of \\(Y\\) given \\(x\\) is linear in \\(x\\). In the model: \\(\\beta_0\\) is the intercept (mean of \\(Y\\) when \\(x=0\\)) \\(\\beta_1\\) is the slope (which is the change in the mean of \\(Y\\) for \\(1\\) unit increase in \\(x\\)) \\(\\varepsilon\\) is the error term (anything that is not explained by \\(x\\)). It is the vertical distance between \\(y\\) and the conditional mean \\(E(Y|X=x)\\). \\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters to be estimated from data How to estimate \\(\\beta_0\\) and \\(\\beta_1\\)? Recall that a line can be specified by the intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)). Therefore, intuitively, we want to find a line that best fits the points. How to define the best fit? Answer: Method of least square We can minimize the residual sum of squares, which is defined by \\[\\begin{equation*} \\sum^n_{i=1} [ y_i - (\\beta_0 + \\beta_1 x_i)]^2. \\end{equation*}\\] The least square estimator for \\((\\beta_0, \\beta_1)\\) is defined as the minimizer of the residual sum of squares. That is, \\[\\begin{equation*} (\\hat{\\beta}_0, \\hat{\\beta}_1) := \\text{argmin}_{\\beta_0, \\beta_1} \\sum^n_{i=1} [ y_i - (\\beta_0 + \\beta_1 x_i)]^2. \\end{equation*}\\] The minimizer can be found by differentiating the objection function with respect to \\(\\beta_0\\) and \\(\\beta_1\\) setting the resulting expressions to \\(0\\) solving the simultaneous equations The above steps lead to a closed-form formula for \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) in terms of \\(x\\)s and \\(y\\)s, which is a special case of the formula given in Section 11.3 Multiple Linear Regression. The residual is \\(\\hat{\\varepsilon}_i = y_i - \\hat{y}_i\\), where \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\). To perform linear regression in R, we use lm(). Examples in R Recall the dataset flights in the package nycflights13. # y = arr_delay, x = dep_delay lm(arr_delay ~ dep_delay, data = flights) ## ## Call: ## lm(formula = arr_delay ~ dep_delay, data = flights) ## ## Coefficients: ## (Intercept) dep_delay ## -5.899 1.019 The regression equation is \\[\\begin{equation*} \\text{arr delay} = -5.899 + 1.019 \\text{dep delay} + \\varepsilon. \\end{equation*}\\] From this equation, we can see that for \\(1\\) minute increase in departure delay, the arrival delay will increase by \\(1.019\\) minute on average. When there is no departure delay, the flights arrive earlier on average (a negative arrival delay means there was an early arrival). Perform a simple linear regression without a dataset # Example 1 lm(flights$arr_delay ~ flights$dep_delay) ## ## Call: ## lm(formula = flights$arr_delay ~ flights$dep_delay) ## ## Coefficients: ## (Intercept) flights$dep_delay ## -5.899 1.019 Another example # Example 2 set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 1 + x + rnorm(100, 0, 1) lm(y ~ x) ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## 0.9623 0.9989 To visualize the regression line using ggplot(), use geom_smooth() and set method = lm. ggplot(mapping = aes(x = x, y= y)) + geom_point() + geom_smooth(method = lm) To remove the confidence interval, set se = FALSE in geom_smooth(). ggplot(mapping = aes(x = x, y= y)) + geom_point() + geom_smooth(method = lm, se = FALSE) Remark: You have seen geom_smooth() in Assignment 3. 12.2 Smoothed Conditional Means In simple linear regression, a linear relationship between the response and predictor is assumed. In many cases, this assumption may not hold. In those situations, we may fit a curve to the data. For example, we can use geom_smooth() (without setting method = lm) to visualize the fitting of a function through the points of a scatterplot that best represents the relationship bewteen the response and the predictor without assuming the linear relationship. cars %&gt;% ggplot(mapping = aes(x = speed, y = dist)) + geom_point() + geom_smooth() You can see the message saying geom_smooth() is using method = \"loess\". loess stands for locally estimated scatterplot smoothing. You may also see lowess, which stands for locally weighted scatterplot smoothing. Basically, a locally weighted regression solves a separate weighted least squares problem at each target point \\(x_0\\): \\[\\begin{equation*} \\min_{\\alpha(x_0), \\beta(x_0)} \\sum^n_{i=1} K_\\lambda(x_0, x_i)[y_i - \\alpha(x_0) - \\beta(x_0) x_i]^2, \\end{equation*}\\] where \\(K_\\lambda(x_0, x_i) = K(|x_i-x_0|/\\lambda)\\) for some kernel function \\(K\\) and \\(\\lambda\\) is a positive number. The estimate is \\(\\hat{f}(x_0) = \\hat{\\alpha}(x_0) + \\hat{\\beta}(x_0)x_0\\). The idea is that data points that are close to \\(x_0\\) will have larger weights \\(K_\\lambda(x_0, x_i)\\) so that the points near \\(x_0\\) are more important in estimating \\(\\alpha(x_0)\\) and \\(\\beta(x_0)\\). This is the meaning of locally weighted. 12.3 Multiple Linear Regression We can add more predictors to explain the response variable better: \\[\\begin{equation*} y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_p x_{ip} + \\varepsilon_i. \\end{equation*}\\] Suppose we have \\(n\\) data, then we can use the matrix notation to represent our model: \\[\\begin{equation*} Y = X \\beta + \\varepsilon, \\end{equation*}\\] where \\[\\begin{equation*} y = \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right), \\quad X = \\left( \\begin{array}{cccc} 1 &amp; x_{11} &amp; \\ldots &amp; x_{1p}\\\\ 1 &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\ldots &amp; x_{np} \\end{array} \\right), \\quad \\text{and } \\varepsilon = \\left( \\begin{array}{c} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array} \\right). \\end{equation*}\\] As in simple linear regression, we estimate \\(\\beta\\) by minimizing the residual sum of squares: \\[\\begin{equation*} \\sum^n_{i=1}(y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2} - \\ldots - \\beta_p x_{ip})^2. \\end{equation*}\\] The least square estimator for \\(\\beta\\) is \\[\\begin{equation*} \\hat{\\beta} = (X^T X)^{-1}X^T Y. \\end{equation*}\\] Examples in R Without a dataframe: set.seed(1) n &lt;- 100 x1 &lt;- rnorm(n, 0, 1) x2 &lt;- rnorm(n, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + rnorm(n, 0, 1) lm(y ~ x1 + x2) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Coefficients: ## (Intercept) x1 x2 ## 1.025 2.021 2.947 With a dataframe: new_data &lt;- tibble(response = y, cov1 = x1, cov2 = x2) # use the column names of your dataframe lm(response ~ cov1 + cov2, data = new_data) ## ## Call: ## lm(formula = response ~ cov1 + cov2, data = new_data) ## ## Coefficients: ## (Intercept) cov1 cov2 ## 1.025 2.021 2.947 To obtain more information about the model: # assign the model object to a variable fit &lt;- lm(y ~ x1 + x2) # summary is one of the most important functions for linear regression summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.94359 -0.43645 0.00202 0.63692 2.63941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0254 0.1052 9.747 4.71e-16 *** ## x1 2.0211 0.1168 17.311 &lt; 2e-16 *** ## x2 2.9465 0.1095 26.914 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.043 on 97 degrees of freedom ## Multiple R-squared: 0.9134, Adjusted R-squared: 0.9116 ## F-statistic: 511.6 on 2 and 97 DF, p-value: &lt; 2.2e-16 From the summary, you can find Estimates of the regression coefficients. The \\(p\\)-value indicates if the regression coefficient is significantly different from \\(0\\). If the \\(p\\)-value is smaller than \\(0.05\\), then we reject the null hypothesis that the regression coefficient is equal to \\(0\\) at \\(0.05\\) significance level. \\(R^2\\) is a measure of the variance of \\(y\\) that is explained by the model. The higher the \\(R^2\\) is, the better is your model. However, adding additional variables will always increase \\(R^2\\) while this may not improve the model in the sense that it may not improve your prediction for new data. The adjusted \\(R^2\\) accounts for the number of variables in the model and is a better measure of the models quality. \\(F\\)-statistic tells you whether your model is statistically significant or not. The null hypothesis is that all coefficients are zero and the alternative hypothesis is that not all coefficients are zero. Objects in fit names(fit) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; Extract the coefficients fit$coefficients # or coef(fit) ## (Intercept) x1 x2 ## 1.025353 2.021110 2.946533 Extract the residuals fit$residuals # or resid(fit) Confidence intervals for regression coefficients confint(fit) ## 2.5 % 97.5 % ## (Intercept) 0.8165723 1.234135 ## x1 1.7893884 2.252832 ## x2 2.7292486 3.163818 12.4 Example: diamonds Consider the diamonds dataset in ggplot2. Lets try to predict the price of an diamond based on its characteristics. In diamonds, cut, color and clarity are categorical features. To use them in regression, a standard way is to use the dummy coding discussed in the kNN chapter. The lm() function can handle this automatically as long as the variable is of a factor type. # When the variable is an ordered factor, the names in the output of # lm() are weird. So I turn them into an unordered factor first. diamonds2 &lt;- diamonds diamonds2$cut &lt;- factor(diamonds2$cut, order = FALSE) diamonds2$clarity &lt;- factor(diamonds2$clarity, order = FALSE) diamonds2$color &lt;- factor(diamonds2$color, order = FALSE) Lets try price ~ cut. The variable cut takes \\(5\\) values: Fair, Good, Very Good, Premium and Ideal. Therefore, we expect to have 5 regresion coefficients (1 for intercept, 4 for cut). fit &lt;- lm(price ~ cut, data = diamonds2) summary(fit) ## ## Call: ## lm(formula = price ~ cut, data = diamonds2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4258 -2741 -1494 1360 15348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4358.76 98.79 44.122 &lt; 2e-16 *** ## cutGood -429.89 113.85 -3.776 0.000160 *** ## cutVery Good -377.00 105.16 -3.585 0.000338 *** ## cutPremium 225.50 104.40 2.160 0.030772 * ## cutIdeal -901.22 102.41 -8.800 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3964 on 53935 degrees of freedom ## Multiple R-squared: 0.01286, Adjusted R-squared: 0.01279 ## F-statistic: 175.7 on 4 and 53935 DF, p-value: &lt; 2.2e-16 From the output, we see that we have \\(4\\) additional predictors named cutGood, cutVery Good, cutPremium and cutIdeal corresponding to the values cut can take. We do not see cutFair because the value fair has been used as the baseline for the regression model. That means if cut is fair, then the mean price is just the intercept value. If cut is Good, then the mean price is \\(4358.76-429.89.\\) The results for the regression coefficients do not make sense because they imply that diamonds with ideal cut are the cheapest on average. The problem is that we have not controlled for other variables. The meaning of control for other variables is that we should also put in other relevant variables so that the interpretation of the regression coefficients will make sense. For example, if we include carat, then the interpretation of the regression coefficient for cutGood is that fixing the weight of the diamond, diamonds with good cut is how much more expensive than diamonds with fair cut on average (because the baseline value is fair). Now, lets try to add carat. fit &lt;- lm(price ~ cut + carat, data = diamonds2) summary(fit) ## ## Call: ## lm(formula = price ~ cut + carat, data = diamonds2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17540.7 -791.6 -37.6 522.1 12721.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3875.47 40.41 -95.91 &lt;2e-16 *** ## cutGood 1120.33 43.50 25.75 &lt;2e-16 *** ## cutVery Good 1510.14 40.24 37.53 &lt;2e-16 *** ## cutPremium 1439.08 39.87 36.10 &lt;2e-16 *** ## cutIdeal 1800.92 39.34 45.77 &lt;2e-16 *** ## carat 7871.08 13.98 563.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1511 on 53934 degrees of freedom ## Multiple R-squared: 0.8565, Adjusted R-squared: 0.8565 ## F-statistic: 6.437e+04 on 5 and 53934 DF, p-value: &lt; 2.2e-16 It makes more sense now although the coefficient of cutVery good is still higher than that of cutPremium. We now add clarity and color. fit &lt;- lm(price ~ cut + carat + clarity + color, data = diamonds2) summary(fit) ## ## Call: ## lm(formula = price ~ cut + carat + clarity + color, data = diamonds2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16813.5 -680.4 -197.6 466.4 10394.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7362.80 51.68 -142.46 &lt;2e-16 *** ## cutGood 655.77 33.63 19.50 &lt;2e-16 *** ## cutVery Good 848.72 31.28 27.14 &lt;2e-16 *** ## cutPremium 869.40 30.93 28.11 &lt;2e-16 *** ## cutIdeal 998.25 30.66 32.56 &lt;2e-16 *** ## carat 8886.13 12.03 738.44 &lt;2e-16 *** ## claritySI2 2625.95 44.79 58.63 &lt;2e-16 *** ## claritySI1 3573.69 44.60 80.13 &lt;2e-16 *** ## clarityVS2 4217.83 44.84 94.06 &lt;2e-16 *** ## clarityVS1 4534.88 45.54 99.59 &lt;2e-16 *** ## clarityVVS2 4967.20 46.89 105.93 &lt;2e-16 *** ## clarityVVS1 5072.03 48.21 105.20 &lt;2e-16 *** ## clarityIF 5419.65 52.14 103.95 &lt;2e-16 *** ## colorE -211.68 18.32 -11.56 &lt;2e-16 *** ## colorF -303.31 18.51 -16.39 &lt;2e-16 *** ## colorG -506.20 18.12 -27.93 &lt;2e-16 *** ## colorH -978.70 19.27 -50.78 &lt;2e-16 *** ## colorI -1440.30 21.65 -66.54 &lt;2e-16 *** ## colorJ -2325.22 26.72 -87.01 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1157 on 53921 degrees of freedom ## Multiple R-squared: 0.9159, Adjusted R-squared: 0.9159 ## F-statistic: 3.264e+04 on 18 and 53921 DF, p-value: &lt; 2.2e-16 Now, in terms of the relative magnitude, all the regression coefficients make sense. Note that the baseline value of color is D (which is the best) so that any other colors should have negative coefficients. For cut and clarity, the baseline values are the worst so that the other variables should have positive coefficients. 12.5 Categorical Predictors See the diamonds example. 12.6 Compare models using ANOVA If you want to compare if the difference between two models are statistically significant or not, you can use anova(). When you compare two models using anova(), one model must be contained within the other. set.seed(100) n &lt;- 100 x1 &lt;- rnorm(n, 0, 1) x2 &lt;- rnorm(n, 0, 1) x3 &lt;- rnorm(n, 0, 1) x4 &lt;- rnorm(n, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + rnorm(n, 0, 1) m1 &lt;- lm(y ~ x1) m2 &lt;- lm(y ~ x1 + x2) m3 &lt;- lm(y ~ x1 + x2 + x3) m4 &lt;- lm(y ~ x1 + x2 + x3 + x4) m1 is contained within m2, m2 is contained within m3, and m3 is contained within m4. Compare m1 and m2: anova(m1, m2) ## Analysis of Variance Table ## ## Model 1: y ~ x1 ## Model 2: y ~ x1 + x2 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 98 618.26 ## 2 97 112.56 1 505.7 435.79 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The difference is statistically significant, implying x2 should be necessary. Compare m2 and m4: anova(m2, m4) ## Analysis of Variance Table ## ## Model 1: y ~ x1 + x2 ## Model 2: y ~ x1 + x2 + x3 + x4 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 112.56 ## 2 95 110.16 2 2.4024 1.0359 0.3589 The difference is not statistically significant, implying x3 and x4 may not be necessary. Compare m3 and m4: anova(m3, m4) ## Analysis of Variance Table ## ## Model 1: y ~ x1 + x2 + x3 ## Model 2: y ~ x1 + x2 + x3 + x4 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 96 110.78 ## 2 95 110.16 1 0.61992 0.5346 0.4665 The difference is not statistically significant, implying x4 may not be necessary. 12.7 Prediction Lets use the flights dataset again. set.seed(1) random_index &lt;- sample(nrow(flights), size = nrow(flights) * 0.7) flights_train &lt;- flights[random_index, ] flights_test &lt;- flights[-random_index, ] fit &lt;- lm(arr_delay ~ dep_delay, data = flights_train) Prediction # prediction predict(fit, flights_test) Since the response is continuous, we cannot use accuracy to measure the performance of our prediction. One possible measure is to use correlation. The higher the correlation, the better the prediction. cor(predict(fit, flights_test), flights_test$arr_delay, use = &quot;complete.obs&quot;) ## [1] 0.9171425 Since there are missing values, we set use = \"complete.obs\". 12.8 Interaction Terms When the effect on the response of one predictor variable depends the levels or values of the other predictor variables, we can include interaction terms. For example, if we have two predictors \\(X_1\\) and \\(X_2\\), we can include an additional interaction term \\(X_1 X_2\\): \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\varepsilon_i. \\end{equation*}\\] To perform the following regression \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\varepsilon_i, \\end{equation*}\\] use lm(y ~ x1 * x2). Example: # simulation set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + 4 * x1 * x2 + rnorm(100, 0, 1) # estimation lm(y ~ x1 * x2) ## ## Call: ## lm(formula = y ~ x1 * x2) ## ## Coefficients: ## (Intercept) x1 x2 x1:x2 ## 1.031 1.966 2.972 3.760 To perform the following regression \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\beta_4 X_{i1} X_{i2} + \\beta_5 X_{i1}X_{i3} + \\beta_6 X_{i2} X_{i3} + \\beta_7 X_{i1} X_{i2} X_{i3} + \\varepsilon_i, \\end{equation*}\\] use lm(y ~ x1 * x2 * x3). Example: # simulation set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) x3 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + 4 * x1 * x2 + 2 * x1 * x2 * x3 + rnorm(100, 0, 1) # estimation fit &lt;- lm(y ~ x1 * x2 * x3) summary(fit) ## ## Call: ## lm(formula = y ~ x1 * x2 * x3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.31655 -0.51032 -0.00288 0.76844 2.01342 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.050754 0.098109 10.710 &lt;2e-16 *** ## x1 1.862611 0.113359 16.431 &lt;2e-16 *** ## x2 2.942130 0.104931 28.039 &lt;2e-16 *** ## x3 0.034585 0.096500 0.358 0.7209 ## x1:x2 3.636972 0.143910 25.272 &lt;2e-16 *** ## x1:x3 -0.246646 0.118356 -2.084 0.0399 * ## x2:x3 0.002958 0.113553 0.026 0.9793 ## x1:x2:x3 1.873330 0.140592 13.325 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9552 on 92 degrees of freedom ## Multiple R-squared: 0.9625, Adjusted R-squared: 0.9597 ## F-statistic: 337.6 on 7 and 92 DF, p-value: &lt; 2.2e-16 We notice that the coefficients of x3 and x2:x3 are not significantly different from \\(0\\). This makes sense because our simulation model does not include these two predictors. However, we see that x1:x3 is significantly different from \\(0\\) (at the \\(0.05\\) level) even though we did not include this predictor in the simulation model. This is because we will commit Type I error. We can also include a specific interaction by using : instead of *. For example, to fit \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\beta_4 X_{i1} X_{i2} X_{i3} + \\varepsilon_i, \\end{equation*}\\] we can use lm(y ~ x1 + x2 + x1:x2 + x1:x2:x3). Example: set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) x3 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + 4 * x1 * x2 + 2 * x1 * x2 * x3 + rnorm(100, 0, 1) lm(y ~ x1 + x2 + x1:x2 + x1:x2:x3) ## ## Call: ## lm(formula = y ~ x1 + x2 + x1:x2 + x1:x2:x3) ## ## Coefficients: ## (Intercept) x1 x2 x1:x2 x1:x2:x3 ## 1.061 1.853 2.981 3.598 1.967 12.9 Variable Transformation We can transform our response or predictors. For example, the model \\[\\begin{equation*} \\log Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\varepsilon_i \\end{equation*}\\] is a special case of the genearl linear regression model because we can define \\(Y_i&#39;\\) as \\(\\log Y_i\\). Then, we obtain \\[\\begin{equation*} Y_i&#39; = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\varepsilon_i. \\end{equation*}\\] Lets generate some data in which \\(x\\) and \\(y\\) do not have a linear relationship. set.seed(1) x &lt;- rnorm(100, 0, 0.5) y &lt;- exp(1 + 2 * x + rnorm(100, 0, 1)) To perform regression for the transformed variable: lm(log(y) ~ x) ## ## Call: ## lm(formula = log(y) ~ x) ## ## Coefficients: ## (Intercept) x ## 0.9623 1.9979 12.10 Polynomial Regression Polynomial regression models are special cases of the general linear regression model. They contain higher-order terms of the predictor variables, making the response function nonlinear. Suppose we only have one predictor \\(X_{1}\\). An example of a polynomial regression model is \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i1}^2 + \\varepsilon_i. \\end{equation*}\\] It is a special case of the general linear regression model because we can define \\(X_{i2}\\) as \\(X_{i1}^2\\) and write \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\varepsilon_i. \\end{equation*}\\] Example: set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x1^2 - 2 * x2 + rnorm(100, 0, 1) lm(y ~ poly(x1, 2, raw = TRUE) + x2) ## ## Call: ## lm(formula = y ~ poly(x1, 2, raw = TRUE) + x2) ## ## Coefficients: ## (Intercept) poly(x1, 2, raw = TRUE)1 poly(x1, 2, raw = TRUE)2 ## 1.044 2.025 2.976 ## x2 ## -2.058 We have to set raw = TRUE in ploy(). Alternatively, include I(x1^2) not x1^2: set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x1^2 - 2 * x2 + rnorm(100, 0, 1) lm(y ~ x1 + I(x1^2) + x2) ## ## Call: ## lm(formula = y ~ x1 + I(x1^2) + x2) ## ## Coefficients: ## (Intercept) x1 I(x1^2) x2 ## 1.044 2.025 2.976 -2.058 Note that lm(y ~ x1 + x1^2 + x2) does not do what you think. If you want to perform \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i1}^3 + \\varepsilon, \\end{equation*}\\] you can use lm(y ~ x1 + I(x1^3)). Example: set.seed(1) x1 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x1^3 + rnorm(100, 0, 1) lm(y ~ x1 + I(x1^3)) ## ## Call: ## lm(formula = y ~ x1 + I(x1^3)) ## ## Coefficients: ## (Intercept) x1 I(x1^3) ## 0.9623 2.0014 2.9990 12.11 Stepwise regression Suppose that there are \\(p\\) predictors, \\(x_1,\\ldots,x_p\\). In many cases, not all of them are useful to model and predict the response. That is, we may only want to use a subset of them to form a model. Stepwise regression is one of the methods to achieve this. To perform stepwise regression, you can use step() in R. This function uses AIC as a criterion. AIC is defined as \\[\\begin{equation*} AIC = -2\\log L(\\hat{\\beta}) + 2k, \\end{equation*}\\] where \\(k\\) is the number of parameters and \\(L(\\hat{\\beta})\\) is the likelihood function evaluated at the MLE \\(\\hat{\\beta}\\). As you can see, there are two components in the formula. We want a large likelihood while keeping a model simple (= fewer parameters = smaller \\(k\\)). Therefore, a model with a smaller AIC is preferred. You can think of the term \\(2k\\) as a penalty that discourages us to use too many predictors. This is because while using more predictors can always increase the value of the likelihood function, it will result in overfitting. We will illustrate step() using the prostate cancer dataset zprostate in the package bestglm. Details of the dataset: a study of 97 men with prostate cancer examined the correlation between PSA (prostate specific antigen) and a number of clinical measurements: lcavol, lweight, lbph, svi, lcp, gleason, pgg45. Backward stepwise regression: starts with the full model (or a model with many variables) and removes the variable that results in the largest AIC until no variables can be removed. full_model &lt;- lm(lpsa ~ . , data = zprostate[, -10]) step(full_model, direction = &quot;backward&quot;) ## Start: AIC=-60.78 ## lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + ## pgg45 ## ## Df Sum of Sq RSS AIC ## - gleason 1 0.0491 43.108 -62.668 ## - pgg45 1 0.5102 43.569 -61.636 ## - lcp 1 0.6814 43.740 -61.256 ## &lt;none&gt; 43.058 -60.779 ## - lbph 1 1.3646 44.423 -59.753 ## - age 1 1.7981 44.857 -58.810 ## - lweight 1 4.6907 47.749 -52.749 ## - svi 1 4.8803 47.939 -52.364 ## - lcavol 1 20.1994 63.258 -25.467 ## ## Step: AIC=-62.67 ## lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45 ## ## Df Sum of Sq RSS AIC ## - lcp 1 0.6684 43.776 -63.176 ## &lt;none&gt; 43.108 -62.668 ## - pgg45 1 1.1987 44.306 -62.008 ## - lbph 1 1.3844 44.492 -61.602 ## - age 1 1.7579 44.865 -60.791 ## - lweight 1 4.6429 47.751 -54.746 ## - svi 1 4.8333 47.941 -54.360 ## - lcavol 1 21.3191 64.427 -25.691 ## ## Step: AIC=-63.18 ## lpsa ~ lcavol + lweight + age + lbph + svi + pgg45 ## ## Df Sum of Sq RSS AIC ## - pgg45 1 0.6607 44.437 -63.723 ## &lt;none&gt; 43.776 -63.176 ## - lbph 1 1.3329 45.109 -62.266 ## - age 1 1.4878 45.264 -61.934 ## - svi 1 4.1766 47.953 -56.336 ## - lweight 1 4.6553 48.431 -55.373 ## - lcavol 1 22.7555 66.531 -24.572 ## ## Step: AIC=-63.72 ## lpsa ~ lcavol + lweight + age + lbph + svi ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 44.437 -63.723 ## - age 1 1.1588 45.595 -63.226 ## - lbph 1 1.5087 45.945 -62.484 ## - lweight 1 4.3140 48.751 -56.735 ## - svi 1 5.8509 50.288 -53.724 ## - lcavol 1 25.9427 70.379 -21.119 ## ## Call: ## lm(formula = lpsa ~ lcavol + lweight + age + lbph + svi, data = zprostate[, ## -10]) ## ## Coefficients: ## (Intercept) lcavol lweight age lbph svi ## 2.4784 0.6412 0.2520 -0.1224 0.1469 0.2960 From the full model, removing gleason results in a model with the smallest AIC. Therefore, the algorithm first removes gleason. Next, removing lcp results in a model with the smallest AIC. Therefore, lcp is removed in the next step. pgg45 is removed because removing it will continue to reduce the AIC. Finally, removing any variable will not reduce the AIC. Therefore, the algorithm stops and the best model is lpsa ~ lcavol + lweight + age + lbph + svi. Forward stepwise regression: starts with the intercept (or a few variables) and adds new ones until the model cannot be improved (in the sense that the AIC will not decrease). null_model &lt;- lm(lpsa ~ 1, data = zprostate[, -10]) # the model with intercept only step(null_model, direction = &quot;forward&quot;, scope = formula(full_model)) ## Start: AIC=28.84 ## lpsa ~ 1 ## ## Df Sum of Sq RSS AIC ## + lcavol 1 69.003 58.915 -44.366 ## + svi 1 41.011 86.907 -6.658 ## + lcp 1 38.528 89.389 -3.926 ## + lweight 1 24.019 103.899 10.665 ## + pgg45 1 22.814 105.103 11.783 ## + gleason 1 17.416 110.502 16.641 ## + lbph 1 4.136 123.782 27.650 ## + age 1 3.679 124.239 28.007 ## &lt;none&gt; 127.918 28.838 ## ## Step: AIC=-44.37 ## lpsa ~ lcavol ## ## Df Sum of Sq RSS AIC ## + lweight 1 7.1726 51.742 -54.958 ## + svi 1 5.2375 53.677 -51.397 ## + lbph 1 3.2658 55.649 -47.898 ## + pgg45 1 1.6980 57.217 -45.203 ## &lt;none&gt; 58.915 -44.366 ## + lcp 1 0.6562 58.259 -43.452 ## + gleason 1 0.4156 58.499 -43.053 ## + age 1 0.0025 58.912 -42.370 ## ## Step: AIC=-54.96 ## lpsa ~ lcavol + lweight ## ## Df Sum of Sq RSS AIC ## + svi 1 5.1737 46.568 -63.177 ## + pgg45 1 1.8158 49.926 -56.424 ## &lt;none&gt; 51.742 -54.958 ## + lcp 1 0.8187 50.923 -54.506 ## + gleason 1 0.7163 51.026 -54.311 ## + age 1 0.6456 51.097 -54.176 ## + lbph 1 0.4440 51.298 -53.794 ## ## Step: AIC=-63.18 ## lpsa ~ lcavol + lweight + svi ## ## Df Sum of Sq RSS AIC ## + lbph 1 0.97296 45.595 -63.226 ## &lt;none&gt; 46.568 -63.177 ## + age 1 0.62301 45.945 -62.484 ## + pgg45 1 0.50069 46.068 -62.226 ## + gleason 1 0.34449 46.224 -61.898 ## + lcp 1 0.06937 46.499 -61.322 ## ## Step: AIC=-63.23 ## lpsa ~ lcavol + lweight + svi + lbph ## ## Df Sum of Sq RSS AIC ## + age 1 1.15879 44.437 -63.723 ## &lt;none&gt; 45.595 -63.226 ## + pgg45 1 0.33173 45.264 -61.934 ## + gleason 1 0.20691 45.389 -61.667 ## + lcp 1 0.10115 45.494 -61.441 ## ## Step: AIC=-63.72 ## lpsa ~ lcavol + lweight + svi + lbph + age ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 44.437 -63.723 ## + pgg45 1 0.66071 43.776 -63.176 ## + gleason 1 0.47674 43.960 -62.769 ## + lcp 1 0.13040 44.306 -62.008 ## ## Call: ## lm(formula = lpsa ~ lcavol + lweight + svi + lbph + age, data = zprostate[, ## -10]) ## ## Coefficients: ## (Intercept) lcavol lweight svi lbph age ## 2.4784 0.6412 0.2520 0.2960 0.1469 -0.1224 Remark: there is also an option direction = \"both\". 12.12 Best subset Given \\(p\\) predictors, the best subset problem is to find out of all the \\(2^p\\) subsets, the best subset according to some goodness-of-fit criterion. Some examples of goodness-of-fit criteria are AIC and BIC. We will use the package bestglm. The function to find out the best subset is also called bestglm(). BIC is defined as \\[\\begin{equation*} BIC = -2\\log L(\\hat{\\beta}) + k \\log (n), \\end{equation*}\\] where \\(k\\) is the number of parameters, \\(n\\) is the number of observations, and \\(L(\\hat{\\beta})\\) is the likelihood function evaluated at the MLE. You can think of \\(k \\log (n)\\) as a penalty to discourage us to use too many predictors to avoid overfitting. library(bestglm) # the last column ind bestglm(zprostate[, -10], IC = &quot;BIC&quot;) ## BIC ## BICq equivalent for q in (0.0561398731346802, 0.759703855996616) ## Best Model: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4783869 0.07184863 34.494560 8.637858e-55 ## lcavol 0.6197821 0.08823567 7.024168 3.493564e-10 ## lweight 0.2835097 0.07524408 3.767867 2.887126e-04 ## svi 0.2755825 0.08573414 3.214385 1.797619e-03 "],["logistic-regression-model.html", "Chapter 13 Logistic Regression Model", " Chapter 13 Logistic Regression Model library(tidyverse) library(mlbench) # for some datasets We have already seen logistic regression model in Chapter 9. Logistic regression is one of the most common methods for classification. Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) is a binary variable and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In logistic regression we assume that \\[\\begin{equation*} P(Y_i = 1|x_i, \\beta) = \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}} = \\frac{ e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}. \\end{equation*}\\] Since \\(Y_i\\) takes only two values, \\[\\begin{equation*} P(Y_i = 0|x_i, \\beta) = \\frac{1}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] We can use one single formula for \\(y = 0, 1\\): \\[\\begin{equation*} P(Y_i = y|x_i, \\beta) = \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The likelihood function (conditional on x) is \\[\\begin{equation*} L(\\beta|y_1,\\ldots,y_n, x_1,\\ldots,x_n) = \\prod^n_{i=1} P(Y_i = y_i|x_i, \\beta) = \\prod^n_{i=1} \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The MLE of \\(\\beta\\) is obtained by maximizing \\(L(\\beta|y,x)\\) with respect to \\(\\beta\\). We usually maximize the natural logarithm of the likelihood function instead of the likelihood function, which is easier. The log likelihood function is \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( (x^T_i \\beta) y_i - \\log(1+e^{x^T_i \\beta}) \\bigg), \\end{equation*}\\] which can be maximized numerically in computer. Odds In statistics, odds of an event are defined as the ratio of the probability that the event will occur to the probability that the event will not occur. For example, if the event of interest is getting a \\(6\\) when you roll a die. The odds are \\((1/6):(5/6) = 1:5\\). In the logistic regression model, the odds of \\(Y=1\\) given \\(x\\) are \\[\\begin{equation*} \\frac{P(Y=1|x)}{P(Y=0|x)} = e^{\\beta^T x}. \\end{equation*}\\] Interpretation of \\(\\beta_j\\): changing the value of \\(x_j\\) by one unit, while keeping other predictors fixed, multiplies the odds by \\(e^{\\beta_j}\\). Log odds The log odds is the log of the odds. In the logistic regression model, the log odds of \\(Y=1\\) given \\(x\\) are \\[\\begin{equation*} \\log \\frac{P(Y=1|x)}{P(Y=0|x)} = \\beta^T x. \\end{equation*}\\] Interpretation of \\(\\beta_j\\): changing the value of \\(x_j\\) by one unit, while keeping other predictors fixed, changes the log odds by \\(\\beta_j\\). logit function The logit function, \\(\\text{logit}:(0, 1) \\rightarrow \\mathbb{R}\\), is defined as \\[\\begin{equation*} \\text{logit}(p) = \\log \\frac{p}{1-p}. \\end{equation*}\\] logistic function The standard logistic function is defined as \\[\\begin{equation*} f(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}. \\end{equation*}\\] It is the inverse of the logit function. Example The package mlbench contains some artificial and real-world machine learning benchmark problems (datasets). See https://cran.r-project.org/web/packages/mlbench/mlbench.pdf We will use the dataset PimaIndiansDiabetes2 from mlbench. Details: pregnant: Number of times pregnant glucose: Plasma glucose concentration (glucose tolerance test) pressure: Diastolic blood pressure (mm Hg) triceps: Triceps skin fold thickness (mm) insulin: 2-Hour serum insulin (mu U/ml) mass: Body mass index (weight in kg/(height in m)^2) pedigree: Diabetes pedigree function age: Age (years) diabetes: Class variable (test for diabetes) data(PimaIndiansDiabetes2) PimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2) # Split the data into training and test set set.seed(1) random_index &lt;- sample(nrow(PimaIndiansDiabetes2), size = nrow(PimaIndiansDiabetes2) * 0.7) train_data &lt;- PimaIndiansDiabetes2[random_index, ] test_data &lt;- PimaIndiansDiabetes2[-random_index, ] Simple model: Fitting a logistic regression model is similar to fitting a linear regression model. In addition to the formula and the data, we use glm() and specify family = binomial. predict(fit_simple, test_data, type = \"response\"): give us the estimated probability ifelse(prob &gt; 0.5, \"pos\", \"neg\"): classify the case as pos when the probability is greater than \\(0.5\\) # Estimation fit_simple &lt;- glm(diabetes ~ glucose, data = train_data, family = binomial) # Summary summary(fit_simple) ## ## Call: ## glm(formula = diabetes ~ glucose, family = binomial, data = train_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2089 -0.7182 -0.4459 0.7004 2.4145 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.419871 0.774032 -8.294 &lt; 2e-16 *** ## glucose 0.044509 0.005775 7.707 1.29e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 348.35 on 273 degrees of freedom ## Residual deviance: 262.14 on 272 degrees of freedom ## AIC: 266.14 ## ## Number of Fisher Scoring iterations: 4 # Prediction prob &lt;- predict(fit_simple, test_data, type = &quot;response&quot;) predicted_class &lt;- ifelse(prob &gt; 0.5, &quot;pos&quot;, &quot;neg&quot;) # Confusion Matrix table(predicted_class, test_data$diabetes) ## ## predicted_class neg pos ## neg 71 23 ## pos 8 16 # Accuracy mean(predicted_class == test_data$diabetes) ## [1] 0.7372881 For this simple model, the accuracy is \\(73.7\\%\\). Now, we will use all the covariates. # Estimation fit &lt;- glm(diabetes ~ ., data = train_data, family = binomial) # Summary summary(fit) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial, data = train_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6139 -0.6178 -0.3523 0.5804 2.1311 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.130e+01 1.558e+00 -7.250 4.18e-13 *** ## pregnant 2.511e-02 6.582e-02 0.381 0.7028 ## glucose 4.038e-02 7.028e-03 5.745 9.17e-09 *** ## pressure 1.368e-02 1.562e-02 0.876 0.3811 ## triceps 1.381e-02 2.151e-02 0.642 0.5209 ## insulin -9.559e-04 1.808e-03 -0.529 0.5970 ## mass 7.334e-02 3.586e-02 2.045 0.0408 * ## pedigree 8.706e-01 5.093e-01 1.709 0.0874 . ## age 3.612e-02 2.323e-02 1.555 0.1200 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 348.35 on 273 degrees of freedom ## Residual deviance: 233.54 on 265 degrees of freedom ## AIC: 251.54 ## ## Number of Fisher Scoring iterations: 5 # Prediction prob &lt;- predict(fit, test_data, type = &quot;response&quot;) predicted_class &lt;- ifelse(prob &gt; 0.5, &quot;pos&quot;, &quot;neg&quot;) # Confusion Matrix table(predicted_class, test_data$diabetes) ## ## predicted_class neg pos ## neg 69 15 ## pos 10 24 # Accuracy mean(predicted_class == test_data$diabetes) ## [1] 0.7881356 The accuracy is \\(78.8\\%\\). "],["k-means-clustering.html", "Chapter 14 k-means Clustering 14.1 Introduction 14.2 Applications", " Chapter 14 k-means Clustering Reference: Ch9 of Machine Learning with R, Ch6 of R Graphics Cookbook Packages used: library(tidyverse) # for read_csv library(factoextra) # visualize clusering results library(jpeg) # readJPEG reads an image from a JPEG file/content into a raster array library(ggpubr) 14.1 Introduction Clustering Clustering is an unsupervised learning task that divides data into clusters, or groups of similar items while data points in different clusters are very different. It is an unsupervised learning method as we do not have labels of the data. We do not have the ground truth to compare the results. Clustering will only tell you which groups of data are similar. One may get some meaningful interpretation of the groups by studying the members in each group, e.g., by calculating some summary statistics and making use of some visualization tools. \\(k\\)-means clustering \\(k\\)-means clustering is one of the most popular clustering methods. It is intended for situations in which all variables are of the numeric type. In \\(k\\)-means clustering, all the examples are assigned to one of the \\(k\\) clusters, where \\(k\\) is a positive integer that has been specified before performing the clustering. The goal is to assign similar data to the same cluster. Some Applications Cluster analysis: Interesting groups may be discovered, such as the groups of motor insurance policy holders with a high average claim cost, or the groups of clients in a banking database having a heavy investment in real estate. In marketing segmentation, we segment customers into groups with similar demographics or buying patterns for targeted marketing campaigns Image segmentation: The goal of segmentation is to partition an image into regions each of which has a reasonably homogeneous visual appearance or which corresponds to objects or parts of objects. Notation and Setting We have \\(n\\) data points \\(x_i = (x_{i1},\\ldots,x_{ip})&#39; \\in \\mathbb{R}^p\\), \\(i=1,\\ldots,n\\). Each observation is uniquely labeled by an integer \\(i \\in \\{1,\\ldots,n\\}\\). We use the notation \\(C_i\\) to denote which cluster the \\(i\\)th observation is assigned to. For example, \\(C_1 = 2\\) means the \\(1\\)st observation is assigned to the \\(2\\)nd cluster, and \\(C_2 = 3\\) means the \\(2\\)nd observation is assigned to the \\(3\\)rd cluster. We use \\(i\\) to index data \\(j\\) to index cluster \\(l\\) to index feature Dissimilarity measure In clustering, one have to define a dissimilarity measure \\(d\\). In \\(k\\)-means clustering, the squared Euclidean distance is used. Given two data points \\(x_i = (x_{i1},\\ldots,x_{ip})\\) and \\(x_{i&#39;} = (x_{i&#39;1},\\ldots,x_{i&#39;p})\\), the squared Euclidean distance is given by \\[\\begin{equation*} d(x_i, x_{i&#39;}) = \\sum^p_{l=1} (x_{il} - x_{i&#39;l})^2 = ||x_i - x_{i&#39;}||^2. \\end{equation*}\\] k-means clustering algorithm (Lloyds algorithm) Specify the number of clusters \\(k\\) Randomly select \\(k\\) data points as the centroids, denoted by \\(\\mu_1,\\ldots,\\mu_k\\). For each \\(i=1,\\ldots,n\\), set \\[ C_i = \\text{argmin}_j ||x_i - \\mu_j||^2. \\] In words, assign the closest cluster to the \\(i\\)th observation. For each \\(j=1,\\ldots,k\\), \\(l=1,\\ldots,p\\), set \\[ \\mu_{jl} = \\frac{\\sum^n_{i=1}I(C_i =j) x_{il}}{\\sum^n_{i=1}I(C_i =j)}. \\] In words, \\(\\mu_j\\) is simply the mean of all the points assigned to \\(j\\)th cluster. Note that \\(\\mu_j = (\\mu_{j1},\\ldots,\\mu_{jp})\\). Repeat Steps 3 - 4 until the assignments do not change. Illustration of the algorithm: Set \\(k=2\\). Randomly select 2 data points as the initial centroids (Brown crosses). For each observation, find the nearest centroid. Compute the mean of all the points assigned to the two cluster to find the new centroids. Repeat Step 3: Repeat Step 4: Remarks \\(\\text{argmin}\\) is the argument of the minimum. \\(\\text{argmin}_j ||x_i - \\mu_j||^2\\) means the value of \\(j\\) such that \\(||x_i - \\mu_j||^2\\) is minimum. \\(I(C_i = j)\\) equals \\(1\\) if \\(C_i = j\\) and equals \\(0\\) otherwise. The \\(I\\) is the indicator function. \\(k\\)-means is sensitive to the number of clusters. See the elbow method. \\(k\\)-means is sensitive to the randomly-chosen cluster centers. Different initial centers may result in different clustering results. As a result, we should use multiple set of initial cluster centers and choose the best result (smallest within-group sum of squared errors, see the end of this chapter). Scale your data before applying \\(k\\)-means clustering. For categorical data: one possible solution is to use \\(k\\)-modes. The \\(k\\)-modes algorithm uses a matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimize the clustering cost functions For mixed data (numeric + categorical features): can use \\(k\\)-prototypes. It integrates the \\(k\\)-means and \\(k\\)-modes algorithms using a combined dissimilarity measure. R package: clustMixType See also Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values by Z. Huang (1998). 14.2 Applications 14.2.1 Cluster Analysis The Mall_Customers.csv (in onQ) contains the gender, age, annual income and spending score of some customers. mall &lt;- read_csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/Mall_Customers.csv&quot;) mall ## # A tibble: 200 x 5 ## CustomerID Gender Age `Annual Income (k$)` `Spending Score (1-100)` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Male 19 15 39 ## 2 2 Male 21 15 81 ## 3 3 Female 20 16 6 ## 4 4 Female 23 16 77 ## 5 5 Female 31 17 40 ## 6 6 Female 22 17 76 ## 7 7 Female 35 18 6 ## 8 8 Female 23 18 94 ## 9 9 Male 64 19 3 ## 10 10 Female 30 19 72 ## # ... with 190 more rows We will use the kmeans() function in R (contained in base R) to perform \\(k\\)-means clustering. x: your dataframe/ matrix centers: the number of clusters nstart: how many random sets should be chosen (the best result will be reported). Recommended setting: always run \\(k\\)-means clustering with a large value of nstart, such as \\(20\\) or \\(50\\), to reduce the chance of obtaining an undesirable local optimum. To be able to reproduce the \\(k\\)-means output, you can also set a random seed using the set.seed() function. Otherwise, the cluster labels can be different each time you run the code. Lets perform a \\(k\\)-means clustering on the customers using Annual Income (k$) and Spending Score (1-100) with \\(k=3\\). mall_kmeans &lt;- kmeans(x = scale(mall[, 4:5]), centers = 3, nstart = 25) # To view the cluster mall_kmeans$cluster ## [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [39] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [77] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [115] 3 3 3 3 3 3 3 3 3 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 ## [153] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 ## [191] 1 2 1 2 1 2 1 2 1 2 # To view the centers mall_kmeans$centers ## Annual Income (k$) Spending Score (1-100) ## 1 1.0066735 -1.22246770 ## 2 0.9891010 1.23640011 ## 3 -0.6246222 -0.01435636 Visualize the clusters: library(factoextra) fviz_cluster(mall_kmeans, data = mall[, 4:5], geom = &quot;point&quot;) By looking at the plot, we see the 3 clusters are high annual income + low spending score high annual income + high spending score low annual income Lets try \\(k=5\\): mall_kmeans &lt;- kmeans(x = scale(mall[, 4:5]), centers = 5, nstart = 25) Visualizing the clusters: fviz_cluster(mall_kmeans, data = mall[, 4:5], geom = &quot;point&quot;) The \\(5\\) clusters are high annual income + low spending score high annual income + high spending score medium annual income + medium spending score low annual income + high spending score low annual income + low spending score Now, lets use one more variable for clustering: mall_kmeans &lt;- kmeans(x = scale(mall[, 3:5]), centers = 5, nstart = 25) To visualize more than 2 variables, we can use a dimension reduction technique called principal component analysis. The function fviz_cluster will automatically perform that and give you a \\(2D\\)-plot using the first two principal comonents. fviz_cluster(mall_kmeans, data = mall[, 3:5], geom = &quot;point&quot;) To understand more about the clusters, we should take a look at some plots and some summary statistics. An example is to use violin plots. Violin Plots A violin plot is a kernel density estimate, mirrored so that it forms a symmetrical shape. It is a helpful tool to compare multiple data distributions when we put several plots side by side. To provide additional information, we can also have box plots overlaid, with a white dot at the median. We first add the cluster information to our dataset. mall_cluster &lt;- mutate(mall, Cluster = factor(mall_kmeans$cluster)) Create violin plots outlier.colour = NA: do not display outliers mall_cluster %&gt;% ggplot(aes(x = Cluster, y = `Annual Income (k$)`)) + geom_violin(trim = FALSE) + geom_boxplot(width = .1, fill = &quot;black&quot;, outlier.colour = NA) + stat_summary(fun = median, geom = &quot;point&quot;, fill = &quot;white&quot;, shape = 21, size = 2.5) The other two variables: Determine the optimal \\(k\\) The elbow method is a heuristic method to determine the number of clusters. We plot the total within-group sum of squared errors against the number of clusters. We pick the elbow of the curve as the number of clusters to use. The elbow (or knee) of a curve is a point where the curve visibly bends. WSS &lt;- rep(0, 10) for (k in 1:10) { # extract the total within-group sum of squared errors WSS[k] = kmeans(x = scale(mall[, 3:5]), centers = k, nstart = 25)$tot.withinss } ggplot(mapping = aes(x = 1:10, y = WSS)) + geom_line() + geom_point() + geom_vline(xintercept = 4) + scale_x_discrete(name = &quot;k&quot;, limits = factor(1:10)) + labs(title = &quot;Elbow Method&quot;) It seems to me that the curve bends at \\(k=4\\). 14.2.2 Image Segementation and Image Compression Image segementation using \\(k\\)-means clustering: Each pixel is a data point in a 3-dimensioal space comprising the intensities of the red, blue, and green channels. Group the pixels into \\(k\\) different clusters For a given value of \\(k\\). the result from the \\(k\\)-means clustering is an image using a paletter of only \\(k\\) colors. Note: a direct use of \\(k\\)-means clustering is not a particular sophisticated approach to image segmentation (e.g., we havent taken into account of the information from spatial proximity) We will use the readJPEG() function from the package jpeg to reads an image from a jpeg file into a raster array. library(jpeg) # read the image into a raster array image &lt;- readJPEG(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/image/flower_s.jpg&quot;) # 3-way array (matrix = 2-way array) (image_dim &lt;- dim(image)) ## [1] 200 300 3 ## [1] 200 300 3 # Assign RGB channels to each pixel image_RGB &lt;- tibble( x = rep(1:image_dim[2], each = image_dim[1]), y = rep(image_dim[1]:1, image_dim[2]), R = as.vector(image[, , 1]), G = as.vector(image[, , 2]), B = as.vector(image[, , 3]) ) # use view(image_RGB) to view the resulting tibble Original Image Apply \\(k\\)-means clustering: Code for creating the above images: cluster_plot &lt;- list() i &lt;- 0 for (k in c(2, 3, 5, 8)) { i &lt;- i + 1 # perform k-means image_kmeans &lt;- kmeans(image_RGB[, c(&quot;R&quot;, &quot;G&quot;, &quot;B&quot;)], centers = k, nstart = 25) # for each pixel, use the colour of the center of the corresponding cluster cluster_color &lt;- rgb(image_kmeans$centers[image_kmeans$cluster, ]) cluster_plot[[i]] &lt;- ggplot(data = image_RGB, aes(x = x, y = y)) + geom_point(colour = cluster_color) + labs(title = paste(&quot;k = &quot;, k)) + xlab(&quot;&quot;) + ylab(&quot;&quot;) } # save the plots to your computer png(paste0(&quot;image/flower_cluster.png&quot;), width = 900, height = 600) ggarrange(cluster_plot[[1]], cluster_plot[[2]], cluster_plot[[3]], cluster_plot[[4]]) dev.off() Another example Original Image: Apply \\(k\\)-means clustering: Image Compression Two types of data (image is a special case of data) compression: lossless data compression: reconstruct the original data exactly from the compressed representation lossy data compression: accept some errors in the reconstruction in return for higher levels of compression than can be achieved in the lossless case To apply \\(k\\)-means clustering to lossy data compression: For each pixel, store only the identity of the cluster to which it is assigned Store the values of the \\(k\\) cluster centers Bit: The bit is the most basic unit of information in computing and digital communications. The bit represents a logical state with one of two possible values. These values are most commonly represented as either 1 or 0. \\(8\\) bits can represent at most \\(2^8 = 256\\) different values. Suppose the original image has \\(n\\) pixels comprising \\(R, G, B\\) values each of which is stored with \\(8\\) bits of precision. To store the whole image, we needs \\(24 n\\) bits. Using \\(k\\)-means clustering, we needs \\(\\log_2 K\\) bits per pixel to store the cluster identity and \\(24\\) bits for each cluster. Total = \\(24K + N \\log_2 K\\). Example: For an image with \\(240 \\times 180 = 43,200\\) pixels, we need \\(1,036,800\\) bits to transmit directly. For the same image, using \\(k\\)-means clustering, we need \\(43,248\\) bits (\\(k=2\\)), compression ratios: \\(4.2\\%\\) \\(86,472\\) bits (\\(k=3\\)), compression ratios: \\(8.3\\%\\) \\(173,040\\) bits (\\(k=10\\)), compression ratios: \\(16.7\\%\\) Explanation of the algorithm The \\(k\\)-means algorithm searches for a partition of the data into \\(k\\) clusters . It tries to minimize the within-group sum of squared errors (WSS): \\[\\begin{equation*} WSS(C, \\mu) = \\sum^k_{j=1} \\sum^n_{i=1} I(C_i = j) ||x_i - \\mu_j||^2, \\end{equation*}\\] where \\(C = (C_1,\\ldots,C_n)\\) and \\(\\mu = \\{\\mu_1,\\ldots,\\mu_k\\}\\). To minimize WGSS, the algorithm iteratively solves two problems: Problem 1. Fix \\(\\mu\\) and minimize \\(WGSS(C, \\mu)\\) with respect to \\(C\\) (Step 3 in the algorithm) Problem 2. Fix \\(C\\) and minimize \\(WGSS(C, \\mu)\\) with respect to \\(\\mu\\) (Step 4 in the algorithm) For Problem 1, the solution is \\[\\begin{equation*} C_i = \\text{argmin}_j ||x_i - \\mu_j||^2. \\end{equation*}\\] For Problem 2, the solution is \\[\\begin{equation*} \\mu_{jl} = \\frac{\\sum^n_{i=1}I(C_i = j) x_{il}}{\\sum^n_{i=1} I(C_i=j)}. \\end{equation*}\\] "],["hierarchical-clustering.html", "Chapter 15 Hierarchical Clustering 15.1 Dissimilarity measure and Linkage 15.2 Alogrithm 15.3 Applications", " Chapter 15 Hierarchical Clustering Reference: Ch12.4 in An introduction to Statistical Leraning with applications in R by James, Witten, Hastie and Tibshirani. One potential disadvantage of \\(k\\)-means clustering: need to pre-sepcify the number of clusters \\(k\\). Hierarchical clustering is a method to perform cluster analysis that seeks to build a hierarchy of clusters. Two features: Does not require us to pre-specify the number of clusters (but see the height of the cut below) Results in an attractive tree-based representation of the observations, called dendrogram Example of dendrogram: Two types: bottom-up (agglomerative) and top-down (divisive) We will describe bottom-up clustering (the most common type), where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Interpreting a Dendrogram The left figure shows the dendrogram (genearlly depicted as an upside-down tree) obtained from hierarchical clustering of \\(45\\) observations. Each leaf of the dendrogram represents one of the \\(45\\) observations As we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, the branches fuse, either with leaves or other branches. The earlier fusions occur, the more similar the groups of observations are to each other. For any two observations, we can look for the point in the tree where branches containing those two observations are first fused. The height of the fusion, as measured on the vertical axis, indicates how different the two observations are. Note: The colors in the figure do not represent the true class labels. In fact, hierarchial clustering does not make use of class labels for the clustering. Finding Clusters from a Dendrogram To identify clusters from a dendrogram, make a horizontal cut across the dendrogram, as shown in the center and right-hand panels of the above figure In the center panel of the above figure, cutting the dendrogram at a height of nine results in two clusters, as shown in distinct colors In the right-hand panel, cutting the dendrogram at a height of five results in three clusters The height of the cut plays the same role as the \\(k\\) in \\(k\\)-means clustering: it controls the number of clusters obtained Now, we can understand the term hierarchical in this method. It refers to the fact that clusters obtained by cutting dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height. This also leads to one potential problem of hierarchical clustering: The resulting hierarchical structure may be unrealistic for some datasets 15.1 Dissimilarity measure and Linkage Just like \\(k\\)-means clustering, we have to define some sort of dissimilarity measure between each pair of observations. Usually, Euclidean distance is used. In addition, how do we measure the dissimilarity between groups of observations? Linkage is a notion to define the dissimilarity between two groups of observations. Given two groups of observations \\(A\\) and \\(B\\) and a dissimilarity measure \\(d\\) between two observations. Four commonly-used types of linkage: Complete: \\(d_l(A, B) = \\max_{x_i \\in A, y_j \\in B} d(x_i, y_j)\\). Single: \\(d_l(A, B) = \\min_{x_i \\in A, y_j \\in B} d(x_i, y_j)\\). Average: \\(d_l(A, B) = \\frac{1}{|A| \\cdot |B|} \\sum_{x_i \\in A} \\sum_{y_j \\in B} d(x_i, y_j)\\). Centroid: \\(d_l(A, B) = d(A_c, B_c)\\), where \\(A_c = \\frac{1}{|A|} \\sum_{x_i \\in A} x_i\\) and \\(B_c = \\frac{1}{|B|} \\sum_{y_j \\in B} y_j\\). \\(|A|\\) denotes the cardinality of \\(A\\), the number of elements in \\(A\\). 15.2 Alogrithm Begin with \\(n\\) observations and a measure of all the \\(C^n_2\\) pairwise dissimilarities. Treat each observation as its own cluster. For \\(i=n,n-1,\\ldots,2\\): Examine all pairwise inter-cluster dissimilarities among the \\(i\\) clusters and identify the pair of clusters that are least dissimilar. Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed. Compute the new pairwise inter-cluster dissimilarities among the \\(i-1\\) remaining clusters. 15.3 Applications In R, use hclust() to implement hierarchical clustering. We begin with some simulated data. set.seed(2) X &lt;- matrix(rnorm(50 * 2), nrow = 50, ncol = 2) X[1:25, ] &lt;- X[1:25, ] + 3 # some mean shift for the first 25 observations To compute the distances between all the observations, use dist(). This will give you a \\(50 \\times 50\\) matrix containing the distance between each pair of observations. dist(X) Perform hierarchical clustering with complete linkage: hc_complete &lt;- hclust(dist(X), method = &quot;complete&quot;) Plot the dendrogram: plot(hc_complete, main = &quot;Complete Linkage&quot;, xlab =&quot;&quot;, sub = &quot;&quot;, cex = 0.6) Use cutree() to determine the cluster labels for each observation associated with a given number of clusters: cutree(hc_complete, k = 2) # given number of clusters = 2 ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [40] 2 2 2 2 2 2 2 2 2 2 2 Use cutree() to determine the cluster labels for each observation associated with a given cut of the dendrogram: cutree(hc_complete, h = 3) # cut at 3 ## [1] 1 2 3 2 1 1 3 2 3 2 1 2 1 2 3 4 1 1 3 1 3 2 3 3 1 5 4 5 4 6 4 6 6 6 6 4 5 5 5 ## [40] 5 4 5 6 6 6 6 4 5 6 5 To perform hierarchical clustering with average linkage: hc_average &lt;- hclust(dist(X), method = &quot;average&quot;) To perform hierarchical clustering with single linkage: hc_single &lt;- hclust(dist(X), method = &quot;single&quot;) To scale the variables before performing hierarchical clustering: hc_complete &lt;- hclust(dist(scale(X)), method = &quot;complete&quot;) 15.3.1 NCI60 Data Install and load the package ISLRT2. It contains NCI60 dataset. NCI60: a cancer cell line microarray data, which consists of \\(6,830\\) gene expression measurements on \\(64\\) cancer cell lines. library(ISLR2) nci_data &lt;- NCI60$data nci_labs &lt;- NCI60$labs Cancer types for the cell lines: table(nci_labs) ## nci_labs ## BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA ## 7 5 7 1 1 6 ## MCF7A-repro MCF7D-repro MELANOMA NSCLC OVARIAN PROSTATE ## 1 1 8 9 6 2 ## RENAL UNKNOWN ## 9 1 Suppose we perform hierarchical clustering on this data with the goal of finding out whether or not the observation cluster into distinct types of cancer. We first scale the data (this step may be optional): nci_data_scale &lt;- scale(nci_data) Dendrogram: hc_complete &lt;- hclust(dist(nci_data_scale), method = &quot;complete&quot;) plot(hc_complete, labels = nci_labs, xlab = &quot;&quot;, sub = &quot;&quot;, ylab = &quot;&quot;, cex = 0.6) Clearly, cell lines within a single cancer type do tend to cluster together. Now, lets cut the dendrogram to obtain, say, \\(4\\) clusters and compare with the true cancer types: hc_cluster &lt;- cutree(hc_complete, k = 4) table(hc_cluster, nci_labs) ## nci_labs ## hc_cluster BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA MCF7A-repro ## 1 2 3 2 0 0 0 0 ## 2 3 2 0 0 0 0 0 ## 3 0 0 0 1 1 6 0 ## 4 2 0 5 0 0 0 1 ## nci_labs ## hc_cluster MCF7D-repro MELANOMA NSCLC OVARIAN PROSTATE RENAL UNKNOWN ## 1 0 8 8 6 2 8 1 ## 2 0 0 1 0 0 1 0 ## 3 0 0 0 0 0 0 0 ## 4 1 0 0 0 0 0 0 Some patterns: All the leukemia cell lines fall in cluster 3 Breast cancer cell lines are spread out over \\(3\\) different clusters Comparing with \\(k\\)-means: set.seed(1) km_cluster &lt;- kmeans(nci_data_scale, 4, nstart = 50)$cluster table(km_cluster, hc_cluster) ## hc_cluster ## km_cluster 1 2 3 4 ## 1 9 0 0 0 ## 2 20 7 0 0 ## 3 0 0 8 0 ## 4 11 0 0 9 Some observations: The \\(4\\) clusters obtained using the two methods are different Cluster 3 in \\(k\\)-means clustering is the same as Cluster 3 in hierarchical clustering Cluster 1 in \\(k\\)-means is a subset of Cluster 1 in hierarchical clustering "],["resampling-methods.html", "Chapter 16 Resampling Methods 16.1 Cross-validation 16.2 Bootstrap", " Chapter 16 Resampling Methods Reference: Ch5 in An introduction to Statistical Leraning with applications in R by James, Witten, Hastie and Tibshirani. Packages used: caret, boot Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each resampled data set to obtain additional information about the fitted model. We will study two methods called cross-validation and bootstrap. Cross-validation can be used to estimate the test error of any learning methods in order to evaluate its performance (model assessment) select a tuning parameter / the appropriate level of flexibility (model selection) Bootstrap can be used to measure the accuracy of a parameter estimate, among other uses. We will see another application of bootstrap when we discuss ensemble methods. 16.1 Cross-validation Recall that in evaluating the model performance in previous chapters, we have partition our data into training and testing datasets. This is known as the holdout method. For quantitative response, we usually use the mean squared error (MSE) to measure the error: \\(\\frac{1}{n} \\sum^n_{i=1} (y_i - \\hat{y}_i)^2\\), where \\(y_i\\) is the observed value and \\(\\hat{y}_i\\) is the predicted value. The test error is the MSE computed using only the test data. However, the test error obtained using the above holdout method depends on the particular training and testing split. That is, if you split the dataset differently, the test error will be different. The following simple example illustrates this general observation. set.seed(1) x &lt;- runif(100, 0, 1) y &lt;- 1 + x + rnorm(100, 0, 1) # Split 1 fit &lt;- lm(y[1:50] ~ x[1:50]) # test error: mean((y[51:100] - predict(fit, data.frame(x[51:100])))^2) ## [1] 1.293077 # Split 2 fit &lt;- lm(y[51:100] ~ x[51:100]) # test error: mean((y[1:50] - predict(fit, data.frame(x[1:50])))^2) ## [1] 1.209196 To get a better test error estimate, we can use repeated holdout. That is, we repeat the whole splitting process several times with several random holdout samples and take the average of the performance measures to evaluate the model performance. A standard practice now is to use \\(k\\)-fold cross-validation (\\(k\\)-fold CV). It is a procedure in which instead of randomly partition the data many times, we divide the data into \\(k\\) random non-overlapping partitions. Then, we combine \\(k-1\\) partitions to form a training dataset and the remaining one to form the testing dataset. We repeat this procedure \\(k\\) times using different partitions and obtain \\(k\\) evaluations. It is common to use \\(10\\)-fold CV. For example, when \\(k=3\\): Randomly split your dataset \\(S\\) into \\(3\\) partitions \\(S_1,S_2,S_3\\). Use \\(S_1, S_2\\) to train your model. Evaluate the performance using \\(S_3\\). Use \\(S_1, S_3\\) to train your model. Evaluate the performance using \\(S_2\\). Use \\(S_2, S_3\\) to train your model. Evaluate the performance using \\(S_1\\). Report the average of the performance measures obtained in Steps 2-4. See https://en.wikipedia.org/wiki/File:KfoldCV.gif for a gif animation showing the \\(k\\)-fold CV. Leave-one-out CV (LOOCV) 5-fold CV 16.1.1 CV in GLM We can perform CV for linear regression and logistic regression using cv.glm() from the package boot. For the purpose of using cv.glm(), instead of using lm() to fit a linear regression, we will use glm() without specifying the family argument. The following two methods will give you the same result. toy_data &lt;- data.frame(y = y, x = x) glm(y ~ x, data = toy_data)$coef ## (Intercept) x ## 0.8206746 1.3123431 lm(y ~ x, data = toy_data)$coef ## (Intercept) x ## 0.8206746 1.3123431 Use cv.glm() without setting the values of \\(K\\) in the argument to perform leave-one-out CV: library(boot) fit &lt;- glm(y ~ x, data = toy_data) cv_error &lt;- cv.glm(data = toy_data, glmfit = fit) cv_error$delta ## [1] 0.9047164 0.9045299 Use cv.glm() with \\(K = 10\\) to perform \\(10\\)-fold CV: cv_error &lt;- cv.glm(data = toy_data, glmfit = fit, K = 10) cv_error$delta ## [1] 0.8879592 0.8868986 16.1.2 General Implementation In general, you may or may not be able to find the CV function that helps you to implement the CV to estimate the test error for a particular method. However, you can implement \\(k\\)-fold CV easily by yourself. For simplicity, we illustrate the implementation using a simple linear regression model. But the idea is the same for other methods. To create the \\(k\\) partitions, we can use createFolds() from the caret package. library(caret) set.seed(1) x &lt;- runif(100, 0, 1) y &lt;- 1 + x + rnorm(100, 0, 1) CV_example &lt;- data.frame(x = x, y = y) folds &lt;- createFolds(y, k = 10) # the result is a list test_error &lt;- rep(0, 10) for (k in 1:10) { # fit the model without one partition fit &lt;- lm(y ~ x, data = CV_example[-folds[[k]], ]) # MSE as the test error test_error[k] &lt;- sum((CV_example$y[folds[[k]]] - predict(fit, CV_example[folds[[k]], ]))^2) / length(folds[[k]]) } # average mean(test_error) ## [1] 0.9071669 16.2 Bootstrap In this section, we illustrate how to use bootstrap to estimate the standard error of an estimate. In later chapter when we discuss ensemble methods, you will see other uses of bootstrap. Suppose there are two stocks \\(A\\) and \\(B\\) with returns \\(r_A\\) and \\(r_B\\), respectively. You decide to invest \\(\\alpha\\) of your money in \\(A\\) and \\(1-\\alpha\\) of your money in \\(B\\). The way that you want to determine \\(\\alpha\\) is to minimize the risk (measured as the standard deviation of the portfolio return). For any \\(\\alpha\\), the portfolio return is given by \\[\\begin{equation*} r_P = \\alpha r_A + (1-\\alpha) r_B. \\end{equation*}\\] The variance of the portfolio return is \\[\\begin{equation*} \\sigma^2_P = \\alpha^2 \\sigma^2_A + (1-\\alpha)^2 \\sigma^2_B + 2\\alpha(1-\\alpha) \\sigma_{AB}, \\end{equation*}\\] where \\(\\sigma^2_A\\) and \\(\\sigma^2_B\\) are the variance of \\(r_A\\) and \\(r_B\\), respectively, and \\(\\sigma_{AB}\\) is the covariance between the two returns. Straightforward calculation shows that the minimizer of \\(\\sigma^2_P\\) is \\[\\begin{equation*} \\alpha = \\frac{\\sigma^2_A - \\sigma_{AB}}{\\sigma^2_A + \\sigma^2_B - 2\\sigma_{AB}}. \\end{equation*}\\] Since in reality \\(\\sigma^2_A, \\sigma^2_B, \\sigma_{AB}\\) are unknown, we have to estimate them using the past data. After obtain these estimates (denoted with the hat in the notation), the estimate of \\(\\alpha\\) will be given by \\[\\begin{equation*} \\hat{\\alpha} = \\frac{\\hat{\\sigma}^2_A - \\hat{\\sigma}_{AB}}{\\hat{\\sigma}^2_A + \\hat{\\sigma}^2_B - 2\\hat{\\sigma}_{AB}}. \\end{equation*}\\] It is natural to wish to quantify the accuracy of our estimate of \\(\\alpha\\). The bootstrap theory tells you that you can sample observations from the original data set repeatedly, compute the corresponding estimate of \\(\\alpha\\) and use the estimates of \\(\\alpha\\) to estimate the standard error of the original estimate \\(\\hat{\\alpha}\\). Nonparametric bootstrap algorithm: Sample with replacement \\(n\\) observations from the original data set Compute an estimate of \\(\\alpha\\). Repeat Step 1-2 \\(B\\) times to obtain \\(\\hat{\\alpha}^{(1)}, \\ldots,\\hat{\\alpha}^{(B)}\\). Compute the sample standard deviation of \\(\\hat{\\alpha}^{(1)}, \\ldots,\\hat{\\alpha}^{(B)}\\), denoted by \\(SE_B(\\hat{\\alpha})\\). Then, use \\(SE_B(\\hat{\\alpha})\\) as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. Use boot() from the package boot to implement bootstrap. To use boot(), we have to create a function which takes our data as input as well as a vector indicating which observations should be used. The output should be our estimate of interest. set.seed(1) rA &lt;- rnorm(100, 0.01, 0.05) rB &lt;- 0.5 * rA + rnorm(100, 0, 0.05) return_data &lt;- cbind(rA, rB) est_alpha &lt;- function(data, index) { rA &lt;- data[index, 1] rB &lt;- data[index, 2] (var(rB) - cov(rA, rB)) / (var(rA) + var(rB) - 2 * cov(rA, rB)) } boot(return_data, est_alpha, R = 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = return_data, statistic = est_alpha, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 0.6390998 0.0002071561 0.07488879 Without using boot(): B &lt;- 1000 alpha_b &lt;- rep(0, B) for (b in 1:B) { index &lt;- sample(1:100, replace = TRUE) alpha_b[b] &lt;- est_alpha(return_data, index) } sd(alpha_b) ## [1] 0.0717954 "],["regularization.html", "Chapter 17 Regularization 17.1 Ridge Regression 17.2 LASSO 17.3 Selecting the tuning parameter 17.4 glmnet", " Chapter 17 Regularization Reference: Ch6 in An introduction to Statistical Leraning with applications in R by James, Witten, Hastie and Tibshirani. An introduction toglmnet: https://glmnet.stanford.edu/articles/glmnet.html For theory behind the methods, study Statistical Learning I and II (STAT462, STAT457). Packages used: glmnet, ISLR2 Introduction: Regularization (or shrinkage) involves fitting a model where the estimated parameters are shrunken towards zero. The aim is to reduce overfitting and improve prediction accuracy. Depending on the regularization method, some of the parameters may be estimated to be exactly zero (which thus improve model interpretability). Hence, regularization methods can also perform variable selection. Variable selection here means the process of determining which variables to be included in your model. Examples that we have seen before are the forward stepwise regression, backward stepwise regression and the best subset selection. Regularization is not limited to linear regression models (although we describe this idea using linear regression in this chpater). It is a general method that is also used in other methods as well (e.g., logistic regression, deep learning) 17.1 Ridge Regression In linear regression, we minimize \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( y_i -\\beta_0 - \\sum^p_{j=1} \\beta_j x_{ij} \\bigg)^2. \\end{equation*}\\] In ridge regression, we minimize \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( y_i -\\beta_0 - \\sum^p_{j=1} \\beta_j x_{ij} \\bigg)^2 + \\lambda \\sum^p_{j=1} \\beta^2_j, \\end{equation*}\\] where \\(\\lambda \\geq 0\\) is a tuning parameter to be determined separately. The term \\(\\lambda \\sum_j \\beta^2_j\\) is called a shrinkage penalty. This penalty is small when \\(\\beta_j\\)s are close to \\(0\\). Thus, it has the effect of shrinking the estimates of \\(\\beta_j\\) towards \\(0\\). \\(\\lambda\\) is a tuning parameter that serves to control the relative impact of the shrinkage penalty. For each value of \\(\\lambda\\), there is a corresponding minimizer \\(\\hat{\\beta}^R_\\lambda\\) of the loss function above in the ridge regression. Note that we do not shrink the intercept \\(\\beta_0\\). Some properties of ridge regression: Efficient to compute Work also when \\(p &gt; n\\). That is, when the number of covariates is greater than the sample size (least squares does not work in this case) Does not select a particular model (i.e., include all \\(p\\) predictors in the final model) Ridge regression usually performs better than the least squares because of the bias-variance trade-off. As \\(\\lambda\\) increases, the bias increases but variance decreases. Ridge regression works best in situations where the least squares estimates have high variance 17.2 LASSO The lasso coefficients \\(\\hat{\\beta}^L_\\lambda\\) minimize the quantity \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( y_i -\\beta_0 - \\sum^p_{j=1} \\beta_j x_{ij} \\bigg)^2 + \\lambda \\sum^p_{j=1} |\\beta_j|. \\end{equation*}\\] Recall the \\(l_1\\) norm of the vector \\(\\beta\\) is given by \\(||\\beta||_1 = \\sum^p_{j=1}|\\beta_j|\\). The lasso penalty \\(\\lambda \\sum^p_{i=1}|\\beta_j|\\) is an \\(l_1\\) penalty. The ridge penalty is an \\(l_2\\) penalty. As with ridge regression, the lasso shrinks the coefficient estiamtes towards \\(0\\). However, the \\(l_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to \\(0\\). Hence, the lasso can be used to perform variable selection. Model generated from the lasso is easier to inerpret than those generated from ridge regression because of this. A sparse model is a model that only involves a fraction of all the variables. Thus, lasso yields sparse models. 17.3 Selecting the tuning parameter A standard way to choose the tuning parameter \\(\\lambda\\) is to use cross-validation. Steps: Choose a grid of \\(\\lambda\\) values Compute the cross-vlidation error for each value of \\(\\lambda\\) Select the tuning parameter value for which the cross-validation error is smallest Refit the model using all of the data and the selected value of the tuning parameter 17.4 glmnet To perform ridge regression and the lasso in R, we can use the glmnet package. The main function in this package is glmnet(). In this case of linear regression, glmnet() solves the problem \\[\\begin{equation*} \\min_{\\beta_0, \\beta} \\frac{1}{n} \\sum^n_{i=1} (y_i - \\beta_0 - x^T_i \\beta)^2 + \\lambda\\{ (1-\\alpha) ||\\beta||^2_2/2 + \\alpha ||\\beta||_1 \\}. \\end{equation*}\\] This penalty is called the elastic net penalty. When \\(\\alpha = 0\\), it becomes the ridge penalty. When \\(\\alpha = 1\\), it becomes the lasso penalty. We will illustrate how to perform ridge regression and lasso using Hitters data set from the package ISLR2. The data set contains \\(322\\) observations of major league players. The aim is to predict the salary of the plyer using other information about the players. The syntax of glmnet() is different from lm() or glm(). In particular, we have to provide the x and y separately in the function. In general, a data set may have some categorical variables. We can prepare the data set to be used in glmnet() using model.matrix(). The data set contains three factors: League, Division, NewLeague: library(ISLR2) # first remove rows with missing values Hitters &lt;- na.omit(Hitters) head(Hitters[, c(&quot;League&quot;, &quot;Division&quot;, &quot;NewLeague&quot;)]) ## League Division NewLeague ## -Alan Ashby N W N ## -Alvin Davis A W A ## -Andre Dawson N E N ## -Andres Galarraga N E N ## -Alfredo Griffin A W A ## -Al Newman N E A Use model.matrix to prepare the data. The first column is the intercept and we remove it when putting into glmnet(). x &lt;- model.matrix(Salary ~., Hitters)[, -1] y &lt;- Hitters$Salary head(x[, c(&quot;LeagueN&quot;, &quot;DivisionW&quot;, &quot;NewLeagueN&quot;)]) ## LeagueN DivisionW NewLeagueN ## -Alan Ashby 1 1 1 ## -Alvin Davis 0 1 0 ## -Andre Dawson 1 0 1 ## -Andres Galarraga 1 0 1 ## -Alfredo Griffin 0 1 0 ## -Al Newman 1 0 0 Generate random indexes to split the data into training and testing data: set.seed(1) index &lt;- sample(nrow(x), nrow(x) * 0.5) # use half of data as training data 17.4.1 Ridge Regression Perform ridge regression with cross-validation. The default option is \\(10\\)-fold CV. Note that by default, the variables will be scaled automatically in the function. library(glmnet) ridge_cv &lt;- cv.glmnet(x[index, ], y[index], alpha = 0) plot(ridge_cv) The above plot shows the cross-validation error (the mean squared error computed using the CV approach) with different values of \\(\\lambda\\) in the \\(\\ln\\) scale. We can pick the \\(\\lambda\\) that results in the smallest cross-validation error using the following code: best_lambda &lt;- ridge_cv$lambda.min Fit the ridge regression for the training data set with the best lambda and evaluate the test error. # ridge regression with a particular value of lambda ridge_best &lt;- glmnet(x[index, ], y[index], alpha = 0, lambda = best_lambda) # form predictions ridge_pred &lt;- predict(ridge_best, s = best_lambda, newx = x[-index, ]) # test error mean((ridge_pred - y[-index])^2) ## [1] 138800.2 Coefficients (when the variables are in the original scale): predict(ridge_best, type = &quot;coefficients&quot;)[1:ncol(x), ] ## (Intercept) AtBat Hits HmRun Runs RBI ## 71.16411532 0.05151174 0.31495026 2.49638228 0.68833226 0.73836757 ## Walks Years CAtBat CHits CHmRun CRuns ## 1.77444807 -0.48852879 0.01327951 0.06333713 0.62913292 0.12448162 ## CRBI CWalks LeagueN DivisionW PutOuts Assists ## 0.15070839 0.16896589 31.16003691 -77.93859127 0.09125514 0.06393766 ## Errors ## -0.33532271 Compare with the least squares approach ls_fit &lt;- lm(Salary ~ ., data = Hitters[index, ]) ls_pred &lt;- predict(ls_fit, Hitters[-index, ]) mean((ls_pred - y[-index])^2) ## [1] 168593.3 We can see that the test error obtained from ridge regression is smaller. 17.4.2 LASSO The code to perform LASSO is essentially the same as that in ridge regression. We only need to change the value of alpha to 1. lasso_cv &lt;- cv.glmnet(x[index, ], y[index], alpha = 1) plot(lasso_cv) # find the best lambda best_lambda &lt;- lasso_cv$lambda.min lasso_best &lt;- glmnet(x[index, ], y[index], alpha = 1, lambda = best_lambda) # form predictions lasso_pred &lt;- predict(lasso_best, s = best_lambda, newx = x[-index, ]) # test error mean((lasso_pred - y[-index])^2) ## [1] 144127.2 The test error obtained from lasso is also smaller than that obtained from the least squares approach. Coefficients (when the variables are in the original scale): predict(lasso_best, type = &quot;coefficients&quot;)[1:ncol(x), ] ## (Intercept) AtBat Hits HmRun Runs ## 126.13087838 0.00000000 0.00000000 3.49595499 0.00000000 ## RBI Walks Years CAtBat CHits ## 0.00000000 3.93919763 -15.95901778 0.00000000 0.23935217 ## CHmRun CRuns CRBI CWalks LeagueN ## 0.87111840 0.00000000 0.39819654 0.00000000 52.95398929 ## DivisionW PutOuts Assists Errors ## -139.60281921 0.12169771 0.14858018 -0.06371655 We can see that the lasso results in a model where the coefficient estimates are sparse: \\(7\\) of the estimates are exactly \\(0\\). Variable selection property of LASSO In the following toy example, the response is only related to the first five features \\(X_1,\\ldots,X_5\\): \\[\\begin{equation*} Y_i = 1 + 3(X_{i1} + \\ldots +X_{i5}) + \\varepsilon_i \\end{equation*}\\] We apply LASSO and see that LASSO can select the true model, that is, a linear model that only includes \\(X_1,\\ldots,X_5\\). set.seed(2) x &lt;- matrix(rnorm(500 * 20), nrow = 500, ncol = 20) y &lt;- 1 + rowSums(x[, 1:5]) * 3 + rnorm(500, 0, 0.1) # Perform CV to find the best lambda lasso_cv &lt;- cv.glmnet(x, y, alpha = 1) best_lambda &lt;- lasso_cv$lambda.min # Refit the data with the best lambda lasso_best &lt;- glmnet(x, y, lambda = best_lambda, alpha = 1) # Estimated regression coefficients predict(lasso_best, type = &quot;coefficients&quot;)[, 1] ## (Intercept) V1 V2 V3 V4 V5 ## 1.023367 2.926722 2.912596 2.917734 2.928092 2.921180 ## V6 V7 V8 V9 V10 V11 ## 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ## V12 V13 V14 V15 V16 V17 ## 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ## V18 V19 V20 ## 0.000000 0.000000 0.000000 "],["decision-trees.html", "Chapter 18 Decision Trees 18.1 Introduction to Classification Tree 18.2 Introduction to regression tree 18.3 Mathematical Formulation 18.4 Examples", " Chapter 18 Decision Trees Reference: Ch8 in An introduction to Statistical Leraning with applications in R by James, Witten, Hastie and Tibshirani. In this chapter, we will load the following packages: library(tidyverse) library(GGally) # to use ggpairs library(ISLR2) Decision trees involve stratifying or segmenting the predictor space into a number of simple regions To make a prediction for a new observation, we typically use the mean (for continuous response) or mode (for categorical response) response value of the training observations in the region to which the new observation belongs Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods. Decision trees can be applied to both regression and classification problems classification tree - for categorical outcomes regression tree - for continuous outcomes A key advantage of the decision tree is its interpretability (compared with other black box models, see Ch20). 18.1 Introduction to Classification Tree Below is a simplified example of determining if you should accept a new job offer using a tree representation. In each branching node of the tree, a specific feature of the data is examined. According to the value of the feature, one of the branches will be followed. The leaf node represents the class label. In the above example, the leaf nodes are Decline offer and Accept offer. The paths from the root to leaf node represent classification rules. Consider the iris dataset, which is available in base R. We have \\(150\\) observations and \\(5\\) variables named Sepal.Length, Sepal.Wdith, Petal.Length, Petal.Width, and Species. Lets take a look at the data. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... The first four variables are numeric features of the flowers. The last column in the dataset is the species of the flower. We will illustrate how to use the numeric features of the flowers to classify the flowers. First, we will split the dataset into a training dataset and a testing dataset. For the moment, we will only use the training dataset. set.seed(6) random_index &lt;- sample(nrow(iris), 100) # select 100 examples for our training dataset iris_train &lt;- iris[random_index, ] iris_test &lt;- iris[-random_index, ] It can be helpful to look at the scatterplot matrix when you wish to look at the relationship between each pair of variables. To create a scatterplot matrix, we can use pairs (base R graphics) or ggpairs (in the package GGally). With pairs(): # we only want to plot the numeric features # the 5th column is the species of the flowers pairs(iris_train[, -5]) With ggpairs(), you can also visualize an additional category by specifying color = with your variable describing the category. library(GGally) # to use ggpairs, install it first if you haven&#39;t done so ggpairs(data = iris_train, aes(color = Species, alpha = 0.8), columns = 1:4) In the above code: columns = 1:4 tells the function only uses the first \\(4\\) columns of the dataset for plotting. color = Species indicates that we map the color to the variable Species alpha = 0.8 controls the level of transparency so that the density estimates do not overlap each other completely From the plots, it seems that the variable Petal.Width and Petal.Length will be useful in classifying the flowers. Lets focus on the scatterplot of these two variables. ggplot(iris_train, aes(x = Petal.Width, y = Petal.Length, color = Species)) + geom_point() From the above plot, we see that we can easily classify the flowers with high accuracy using these two features. For example, if we draw the horizontal line \\(y = 1.9\\), we could classify any iris lying on or below this line as setosa with \\(100\\%\\) accuracy. Next, we draw the vertical line \\(x = 1.7\\). We could classify any iris lying on or to the left of this line as versicolor. We get a pretty good classification with only 3 mistakes. The above classification is an example of classification trees (because the outcome is categorical) and can be visualized as 18.2 Introduction to regression tree To describe the regression trees, we consider the dataset Hitters from the package ISLR2, a baseball player dataset with \\(322\\) observations of major league players from the 1986 and 1987 seasons. In this dataset, there are \\(20\\) variables. library(ISLR2) str(Hitters) ## &#39;data.frame&#39;: 263 obs. of 20 variables: ## $ AtBat : int 315 479 496 321 594 185 298 323 401 574 ... ## $ Hits : int 81 130 141 87 169 37 73 81 92 159 ... ## $ HmRun : int 7 18 20 10 4 1 0 6 17 21 ... ## $ Runs : int 24 66 65 39 74 23 24 26 49 107 ... ## $ RBI : int 38 72 78 42 51 8 24 32 66 75 ... ## $ Walks : int 39 76 37 30 35 21 7 8 65 59 ... ## $ Years : int 14 3 11 2 11 2 3 2 13 10 ... ## $ CAtBat : int 3449 1624 5628 396 4408 214 509 341 5206 4631 ... ## $ CHits : int 835 457 1575 101 1133 42 108 86 1332 1300 ... ## $ CHmRun : int 69 63 225 12 19 1 0 6 253 90 ... ## $ CRuns : int 321 224 828 48 501 30 41 32 784 702 ... ## $ CRBI : int 414 266 838 46 336 9 37 34 890 504 ... ## $ CWalks : int 375 263 354 33 194 24 12 8 866 488 ... ## $ League : Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 2 1 2 2 1 2 1 2 1 1 ... ## $ Division : Factor w/ 2 levels &quot;E&quot;,&quot;W&quot;: 2 2 1 1 2 1 2 2 1 1 ... ## $ PutOuts : int 632 880 200 805 282 76 121 143 0 238 ... ## $ Assists : int 43 82 11 40 421 127 283 290 0 445 ... ## $ Errors : int 10 14 3 4 25 7 9 19 0 22 ... ## $ Salary : num 475 480 500 91.5 750 ... ## $ NewLeague: Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 2 1 2 2 1 1 1 2 1 1 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:59] 1 16 19 23 31 33 37 39 40 42 ... ## ..- attr(*, &quot;names&quot;)= chr [1:59] &quot;-Andy Allanson&quot; &quot;-Billy Beane&quot; &quot;-Bruce Bochte&quot; &quot;-Bob Boone&quot; ... We will consider the problem of predicting players Salary (in the log scale so that its distribution has more of a typical bell-shape) based on the other variables. In particular, to illustrate the regression trees, we shall first use only two variables Years and Hits. Years: the number of years that the player has played in the major leagues Hits: the number of hits that he made in the previous year The following figure shows an example of regression tree using Years and Hits. The split at the top of the tree results in two large branches. The left-hand branch corresponds to Years &lt; 4.5, and the right-hand branch corresponds to Years &gt;= 4.5. The tree has two internal nodes and three terminal nodes, or leaves. The number in each leaf is the mean of the response for the observations that fall there. For players with Years &lt; 4.5, the mean log salary is \\(5.11\\) and so we make a prediction of \\(e^{5.11}\\) (thousands of dollars). Players with Years &gt;= 4.5 are assigned to the right branch, and that group is further subdidived by Hits. The prediction of salary for players with Years &gt;= 4.5 and Hits &lt; 117.5 is \\(e^{6}\\) and the prediction of salary for players with Years &gt;= 4.5 and Hits &gt;= 117.5 is \\(e^{6.74}\\). The following figure shows that partition of the predictor space: 18.3 Mathematical Formulation We first describe the regression tree. The idea behind a regression tree consists of two steps: Divide the predictor space, which is the set of possible values for the \\(p\\) features \\(X_1,\\ldots,X_p\\), into \\(J\\) distinct and non-overlapping regions \\(R_1,\\ldots,R_J\\) For each observation that falls into the region \\(R_j\\), we make the same prediction, usually using the mean of the response values for the training observations in \\(R_j\\) (because it minimizes the sum of squares error). The first step is however computationally untractable if we allow arbitrary shapes of non-overlapping regions. First simplification: divide the predictor space into high-dimensional rectangles or boxes only The goal then is to find boxes \\(R_1,\\ldots,R_J\\) that minimize the RSS \\[\\begin{equation*} \\sum^J_{j=1} \\sum_{i : x_i \\in R_j} (y_i - \\hat{y}_{R_j})^2, \\end{equation*}\\] where \\(\\hat{y}_{R_j}\\) is the mean of the response for the training observation in \\(R_j\\). Second simplification: since it is also computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes, we have to take a top-down, greedy approach that is known as recursive binary splitting. Steps: For any \\(j\\) and \\(s\\), define the pair of half-planes \\[\\begin{equation*} R_1(j, s) := \\{X| X_j &lt; s\\} \\text{ and } R_2(j, s) := \\{X|X_j \\geq s \\}, \\end{equation*}\\] and we seek the value of \\(j\\) and \\(s\\) that minimize the expression \\[\\begin{equation*} \\sum_{i: x_i \\in R_1(j, s)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i : x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2, \\end{equation*}\\] where \\(\\hat{y}_{R_k}\\) is the mean response for the training observations in \\(R_k(j, s)\\) for \\(k=1,2\\). Next, we repeat the above process within each of the resulting regions. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than \\(5\\) observations. Finally, tree pruning is performed to avoid the model becomes too complex, which overfits the training data and leads to poor test performance. Remarks: The idea of tree pruning is similar to regularization in Ch17. We add an additional tuning parameter that penalizes a tree with many terminal nodes (large tree) and find the optimal tuning parameter using cross-validation. The details are omitted. We can grow a classification tree similar to a regression tree except that our prediction of each new observation is now the most commonly occurring class of training observations in the region to which it belongs we replace the RSS by classification error rate/ Gini index/ entropy (see the reference for details) 18.4 Examples 18.4.1 Classification Tree Lets begin with the iris dataset. To build a classification tree, we can use the function tree() from the package tree. The syntax of tree() is similar to that of lm(). library(tree) # split the data into training data and testing data set.seed(10) index &lt;- sample(nrow(iris), 100) tree_iris &lt;- tree(Species ~. , data = iris[index, ]) # display the result tree_iris ## node), split, n, deviance, yval, (yprob) ## * denotes terminal node ## ## 1) root 100 217.70 virginica ( 0.29000 0.31000 0.40000 ) ## 2) Petal.Length &lt; 4.75 58 80.41 setosa ( 0.50000 0.50000 0.00000 ) ## 4) Petal.Length &lt; 2.5 29 0.00 setosa ( 1.00000 0.00000 0.00000 ) * ## 5) Petal.Length &gt; 2.5 29 0.00 versicolor ( 0.00000 1.00000 0.00000 ) * ## 3) Petal.Length &gt; 4.75 42 16.08 virginica ( 0.00000 0.04762 0.95238 ) ## 6) Petal.Width &lt; 1.75 5 6.73 virginica ( 0.00000 0.40000 0.60000 ) * ## 7) Petal.Width &gt; 1.75 37 0.00 virginica ( 0.00000 0.00000 1.00000 ) * The summary() function lsts the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate. summary(tree_iris) ## ## Classification tree: ## tree(formula = Species ~ ., data = iris[index, ]) ## Variables actually used in tree construction: ## [1] &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## Number of terminal nodes: 4 ## Residual mean deviance: 0.07011 = 6.73 / 96 ## Misclassification error rate: 0.02 = 2 / 100 Plot the tree: plot(tree_iris) text(tree_iris, pretty = 0) Prediction and accuracy: # prediction predict_iris &lt;- predict(tree_iris, iris[-index, ], type = &quot;class&quot;) (result &lt;- table(iris[-index, 5], predict_iris)) ## predict_iris ## setosa versicolor virginica ## setosa 21 0 0 ## versicolor 0 15 4 ## virginica 0 1 9 # accuracy sum(diag(result)) / sum(result) ## [1] 0.9 Now, lets consider Hitters dataset from the package ISLR2. We have used this dataset in Ch17. We will create an additional categorical variable indicating if Salary is higher than the upper quartile of Salary library(ISLR2) # remove rows with missing values Hitters &lt;- na.omit(Hitters) # create an additional variable Hitters$High &lt;- factor(ifelse(Hitters$Salary &gt;= quantile(Hitters$Salary, 0.75), &quot;High&quot;, &quot;Low&quot;)) # split the dataset set.seed(1) index &lt;- sample(nrow(Hitters), nrow(Hitters) * 0.5) Hitters_train &lt;- Hitters[index, ] Hitters_test &lt;- Hitters[-index, ] # fit classification tree fit &lt;- tree(High ~. - Salary, data = Hitters_train) summary(fit) ## ## Classification tree: ## tree(formula = High ~ . - Salary, data = Hitters_train) ## Variables actually used in tree construction: ## [1] &quot;CAtBat&quot; &quot;CWalks&quot; &quot;AtBat&quot; &quot;Hits&quot; &quot;CRuns&quot; &quot;PutOuts&quot; &quot;Assists&quot; ## Number of terminal nodes: 8 ## Residual mean deviance: 0.3054 = 37.56 / 123 ## Misclassification error rate: 0.08397 = 11 / 131 (result &lt;- table(Hitters_test$High, predict(fit, Hitters_test, type = &quot;class&quot;))) ## ## High Low ## High 20 12 ## Low 13 87 sum(diag(result)) / sum(result) ## [1] 0.8106061 The classification accuracy in the training data is \\((131 - 11)/131 = 0.916\\). The classification accuracy in the testing data is \\(0.811\\). Visualize the classification rules using tree: plot(fit) text(fit) Lets try to see if pruning the tree will improve the testing accuracy. Set FUN = prune.misclass to tell the function to use misclassification error as the evaluation metric in CV. cv_fit &lt;- cv.tree(fit, FUN = prune.misclass) cv_fit ## $size ## [1] 8 4 3 1 ## ## $dev ## [1] 27 27 20 40 ## ## $k ## [1] -Inf 0 1 13 ## ## $method ## [1] &quot;misclass&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;prune&quot; &quot;tree.sequence&quot; plot(cv_fit$size, cv_fit$dev, type = &quot;b&quot;) size: number of terminal nodes of the tree dev: cross-validation errors We will choose the size with the minimum dev. prune_fit &lt;- prune.misclass(fit, best = 3) (result &lt;- table(Hitters_test$High, predict(prune_fit, Hitters_test, type = &quot;class&quot;))) ## ## High Low ## High 21 11 ## Low 16 84 sum(diag(result)) / sum(result) ## [1] 0.7954545 For this particular example, the classification accuracy actually becomes lower when we prune the tree. However, the pruned tree has only \\(3\\) terminal nodes, which can be easier for interpretation. plot(prune_fit) text(prune_fit) 18.4.2 Regression Tree Fitting regression tree and finding the MSE in test data # Fit regression tree reg_tree &lt;- tree(Salary ~. - High, Hitters_train) # MSE in test data mean((Hitters_test$Salary - predict(reg_tree, Hitters_test))^2) ## [1] 122872.5 Tree without pruning plot(reg_tree) text(reg_tree) Use cross validation to determine the optimal choice of size (the number of terminal nodes) set.seed(3) # the result of CV is also random because we partition the data randomly cv &lt;- cv.tree(reg_tree) plot(cv$size, cv$dev, type = &quot;b&quot;) Prune the tree: # best size cv$size[which.min(cv$dev)] ## [1] 5 prune_reg_tree &lt;- prune.tree(reg_tree, best = cv$size[which.min(cv$dev)]) # MSE in test data mean((Hitters_test$Salary - predict(prune_reg_tree, Hitters_test))^2) ## [1] 122267.2 Pruned tree: plot(prune_reg_tree) text(prune_reg_tree) The performance (in terms of MSE) of the pruned tree in test data is similar to the tree without pruning. "],["ensemble-methods.html", "Chapter 19 Ensemble Methods 19.1 Bagging 19.2 Random Forest 19.3 Example", " Chapter 19 Ensemble Methods Reference: Ch8 in An introduction to Statistical Leraning with applications in R by James, Witten, Hastie and Tibshirani. For more details, study STAT457/ 462. Package used: library(ISLR2) library(randomForest) library(tidyverse) library(tree) Ensemble methods involve pooling together the predictions of a set of many models. In this chapter, we illustrate this idea by pooling together many decision trees, resulting in a method called random forest. However, it is important to note that ensemble method is a generally method that can be applied not only to tree-based methods but virtually any models. For example, you can combine the predictions from a neural network and a random forest. In fact, many of the winners of the machine-learning competitions on Kaggle use very large ensembles of models. Remark: of course, to get a good performance, the weights used to combine the models should be optimized on the validation data. The main idea of ensemble methods is to combine models that are as good as possible while being as different as possible. Analogy: combining experts from different fields to solve a difficult problem is usually more effective than one single expert or a group of experts in the same field. 19.1 Bagging Bagging = Bootstrap aggregation a general-purpose procedure for reducing the variance of a statistical learning method Suppose we can compute a prediction \\(\\hat{f}(x)\\) given data \\(\\{(x_i, y_i)\\}^n_{i=1}\\). Bagging steps: Sample with replacement \\(n\\) data from \\(\\{(x_i, y_i)\\}^n_{i=1}\\). Denote the resampled data to be \\(\\{(x^{(b)}_i, y^{(b)}_i)\\}^n_{i=1}\\). Compute \\(\\hat{f}^{(b)}(x)\\) using \\(\\{(x^{(b)}_i, y^{(b)}_i)\\}^n_{i=1}\\). Repeat Step 1-2 \\(B\\) times to obtain \\(\\hat{f}^{(b)}(x)\\) for \\(b =1,\\ldots,B\\). Final model is \\[\\begin{equation*} \\hat{f}_{Bagging}(x) = \\frac{1}{B} \\sum^B_{i=1} \\hat{f}^{(b)}(x). \\end{equation*}\\] 19.2 Random Forest Bagging decision trees uses the same model and variables repeatedly. Thus, the models lack diversity. In fact, the bagged trees are highly correlated. To improve the prediction accuracy, we want to combine trees that are different. Random forest includes a small tweak that decorrelates the trees used in the ensemble. Ideas of random forest: As in bagging, a number of decision trees are build on boostrapped training samples. But when building these decision trees, each time a split in a tree is considered, only a random sample of \\(m\\) predictors is chosen as split candidates from the full set of \\(p\\) predictors. A fresh sample of \\(m\\) predictors is taken each split. In this way, the correlation between the predictions from the trees will be reduced because each tree is built using only a subset of predictors at each split. Remark: when \\(m = p\\), random forest is the same as bagging decision tree. 19.3 Example We will use the Hitters dataset from ISLR2 to illustrate how to perform random forest with the function randomForest() in the package randomForest. Hitters &lt;- na.omit(Hitters) # split the dataset set.seed(1) index &lt;- sample(nrow(Hitters), nrow(Hitters) * 0.5) Hitters_train &lt;- Hitters[index, ] Hitters_test &lt;- Hitters[-index, ] Fitting a random forest: rf_fit &lt;- randomForest(Salary ~., data = Hitters_train, mtry = (ncol(Hitters_train) - 1)/ 3, ntree = 1000, importance = TRUE) Two important parameters: mtry: Number of variables randomly sampled as candidates at each split. The default values for classification and regression are \\(\\sqrt{p}\\) and \\(p/3\\), respectively. ntree: Number of trees to grow. The default is \\(500\\). More trees will require more time. Prediction: rf_pred &lt;- predict(rf_fit, Hitters_test) mean((Hitters_test$Salary - rf_pred)^2) ## [1] 62219.41 Compared with least squares: ls_fit &lt;- lm(Salary ~., data = Hitters_train) ls_pred &lt;- predict(ls_fit, Hitters_test) mean((Hitters_test$Salary - ls_pred)^2) ## [1] 93373.95 Compared with one tree: # Fit regression tree tree_fit &lt;- tree(Salary ~., Hitters_train) # MSE in test data tree_pred &lt;- predict(tree_fit, Hitters_test) mean((Hitters_test$Salary - tree_pred)^2) ## [1] 83073.2 A plot showing the predictions by different methods and the corresponding observed values. ## [1] FALSE Several observations: The regression tree can only use a few distinct values as the prediction values (the number of such values equals the number of terminal nodes). Hence, you can see the blue points are all located at several vertical lines. Linear regression can produce predictions with values depending on the features. Thus, the points will not lie on several vertical lines. The closer the points are to the diagonal line, the better the predictions are. For observed values that are small, random forest is doing a much better job than linear regression. Variable Importance Plot Random forest typically improves the accuracy over predictions using a single tree. However, it can be difficult to interpret the resulting model, losing the advantage of using a decision tree. On the other hand, one can still obtain an overall summary of the importance of each feature using the RSS (for regression problems) or the Gini index (for classification problems). Basically, we can record the total amount that the measure (RSS or Gini index) decreases due to splits over a given predictor, averaged over all \\(B\\) trees. A large value indicates an important predictor. This importance measure is given in the second column of importance(rf_fit). importance(rf_fit) ## %IncMSE IncNodePurity ## AtBat 3.35182212 524974.45 ## Hits 1.13094383 611669.31 ## HmRun 2.85844392 631697.80 ## Runs 2.65375662 712993.07 ## RBI 0.91376847 858711.10 ## Walks 5.10158070 938898.67 ## Years 5.38718804 295859.22 ## CAtBat 12.34565397 1501003.50 ## CHits 12.66787049 1861108.04 ## CHmRun 4.50720662 1086255.87 ## CRuns 13.31276946 2103022.87 ## CRBI 11.11503712 2101151.04 ## CWalks 9.40813964 1447656.21 ## League 0.57926429 20002.08 ## Division 2.04714437 45064.15 ## PutOuts 3.67103105 296237.51 ## Assists -0.26947886 152545.23 ## Errors -0.07960768 142009.27 ## NewLeague 0.11678271 23564.81 ## High 29.37244111 7814171.92 Visualizing the importance of the features: varImpPlot(rf_fit) "],["deep-learning.html", "Chapter 20 Deep Learning 20.1 Introduction 20.2 Single Layer Neural Netowrks 20.3 Multilayer Neural Networks 20.4 Examples", " Chapter 20 Deep Learning References: Ch10: in An introduction to Statistical Leraning with applications in R by James, Witten, Hastie and Tibshirani (some basic introduction with some math equations) Deep Learning with R by Francois Chollet with J.J. Allaire. (no math equations but offer practical guildlines in implementation) https://tensorflow.rstudio.com/tutorials/beginners/ 20.1 Introduction In the past few years, artificial intelligence (AI), machine learning, deep learning have been subjects of intense media hype. One may define AI as the study to automate intellectual tasks normally performed by humans. AI encompasses machine learning but also includes other approaches that dont involve any learning. For example, early chess programs only involved hardcoded rules crafted by programmers. For a fairly long time, many experts believed that human-level AI could be achieved by having programmers handcraft a sufficiently large set of explicit rules for manipulating knowledge. This approach is known as symbolic AI. It turns out it is intractable to figure out explicit rules for solving more complex problems, such as image classification, speech recognition, and language translation. A new appraoch is to use machine learning. You have already studied various machine learning methods and how to apply them with R: \\(k\\)-NN, linear regression, logistic regression, \\(k\\)-means clustering, hierarchical clustering, ridge regression, LASSO, decision trees, random forest. These approaches are also examples of shallow learning. So, what is deep learning? Deep learning is a subset of machine learning The deep in deep learning refers to the idea of successive layers of representations (see next section) These layered representations are learned via neural networks, which is the cornerstone of deep learning Some important factors that drive the advances in machine learning Hardware: faster CPU (central processing units) and GPU (graphics processing unit), TPU (tensor processing unit) Datasets: the rise of internet allows people to collect many data Benchmarks: competitions such as Kaggle allow people to have benchmarks that researchers compete to beat Algorithmic advances 20.2 Single Layer Neural Netowrks A neural network takes an input vector of \\(p\\) variables \\(X = (X_1,\\ldots,X_p)\\) and builds a nonlinear function \\(f(X)\\) to predict the response \\(Y\\). A linear function is a function of the form \\[f(X) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p.\\] In general, a linear function is not enough to describe the complex relationship between \\(X\\) and \\(Y\\). Graphical illustration of a single layer neural network: Input layer: consists of \\(p\\) input variables (the picture shows \\(4\\) but we can have many more) Hidden layer: consists of \\(K\\) hidden units (the picture shows \\(5\\) but we can have many more) Output layer: prediction of the response (we can have more than one output variable) Activations \\(A_k\\), \\(k=1,\\ldots,K\\) are computed as functions of the input features \\[\\begin{equation*} A_k = g\\left(w_{k0} + \\sum^p_{j=1} w_{kj}X_j \\right), \\end{equation*}\\] where \\(g\\) is a nonlinear activation function that is specified in advance and the weights \\(w_{k0},\\ldots,w_{kp}\\) have to be estimated from the data. For the function \\(g\\), it is common to use ReLU (rectified linear unit), which takes the form \\[\\begin{equation*} g(z) = z I(z \\geq 0). \\end{equation*}\\] With nonlinear activation functions, it is possible for the model to capture complex nonlinearities and interaction effects (e.g.Â \\(X_1 X_2\\)). Representation: We can think of each \\(A_k\\) as a different transformation \\(h_k(X) = g\\left(w_{k0} + \\sum^p_{j=1} w_{kj}X_j \\right)\\) of the original features. For regression problem, the output is \\[\\begin{equation*} f(X) = \\beta_0 + \\sum^K_{k=1} \\beta_k A_k. \\end{equation*}\\] If you want to write in terms of the original \\(X\\): \\[\\begin{equation*} f(X) = \\beta_0 + \\sum^K_{k=1} \\beta_k g\\left(w_{k0} + \\sum^p_{j=1} w_{kj}X_j \\right). \\end{equation*}\\] Estimation Parameters in the model: \\(\\beta_0,\\ldots,\\beta_K, w_{10},\\ldots,w_{Kp}\\). Suppose we have \\(n\\) data \\(\\{ (x_i, y_i)\\}^n_{i=1}\\). To estimate these parameters, the squared-error loss is typically used. That is, we wish to find parameters to minimize \\[\\begin{equation*} \\sum^n_{i=1} (y_i - f(x_i))^2. \\end{equation*}\\] We will describe how to use neural network in classification problems in the next section. Remark The name neural network originally derived from thinking of the model as analogous to neurons in the brain. But there is no need to think in that way. 20.3 Multilayer Neural Networks Modern neural networks typically have more than one hidden layer, and many units per layer. In general, you can also have more than one output variable. In particular, if you have a classification problem at hand with \\(M\\) classes. You will need \\(M\\) output variables to model the probabilities of the response being in the \\(M\\) classes. A graphical illustration of a neural network with \\(2\\) hidden layers: Input layer: consists of \\(p\\) input variables \\(1\\)st Hidden layer: consists of \\(K_1\\) hidden units \\(2\\)st Hidden layer: consists of \\(K_2\\) hidden units Output layer: prediction of the responses The activations in the first hidden layer: \\[\\begin{equation*} A^{(1)}_k = g\\left(w^{(1)}_{k0} + \\sum^p_{j=1} w^{(1)}_{kj} X_j\\right) \\end{equation*}\\] for \\(k=1,\\ldots,K_1\\). The activations in the second hidden layer treats the activations \\(A^{(1)}_k\\) of the first hidden layer as inputs and computes new activations: \\[\\begin{equation*} A^{(2)}_k = g\\left(w^{(2)}_{k0} + \\sum^{K_1}_{k=1} w^{(2)}_{kj} A_k^{(1)}\\right) \\end{equation*}\\] Output layer: For regression problem: \\[\\begin{equation*} f_m(X) = \\beta_{m0} + \\sum^{K_2}_{l=1} \\beta_{ml} A^{(2)}_l,\\quad \\text{for } m = 1,\\ldots,M. \\end{equation*}\\] In terms of the original \\(X\\), we have \\[\\begin{equation*} f_m(X)= \\beta_{m0} + \\sum^{K_2}_{l=1} \\beta_{ml} g\\left(w^{(2)}_{k0} + \\sum^{K_1}_{k=1} w^{(2)}_{kj} g\\left(w^{(1)}_{k0} + \\sum^p_{j=1} w^{(1)}_{kj} X_j\\right)\\right). \\end{equation*}\\] For classification problem, we do not directly output the predicted class but we try to predict the probability \\(P(Y=m|X)\\). Hence, we use \\[\\begin{equation*} Z_m = \\beta_{m0} + \\sum^{K_2}_{l=1} \\beta_{ml} A^{(2)}_l,\\quad \\text{for } m = 1,\\ldots,M \\end{equation*}\\] and set \\[\\begin{equation*} f_m(Z) = \\frac{e^{Z_m}}{\\sum^M_{m=1} e^{Z_m}}, \\text{for } m=1,\\ldots,M. \\end{equation*}\\] The function in the last displayed equation is called the softmax activation function. This ensures all the outputs behave like probabilities (non-negative and sum to one). For classification problem, we use the one-hot encoding for \\(y_i\\) so that \\(y_{im} = 1\\) if \\(y_i = m\\) and \\(y_{il} = 0\\) for \\(l \\neq m\\). Then we estimate the parameters using the cross-entropy: \\[\\begin{equation*} - \\sum^n_{i=1} \\sum^M_{m=1} y_{im} \\log (f_m(x_i)), \\end{equation*}\\] which is the negative of the likelihood of a multinomial distribution. Remark The multinomial distribution is a generalization of the binomial distribution. Recall that a binomial distribution models the number of successes in \\(n\\) independent Bernoulli trials. In a Bernoulli trial, there are only two possible outcomes, success and failure. When we have \\(M\\) classes and \\(n\\) independent trials, we obtain a multinomial distribution. A special case is when we only have one trial. In that case, it is called a categorical distribution and the probability mass function is \\[\\begin{equation*} P(Y = m) = p_m, \\quad m =1,\\ldots,M, \\end{equation*}\\] where \\(p_m \\geq 0, \\sum^M_{m=1} p_m = 1\\). When you observe \\(y_i\\), the likelihood becomes \\[\\begin{equation*} \\prod^M_{m=1} p_m^{I(y_i = m)}. \\end{equation*}\\] The log-likelihood is \\[\\begin{equation*} \\sum^M_{m=1} I(y_i = m ) \\log p_m = \\sum^M_{m=1} y_{im} \\log p_m. \\end{equation*}\\] 20.4 Examples 20.4.1 MNIST We will use Keras, a deep-learning framework that provides a convenient way to define and train deep-learning models. One of the backend implementations in Keras is TensorFlow, which is one of the primary platforms for deep learning today. Almost every recent deep-learning competition has been won using Keras models. For serious deep learning users, need to train deep learning models with GPUs. To install Keras, follow the installation guide on https://tensorflow.rstudio.com/guide/keras/ We will fit a neural network to recognize handwritten digits from the MNIST dataset. MNIST consists of \\(28 \\times 28\\) greayscale images of handwritten digits like these: The MNIST dataset is a classic dataset in the machine-learning community which has been around almost as long as the field itself and has been intensively studied. Its a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of solving MNIST as the Hello World of deep learningits what you do to verify that your algorithms are working as expected. Load the data: library(keras) mnist &lt;- dataset_mnist() train_images &lt;- mnist$train$x train_labels &lt;- mnist$train$y test_images &lt;- mnist$test$x test_labels &lt;- mnist$test$y The images are encoded as 3D arrays, and the labels are a 1D array of digits, ranging from \\(0\\) to \\(9\\). There is a one-to-one correspondence between the images and the labels. To view the data of the first image (which is a \\(28 \\times 28\\) matrix of integers in \\([0, 255]\\)): # this is how we access the 1st element in the 1st dimension of an 3D array train_images[1, , ] 20.4.2 Encoding the data For each image, we first turn the matrix of values into a vector of values. We also rescale the data to have values in \\([0, 1]\\). The data originally take integer values in \\([0, 255]\\). train_images &lt;- array_reshape(train_images, c(60000, 28 * 28)) train_images &lt;- train_images / 255 test_images &lt;- array_reshape(test_images, c(10000, 28 * 28)) test_images &lt;- test_images / 255 The original labels take integer values from \\(0\\) to \\(9\\). We will apply one-hot encoding to the labels: train_labels &lt;- to_categorical(train_labels) test_labels &lt;- to_categorical(test_labels) For example, if there are only \\(3\\) classes labeled as \\(1,2,3\\) and \\(y = 2\\). Then we want to turn \\(y\\) into \\((0, 1, 0)\\). If \\(y = 1\\), we want to turn it into \\((1, 0, 0)\\). 20.4.3 Implementation Model specification model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 512, activation = &quot;relu&quot;, input_shape = c(28 * 28)) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) The above code means we specify a model with One hidden layer with \\(512\\) units, the activation function is ReLU (rectified linear unit) The output layer has \\(10\\) units, the activation is the softmax function because we are dealing with a multiclass classification problem. Compiling the model model %&gt;% compile( optimizer = &quot;rmsprop&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = c(&quot;accuracy&quot;) ) For multiclass classification, the above setting is in general ok. Setting aside a validation set Setting aside a validation set to choose the number of epochs (each iteration over all the training data is called an epoch). The number of epochs is an important tunning parameter that one has to decide in order not to overfit the training data. Note: since we have a lot of data, we use a simple validation approach instead of the cross-validation. index &lt;- sample(dim(train_images)[1], 10000) partial_train_images &lt;- train_images[-index, ] partial_train_labels &lt;- train_labels[-index, ] validation_images &lt;- train_images[index, ] validation_labels &lt;- train_labels[index, ] Train the model with \\(15\\) epochs and evaluate the loss using the validation data: history &lt;- model %&gt;% fit( partial_train_images, partial_train_labels, epochs = 15, bitch_size = 128, validation_data = list(validation_images, validation_labels) ) Create a plot to visualize the relationship between the number of epoch and loss as well as accuracy in the validation data (and training data). plot(history) From the above plot, we can see that the training error keeps on decreasing when we train the model with more epochs. However, it seems that after \\(4\\) epochs, the model starts to overfit the data as the validation loss starts to increase. Therefore, we shall only train the model with \\(4\\) epochs. Now, we will retrain our model using all the training data but set epochs = 4. # same as before model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 512, activation = &quot;relu&quot;, input_shape = c(28 * 28)) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) # same as before model %&gt;% compile( optimizer = &quot;rmsprop&quot;, loss = &quot;categorical_crossentropy&quot;, metrics = c(&quot;accuracy&quot;) ) # no need to use validation data now model %&gt;% fit( partial_train_images, partial_train_labels, epochs = 4, bitch_size = 128 ) To form the predictions: # prediction prediction &lt;- model %&gt;% predict(test_images) # these are the predicted probabilities of each class for the 1st observation prediction[1, ] ## [1] 5.935057e-14 3.829296e-17 7.510276e-10 2.582575e-09 4.891600e-17 1.275439e-14 ## [7] 1.665955e-23 1.000000e+00 1.632768e-14 2.574293e-09 Evaluate the performance: result &lt;- model %&gt;% evaluate(test_images, test_labels) result ## loss accuracy ## 0.08298111 0.97899997 We see that our basic neural network model is able to achieve more than \\(97\\%\\) accuracy in a \\(10\\)-class classification problem. 20.4.4 Multilayer Neural Network If you want to fit a model with two hidden layers, where the first hidden layer has \\(512\\) units and the second layer has \\(128\\) units: model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 512, activation = &quot;relu&quot;, input_shape = c(28 * 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;, input_shape = c(28 * 28)) %&gt;% layer_dense(units = 10, activation = &quot;softmax&quot;) Now, you should understand how to include more layers and change the number of units. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
