[["index.html", "STAT 362 R for Data Science Syllabus", " STAT 362 R for Data Science Brian Ling 2022-01-10 Syllabus STAT 362 R for Data Science Department of Mathematics and Statistics, Queens University Course Description: Introduction to R, data creation and manipulation, data import and export, scripts and functions, control flow, debugging and profiling, data visualization, statistical inference, Monte Carlo methods, decision trees, support vector machines, neural network, numerical methods. For details, see onQ "],["introduction.html", "Chapter 1 Introduction 1.1 What is R and RStudio? 1.2 What will you learn in this course? 1.3 Lets Get Started 1.4 R Data Structures 1.5 Operators 1.6 Built-in Functions 1.7 Some Useful RStudio Shortcuts 1.8 Exercises 1.9 Comments to Exercises", " Chapter 1 Introduction 1.1 What is R and RStudio? R R is a language and environment for statistical computing and graphics. R is an interpreted language (individual language expressions are read and then executed immediately as soon as the command is entered) To download R, go to https://cloud.r-project.org/ RStudio is an integrated development environment (IDE) for R programming Install R first, then go to https://rstudio.com/products/rstudio/download/ and download RStudio While you can work in R directly, it is recommended to work in RStudio. 1.2 What will you learn in this course? Note: we do not assume you know R or any programming language before. 1.2.1 R and R as a programming language operators control flow (if..else.., for loop) defining a function 1.2.2 Data Wrangling Data wrangling = the process of tidying and transforming the data 1.2.3 Data Visualization Graphs are powerful to illustrate features of the data. You will learn how to create some basic plots as well as using the package ggplot2 to create more elegant plots. Consider a dataset about cars. library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 4.0.3 ## Need help? Try Stackoverflow: https://stackoverflow.com/tags/ggplot2 mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compact ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 audi a4 quattro 1.8 1999 4 manual(m5) 4 18 26 p compact ## 9 audi a4 quattro 1.8 1999 4 auto(l5) 4 16 25 p compact ## 10 audi a4 quattro 2 2008 4 manual(m6) 4 20 28 p compact ## # ... with 224 more rows Among the variables in mpg are: displ, a cars engine size, in litres. hwy, a cars fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. Scatterplot Scatterplot, points are labeled with colors according to the class variable Scatterplots Line Chart Bar chart Another Bar Chart Boxplot Histogram 1.2.4 Statistical Inference Many problems in different domains can be formulated into hypothesis testing problems. Are university graduates more likely to vote for Candidate A? Is a treatment effective in reducing weights? Is a drug effective in reducing mortality rate? We want to answer these questions that take into account of the intrinsic variability. Formally, we can perform hypothesis testing and compute the confidence intervals. These are what you learned in STAT 269. It is ok if you havent taken the STAT 269. The topics will be briefly reviewed. We will focus on the applications using R. 1.2.5 Machine Learning We will illustrate some machine learning methods using real datasets. For example, Diagnoising breast cancer with the k-NN algorithm Employ Naive Bayes to build an SMS junk message filter (text data) A wordcloud of text data Use neural network to predict the compressive strength of concrete 1.2.6 Some Numerical Methods Monte Carlo simulation (estimate probabilities, expectations, integrals) numerical optimizaiton methods (e.g. maximizing a multi-parameter likelihood function using optim) 1.2.7 Lastly It is important to communicate your results to other after performing the data analysis. Therefore, you will do a project with presentation and report. 1.3 Lets Get Started The best way to learn R is to get started immediately and try the code by yourselves. We will not discuss every topic in detail at the beginning, which is not interesting and unnecessary. We shall revisit the topics when we need additional knowledge. Simple arithmetic expression # can be used a simple calculator 3 + 5 ## [1] 8 4 * 2 ## [1] 8 10 / 2 ## [1] 5 Comment a code: use the hash mark # # this is a comment, R will not run the code behine # Function for combining c(4, 2, 3) # &quot;c&quot; is to &quot;combine&quot; the numbers ## [1] 4 2 3 Assignment (&lt;- is the assignment operator like = in many other programming languages) y &lt;- c(4, 2, 3) # create a vector called y with elements 4, 2, 3 c(1, 3, 5) -&gt; v # c(1,3,5) is assigned to v Output y ## [1] 4 2 3 v ## [1] 1 3 5 R is case-sensitive. When you type Y, you will see an error message: object Y not found Y ## Error in eval(expr, envir, enclos): object &#39;Y&#39; not found 1.4 R Data Structures Reading: ML with R Ch2 Most frequently used data structures in R: vectors, factors, lists, arrays, matrices, data frames 1.4.1 Vectors Vector fundamental R data structure stores an ordered set of values called elements elements must be of the same type Type: integer, double, character, logical Integer, double, logical, character vectors x &lt;- 1:2 # integer vector, we use a:b to form the sequence of integers from a to b typeof(x) # type of the vector ## [1] &quot;integer&quot; x &lt;- c(1.1, 1.2) # double vector typeof(x) ## [1] &quot;double&quot; length(x) # length of the vector x ## [1] 2 x &lt; 2 # logical (TRUE/FALSE) ## [1] TRUE TRUE p &lt;- c(TRUE, FALSE) subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) # character vector Combine two vectors y &lt;- c(2, 4, 6) c(x, y) # note that we created x above ## [1] 1.1 1.2 2.0 4.0 6.0 c(y, subject_name) # 2, 4, 6 become characters &quot;2&quot;, &quot;4&quot;, &quot;6&quot; ## [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; Assessing elements in the vectors y &lt;- c(2, 4, 6) y[2] # second element ## [1] 4 y[3] # third element ## [1] 6 1.4.2 Factors A factor is a special type of vector that is solely used for representing categorical (male, female/group 1, group 2, group 3) or ordinal (cold, warm, hot/ low, medium, high) variables. Reasons for using factor the category labels are stored only once. E.g., rather than storing MALE, MALE, MALE, FEMALE, the computer may store 1,1,1,2(save memory) many machine learning algorithms treat categorical/ordinal and numeric features differently and may require the input as a factor Create a factor gender &lt;- factor(c(&quot;MALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;)) gender ## [1] MALE MALE FEMALE MALE ## Levels: FEMALE MALE # compared with c(&quot;MALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;) ## [1] &quot;MALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; 1.4.3 Matrix Matrix a collection of numbers in a rectangular form A matrix with dimension n by m means the matrix has n rows and m columns. Create Matrix: To create a \\(3\\times 4\\) matrix with elements 1:12 filled in column-wise A &lt;- matrix(1:12, nrow = 3, ncol = 4) # note that we use = instead of &lt;- A ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Dimension, number of rows, number of columns of a matrix # again R is case-sensitive, a and A are different dim(A) # to find the dimension of A ## [1] 3 4 nrow(A) # to find the number of row in A ## [1] 3 ncol(A) # to find the number of column in A ## [1] 4 By default, the matrix is filled column-wise. You can change to row-wise by adding byrow = TRUE B &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE) B ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 Select rows, columns, submatrix, element A[1, 2] # select the element in the 1st row and 2nd column ## [1] 4 A[2, ] # select 2nd row ## [1] 2 5 8 11 A[, 3] # select 3rd column ## [1] 7 8 9 A[1:2, 3:4] # select a submatrix ## [,1] [,2] ## [1,] 7 10 ## [2,] 8 11 Try: A[c(1, 2), c(1, 3, 4)] A[-1, ] A[, -2] Combine Two Matrices cbind(A, B) # combine column-wise ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 1 4 7 10 1 2 3 4 ## [2,] 2 5 8 11 5 6 7 8 ## [3,] 3 6 9 12 9 10 11 12 rbind(A, B) # combine row-wise ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [4,] 1 2 3 4 ## [5,] 5 6 7 8 ## [6,] 9 10 11 12 Try: rbind(B, A) Transpose x &lt;- c(1, 2, 3) t(x) # transpose ## [,1] [,2] [,3] ## [1,] 1 2 3 Q &lt;- matrix(1:4, 2, 2) Q ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 t(Q) # transpose ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 Matrix Addition A &lt;- matrix(1:6, nrow = 2, ncol = 3) B &lt;- matrix(2:7, nrow = 2, ncol = 3) A + B ## [,1] [,2] [,3] ## [1,] 3 7 11 ## [2,] 5 9 13 A - B ## [,1] [,2] [,3] ## [1,] -1 -1 -1 ## [2,] -1 -1 -1 A + 2 ## [,1] [,2] [,3] ## [1,] 3 5 7 ## [2,] 4 6 8 c &lt;- c(1, 2) A + c ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 4 6 8 Elementwise Product A &lt;- matrix(1:6, nrow = 2, ncol = 3) B &lt;- matrix(1:2, nrow = 2, ncol = 3) A * B ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 4 8 12 c &lt;- 2 A * c ## [,1] [,2] [,3] ## [1,] 2 6 10 ## [2,] 4 8 12 c &lt;- c(10, 100) A * c ## [,1] [,2] [,3] ## [1,] 10 30 50 ## [2,] 200 400 600 c &lt;- c(10, 100, 1000) A * c # do you notice the pattern? ## [,1] [,2] [,3] ## [1,] 10 3000 500 ## [2,] 200 40 6000 Matrix Multiplication A &lt;- matrix(1:12, nrow = 3, ncol = 4) # 3x4 matrix t(A) # 4x3 matrix ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 t(A) %*% A #3x3 matrix, %*% = matrix multiplication ## [,1] [,2] [,3] [,4] ## [1,] 14 32 50 68 ## [2,] 32 77 122 167 ## [3,] 50 122 194 266 ## [4,] 68 167 266 365 B &lt;- matrix(1:9, nrow = 3, ncol = 3) B %*% A ## [,1] [,2] [,3] [,4] ## [1,] 30 66 102 138 ## [2,] 36 81 126 171 ## [3,] 42 96 150 204 A %*% B # error, non-conformable arguments ## Error in A %*% B: non-conformable arguments Diagonal Matrix diag(1:4) # diagonal matrix with diagonal elements being 1:4 ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 2 0 0 ## [3,] 0 0 3 0 ## [4,] 0 0 0 4 A &lt;- matrix(1:9, 3, 3) A ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 diag(A) # find the diagonal of A ## [1] 1 5 9 How to create an identity matrix in R? Inverse The inverse of a \\(n \\times n\\) matrix \\(A\\), denoted by \\(A^{-1}\\), is a \\(n \\times n\\) matrix such that \\(AA^{-1} = A^{-1} A = I_n\\), where \\(I_n\\) is the \\(n\\times n\\) identity matrix. To find the inverse of \\(A\\) in R: solve A &lt;- matrix(c(1, 0, 0, 3), 2, 2) solve(A) ## [,1] [,2] ## [1,] 1 0.0000000 ## [2,] 0 0.3333333 Some Statistical Applications I will mention a few connections of matrices with statistics. A dataset is naturally a matrix. Suppose that you have \\(n\\) people. You collected their health information: blood pressure, height, weight, age, whether they smoke (1 if yes, 0 if no), whether they drink (1/0), etc. Linear regression: we observe \\((x,y)\\), where \\(x\\) is a vector of covariates and \\(y\\) is your response. For example, \\(y\\) is the blood pressure, \\(x\\) is the collection of other health information. The linear regression model assumes that \\[y = \\beta_0 + x^T \\beta_1 + \\varepsilon,\\] where \\(\\varepsilon\\) is the error term. Our goal is to estimate \\(\\beta:=(\\beta_0, \\beta_1)\\). Let \\(X\\) be the design matrix. That is \\(X\\) is a \\(n \\times p\\) matrix where \\(n\\) is the number of observation, \\(p-1\\) is the number of covariates. The least squares solution for \\(\\beta\\) is \\[\\hat{\\beta} = (X^T X)^{-1}X^TY.\\] We will revisit linear regression later (I know some of you may not know linear regression). Correlation matrix, Covariance matrix. Let \\(X\\) be a random vector (column vector). The covariance matrix is defined as \\[\\Sigma := E[(X-E(X))(X-E(X))^T].\\] 1.4.4 Lists store an ordered set of elements like a vector can store different R data types (unlike a vector) # let&#39;s create some vectors (of different types) subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) # at this point, you should notice that meaningful names should be used for the variables temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) # notice how we use _ to separate two words # this is one of the styles in coding, you should be consistent with your style data &lt;- list(fullname = subject_name, temperature = temperature, flu_status = flu_status) # you may wonder what is the meaning of temperature = temperature # in &quot;fullname = subject_name&quot; # on the left of = is the name of the 1st element of your list # on the right of = is the name of the variable that you want to # assign the value to the 1st element data ## $fullname ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; ## ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE To assess the element of a list: data$flu_status ## [1] FALSE FALSE TRUE data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE data[2:3] # if you don&#39;t have the names ## $temperature ## [1] 98.1 98.6 101.4 ## ## $flu_status ## [1] FALSE FALSE TRUE 1.4.5 Data frames Data frame can be understood as a list of vectors, each having exactly the same number of values, arranged in a structure like a spreadsheet or database gender &lt;- c(&quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;) blood &lt;- c(&quot;O&quot;, &quot;AB&quot;, &quot;A&quot;) pt_data &lt;- data.frame(subject_name, temperature, flu_status, gender, blood) pt_data ## subject_name temperature flu_status gender blood ## 1 John 98.1 FALSE MALE O ## 2 Jane 98.6 FALSE FEMALE AB ## 3 Steve 101.4 TRUE MALE A colnames(pt_data) ## [1] &quot;subject_name&quot; &quot;temperature&quot; &quot;flu_status&quot; &quot;gender&quot; &quot;blood&quot; pt_data$subject_name ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; pt_data[c(&quot;temperature&quot;, &quot;flu_status&quot;)] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE pt_data[1, ] # like a matrix ## subject_name temperature flu_status gender blood ## 1 John 98.1 FALSE MALE O pt_data[, 2:3] ## temperature flu_status ## 1 98.1 FALSE ## 2 98.6 FALSE ## 3 101.4 TRUE Create a new column pt_data$temp_c &lt;- (pt_data$temperature - 32) * 5 / 9 pt_data ## subject_name temperature flu_status gender blood temp_c ## 1 John 98.1 FALSE MALE O 36.72222 ## 2 Jane 98.6 FALSE FEMALE AB 37.00000 ## 3 Steve 101.4 TRUE MALE A 38.55556 pt_data$fever &lt;- (pt_data$temp_c &gt; 37.6) pt_data ## subject_name temperature flu_status gender blood temp_c fever ## 1 John 98.1 FALSE MALE O 36.72222 FALSE ## 2 Jane 98.6 FALSE FEMALE AB 37.00000 FALSE ## 3 Steve 101.4 TRUE MALE A 38.55556 TRUE 1.5 Operators Priority Operator Meaning 1 $ component selection 2 [ [[ subscripts, elements 3 ^ (caret) exponentiation 4 - unary minus 5 : sequence operator 6 %% %/% %*% modulus, integer divide, matrix multiply 7 * / multiply, divide 8 + - add, subtract 9 &lt; &gt; &lt;= &gt;= == != comparison 10 ! not 11 &amp; | &amp;&amp; || logical and, logical or 12 &lt;- -&gt; = assignments # $ for list, data frame, etc subject_name &lt;- c(&quot;John&quot;, &quot;Jane&quot;, &quot;Steve&quot;) temperature &lt;- c(98.1, 98.6, 101.4) flu_status &lt;- c(FALSE, FALSE, TRUE) data &lt;- list(fullname = subject_name, temperature = temperature, flu_status = flu_status) data$fullname ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; pt_data &lt;- data.frame(subject_name, temperature, flu_status) pt_data$temperature ## [1] 98.1 98.6 101.4 # [ ], [[]] x &lt;- c(1, 5, 7) x[2] ## [1] 5 data[[1]] ## [1] &quot;John&quot; &quot;Jane&quot; &quot;Steve&quot; # x^r = x to the power of r x &lt;- 2 x^4 # 16 ## [1] 16 # modulus 7 %% 2 # 7 divided by 2 equals 3 but it remains 1, modulus = reminder ## [1] 1 10 %% 3 ## [1] 1 20 %% 2 ## [1] 0 # integer division 7 %/% 2 ## [1] 3 20 %/% 3 ## [1] 6 Comparison # &lt;, &gt;, &lt;=, &gt;=, ==, != x &lt;- 2 x &gt; 3 ## [1] FALSE x &lt; 4 ## [1] TRUE x &lt;- c(1, 5, 7) x &lt; 3 # compare each element with 3 ## [1] TRUE FALSE FALSE x &gt;= 5 ## [1] FALSE TRUE TRUE x == 5 # if x is equal to 5, not x = 5 ## [1] FALSE TRUE FALSE x != 5 # if x is not equal to 5 ## [1] TRUE FALSE TRUE x &lt;- TRUE !x # not x ## [1] FALSE x &lt;- 2 x &lt;= 2 ## [1] TRUE !(x &lt;= 2) ## [1] FALSE &amp; and &amp;&amp; indicate logical AND. The shorter form performs elementwise comparisons. The longer form examine only the first element of each vector. x &lt;- c(TRUE, FALSE, TRUE) y &lt;- c(FALSE, FALSE, TRUE) x &amp; y ## [1] FALSE FALSE TRUE x &amp;&amp; y ## [1] FALSE z &lt;- c(TRUE) x &amp;&amp; z ## [1] TRUE | and || indicate logical OR. The shorter form performs elementwise comparisons. The longer form examine only the first element of each vector. x &lt;- c(TRUE, FALSE, TRUE) y &lt;- c(FALSE, FALSE, TRUE) x | y ## [1] TRUE FALSE TRUE x || y ## [1] TRUE z &lt;- c(TRUE) x || z ## [1] TRUE Assignment # these assignments are the same, it is recommended to use &lt;- a &lt;- 2 a ## [1] 2 2 -&gt; b b ## [1] 2 c = 2 c ## [1] 2 Do !(x &gt; 1) &amp; (x &lt; 4) and !((x &gt; 1) &amp; (x &lt; 4)) give different results? 1.5.1 Vectorized Operators An important property of many of the operators is that they are vectorized. This means that the operation will be performed elementwise. x &lt;- c(1, 2, 3) y &lt;- c(5, 6, 7) x + y ## [1] 6 8 10 x * y ## [1] 5 12 21 2 * x # you do not need to use c(2,2,2)*x ## [1] 2 4 6 y / 2 # you do not need to use y/c(2,2,2) ## [1] 2.5 3.0 3.5 A &lt;- matrix(1:9, nrow = 3, ncol = 3) B &lt;- matrix(1:9, nrow = 3, ncol = 3) A + B ## [,1] [,2] [,3] ## [1,] 2 8 14 ## [2,] 4 10 16 ## [3,] 6 12 18 A * B # this is not matrix multiplication, but elementiwse multiplication ## [,1] [,2] [,3] ## [1,] 1 16 49 ## [2,] 4 25 64 ## [3,] 9 36 81 x &lt;- c(1, 3, 5) y &lt;- c(2, 2, 9) x &lt; y ## [1] TRUE FALSE TRUE 1.6 Built-in Functions Common mathematical functions sqrt, abs, sin, cos, log, exp. To get help on the usage of a function. Use ?. For example, if you want to know more about log. Type ?log in the console. You will then see that by default, log computes the natrual logarithms. Other useful functions Name Operations ceiling smallest integer greater than or equal to element floor largest integer less than or equal to element trunc ignore the decimal part round round up for positive and round down for negative sort sort the vector in ascending or descending order sum, prod sum and produce of a vector cumsum, cumprod cumulative sum and product min, max return the smallest and largest values range return a vector of length 2 containing the min and max mean return the sample mean of a vector var return the sample variance of a vector sd return the sample standard deviation of a vector seq generate a sequence of number rep replicate elements in a vector Note: If you have data \\(x_1,\\ldots,x_n\\), the sample variance is defined as \\[ S^2_n := \\frac{1}{n-1} \\sum^n_{i=1}(x_i-\\overline{x}_n)^2. \\] Note that we divide the sum by \\(n-1\\) but not \\(n\\). The sample standard deviation is the square root of the sample variance. x &lt;- 1:5 y &lt;- sqrt(x) y ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 ceiling(y) ## [1] 1 2 2 2 3 sum(x) ## [1] 15 prod(x) ## [1] 120 cumsum(x) ## [1] 1 3 6 10 15 cumprod(x) ## [1] 1 2 6 24 120 min(x) ## [1] 1 max(x) ## [1] 5 range(x) ## [1] 1 5 mean(x) ## [1] 3 var(x) ## [1] 2.5 rep(0, 10) # create a vector of length 10 with all elements being 0 ## [1] 0 0 0 0 0 0 0 0 0 0 rep(1, 10) # create a vector of length 10 with all elements being 1 ## [1] 1 1 1 1 1 1 1 1 1 1 1.6.1 sort() x &lt;- c(1, 5, 3, 10) sort(x) # default = ascending order ## [1] 1 3 5 10 sort(x, decreasing = TRUE) # descending order ## [1] 10 5 3 1 1.6.2 seq() This is an example of function with more than one argument. # seq(from, to) seq(1:5) ## [1] 1 2 3 4 5 seq(from = 1, to = 5) ## [1] 1 2 3 4 5 # seq(from, to, by) seq(1, 5, 2) ## [1] 1 3 5 seq(from = 1, to = 5, by = 2) ## [1] 1 3 5 # seq(from, to, length) seq(0, 10, length = 21) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 ## [20] 9.5 10.0 seq(from = 0, to = 10, length = 21) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 ## [20] 9.5 10.0 1.6.3 rep() # rep(data, times), try ?rep rep(0, 10) ## [1] 0 0 0 0 0 0 0 0 0 0 rep(c(1, 2, 3), 3) ## [1] 1 2 3 1 2 3 1 2 3 1.6.4 pmax, pmin x &lt;- c(1, 3, 5) y &lt;- c(2, 4, 4) max(x, y) # maximum of x and y ## [1] 5 min(x, y) # minimum of x and y ## [1] 1 pmax(x, y) # elementwise comparison ## [1] 2 4 5 pmin(x, y) # elementwise comparison ## [1] 1 3 4 1.7 Some Useful RStudio Shortcuts See also https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts Ctrl + 1: Move focus to the Source Editor (when you are in the Console) Ctrl + 2: Move focus to the Console (when you are in the source window) \\(\\uparrow\\) (the up arrow key on the keyboard): go to the previous command (in the console) \\(\\downarrow\\) (the down arrow key on the keyboard): go to the next command (in the console) Esc: Delete the current command/ Interrupt currently executing command Ctrl + Tab: go to the next tab 1.8 Exercises To test your understanding, try to evaluate the following code by hand and then check with the output from R. x &lt;- (10:1)[c(-1, -4)] x &lt;- x^2 x[5] # what do you expect to see? a&lt;-c(1:3, 6, 3 + 3, &quot;sta&quot;, 3 &gt; 5) a #? typeof(a) #? length(a) #? x&lt;-rep(1:6, rep(1:3, 2)) x %% 2 == 0 #? x[x %% 2 == 0] #? round(-3.7) #? trunc(-3.7) #? floor(-3.7) #? ceiling(-3.7) #? round(3.8) #? trunc(3.8) #? floor(3.8) #? ceiling(3.8) #? x &lt;- c(4, 3, 8, 7, 5, 6, 2, 1) sort(x) #? order(x) #? sum(x) + prod(x) #? cumsum(x) + cumprod(x) #? max(x) + min(x) #? 1.9 Comments to Exercises These are comments but not answers but you can get the answers immediately by running the code. x &lt;- (10:1)[c(-1, -4)] # 10:1 will give you a vector with elements 10, 9, 8,...,1 10:1 ## [1] 10 9 8 7 6 5 4 3 2 1 # [c(-1, -4)] will remove the 1st and 4th elements in the vector # therefore 10 and 7 will be removed from 10:1 given x ## [1] 9 8 6 5 4 3 2 1 x &lt;- x^2 # ^2 will square each of the elements in the vector x[5] # 36 ## [1] 16 a&lt;-c(1:3, 6, 3 + 3, &quot;sta&quot; , 3 &gt; 5) a # when you combine numeric, characters and logical values, the results become character ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;6&quot; &quot;6&quot; &quot;sta&quot; &quot;FALSE&quot; typeof(a) # character ## [1] &quot;character&quot; length(a) # ## [1] 7 x&lt;-rep(1:6, rep(1:3, 2)) # we first evaluate rep(1:3, 2), which is # 1 2 3 1 2 3 # rep then replicates the elements by the corresponding number of times # 1 is repeated 1 time # 2 is repeated 2 times # 3 is repeated 3 times # 4 is repeated 1 time # 5 is repeated 2 times # 6 is repeated 3 times x %% 2 == 0 # check if the modulus is 0 when x is divided by 2 ## [1] FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE # this is equivalent to ask if the elements are even x[x %% 2 == 0] # find out all the even elements ## [1] 2 2 4 6 6 6 # if you need to use these functions # you may try with a positive number and a negative number # to see if the results are what you want round(-3.7) # try round(-3.5), round(-3.4) ## [1] -4 trunc(-3.7) ## [1] -3 floor(-3.7) ## [1] -4 ceiling(-3.7) ## [1] -3 round(3.8) ## [1] 4 trunc(3.8) ## [1] 3 floor(3.8) ## [1] 3 ceiling(3.8) ## [1] 4 x&lt;-c(4, 3, 8, 7, 5, 6, 2, 1) sort(x) # asecending order ## [1] 1 2 3 4 5 6 7 8 order(x) # from the result, can you guess what it does? ## [1] 8 7 2 1 5 6 4 3 # order(x) returns a permutation which rearranges its first argument into ascending or descending order # that is, x[order(x)] = sort(x) sum(x) + prod(x) ## [1] 40356 cumsum(x) + cumprod(x) ## [1] 8 19 111 694 3387 20193 40355 40356 max(x) + min(x) ## [1] 9 "],["probability.html", "Chapter 2 Probability 2.1 Probability Distributions 2.2 Simulation", " Chapter 2 Probability Optional Reading: R Cookbook Ch8 2.1 Probability Distributions Using the normal distribution as an example: Function Purpose dnorm Normal density pnorm Normal CDF qnorm Normal quantile function rnorm Normal random variables Examples Density of \\(N(2, 3^2)\\) at \\(5\\). dnorm(5, mean = 2, sd = 3) ## [1] 0.08065691 \\(P(X \\leq 3)\\), where \\(X \\sim N(2, 3^2)\\) pnorm(3, mean = 2, sd = 3) ## [1] 0.6305587 # &quot;mean =&quot; and &quot;sd =&quot; are optional pnorm(3, 2, 3) ## [1] 0.6305587 Generate 10 random variables, each follows \\(N(3, 4^2)\\). rnorm(10, 3, 4) ## [1] 0.1277022 3.9227790 -1.1267426 4.7462358 11.1884196 -0.6558544 3.7532878 3.5045187 ## [9] 3.2762792 8.9488171 95th percenttile of \\(N(0, 1)\\). Find \\(q\\) such that \\(P(Z \\leq q) = 0.95\\) qnorm(0.95, 0, 1) ## [1] 1.644854 Plotting the normal density x &lt;- seq(-4, 4, by = 0.1) plot(x, dnorm(x), type = &quot;l&quot;, main = &quot;Density of N(0,1)&quot;) # &quot;l&quot; for lines 2.1.1 Common Distributions Common discrete distributions Discrete distribution R name Parameters Binomial binom n = number of trials; p = probability of success for one trial Geometric geom p = probability of success for one trial Negative binomial (NegBinomial) nbinom size = number of successful trials; either prob = probability of successful trial or mu = mean Poisson pois lambda = mean Common continuous distributions Continuous distribution R name Parameters Beta beta shape1; shape2 Cauchy cauchy location; scale Chi-squared (Chisquare) chisq df = degrees of freedom Exponential exp rate F f df1 and df2 = degrees of freedom Gamma gamma rate; either rate or scale Log-normal (Lognormal) lnorm meanlog = mean on logarithmic scale; sdlog = standard deviation on logarithmic scale Logistic logis location; scale Normal norm mean; sd = standard deviation Students t (TDist) t df = degrees of freedom Uniform unif min = lower limit; max = upper limit To get help on the distributions: ?dnorm ?dbeta ?dcauchy # the following distributions need to use different code ?TDist ?Chisquare ?Lognormal Examples (Using Binomial as an Example) dbinom(2, 10, 0.6) # p_X(2), p_X is the pmf of X, X ~ Bin(n=10, p=0.6) ## [1] 0.01061683 pbinom(2, 10, 0.6) # F_X(2), F_X is the CDF of X, X ~ Bin(n=10, p=0.6) ## [1] 0.01229455 qbinom(0.5, 10, 0.6) # 50th percentile of X ## [1] 6 rbinom(4, 10, 0.6) # generate 4 random variables from Bin(n=10, p=0.6) ## [1] 7 5 7 4 x &lt;- 0:10 plot(x, dbinom(x, 10, 0.6), type = &quot;h&quot;) # &quot;h&quot; for histogram like vertical lines 2.1.2 Exercises The average number of trucks arriving on any one day at a truck depot in a certain city is known to be 12. Assuming the number of trucks arriving on any one day has a Poisson distribution, what is the probability that on a given day fewer than 9 (strictly less than 9) trucks will arrive at this depot? ppois(8, 12) ## [1] 0.1550278 Let \\(Z \\sim N(0, 1)\\). Find \\(c\\) such that \\(P(Z \\leq c) = 0.1151\\) qnorm(0.1151) ## [1] -1.199844 \\(P(1\\leq Z \\leq c) = 0.1525\\) c &lt;- qnorm(pnorm(1) + 0.1525) # draw a graph # test the answer pnorm(c) - pnorm(1) ## [1] 0.1525 \\(P(-c \\leq Z \\leq c) = 0.8164\\). # P(0 &lt;= Z &lt;= c) = 0.8164/2 # P(Z &lt;= c) = 0.8164/2 + 0.5 c &lt;- qnorm(0.8164 / 2 + 0.5) # test our answer pnorm(c)- pnorm(-c) ## [1] 0.8164 Plot the density of a chi-squared distribution with degrees of freedom \\(4\\), from \\(x=0\\) to \\(x=10\\). Find the 95th percentile of this distribution. # note that a chi-squared r.v. is nonnegative x &lt;- seq(0, 10, by = 0.1) plot(x, dchisq(x, df = 4), type = &quot;l&quot;) qchisq(0.95, df = 4) ## [1] 9.487729 Simulate \\(10\\) Bernoulli random variables with parameter \\(0.6\\). # Bernoulli(p) = Bin(1, p) rbinom(10, size = 1, prob = 0.6) ## [1] 0 0 1 1 1 1 0 1 0 1 Plot the Poisson pmf with parameter \\(2\\) from \\(x = 0\\) to \\(x = 10\\). x &lt;- 0:10 plot(x, dpois(x, 2), type = &quot;h&quot;) Draw a plot to illustrate that the 97.5th percentile of the t distribution will be getting closer to that of the standard normal distribution when the degrees of freedom increases. x &lt;- 10:200 plot(x, qt(0.975, df = x), type = &quot;l&quot;, ylim = c(1.9,2.3)) # add a horizontal line with value at qnorm(0.975) # lty = 2 for dashed line, check ?par abline(h = qnorm(0.975), lty = 2) # Therefore, for a large sample, t-test and z-test will give you similar result. 2.2 Simulation We have already seen how to use functions like runif, rnorm, rbinom to generate random variables. R actually generates pseudo-random number sequence (deterministic sequence of numbers that approximates the properties of random numbers) The pseduo-random number sequence will be the same if it is initialized by the same seed (can be used to reproduce the same simulation results or used to debug). # every time you run the first two lines, you get the same result set.seed(1) runif(5) ## [1] 0.2655087 0.3721239 0.5728534 0.9082078 0.2016819 # every time you run the following code, you get a different result runif(5) ## [1] 0.89838968 0.94467527 0.66079779 0.62911404 0.06178627 Sampling from discrete distributions Usage of sample: sample(x, size, replace = FALSE, prob = NULL) See also ?sample. sample(10) # random permutation of integers from 1 to 10 ## [1] 3 1 5 8 2 6 10 9 4 7 sample(10, replace = T) # sample with replacement ## [1] 5 9 9 5 5 2 10 9 1 4 sample(c(1, 3, 5), 5, replace = T) ## [1] 5 3 3 3 3 # simulate 20 random variables from a discrete distribution sample(c(-1,0,1), size = 20, prob = c(0.25, 0.5, 0.25), replace = T) ## [1] -1 1 1 -1 0 0 1 1 0 -1 0 0 0 0 0 1 1 0 -1 0 Example: Suppose we have a fair coin and we play a game. We flip the coin. We win $1 if the result is head and lose $1 if the result is tail. You play the game 100 times. You are interested in the cumulative profit. set.seed(1) # R actually generates pseudo random numbers # setting the seed ensure that each time you will get the same result # for illustration, code debugging, reproducibility profit &lt;- sample(c(-1, 1), size = 100, replace = T) plot(cumsum(profit), type = &quot;l&quot;) set.seed(2) profit &lt;- sample(c(-1, 1), size = 100, replace = T) plot(cumsum(profit), type = &quot;l&quot;) Example: You have two dice \\(A\\) and \\(B\\). For die \\(A\\), there are \\(6\\) sides with numbers \\(1,2,3,4,5,6\\) and the corresponding probability of getting these values are \\(0.1,0.1,0.1,0.1,0.1,0.5\\). For die \\(B\\), there are \\(4\\) sides with numbers \\(1,2,3,7\\) and the corresponding probability of getting these values are \\(0.3,0.2,0.3,0.2\\). You roll the two dice independently. Estimate \\(P(X &gt; Y)\\) using simulation, where \\(X\\) is the result from die \\(A\\) and \\(Y\\) is the result from die \\(B\\). n &lt;- 10000 # number of simulations X &lt;- sample(1:6, size = n, replace = TRUE, prob = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.5)) Y &lt;- sample(c(1, 2, 3, 7), size = n, replace = TRUE, prob = c(0.3, 0.2, 0.3, 0.2)) mean(X &gt; Y) ## [1] 0.6408 Why the sample mean approximates the required probability? Recall the strong law of large numbers (SLLN). Let \\(X_1,\\ldots,X_n\\) be independent and identically distributed random variables with mean \\(\\mu:=E(X)\\). Let \\(\\overline{X}_n:= \\frac{1}{n} \\sum^n_{i=1}X_i\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\rightarrow} \\mu.\\] Note: a.s. means almost surely. The above convergence means \\(P(\\lim_{n\\rightarrow \\infty} \\overline{X}_n = \\mu) = 1\\). Note that (the expectation of an indicator random variable is the probability that the corresponding event will happen) \\[ P(X&gt;Y) = E(I(X&gt;Y)).\\] To apply SLLN, we just need to recognize the underlying random variable is \\(I(X&gt;Y)\\). Then, with probability \\(1\\), the sample mean \\[ \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i) \\rightarrow P(X&gt;Y).\\] The quantity on the LHS is what we compute in mean(X&gt;Y) (note that we are using vectorized comparison). We will see additional simulation examples after we talk about some programming in R There are many important topics that we will not discuss algorithms for simulating random variables inverse transform acceptance rejection Markov Chain Monte Carlo methods to reduce variance in simulation Control variates Antithetic variates Importance sampling "],["programming-in-r.html", "Chapter 3 Programming in R 3.1 Writing functions in R 3.2 Control Flow 3.3 Automatically Reindent Code 3.4 Speed Consideration 3.5 Another Simulation Example", " Chapter 3 Programming in R Optional reading: R Cookbook Ch 15, R for data science https://r4ds.had.co.nz/program-intro.html Hands-on programming with R: https://rstudio-education.github.io/hopr/ 3.1 Writing functions in R When you have to copy and paste some code more than 2 times, you should consider writing a function Writing a function can simplify your code and isolate the main part of your program General format of a function function_name &lt;- function(argument1, argument2) { statements } Example: Lets try to write a function to compute the sample variance. my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2)/(n - 1)) } y &lt;- 1:9 my_var(y) ## [1] 7.5 var(y) # compared with the bulit-in function ## [1] 7.5 x &lt;- rnorm(1000, mean = 0, sd = 2) my_var(x) ## [1] 4.262595 var(x) # why the result is not equal to 2? ## [1] 4.262595 We can also write my_var2 &lt;- function(x){sum((x - mean(x))^2)/(length(x) - 1)} The variable x is the argument to be passed into the function. The variables mean_x and n are local variables whose scope is within this function. y &lt;- 2 f &lt;- function(x) { y &lt;- x x &lt;- 4 y } y ## [1] 2 f(3) # output the value f(3) ## [1] 3 y # y is unchanged, y is defined in the global environment ## [1] 2 x ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found We shall write code using proper indentation (easier to read and debug) # with indentation (use this one) my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2)/(n - 1)) } # no indentation my_var &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) return(sum((x - mean_x)^2)/(n - 1)) } The number of arguments passed to a function can be more than one Example: Write a function to compute the pooled sample standard deviation of two independent samples \\(x_1,\\ldots,x_n\\) and \\(y_1,\\ldots,y_m\\) of sizes \\(n\\) and \\(m\\). Recall that the pooled sample standard deviation is defined as: \\[ S_p := \\sqrt{\\frac{(n-1)S^2_X + (m-1)S^2_Y}{m+n-2}}, \\] where \\(S^2_X\\) and \\(S^2_Y\\) are the sample variances of \\(x_1,\\ldots,x_n\\) and \\(y_1,\\ldots,y_m\\), respectively. pooled_sd &lt;- function(x, y) { n &lt;- length(x) m &lt;- length(y) return(sqrt(((n - 1) * var(x) + (m - 1) * var(y))/(m + n - 2))) } Remark: if the final statement will output something, it will be the output of the function. You can also use return() as above. That is, pooled_sd and pooled_sd2 are exactly the same. pooled_sd2 &lt;- function(x, y) { n &lt;- length(x) m &lt;- length(y) sqrt(((n - 1) * var(x) + (m - 1) * var(y))/(m + n - 2)) } You can return more than one value in a function my_var_sd &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) my_var &lt;- sum((x - mean_x)^2)/(n - 1) return(c(my_var, sqrt(my_var))) } You may also return a list my_var_sd &lt;- function(x){ mean_x &lt;- mean(x) n &lt;- length(x) my_var &lt;- sum((x - mean_x)^2)/(n - 1) output &lt;- list(var = my_var, sd = sqrt(my_var)) return(output) } Example: write a function called my_summary that will output a list with elements being the mean, sd, median, min and max of a given vector. my_summary &lt;- function(x){ output &lt;- list(mean = mean(x), sd = sd(x), median = median(x), min = min(x), max = max(x)) return(output) } my_summary(1:10) ## $mean ## [1] 5.5 ## ## $sd ## [1] 3.02765 ## ## $median ## [1] 5.5 ## ## $min ## [1] 1 ## ## $max ## [1] 10 Define a function with default value my_power = function(x, p = 2) { return(x^p) } my_power(3) # by default, p = 2, will compute 3^2 ## [1] 9 my_power(3, 3) # will compute 3^3 ## [1] 27 3.2 Control Flow 3.2.1 for loop You can use a for loop when you know how many times you will loop. Syntax: for (var in sequence) { statement # do this statement for each value of i } Examples: for (i in 1:5) { # note: you do not have to define i beforehand print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 for (i in c(1, 3, 6)) { print(i) } ## [1] 1 ## [1] 3 ## [1] 6 Examples Write R code to find \\(\\sum^{10}_{i=1} \\sum^4_{j=1} \\frac{i^2}{(i+j)^2}\\). sum &lt;- 0 for (i in 1:10) { for (j in 1:4) { sum &lt;- sum + i^2/(i + j)^2 } } sum ## [1] 18.26491 Write R code to find \\(\\sum^{10}_{i=1} \\sum^i_{j=1} \\frac{i^2}{(i+j)^3}\\). sum &lt;- 0 for (i in 1:10) { for (j in 1:i) { sum &lt;- sum + i^2/(i + j)^3 } } sum ## [1] 2.779252 3.2.2 while loop You can use a while loop if you want to loop until a specific condition is met. For example, when you minimize a function numerically using some iterative algorithm, you may want to stop when the objective value does not change much. You may not know how many loops are required in advance so that a while loop may be better than a for loop in this application. Syntax: while (condition) { statement # while the condition is TRUE, do this } A simple example: i &lt;- 1 while (i &lt; 6) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 # What happen if you change &quot;i &lt; 6&quot; to &quot;i &lt;= 6&quot;? Ans: will print 1 to 6 # What happen if you change &quot;i &lt; 6&quot; to &quot;i &lt;= 5&quot;? Ans: outputs are the same Another example: # find the smallest n such that 1^2+ 2^2+ ... + n^2 &gt; 65 sum &lt;- 0 i &lt;- 1 while (sum &lt; 65) { sum &lt;- sum + i^2 print(c(i, sum)) i &lt;- i + 1 } ## [1] 1 1 ## [1] 2 5 ## [1] 3 14 ## [1] 4 30 ## [1] 5 55 ## [1] 6 91 i # 6 ## [1] 7 3.2.3 if (cond) Syntax: if (condition) { statement # do this if the condition is TRUE } Example: write a function that outputs positive if a positive number is entered. # check if a number if positive my_pos &lt;- function(x) { if (x &gt; 0) { print(&quot;positive&quot;) } } my_pos(-2) my_pos(2) ## [1] &quot;positive&quot; 3.2.4 if (cond) else expr Syntax if (condition) { statement1 # do this if condition is TRUE } else { statement2 # do this if condition is FALSE } Example: # write my own absolute value function my_abs &lt;- function(x) { if (x&gt;=0) { return(x) } else { return(-x) } } my_abs(-2) ## [1] 2 my_abs(3) ## [1] 3 my_abs(0) ## [1] 0 Error-handling in a function: my_sqrt = function(x) { if (x &gt;= 0) { print(sqrt(x)) # do this if x &gt;= 0 } else { cat(&quot;Error: this is a negative number!&quot;) # do this otherwise } } my_sqrt(-2) ## Error: this is a negative number! 3.2.5 If else ladder Syntax # Example if (condition1) { statement1 } else if (condition2) { statement2 } else if (condition1) { statement3 } Example: score_to_grade = function(x) { if (x&gt;=90) { cat(&quot;A+&quot;) } else if (x &gt;= 85) { cat(&quot;A&quot;) } else if (x &gt;= 80) { cat(&quot;A-&quot;) } else { cat(&quot;B+ or below&quot;) } } # after you write the function, you should check each case carefully score_to_grade(92) ## A+ score_to_grade(88) ## A score_to_grade(83) ## A- score_to_grade(78) ## B+ or below 3.3 Automatically Reindent Code To indent a block of code, highlight the text in RStudio, then press Ctrl+i (Windows or Linux) or press Cmd+i (Mac). Poor indentation, difficult to read for (i in 1:5) { if (i &gt;= 3) { print(i*2) } else { print(i * 3) } } ## [1] 3 ## [1] 6 ## [1] 6 ## [1] 8 ## [1] 10 Highlight the block of code, press Ctrl+i or Cmd+i for (i in 1:5) { if (i &gt;= 3) { print(i*2) } else { print(i * 3) } } ## [1] 3 ## [1] 6 ## [1] 6 ## [1] 8 ## [1] 10 3.4 Speed Consideration While the computing power is getting stronger and stronger, we should still write code that runs efficiently. # suppose we want to simulate 200,000 normal random variables n &lt;- 200000 x &lt;- rep(0, n) # create a vector for storage of the values initial_time &lt;- proc.time() for (i in 1:n) { x[i] &lt;- rnorm(1) } proc.time() - initial_time ## user system elapsed ## 0.42 0.00 0.43 n &lt;- 200000 x &lt;- rep(0, n) # create a vector for storage of the values # Alternatively system.time({ for (i in 1:n) { x[i] &lt;- rnorm(1) } }) ## user system elapsed ## 0.31 0.00 0.31 The user time is the CPU time charged for the execution of user instructions of the calling process. The system time is the CPU time charged for execution by the system on behalf of the calling process. A much more efficient way for the same task is to use system.time({ n &lt;- 200000 x &lt;- rnorm(n) }) ## user system elapsed ## 0.01 0.00 0.02 Another example: set.seed(1) x &lt;- rnorm(2e6) y &lt;- rnorm(2e6) v &lt;- rep(0, 2e6) system.time({ for (i in 1:length(x)){ v[i] &lt;- x[i]+y[i] } }) ## user system elapsed ## 0.12 0.01 0.14 system.time(v &lt;- x + y) ## user system elapsed ## 0 0 0 The general rule is to use vectorized operations whenever possible and to avoid using for loops. We use a for loop when the code is not time-consuming or when the code is hard to write without using a for loop. A more advanced option is to combine C++ with R using the package rcpp. That is, we can write the most time-consuming part of the R code in C++, which could run many times faster (will not be discussed in this course). See also http://www.noamross.net/archives/2014-04-16-vectorization-in-r-why/ 3.5 Another Simulation Example A simple model on the stock return assumes that (i) \\[r_{t+1} := \\log \\frac{P_{t+1}}{P_t} \\sim N(\\mu,\\sigma^2), \\] where \\(r_{t+1}\\) is the log-return at Day \\(t+1\\), \\(P_t\\) is the stock price at the end of Day \\(t\\); (ii) \\(r_1,r_2,\\ldots\\) are iid. Simple algebra shows that \\[P_{t+1} = P_t e^{r_{t+1}}\\] and \\[P_{t+1} = P_0 e^{ \\sum^{t+1}_{i=1} r_i}.\\] Suppose that the current price of a certain stock \\(P_0\\) is \\(100\\), \\(\\mu = 0.0002\\) and \\(\\sigma = 0.015\\). Using simulation, estimate the probability that the price is below $95 at the close of at least one of the next 30 trading days. no_sim &lt;- 10000 # number of simulation below &lt;- rep(0, no_sim) for (i in 1:no_sim) { price &lt;- 100 * exp(cumsum(rnorm(30, mean = 0.0002, sd = 0.015))) below[i] &lt;- min(price) &lt; 95 } mean(below) ## [1] 0.4384 "],["creating-some-basic-plots.html", "Chapter 4 Creating Some Basic Plots 4.1 Scatter Plot 4.2 Line Graph 4.3 Bar Chart 4.4 Histogram 4.5 Box Plot 4.6 Plotting a function curve 4.7 More on plots with base R 4.8 Summary of ggplot", " Chapter 4 Creating Some Basic Plots The base R contains many basic methods for producing graphics. We will learn some of them in this chapter. For more elegant plots, we will use the package ggplot2. We will use some simple datasets in base R to illustrate how to create some basic plots in this chapter. In this next chapter, we will discuss how to input our own data and transform them. Reference: R graphics cookbook, R for data science, R Cookbook. We will load the package tidyverse. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. You have to install the package before you can use it. install.packages(&quot;tidyverse&quot;) Load the library library(tidyverse) # load the tidyverse package (which contains ggplot2) if we want to make it clear what package an object/ function comes from, use package name followed by two colons, like dplyr::mutate(). 4.1 Scatter Plot Lets take a look at the mtcars dataset. This dataset comes with base R. head(mtcars) # this is a data frame ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) # display the structure of the data frame ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... mpg: miles/gallon wt: weight (1000lbs) Scatter plot with base graphics # x-axis: mtcars$wt # y-axis: mtcars$mpg plot(x = mtcars$wt, y = mtcars$mpg) # &quot;x=&quot;, &quot;y=&quot; are optional You can produce the same plot with plot(mtcars$wt, mtcars$mpg) Scatter plot with ggplot2 ggplot(data = mtcars, aes(x = wt, y = mpg)) + geom_point() Scatter plot with base graphics (when you only have \\(x\\) and \\(y\\) but not a dataset) set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 2 * x + rnorm(100, 0, 1) plot(x, y) Scatter plot with ggplot2 (when you only have \\(x\\) and \\(y\\) but not a dataset) set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 2*x + rnorm(100, 0, 1) ggplot(mapping=aes(x = x, y = y)) + # by default, data = NULL geom_point() 4.2 Line Graph The dataset pressure (also in base R) contains the relation between temperature in degrees Celsius and vapor pressure of mercury in millimeters (of mercury). Line graph with base graphics # the only difference from a scatter plot is that we add type=&quot;l&quot; plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) #l = line Line graph with base graphics with points plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) points(pressure$temperature, pressure$pressure) # add some points Line graph with base graphics with another line and points (with color) plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;) points(pressure$temperature, pressure$pressure) # the additional line may not have a physical meaningful # just an illustration of how to add a line wit base graphics lines(pressure$temperature, pressure$pressure / 2, col = &quot;red&quot;) points(pressure$temperature, pressure$pressure / 2, col = &quot;red&quot;) Colors in R You can go to https://www.r-graph-gallery.com/ggplot2-color.html and read more about colors in R. For example, you can specify the color by name, rgb, number and hex code. plot(pressure$temperature, pressure$pressure, type = &quot;l&quot;, col = rgb(0.1, 0.2,0.5,1)) lines(pressure$temperature, pressure$pressure / 2, type = &quot;l&quot;, col = 2) lines(pressure$temperature, pressure$pressure * 2, col = &quot;#8B2813&quot;) lines(pressure$temperature, pressure$pressure * 3, col = &quot;cornflowerblue&quot;) Line graph with ggplot2 ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() Line graph with ggplot2 with points ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() + geom_point() Line graph with ggplot2 with another line and points (with color) ggplot(data = pressure, aes(x = temperature, y = pressure)) + geom_line() + geom_point() + geom_line(aes(x = temperature, y = pressure / 2), color =&quot;red&quot;) + geom_point(aes(x = temperature, y = pressure / 2), color = &quot;#8B2813&quot;) Remark: it is common with ggplot() to split the command on multiple lines, ending each line with a + so that R knows that the command will continue on the next line. 4.3 Bar Chart Two types of bar charts: Bar chart of values. x-axis: discrete variable, y-axis: numeric data (not necessarily count data) Bar chart of count. x-axis: discrete variable, y-axis: count of cases in the discrete variable Remark: for histogram: x-axis = continuous variable, y-axis = count of cases in the interval. The BOD data set has 6 rows and 2 columns giving the biochemical oxygen demand versus time in an evaluation of water quality. # instead of using head(BOD) and str(BOD), we can also change it to a &quot;tibble&quot; # we can view the first few lines and the data structure as_tibble(BOD) ## # A tibble: 6 x 2 ## Time demand ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 8.3 ## 2 2 10.3 ## 3 3 19 ## 4 4 16 ## 5 5 15.6 ## 6 7 19.8 Bar chart of values with base graphics # names.arg = a vector of names to be plotted below each bar or group of bars. barplot(BOD$demand, names.arg = BOD$Time) Bar chart of counts with base graphics In the dateset mtcars, cylis the number of cylinders in the car. The possible values are \\(4, 6\\), and \\(8\\). We first find the count of each unique value in mtcars$cyl: table(mtcars$cyl) ## ## 4 6 8 ## 11 7 14 To plot the bar chart, we use barplot(table(mtcars$cyl)) Bar chart with values with ggplot2 We first make the Time variable to a factor: BOD2 = BOD BOD2$Time = as.factor(BOD2$Time) ggplot(BOD2, aes(x = Time, y = demand)) + geom_col() What will happen if we do not make Time as a factor: ggplot(BOD, aes(x = Time, y = demand)) + geom_col() Bar chart of counts with ggplot2 # the y position is calculated by counting the number of rows for each value of cyl ggplot(mtcars, aes(x = cyl)) + geom_bar() 4.4 Histogram mpg in mtcars is the miles/gallon of the car. It is a continuous variable. Histogram with base graphics hist(mtcars$mpg) Histogram with base graphics # Specify approximate number of bins with &quot;breaks&quot; hist(mtcars$mpg, breaks = 10) Histogram with ggplo2 ggplot(mtcars, aes(x = mpg)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(mtcars, aes(x = mpg)) + geom_histogram(binwidth = 2.5) Remark: different bin widths will give you histograms with different looks. 4.5 Box Plot Lets take a look at another dataset ToothGrowth. In particular, supp is a factor. as_tibble(ToothGrowth) ## # A tibble: 60 x 3 ## len supp dose ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 ## 6 10 VC 0.5 ## 7 11.2 VC 0.5 ## 8 11.2 VC 0.5 ## 9 5.2 VC 0.5 ## 10 7 VC 0.5 ## # ... with 50 more rows Box plot with basic graphics (using plot) # if x is a factor, use the following code plot(x = ToothGrowth$supp, y = ToothGrowth$len) Box plot with basic graphics (using boxplot) # len ~ supp is an example of a &quot;formula&quot; (y ~ x) boxplot(len ~ supp, data = ToothGrowth) Box plot with basic graphics + interaction of two variables on x-axis (using boxplot) boxplot(len ~ supp + dose, data = ToothGrowth) Box plot with ggplot2 ggplot(ToothGrowth, aes(x = supp, y = len)) + geom_boxplot() Box plot with ggplot2 + interaction of two variables on x-axis ggplot(ToothGrowth, aes(x = interaction(supp, dose), y = len)) + geom_boxplot() What will happen if dose take a lot more values? ggplot(ToothGrowth, aes(x = interaction(supp, dose + 1:5), y = len)) + geom_boxplot() 4.6 Plotting a function curve curve(x^3 - 5 * x, from = -4, to = 4) Alternatively: x &lt;- seq(-4, 4, len = 1000) plot(x, x^3 - 5 * x, type = &quot;l&quot;) Plotting a built-in function curve(dnorm(x), from = -4, to = 4) Plotting a self-defined function my_function &lt;- function(x) { 1 / (1 + exp(-x + 10)) } curve(my_function, from = 0, to = 20) Plotting a function with additional arguments curve(dnorm(x, mean = 2, sd = 3), from = -4, to = 4) 4.7 More on plots with base R 4.7.1 Multi-frame plot To create a 3x2 multi-frame plot. Use par(mfrow = c(3, 2)). set.seed(1) x &lt;- rnorm(100, 50, 5) y &lt;- x + rnorm(100, 2, 2) # create a 2x2 multi-frame plot par(mfrow=c(2, 2)) hist(x) hist(y,breaks = 10) plot(x, y) boxplot(x, y) 4.7.2 Type of Plot Option Type type=\"p\" Points (default) type=\"l\" Lines connecting the data points type=\"b\" Points and non-overlapping lines type=\"h\" Height lines type=\"o\" Points and overlapping lines par(mfrow=c(3, 2)) x &lt;- -5:5 y &lt;- x^2 plot(x, y) plot(x, y, type = &quot;p&quot;) plot(x, y, type = &quot;l&quot;) plot(x, y, type = &quot;b&quot;) plot(x, y, type = &quot;h&quot;) plot(x, y, type = &quot;o&quot;) 4.7.3 Parameters of a plot Parameter Meaning type See Type of Plot main Title sub Subtitle xlab x-axis label ylab y-axis label xlim x-axis range ylim y-axis range pch Symbol of data points col Color of data points lty Type of the line To illustrate some of the components: set.seed(1) x &lt;- rnorm(100, 50, 15) y &lt;- x + rnorm(100, 2, 13) plot(x, y, col = &quot;red&quot;, pch = 15, main = &quot;This is the title&quot;, xlim = c(0, 100), ylim = c(0,100), xlab = &quot;name of x-axis&quot;, ylab = &quot;name of y-axis&quot;) 4.7.4 Elements on plot Function Description abline(c,m) plot the line y = mx +c abline(h = a) plot the line y = a abline(v = b) plot the line x = b lines(x,y) line joining points with coordinates (x,y) set.seed(1) x &lt;- rnorm(100, 50, 15) y &lt;- x + rnorm(100, 2, 13) plot(x, y) # connect the points (20, 20), (30, 80), (40, 40) with a line lines(x = c(20, 30, 40),y = c(20, 80, 40), col = &quot;red&quot;) abline(v = 60, col = &quot;blue&quot;) 4.8 Summary of ggplot Plot geom scatter plot geom_point() line graph geom_line() bar chart of values geom_col() bar chart of counts geom_bar() histogram geom_histogram() box plot geom_boxplot() "],["managing-data-with-r.html", "Chapter 5 Managing Data with R 5.1 Missing Values 5.2 Saving, loading, and removing R data structures 5.3 Importing and saving data from CSV files", " Chapter 5 Managing Data with R Optional Reading: ML with R Ch2 5.1 Missing Values Missing values are common in real datasets. NA is used to denote missing values. (x &lt;- c(1, 2, 3, NA, 4, NA, 4)) # we can use (x&lt;-1) to assign 1 to x and display x at the same time ## [1] 1 2 3 NA 4 NA 4 mean(x) # mean cannot be computed when missing values exist ## [1] NA mean(x, na.rm = TRUE) # NA values will be removed before computing mean ## [1] 2.8 sd(x) # sd cannot be computed when missing values exist ## [1] NA sd(x, na.rm = TRUE) # NA values will be removed before computing SD ## [1] 1.30384 is.na(x) # logical vector ## [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE x[!is.na(x)] #select the elements with non-missing valuess ## [1] 1 2 3 4 4 na.omit(x) # select the elements with non-missing valuess ## [1] 1 2 3 4 4 ## attr(,&quot;na.action&quot;) ## [1] 4 6 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; 5.2 Saving, loading, and removing R data structures Removing all objects in R: rm(list=ls()) ls() returns a vector of all data structures currently in memory x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) ls() ## [1] &quot;x&quot; &quot;y&quot; To remove x from the memory rm(x) x # because we have deleted x, an error message occurs ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found Saving objects to a file (regardless of whether they are vectors, factors, lists, etc) A &lt;- matrix(1:9, 3, 3) f &lt;- function(x){ return(1) } save(A, f, file = &quot;my_data.RData&quot;) Loading objects from a .RData file. rm(list=ls()) # remove everything load(&quot;my_data.RData&quot;) A ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 f ## function(x){ ## return(1) ## } 5.3 Importing and saving data from CSV files Finding current directory getwd() ## [1] &quot;C:/Queens Teaching/Teaching/STAT 362 W22/01c_published_webiste&quot; If you use mac, you will probably see \"/Users/..../\". Setting working directory setwd(&quot;C:/Queens Teaching/Teaching/STAT 362/Notes&quot;) # use /, not \\ If you use mac, change the above code accordingly. Writing to a file my_data &lt;- data.frame(x = c(1, 2, 3), y = c(4, 5, 6)) my_data ## x y ## 1 1 4 ## 2 2 5 ## 3 3 6 library(tidyverse) write_csv(my_data, &quot;C:/Queens Teaching/Teaching/STAT 362/my_data.csv&quot;) Reading a csv file A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. If we use read_csv from the package tidyverse, the resulting object is a tibble. If we use read.csv from base R, the resulting object is a data frame. See R for data science (https://r4ds.had.co.nz/data-import.html) for a discussion on the differences between read.csv and read_csv. A tibble allows us to perform different types of transformation (next chapter). my_data &lt;- read_csv(&quot;C:/Queens Teaching/Teaching/STAT 362/my_data.csv&quot;) ## ## -- Column specification ----------------------------------------------------------------------------- ## cols( ## x = col_double(), ## y = col_double() ## ) "],["review-1-lect-1-8.html", "Chapter 6 Review 1 (Lect 1-8) 6.1 Simulation 6.2 Matrix 6.3 Basic Operation 6.4 Some basic plots in R", " Chapter 6 Review 1 (Lect 1-8) 6.1 Simulation Basic vectorized comparison X &lt;- c(1, 1, 5) Y &lt;- c(5, 5, 1) X &gt; Y # FALSE FALSE TRUE ## [1] FALSE FALSE TRUE sum(X &gt; Y) # TRUE = 1, FALSE = 0 ## [1] 1 mean(X &gt; Y) # 1/3 ## [1] 0.3333333 The code mean(X &gt; Y) is computing \\[\\begin{equation*} \\frac{1}{3}( I(X_1 &gt; Y_1) + I(X_2 &gt; Y_2) + I(X_3 &gt; Y_3)), \\end{equation*}\\] where \\(I(\\cdot)\\) is the indicator function, that is, \\(I(X_1 &gt; Y_1) = 1\\) if \\(X_1 &gt; Y_1\\) and \\(I(X_1 &gt; Y_1) = 0\\) if \\(X_1 \\leq Y_1\\). Example: Let \\(X \\sim N(mean = 0, sd = 2)\\) and \\(Y \\sim Exp(rate = 3)\\). Estimate \\(P(X &gt; Y)\\) using simulation. n &lt;- 10000 # number of simulations X &lt;- rnorm(n, 0, 2) Y &lt;- rexp(n, 3) mean(X &gt; Y) ## [1] 0.4372 The code mean(X &gt; Y) is computing \\[\\begin{equation*} \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i), \\end{equation*}\\] which is an approximation of \\(P(X&gt;Y)\\). Theory Why the sample mean \\(\\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i)\\) approximates the required probability \\(P(X&gt;Y)\\)? Recall the strong law of large numbers (SLLN): Let \\(X_1,\\ldots,X_n\\) be independent and identically distributed random variables with mean \\(\\mu:=E(X)\\). Let \\(\\overline{X}_n:= \\frac{1}{n} \\sum^n_{i=1}X_i\\). Then \\[\\overline{X}_n \\stackrel{a.s.}{\\rightarrow} \\mu.\\] Note: a.s. means almost surely (probability = 1). The above convergence means \\(P(\\lim_{n\\rightarrow \\infty} \\overline{X}_n = \\mu) = 1\\). Note that (the expectation of an indicator random variable is the probability that the corresponding event will happen) \\[ P(X&gt;Y) = E(I(X&gt;Y)).\\] To apply SLLN, we just need to recognize the underlying random variable is \\(I(X&gt;Y)\\) (which follows a Bernoulli distribution with parameter \\(P(X&gt;Y)\\)). Then, with probability \\(1\\), the sample mean \\[ \\frac{1}{n} \\sum^n_{i=1} I(X_i &gt; Y_i) \\rightarrow P(X&gt;Y).\\] The quantity on the LHS is what we compute in mean(X&gt;Y) (note that we are using vectorized comparison). This is the reason why we can use mean(X&gt;Y) to estimate \\(P(X&gt;Y)\\). Ex 1: Let X ~ N(mean = 2, sd =1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(\\max(X,Y)&gt;Z)\\). Recall the difference between pmax and max: # always try with simple examples when you test the usage of functions x &lt;- c(1, 2, 3) y &lt;- c(0, 5, 10) max(x, y) ## [1] 10 pmax(x, y) ## [1] 1 5 10 n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(pmax(x,y) &gt; z) ## [1] 0.5131 Ex 2 Let X ~ N(mean = 2, sd = 1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(\\min(X,Y)&gt;Z)\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(pmin(x, y) &gt; z) # what is the difference between pmin and min? ## [1] 0.11471 Ex 3 Let X ~ N(mean = 2, sd = 1), Y ~ Exp(rate = 2), Z ~ Unif(0, 4) (continuous uniform distribution on [0,4]). Estimate \\(P(X^2 Y&gt;Z)\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) z &lt;- runif(n, 0, 4) mean(x^2 * y &gt; z) ## [1] 0.41099 Ex 4 Person \\(A\\) generates a random variable \\(X \\sim N(2, 1)\\) and Person \\(B\\) generates a random variable \\(Z \\sim Unif(0, 4)\\). If \\(X &lt; Z\\), Person \\(A\\) will discard \\(X\\) and generate another random variable \\(Y \\sim Exp(0.5)\\). Find the probability that the number generated by \\(A\\) is greater than that by \\(B\\). n &lt;- 100000 greater &lt;- rep(0, n) for (i in 1:n) { X &lt;- rnorm(1, 2, 1) Z &lt;- runif(1, 0, 4) if (X&lt; Z) { Y &lt;- rexp(1, rate = 0.5) greater[i] = Y &gt; Z } else { greater[i] = 1 # 1 means A&#39;s no &gt; B&#39;s no } } mean(greater) ## [1] 0.63699 Remark: you may find that the following code gives you almost the same answer. Why? n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 0.5) z &lt;- runif(n, 0, 4) mean(pmax(x, y) &gt; z) ## [1] 0.63969 Ex 5 Let X ~ N(mean = 2, sd = 1) and Y ~ Exp(rate = 2). Estimate \\(E(min(X,Y))\\). n &lt;- 100000 x &lt;- rnorm(n, 2, 1) y &lt;- rexp(n, 2) mean(pmin(x, y)) ## [1] 0.445328 The code mean(pmin(x, y)) computes \\[\\begin{equation*} \\frac{1}{n} \\sum^n_{i=1} \\min(X_i, Y_i), \\end{equation*}\\] which approximates \\(E(\\min(X,Y))\\) by the SLLN. 6.2 Matrix Ex6: Write a function called matrix_times_vector to compute \\(X Y\\), where \\(X\\) is a matrix and \\(Y\\) is a vector. The output should be a vector. matrix_times_vector = function(X, Y) { as.vector(X %*% Y) } # e.g. X &lt;- matrix(1:12, 3, 4) Y &lt;- 1:4 X ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Y ## [1] 1 2 3 4 matrix_times_vector(X, Y) ## [1] 70 80 90 Note: X%*%Y will return a matrix. We can use as.vector to change it into a vector. It is common to see the error non-conformable arguments. This is because the dimensions of your matrices/vectors do not match. If you have a \\(n\\times p\\) matrix \\(A\\) and \\(m \\times q\\) matrix \\(B\\), you can do the matrix multiplication \\(AB\\) only if \\(p = m\\). In R, if this is not the case, there will be an error. Similarly, if you have a vector \\(d\\) of length \\(m\\). You can do the matrix multiplication \\(A d\\) only if \\(p = m\\). Ex7: Write a function called matrix_times_vector2 to compute \\(X Y\\), where \\(X\\) is a matrix and \\(Y\\) is a vector. The output should be a vector. However, you should check if the dimensions of the inputs are appropriate before you perform the calculation. Display an error message The dimensions do not match if this is not the case. matrix_times_vector2 = function(X, Y) { p &lt;- ncol(X) m &lt;- length(Y) if (p == m) { return(as.vector(X %*% Y)) } else { cat(&quot;The dimensions do not match&quot;) } } # e.g. X &lt;- matrix(1:12, 4, 3) Y &lt;- 1:4 matrix_times_vector2(X, Y) ## The dimensions do not match X &lt;- matrix(1:12, 3, 4) Y &lt;- 1:4 matrix_times_vector2(X, Y) ## [1] 70 80 90 Example: if A is matrix and x is a vector, you can find the sums of the elements in A and x by sum(A) and sum(x) respectively. x &lt;- 1:10 sum(x) ## [1] 55 A &lt;- matrix(1:10, 5, 2) sum(A) ## [1] 55 Ex 8: Find \\(\\sum^{5}_{x=1}\\sum^{4}_{y=1} \\frac{x}{x+y}\\) without any loops. (x &lt;- matrix(1:5, nrow = 4, ncol = 5, byrow = TRUE)) # define and display at the same time using (...) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 1 2 3 4 5 ## [3,] 1 2 3 4 5 ## [4,] 1 2 3 4 5 (y &lt;- matrix(1:4, nrow = 4, ncol = 5, byrow = FALSE)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1 ## [2,] 2 2 2 2 2 ## [3,] 3 3 3 3 3 ## [4,] 4 4 4 4 4 x / (x + y) # vectorized operation ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.5000000 0.6666667 0.7500000 0.8000000 0.8333333 ## [2,] 0.3333333 0.5000000 0.6000000 0.6666667 0.7142857 ## [3,] 0.2500000 0.4000000 0.5000000 0.5714286 0.6250000 ## [4,] 0.2000000 0.3333333 0.4285714 0.5000000 0.5555556 sum(x / (x + y)) ## [1] 10.72817 6.3 Basic Operation Example Let v be a vector of integers. Write a one-line R code to compute the product of all the even integers in v. To illustrate how to solve the question step by step: v &lt;- -10:10 # begin writing your code by setting some integers v %% 2 # find the remainder, if the remainder is 0, it is an even number ## [1] 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 v %% 2 == 0 # check which elements is 0 ## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE ## [17] TRUE FALSE TRUE FALSE TRUE v[v %% 2 == 0] # select the elements which are &quot;TRUE&quot; ## [1] -10 -8 -6 -4 -2 0 2 4 6 8 10 prod(v[v %% 2 == 0]) # find the product ## [1] 0 # the result is 0. v may not be a good example to check if the code is correct # let&#39;s change to some other vector v &lt;- 2:8 prod(v[v %% 2 == 0]) ## [1] 384 2 * 4 * 6 * 8 #check ## [1] 384 # now, the final answer is prod(v[v%%2 == 0]) Ex 9a: Given that x = 1:100. Write a one-line R code to copmute \\[\\begin{equation*} S := 1^2 - 2^2 + 3^2 - \\ldots + 99^2 - 100^2. \\end{equation*}\\] x &lt;- 1:100 sum((x[x %% 2 == 1])^2) - sum((x[x %% 2 == 0])^2) ## [1] -5050 Ex 9b (optional) Can you do Ex 9a without any program or calculator? Ex 10: Write a function with inputs n and p to compute \\[\\begin{equation*} S(n, p) := 1^p - 2^p + 3^p -\\ldots + (-1)^{n-1} (n-1)^p + (-1)^n n^p. \\end{equation*}\\] snp &lt;- function(n, p) { x &lt;- 1:n return(sum((x[x %% 2 == 1])^p) - sum((x[x %% 2 == 0])^p)) } 6.4 Some basic plots in R Example Write a function called my_summary_plot with input being two numeric vectors x,y and outputs a \\(2 \\times 2\\) multi-frame plot with (i) histogram of x, (ii) histogram of y, (iii) scatter plot of y versu x, and (iv) boxplots of x and y. # when you write a function, you can begin with some sample x and y x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) par(mfrow=c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) # after that, you wrap the code into a function without defining x, y my_summary_plot = function(x, y){ par(mfrow=c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) } # after you write the function, you should try if it will work or not rm(list=ls()) # let&#39;s remove everything in the memory my_summary_plot =function(x, y){ par(mfrow=c(2, 2)) hist(x) hist(y) plot(x, y) boxplot(x, y) } x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) my_summary_plot(x,y) # try another example x &lt;- rnorm(100, 0, 1) y &lt;- 5 * x + rnorm(100, 0, 1) my_summary_plot(x, y) "],["data-transformation-with-dplyr.html", "Chapter 7 Data Transformation with dplyr 7.1 Introduction 7.2 arrange() 7.3 filter() 7.4 select() 7.5 mutate() 7.6 summarize() 7.7 Summary", " Chapter 7 Data Transformation with dplyr 7.1 Introduction Reference: see https://r4ds.had.co.nz/transform.html Preparation We will use a dataset in the package nycflights13. To install it: install.packages(&quot;nycflights13&quot;) To use the dataset or functions in the package, we first load the library: library(nycflights13) In Chapter 4, we have installed tidyverse, which contains the package dplyr. Now, load the package library(tidyverse) ## function (.data, ...) ## { ## UseMethod(&quot;select&quot;) ## } ## &lt;bytecode: 0x0000025665c9b820&gt; ## &lt;environment: namespace:dplyr&gt; nycflights13 The dataset flights in the package nyclfights13 contains all \\(336,776\\) flights that departed from New York City in 2013. Check ?flights for details. # let&#39;s view the dataset flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA ## 2 2013 1 1 533 529 4 850 830 20 UA ## 3 2013 1 1 542 540 2 923 850 33 AA ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 ## 5 2013 1 1 554 600 -6 812 837 -25 DL ## 6 2013 1 1 554 558 -4 740 728 12 UA ## 7 2013 1 1 555 600 -5 913 854 19 B6 ## 8 2013 1 1 557 600 -3 709 723 -14 EV ## 9 2013 1 1 557 600 -3 838 846 -8 B6 ## 10 2013 1 1 558 600 -2 753 745 8 AA ## # ... with 336,766 more rows, and 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, ## # dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; flights is a tibble. Tibbles are data frames with better properties. Optional: If you are interested in the differences between a data frame and a tibble, you can go to https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html To view the complete dataset, use View(flights). Five key dplyr functions arrange(): reorder the rows fliter(): pick observations by their values select(): pick variables by their names mutate(): create new variables with functions of existing variables summarize(): collapse many values down to a single summary All functions work similarly: The first argument is a data frame/ tibble The subsequent argument describe what to do with the data frame, using the variable names (without quotes). The result is a new data frame. Of course, it is also possible to perform the same tasks without using dplyr functions. We will also discuss briefly how to use the base subsetting with [] to select the data. In general, the functions in dplyr are designed to transform the data more easily. 7.2 arrange() arrange() orders your dataset. If more than one column name is provided, each additional column will be used to break ties in the values of preceding columns. To reorder by a column in ascending order: arrange(flights, year, month, day) Lets create a simple dataset to illustrate this because flights is already sorted in year, month and day. (data &lt;- tibble(x = c(2,2,1,4,5), y = c(2,3,10,10,10))) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 ## 2 2 3 ## 3 1 10 ## 4 4 10 ## 5 5 10 arrange(data, x) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 ## 2 2 2 ## 3 2 3 ## 4 4 10 ## 5 5 10 # first sort in ascending order of x, use y to break any ties and sort in descending order arrange(data, x, desc(y)) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 ## 2 2 3 ## 3 2 2 ## 4 4 10 ## 5 5 10 To reorder by a column in descending order, use desc(): arrange(flights, desc(arr_delay)) Missing values are always sorted at the end # create a tibble with one column called x with values 5,2,NA df &lt;- tibble(x = c(5, 2, NA)) arrange(df, x) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 2 ## 2 5 ## 3 NA arrange(df, desc(x)) ## # A tibble: 3 x 1 ## x ## &lt;dbl&gt; ## 1 5 ## 2 2 ## 3 NA 7.2.1 Exercises Sort flights to find the most delayed flights. Find the flights that left earliest. arrange(flights, desc(dep_delay)) arrange(flights, dep_delay) Which flights traveled the longest? Which traveled the shortest? arrange(flights, desc(distance)) arrange(flights, distance) 7.3 filter() filter() only includes rows where the condition is TRUE Select all flights on Jan 1st: # flights is the name of your data frame # month == 1, day == 1 is the condition (jan1 &lt;- filter(flights, month == 1, day == 1)) ## # A tibble: 842 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA ## 2 2013 1 1 533 529 4 850 830 20 UA ## 3 2013 1 1 542 540 2 923 850 33 AA ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 ## 5 2013 1 1 554 600 -6 812 837 -25 DL ## 6 2013 1 1 554 558 -4 740 728 12 UA ## 7 2013 1 1 555 600 -5 913 854 19 B6 ## 8 2013 1 1 557 600 -3 709 723 -14 EV ## 9 2013 1 1 557 600 -3 838 846 -8 B6 ## 10 2013 1 1 558 600 -2 753 745 8 AA ## # ... with 832 more rows, and 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, ## # dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Lets use a simple dataset to see how to perform the same task without filter: # just a simple dataset (data &lt;- tibble(x = c(1,3,5,5,3), y = 1:5)) ## # A tibble: 5 x 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 1 1 ## 2 3 2 ## 3 5 3 ## 4 5 4 ## 5 3 5 data$x == 5 # logical vector ## [1] FALSE FALSE TRUE TRUE FALSE data[data$x == 5,] # select the rows with value &quot;TRUE&quot; ## # A tibble: 2 x 2 ## x y ## &lt;dbl&gt; &lt;int&gt; ## 1 5 3 ## 2 5 4 # returning to the flights dataset base_jan1 &lt;- flights[(flights$month == 1 &amp; flights$day == 1), ] (flights$month == 1 &amp; flights$day == 1) is a logical vector indicating if the corresponding flight was on Jan 1 (TRUE if yes). Now, lets check if jan1 and base_jan1 are the same using identical: identical(jan1, base_jan1) # TURE means they are the same, FALSE means they are not the same ## [1] TRUE More Examples Select flights that departed in Nov or Dec filter(flights, month == 11 | month == 12) # alternatively, simpler code filter(flights, month %in% c(11,12)) How to use the operator %in%? y &lt;- c(1,3,5) x &lt;- 1 x%in%y #whether 1 is in {1,3,5} ## [1] TRUE x &lt;- c(1,3,2,4,1) x%in%y # check if whether each element in x is in {1,3,5} ## [1] TRUE TRUE FALSE FALSE TRUE %in% also works with characters c(&quot;a&quot;, &quot;b&quot;) %in% c(&quot;a&quot;, &quot;c&quot;, &quot;d&quot;) ## [1] TRUE FALSE The result is TRUE FALSE because \"a\" is in c(\"a\", \"c\", \"d\") and \"b\" is not in c(\"a\", \"c\", \"d\"). Perform the same task without filter: flights[flights$month == 11 | flights$month == 12,] # or flights[flights$month %in% c(11, 12), ] Flights that were not delayed (on arrival or departure) by more than two hours: delay1 &lt;- filter(flights, arr_delay &lt;= 120, dep_delay &lt;= 120) Without using filter delay2 &lt;- flights[flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120, ] Lets check if delay1 and delay2 are the same. identical(delay1, delay2) ## [1] FALSE The result is FALSE, meaning delay1 and delay2 are not the same. Why? Because some elements in flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120 are NA. # to find out the number of NA values sum(is.na(flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120)) ## [1] 9304 As a result, with the base subsetting method, a row with all NA values will be selected. On the other hand, for filter, when a condition evaluates to NA, the row will be dropped. Lets create a small dataset to illustrate this. From now on, lets try to use tibble instead of data.frame. data &lt;- tibble(x = 1:4, y = c(1, 2, NA, 4)) data[data$y &lt;= 3, ] ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 NA NA # to avoid the above problem # which() returns which elements are TRUE which(data$y &lt;= 3) # the result is 1, 2 ## [1] 1 2 data[which(data$y &lt;= 3), ] # select row 1, row 2 ## # A tibble: 2 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 # using filter filter(data, y &lt;= 3) ## # A tibble: 2 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 # if we want to include the row where the value of y is NA # recall that | means &quot;or&quot; data[which(data$y &lt;= 3 | is.na(data$y)), ] ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA # using filter filter(data, y &lt;= 3 | is.na(y)) ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA If you want to drop the NA values with base subsetting[], you may use delay3 &lt;- flights[which((flights$arr_delay &lt;= 120 &amp; flights$dep_delay &lt;= 120) == TRUE), ] To see if delay1 and delay3 are exactly the same: identical(delay1, delay3) # TRUE means exactly the same ## [1] TRUE At this point, you should see that filter could perform the same tasks with simpler code. 7.3.1 Exercises 1a. Find all flights that had an arrival delay of two or more hours (drop the rows with NA in arr_delay). # Using &quot;filter&quot; filter(flights, arr_delay &gt;= 120) # Without using &quot;filter&quot; flights[which(flights$arr_delay &gt;= 120), ] 1b. Find all flights that flew to Houston (IAH or HOU). # Using &quot;filter&quot; filter(flights, dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)) # Without using &quot;filter&quot; flights[which(flights$dest %in% c(&quot;IAH&quot;, &quot;HOU&quot;)), ] 1c. Find all flights that were operated by United, American, or Delta # find all the sorted carrier codes in the dataset sort(unique(flights$carrier)) ## [1] &quot;9E&quot; &quot;AA&quot; &quot;AS&quot; &quot;B6&quot; &quot;DL&quot; &quot;EV&quot; &quot;F9&quot; &quot;FL&quot; &quot;HA&quot; &quot;MQ&quot; &quot;OO&quot; &quot;UA&quot; &quot;US&quot; &quot;VX&quot; &quot;WN&quot; &quot;YV&quot; # look up airline names from their carrier codes airlines ## # A tibble: 16 x 2 ## carrier name ## &lt;chr&gt; &lt;chr&gt; ## 1 9E Endeavor Air Inc. ## 2 AA American Airlines Inc. ## 3 AS Alaska Airlines Inc. ## 4 B6 JetBlue Airways ## 5 DL Delta Air Lines Inc. ## 6 EV ExpressJet Airlines Inc. ## 7 F9 Frontier Airlines Inc. ## 8 FL AirTran Airways Corporation ## 9 HA Hawaiian Airlines Inc. ## 10 MQ Envoy Air ## 11 OO SkyWest Airlines Inc. ## 12 UA United Air Lines Inc. ## 13 US US Airways Inc. ## 14 VX Virgin America ## 15 WN Southwest Airlines Co. ## 16 YV Mesa Airlines Inc. # after looking up the names, we know UA = United, AA = American, DL = Delta filter(flights, carrier %in% c(&quot;UA&quot;, &quot;AA&quot;, &quot;DL&quot;)) ## # A tibble: 139,504 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA ## 2 2013 1 1 533 529 4 850 830 20 UA ## 3 2013 1 1 542 540 2 923 850 33 AA ## 4 2013 1 1 554 600 -6 812 837 -25 DL ## 5 2013 1 1 554 558 -4 740 728 12 UA ## 6 2013 1 1 558 600 -2 753 745 8 AA ## 7 2013 1 1 558 600 -2 924 917 7 UA ## 8 2013 1 1 558 600 -2 923 937 -14 UA ## 9 2013 1 1 559 600 -1 941 910 31 AA ## 10 2013 1 1 559 600 -1 854 902 -8 UA ## # ... with 139,494 more rows, and 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, ## # dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1d. Find all flights that departed in summer (July, August, and September) filter(flights, month %in% c(7, 8, 9)) # Alternative Method filter(flights, between(month, 7, 9)) 1e. Find all flights that arrived more than two hours late, but didnt leave late filter(flights, arr_delay &gt; 120, dep_delay &lt;= 0) The next two exercises are trickier. 1f. Find all flights that were delayed by at least an hour, but made up over 30 minutes in flight. First, the flight was delayed by at least an hour is the same as dep_delay &gt;=60. Second, if a flight made up over 30 minutes in flight, the arrival delay must be at least 30 minutes less than the departure delay, which is the same as dep_delay - arr_delay &gt; 30. filter(flights, dep_delay &gt;= 60, dep_delay - arr_delay &gt; 30) 1g. Find all flights that departed between midnight and 6 a.m. (inclusive). The first question that should come to your mind is how is midnight represented in the dataset? Lets take a look at summary(flights$dep_time). summary(flights$dep_time) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1 907 1401 1349 1744 2400 8255 The minimum is 1 and the maximum is 2400. Therefore, you know midnight is represented by 2400 instead of 0 in this dataset. The answer to the question would be filter(flights, dep_time &lt;= 600 | dep_time == 2400) 7.4 select() Very often, we are only interested in some variables in a dataset. In that case, we can focus on the variables by creating a new dataset with those variables only. select() is to select the columns in a dataset by the name of the columns Selecting Variables Suppose you want to select the following \\(3\\) columns in flights: year, month, day: select(flights, year, month, day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows The usual way without using select() is flights[c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] # or flights[,c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows Select all columns between year and day (inclusive) select(flights, year:day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows Excluding Variables Select all columns except those from year to day (inclusive) select(flights, -(year:day)) select(flights, -year, -month, -day) Without using select() flights[,!(colnames(flights) %in% c(&quot;year&quot;, &quot;day&quot;, &quot;month&quot;))] 7.4.1 Exercises You can also use starts_with(\"abc\") matches names that begin with \"abc\" ends_with(\"xyz\") matches names that end with \"xyz\" contains(\"ijk\") mathces names that contain \"ijk\" Ex: select all columns that end with \"delay\". select(flights, ends_with(&quot;delay&quot;)) ## # A tibble: 336,776 x 2 ## dep_delay arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 ## 2 4 20 ## 3 2 33 ## 4 -1 -18 ## 5 -6 -25 ## 6 -4 12 ## 7 -5 19 ## 8 -3 -14 ## 9 -3 -8 ## 10 -2 8 ## # ... with 336,766 more rows Ex: select all columns that start with \"a\". select(flights, starts_with(&quot;a&quot;)) ## # A tibble: 336,776 x 3 ## arr_time arr_delay air_time ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 830 11 227 ## 2 850 20 227 ## 3 923 33 160 ## 4 1004 -18 183 ## 5 812 -25 116 ## 6 740 12 150 ## 7 913 19 158 ## 8 709 -14 53 ## 9 838 -8 140 ## 10 753 8 138 ## # ... with 336,766 more rows Does the result of running the following code surprise you? select(flights, contains(&quot;TIME&quot;)) ## # A tibble: 336,776 x 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time time_hour ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 517 515 830 819 227 2013-01-01 05:00:00 ## 2 533 529 850 830 227 2013-01-01 05:00:00 ## 3 542 540 923 850 160 2013-01-01 05:00:00 ## 4 544 545 1004 1022 183 2013-01-01 05:00:00 ## 5 554 600 812 837 116 2013-01-01 06:00:00 ## 6 554 558 740 728 150 2013-01-01 05:00:00 ## 7 555 600 913 854 158 2013-01-01 06:00:00 ## 8 557 600 709 723 53 2013-01-01 06:00:00 ## 9 557 600 838 846 140 2013-01-01 06:00:00 ## 10 558 600 753 745 138 2013-01-01 06:00:00 ## # ... with 336,766 more rows Yes, because we used TIME but not time and still get the selected columns. If you check ?contains, you can see that the default is to ignore the case. To change the default: select(flights, contains(&quot;TIME&quot;, ignore.case = FALSE)) ## # A tibble: 336,776 x 0 Ex: without using select(), select all the columns that contain time. flights[grep(&quot;time&quot;, names(flights))] ## # A tibble: 336,776 x 6 ## dep_time sched_dep_time arr_time sched_arr_time air_time time_hour ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 517 515 830 819 227 2013-01-01 05:00:00 ## 2 533 529 850 830 227 2013-01-01 05:00:00 ## 3 542 540 923 850 160 2013-01-01 05:00:00 ## 4 544 545 1004 1022 183 2013-01-01 05:00:00 ## 5 554 600 812 837 116 2013-01-01 06:00:00 ## 6 554 558 740 728 150 2013-01-01 05:00:00 ## 7 555 600 913 854 158 2013-01-01 06:00:00 ## 8 557 600 709 723 53 2013-01-01 06:00:00 ## 9 557 600 838 846 140 2013-01-01 06:00:00 ## 10 558 600 753 745 138 2013-01-01 06:00:00 ## # ... with 336,766 more rows Basic Usage of grep: grep(pattern, x). pattern: character string. e.g., time, delay, air x: a character vector where matches are sought. e.g., the colnames of a dataframe. Output: a vector of the indicaes of the elements of x that yielded a match. some_names &lt;- c(&quot;ab&quot;, &quot;bc&quot;, &quot;cd&quot;) grep(&quot;b&quot;, some_names) ## [1] 1 2 grep(&quot;d&quot;, some_names) ## [1] 3 grep(&quot;e&quot;, some_names) ## integer(0) 7.5 mutate() Add new variables with mutate() Very often, we want to create a new variable based on existing variables. For example, if we have distance and time, we can compute the speed by distance/time. mutate() always adds new columns at the end of the dataset. Lets create a narrower dataset so that we can see the result of mutate without use View(). # create a smaller dataset flights_sml &lt;- select(flights, year:day, arr_delay, dep_delay, distance, air_time) Now, lets use mutate() to create a variable called gain (how much time we gain in flight) defined as arr_delay-dep_delay and a variable called speed (miles/hour) defined as distance/air_time. mutate(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 x 9 ## year month day arr_delay dep_delay distance air_time gain speed ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 11 2 1400 227 9 370. ## 2 2013 1 1 20 4 1416 227 16 374. ## 3 2013 1 1 33 2 1089 160 31 408. ## 4 2013 1 1 -18 -1 1576 183 -17 517. ## 5 2013 1 1 -25 -6 762 116 -19 394. ## 6 2013 1 1 12 -4 719 150 16 288. ## 7 2013 1 1 19 -5 1065 158 24 404. ## 8 2013 1 1 -14 -3 229 53 -11 259. ## 9 2013 1 1 -8 -3 944 140 -5 405. ## 10 2013 1 1 8 -2 733 138 10 319. ## # ... with 336,766 more rows To keep the new variables only, use transmute(): transmute(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 x 2 ## gain speed ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9 370. ## 2 16 374. ## 3 31 408. ## 4 -17 517. ## 5 -19 394. ## 6 16 288. ## 7 24 404. ## 8 -11 259. ## 9 -5 405. ## 10 10 319. ## # ... with 336,766 more rows Without using select and mutate(), one may use flights_sml2 &lt;- flights[c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;arr_delay&quot;, &quot;dep_delay&quot;, &quot;distance&quot;, &quot;air_time&quot;)] flights_sml2$gain &lt;- flights_sml2$arr_delay - flights_sml2$dep_delay flights_sml2$speed &lt;- flights_sml2$distance/flights_sml2$air_time*60 7.5.1 Exercises Ex: Currently, dep_time and sched_dep_time are convenient to look at, but hard to compute with because they are not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Recall that dep_time and sched_dep_time are in HHMM format. For example, 1304 means 1:04pm. The number of minutes since midnight is \\(13\\times 60 + 4 = 784\\). In general, we can use the integer division to find the number of hours since midnight, then multiply by \\(60\\), and finally add the remainder for the minutes. For example, 1304 %/% 100 # get the number of hours since midnight ## [1] 13 1304 %% 100 # get the remainder ## [1] 4 1304 %/% 100 * 60 + 1304 %% 100 # number of minutes since midnight ## [1] 784 Recall that midnight is represented as 2400 in the dataset and the number of minutes since midnight should be 0. However, if we use the above method for midnight, we get 2400 %/% 100 * 60 + 2400 %% 100 # this is not correct for midnight ## [1] 1440 Therefore, we also have to deal with this case. One possible solution is to do another integer division by 1440 (24x60 = 1440): (1304 %/% 100 * 60 + 1304 %% 100) %% 1440 # will not change the result if the time is not midnight ## [1] 784 (2400 %/% 100 * 60 + 2400 %% 100) %% 1440 # this is correct for midnight ## [1] 0 Go back to flights: # let&#39;s keep only the dep_time and sched_dep_time flights_time &lt;- select(flights, dep_time, sched_dep_time) mutate(flights_time, min_dep_time = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, min_sched_dep_time = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440) ## # A tibble: 336,776 x 4 ## dep_time sched_dep_time min_dep_time min_sched_dep_time ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 515 317 315 ## 2 533 529 333 329 ## 3 542 540 342 340 ## 4 544 545 344 345 ## 5 554 600 354 360 ## 6 554 558 354 358 ## 7 555 600 355 360 ## 8 557 600 357 360 ## 9 557 600 357 360 ## 10 558 600 358 360 ## # ... with 336,766 more rows The above code doesnt look good because we have written the formula to complete the same task twice. We can define a function to do this: time_to_minutes &lt;- function(x) { (x %/% 100 * 60 + x %% 100) %% 1440 } With the function time_to_minutes, we have: mutate(flights, min_dep_time = time_to_minutes(dep_time), min_sched_dep_time = time_to_minutes(sched_dep_time)) If you cannot think of using %%1440 to deal with the midnight cases, we can write: # ind for indicator approach time_to_minutes_ind &lt;- function(x) { (x %/% 100 * 60 + x %% 100) * (x != 2400) } # Explanation # if x is 2400, (x!=2400) is FALSE, FALSE times a number y is 0 # if x is not 2400, (x!=2400) is TRUE, TRUE times a number y is y # because &quot;FALSE=0, TRUE=1&quot; mutate(flights, min_dep_time = time_to_minutes_ind(dep_time), min_sched_dep_time = time_to_minutes_ind(sched_dep_time)) An alternative way is to use ifelse (which is a vectorized function). Usage of ifelse: ifelse(test, yes, no) time_to_minutes_ifelse &lt;- function(x) { ifelse(x != 2400, x %/% 100 * 60 + x %% 100, 0) } mutate(flights, min_dep_time = time_to_minutes_ifelse(dep_time), min_sched_dep_time = time_to_minutes_ifelse(sched_dep_time)) ## # A tibble: 336,776 x 21 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA ## 2 2013 1 1 533 529 4 850 830 20 UA ## 3 2013 1 1 542 540 2 923 850 33 AA ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 ## 5 2013 1 1 554 600 -6 812 837 -25 DL ## 6 2013 1 1 554 558 -4 740 728 12 UA ## 7 2013 1 1 555 600 -5 913 854 19 B6 ## 8 2013 1 1 557 600 -3 709 723 -14 EV ## 9 2013 1 1 557 600 -3 838 846 -8 B6 ## 10 2013 1 1 558 600 -2 753 745 8 AA ## # ... with 336,766 more rows, and 11 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, ## # dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ## # min_dep_time &lt;dbl&gt;, min_sched_dep_time &lt;dbl&gt; A less efficient way with for loop and if-else: # if for if-else approach time_to_minutes_if &lt;- function(x) { n &lt;- length(x) output &lt;- rep(0, n) for (i in 1:n) { if(is.na(x[i])){ # check for NA output[i] &lt;- NA } else if (x[i] != 2400) { # if not equal to 2400 output[i] &lt;- x[i] %/% 100 * 60 + x[i] %% 100 } else { # if equal to 2400 output[i] &lt;- 0 } } return(output) } mutate(flights, min_dep_time = time_to_minutes_if(dep_time), min_sched_dep_time = time_to_minutes_if(sched_dep_time)) Compare the efficiency: system.time(mutate(flights, min_dep_time = time_to_minutes(dep_time), min_sched_dep_time = time_to_minutes(sched_dep_time))) ## user system elapsed ## 0.05 0.00 0.05 system.time(mutate(flights, min_dep_time = time_to_minutes_if(dep_time), min_sched_dep_time = time_to_minutes_if(sched_dep_time))) ## user system elapsed ## 0.36 0.00 0.36 7.6 summarize() Average delay over the year (not useful): summarize(flights, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 x 1 ## delay ## &lt;dbl&gt; ## 1 12.6 Using summarize with group_by can result in more useful statistics. Use group_by. First argument is your dataset. Subsequent arguments indicate how you want to group the data. In summarize, the dataset becomes the dataset from group_by. E.g.: Average delay per date (more useful): by_day &lt;- group_by(flights, year, month, day) summarize(by_day, delay = mean(dep_delay, na.rm = TRUE)) ## `summarise()` regrouping output by &#39;year&#39;, &#39;month&#39; (override with `.groups` argument) ## # A tibble: 365 x 4 ## # Groups: year, month [12] ## year month day delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.5 ## 2 2013 1 2 13.9 ## 3 2013 1 3 11.0 ## 4 2013 1 4 8.95 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.55 ## 9 2013 1 9 2.28 ## 10 2013 1 10 2.84 ## # ... with 355 more rows E.g: Average delay per month: by_month &lt;- group_by(flights, year, month) summarize(by_month, delay = mean(dep_delay, na.rm = TRUE)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) ## # A tibble: 12 x 3 ## # Groups: year [1] ## year month delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 10.0 ## 2 2013 2 10.8 ## 3 2013 3 13.2 ## 4 2013 4 13.9 ## 5 2013 5 13.0 ## 6 2013 6 20.8 ## 7 2013 7 21.7 ## 8 2013 8 12.6 ## 9 2013 9 6.72 ## 10 2013 10 6.24 ## 11 2013 11 5.44 ## 12 2013 12 16.6 # we only have 1 year, not necessary to use year by_month &lt;- group_by(flights, month) summarize(by_month, delay = mean(dep_delay,na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 12 x 2 ## month delay ## &lt;int&gt; &lt;dbl&gt; ## 1 1 10.0 ## 2 2 10.8 ## 3 3 13.2 ## 4 4 13.9 ## 5 5 13.0 ## 6 6 20.8 ## 7 7 21.7 ## 8 8 12.6 ## 9 9 6.72 ## 10 10 6.24 ## 11 11 5.44 ## 12 12 16.6 Ex: Find the average weights of the cars grouped by the number of gears. mtcars_gear &lt;- group_by(mtcars, gear) summarize(mtcars_gear, avg_wt = mean(wt, na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 2 ## gear avg_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3.89 ## 2 4 2.62 ## 3 5 2.63 Ex: Find the sample standard deviation of the weights of the cars grouped by the number of gears. mtcars_gear &lt;- group_by(mtcars, gear) summarize(mtcars_gear, avg_wt = sd(wt, na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 2 ## gear avg_wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 0.833 ## 2 4 0.633 ## 3 5 0.819 7.7 Summary Functions and operators learned in this chapter: arrange, filter, identical, %in%, tibble, ifelse, mutate, transmute, select, grep, group_by, summarize "],["data-visualization-with-ggplot2.html", "Chapter 8 Data Visualization with ggplot2 8.1 Combining Multiple Operations with Pipe %&gt;% 8.2 Bar charts 8.3 Line Graph 8.4 Scatter Plots 8.5 Summarizing Data Distributions 8.6 Saving your plots 8.7 Summary", " Chapter 8 Data Visualization with ggplot2 Main reference for this chapter: R graphics cookbook (https://r-graphics.org/) In Chapter 4, we learned how to create some basic plots with base R and the function ggplot (in the package ggplot2, which is also contained in the package tidyverse). In this chapter, we will study how to use ggplot for data visualization in more detail. We will use some of the datasets from the the package gcookbook. Therefore, we will install it now. install.packages(&quot;gcookbook&quot;) Load the packages gcookbook, tidyverse and nycflights13. library(gcookbook) # contains some datasets for illustration library(tidyverse) # contains ggplot2 and dplyr library(nycflights13) # contains the dataset &quot;flights&quot; 8.1 Combining Multiple Operations with Pipe %&gt;% You can use the shortcut Ctrl/Cmd + Shift + M to insert the pipe operator %&gt;%. x %&gt;% f(y) turns into f(x,y) For example, flights %&gt;% filter(month == 1, day == 1) is the same as filter(flights, month == 1, day == 1) x %&gt;% f(y) %&gt;% g(z) turns into g(f(x,y),z). For example, flights %&gt;% group_by(year, month) %&gt;% summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) is the same as summarize(group_by(flights, year, month), mean_dep_delay = mean(dep_delay, na.rm = TRUE)) Using the pipe can avoid creating and naming intermediate objects that we dont need. Instead, the pipe focuses on the sequence of actions, not the object that the actions being performed on. It also tends to make the code easier to read. For the above example, you can read it as: for the dataset flights, we first group the data by year and month, then summarize the data by finding the mean. We can think of the pipe %&gt;% as then. Pipe %&gt;% can also be used with ggplot: flights %&gt;% group_by(year, month) %&gt;% summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ggplot(aes(x = month, y = mean_dep_delay)) + geom_col() # notice the spacing for good indentation Notice the difference between %&gt;% and + in the above code. Note: if you have to manipulate some intermediate objects, it may make sense not to use the pipe in that situation. 8.2 Bar charts We will start with bar charts. Many of the usages discussed in this section can also be transferable to create other plots. Recall that there are two types of bar charts: Bar chart of values. x-axis: discrete variable, y-axis: numeric data (not necessarily count data) Bar chart of counts. x-axis: discrete variable, y-axis: count of cases in the discrete variable Using ggplot: For bar chart of values, we use geom_col(), which is the same as using geom_bar(stat = \"identity\"). For bar chart of counts, we use geom_bar(), which is the same as using geom_bar(stat = \"count\"). That is, the default for geom_bar() is to use stat = \"count\". Bar chart of values: pg_mean is a simple dataset with groupwise means of some plant growth data. pg_mean ## group weight ## 1 ctrl 5.032 ## 2 trt1 4.661 ## 3 trt2 5.526 ggplot(data = pg_mean, mapping = aes(x = group, y = weight)) + geom_col() Recall the mtcars dataset. Lets create a bar chart of values for the mean weights grouped by the number of gears. First, we summarize the data using summarize. by_gear &lt;- group_by(mtcars, gear) mtcars_wt &lt;- summarize(by_gear, mean_wt_by_gear = mean(wt)) # Alternatively, using %&gt;% mtcars_wt &lt;- mtcars %&gt;% group_by(gear) %&gt;% summarize(mean_wt_by_gear = mean(wt)) Create the bar chart: ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col() To change the colour of the bars, use fill. ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(fill = &quot;lightblue&quot;) By default, there is no outline around the fill. To add an outline, use colour (or color). ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(color = &quot;red&quot;) Of course, you can combine the two settings: ggplot(mtcars_wt, aes(x = gear, y = mean_wt_by_gear)) + geom_col(fill = &quot;lightblue&quot;, color = &quot;red&quot;) Graph with grouped bars The most basic bar chart of values have one categorical variable on the x-axis and one continuous variable on the y-axis. If you want to include another categorical variable to divide up the data, you can use a graph with grouped bars. In mtcars, vs represents the engine of the car with 0 = V-shaped and 1 = straight. We can use vc to divide up the data in addition to gear using fill. To create a grouped bar chart, set position = \"dodge\" in geom_col(); otherwise, you will get a stacked bar chart. # prepare the data by_gear_vs &lt;- group_by(mtcars, gear, vs) mtcars_wt2 &lt;- summarize(by_gear_vs, mean_wt = mean(wt)) # convert to factor in the data mtcars_wt2$vs &lt;- as.factor(mtcars_wt2$vs) # using pipe %&gt;% to prepare the data mtcars_wt2 &lt;- mtcars %&gt;% group_by(gear, vs) %&gt;% summarize(mean_wt = mean(wt)) %&gt;% ungroup() %&gt;% mutate(vs = as.factor(vs)) # plot ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) Without position = \"dodge\", we get a stacked bar chart: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col() You can also convert vs to factor in call to ggplot(): # prepare the data by_gear_vs &lt;- group_by(mtcars, gear, vs) mtcars_wt3 &lt;- summarize(by_gear_vs, mean_wt = mean(wt)) # plot ggplot(mtcars_wt3, aes(x = gear, y = mean_wt, fill = factor(vs))) + geom_col(position = &quot;dodge&quot;) To change the colours of the bars: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Pastel2&quot;) You can try with different palettes: library(RColorBrewer) display.brewer.all() Using palette = \"Oranges\": ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_brewer(palette = &quot;Oranges&quot;) Using a manually defined palette: ggplot(mtcars_wt2, aes(x = gear, y = mean_wt, fill = vs)) + geom_col(position = &quot;dodge&quot;) + scale_fill_manual(values = c(&quot;#cc6666&quot;, &quot;#66cccc&quot;)) # also see Section 4.2 for the color Bar Charts of Counts Creating a bar chart of counts is very similar to creating a bar chart of values. Bar chart of the number of cars by gear in mtcars: ggplot(mtcars, aes(x = gear)) + geom_bar() Bar chart of the number of flights by each month in nycflights13: ggplot(flights, aes(x = month)) + geom_bar(fill = &quot;lightblue&quot;) Controlling the width (by default, width = 0.9): ggplot(flights, aes(x = month)) + geom_bar(fill = &quot;lightblue&quot;, width = 0.5) Bar chart of the number of flights by origin and month: # have to convert month to factor ggplot(flights, aes(x = origin, fill = factor(month))) + geom_bar(position = &quot;dodge&quot;, color = &quot;black&quot;) 8.3 Line Graph Suppose you want to make a line graph of the daily average departure delay in flights. From now on, we will use %&gt;% whenever it is appropriate. avg_delay &lt;- flights %&gt;% group_by(month, day) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() %&gt;% mutate(Time = 1:365) ggplot(avg_delay, aes(x = Time, y = delay)) + geom_line() Labeling the graph: # notice how we put each argument on its own line when the arguments # do not all fit on one line ggplot(avg_delay, aes(x=Time, y=delay)) + geom_line() + labs( y = &quot;Average Delay&quot;, title = &quot;Daily Average Departure Delay of Flights from NYC in 2013&quot; ) By default, the range of the y-axis of a line graph is just enough to include all the y values in the data. Sometimes, you may want to change the range manually. For example, the range of the y-axis in the following graph does not include 0. ggplot(BOD, aes(x = Time, y = demand)) + geom_line() If you want to include 0 in the y range, you can use ylim: ggplot(BOD, aes(x = Time, y = demand)) + geom_line() + ylim(0, max(BOD$demand)) Line Graph with multiple lines Suppose we want to create a line graph showing the daily average departure delay from the 3 airports in flights. # prepare the data flights_delay &lt;- flights %&gt;% group_by(month, origin) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) %&gt;% ungroup() Line Graph: ggplot(flights_delay, aes(x = month, y = delay, color = origin)) + geom_line() With different line types: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin)) + geom_line() Add the points on top of the lines: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin)) + geom_line() + geom_point() Change the point shapes according to origin: ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin, shape = origin)) + geom_line() + geom_point() To use one single shape for the points, we can specify the shape in geom_point(). The default shape is shape = 16. The default size is size = 2. fill is only applicable for shape = 21 to 25. ggplot(flights_delay, aes(x = month, y = delay, linetype = origin, color = origin)) + geom_line() + geom_point(shape = 22, size = 3, fill = &quot;white&quot;, color = &quot;darkred&quot;) Using another colour palette and changing the size of the lines: ggplot(flights_delay, aes(x = month, y = delay, color = origin)) + geom_line(size = 2) + geom_point(shape = 22, size = 3, fill = &quot;white&quot;, color = &quot;darkred&quot;)+ scale_colour_brewer(palette = &quot;Set2&quot;) 8.4 Scatter Plots Scatter plots are often used to visualize the relationship between two continuous variables. It is also possible to use a scatter plot when either or both variables are discrete. The dataset heightweight contains sex, age, height and weight of some schoolchildren. head(heightweight) ## sex ageYear ageMonth heightIn weightLb ## 1 f 11.92 143 56.3 85.0 ## 2 f 12.92 155 62.3 105.0 ## 3 f 12.75 153 63.3 108.0 ## 4 f 13.42 161 59.0 92.0 ## 5 f 15.92 191 62.5 112.5 ## 6 f 14.25 171 62.5 112.0 To create a basic scatter plot, use geom_point(): ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point() You can control the shape, size, and color of the points as illustrated in the last section. ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point(size = 1.5, shape = 4, color = &quot;blue&quot;) If shape = 21-25, you can control the color in the points and outline of the points using fill and color, respectively. ggplot(heightweight, aes(x = ageYear, y = heightIn)) + geom_point(size = 1.5, shape = 22, fill = &quot;red&quot;, color = &quot;blue&quot;) Visualizing an additional discrete variable Suppose you want to use different colours for the points according to different categories of sex. ggplot(heightweight, aes(x = ageYear, y = heightIn, color = sex)) + geom_point() Suppose you want to use different shapes for the points according to different categories of sex. ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex)) + geom_point() You can use colours and shapes at the same time: ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, color = sex)) + geom_point() You can change the shapes or colours manually: ggplot(heightweight, aes(x = ageYear, y = heightIn, shape = sex, color = sex)) + geom_point() + scale_shape_manual(values = c(21,22)) + scale_colour_brewer(palette = &quot;Set2&quot;) Visualizing an additional continuous variable You may map an additional continuous variable to color. ggplot(heightweight, aes(x = ageYear, y = heightIn, color = weightLb)) + geom_point() Visualizing two additional discrete variables Lets create a new column to indicate if the child weights &lt; 100 or &gt;= 100 pounds (this is a discrete variable). heightweight2 &lt;- heightweight %&gt;% mutate(weightgroup = ifelse(weightLb &lt; 100, &quot;&lt; 100&quot;, &quot;&gt;= 100&quot;)) Now, we can add both sex and weightgroup in the plot in the following way: ggplot(heightweight2, aes(x = ageYear, y = heightIn, shape = sex, fill = weightgroup)) + geom_point() + scale_shape_manual(values = c(21, 24)) + scale_fill_manual( values = c(&quot;red&quot;, &quot;black&quot;), guide = guide_legend(override.aes = list(shape = 21)) # to change the legend ) Changing the mark ticks, limits and labels of the x-axis and y-axis: ggplot(heightweight2, aes(x = ageYear, y = heightIn, shape = sex, fill = weightgroup)) + geom_point() + scale_shape_manual(values = c(21, 24)) + scale_fill_manual( values = c(&quot;red&quot;, &quot;black&quot;), guide = guide_legend(override.aes = list(shape = 21)) # to change the legend ) + scale_x_continuous(name = &quot;Age (Year)&quot;, breaks = 11:18, limits = c(11, 18)) + scale_y_continuous(name = &quot;Height (In)&quot;, breaks = seq(50, 70, 5), limits = c(50, 73)) 8.4.1 Overplotting Overplotting refers to the situation when you have a large dataset so that the points in a scatter plot overlap and obscure each other. # We can create a variable to store the &quot;ggplot&quot; diamonds_ggplot &lt;- ggplot(diamonds, aes(x = carat, y = price)) diamonds_ggplot + geom_point() Possible solutions for overplotting: Use smaller points (size) # with diamonds_ggplot, we do not have to type # ggplot(diamonds, aes(x = carat, y = price)) diamonds_ggplot + geom_point(size = 0.1) Make the points semitransparent (alpha) diamonds_ggplot + geom_point(alpha = 0.05, size = 0.1) # 0.05 = 95% transparent We can see some vertical bands at some values of carats, meaning that diamonds tend to be cut to those sizes. Bin the data into rectangles (stat_bin2d) bins controls the number of bins in the x and y directions. The color of the rectangle indicates how many data points there are in the region. # by default, bins = 30 diamonds_ggplot + stat_bin2d(bins = 3) With bins = 50: diamonds_ggplot + stat_bin2d(bins = 50) + scale_fill_gradient(low = &quot;lightblue&quot;, high = &quot;red&quot;) Overplotting can also occur when the data is discrete on one or both axes. In the following example, we use the dataset ChickWeight, where Time is a discrete variable. head(ChickWeight) ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 # create a base plot cw_ggplot &lt;- ggplot(ChickWeight, aes(x = Time, y = weight)) cw_ggplot + geom_point() You may randomly jitter the points: cw_ggplot + geom_point(position = &quot;jitter&quot;) Jittering the points means a small amount of random variation is added to the location of each point. If you only want to jitter in the x-direction: cw_ggplot + geom_point(position = position_jitter(width = 0.5, height = 0)) 8.4.2 Labelling points in a scatter plot We can use annotate() or geom_text_repel() to label points in a scatter plot. For the latter, we have to install the package ggrepel. We will use the countries dataset in the package gcookbook and visualize the relationship between health expenditures and infant mortality rate. We will consider a subset of data by focusing the data from 2009 and countries with more than \\(2,000\\) USD health expenditures per capita: countries_subset &lt;- countries %&gt;% filter(Year == 2009, healthexp &gt; 2000) Using annotate: # find out the x and y coordinates for the point corresponding to Canada canada_x &lt;- filter(countries_subset, Name == &quot;Canada&quot;)$healthexp canada_y &lt;- filter(countries_subset, Name == &quot;Canada&quot;)$infmortality ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + annotate(&quot;text&quot;, x = canada_x, y = canada_y + 0.2, label = &quot;Canada&quot;) # + 0.2 is to avoid the label placing on top of the point Label all the points with geom_text_repel: # to use geom_text_repel, load the package ggrepel library(ggrepel) ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + geom_text_repel(aes(label = Name), size = 3) Label all the points with geom_label_repel (with a box around the label): # geom_label_repel also depends on the package ggrepel ggplot(countries_subset, aes(x = healthexp, y = infmortality)) + geom_point() + geom_label_repel(aes(label = Name), size = 3) 8.5 Summarizing Data Distributions 8.5.1 Histogram Histogram can be used to visualize the distribution of a variable. We will illustrate how to create histograms using the dataset birthwt from the package MASS. library(MASS) birthwt contains data of 189 birth weights with some covariates of the mothers. Take a look at the dataset: head(birthwt) ## low age lwt race smoke ptl ht ui ftv bwt ## 85 0 19 182 2 0 0 0 1 0 2523 ## 86 0 33 155 3 0 0 0 0 3 2551 ## 87 0 20 105 1 1 0 0 0 1 2557 ## 88 0 21 108 1 1 0 0 1 2 2594 ## 89 0 18 107 1 1 0 0 1 0 2600 ## 91 0 21 124 3 0 0 0 0 0 2622 Basic histogram: ggplot(birthwt, aes(x=bwt)) + geom_histogram() Plot a histogram with density (not frequency): ggplot(birthwt, aes(x=bwt)) + geom_histogram(aes(y = ..density..)) To compare two histograms Use facet_grid() to display two histograms in the same plot. Suppose we group the data according to the smoking status during pregnancy and we want to display the two histograms of the birth weight: ggplot(birthwt, aes(x = bwt)) + geom_histogram() + facet_grid(smoke ~ .) To change the label, we can change the content of the variable: # create another dataset birthwt_mod &lt;- birthwt birthwt_mod$smoke &lt;- ifelse(birthwt_mod$smoke == 1, &quot;Smoke&quot;, &quot;No Smoke&quot;) ggplot(birthwt_mod, aes(x = bwt)) + geom_histogram() + facet_grid(smoke ~ .) Alternatively, we can use recode_factor: birthwt_mod$smoke &lt;- recode_factor(birthwt_mod$smoke, &quot;0&quot; = &quot;No Smoke&quot;, &quot;1&quot; = &quot;Smoke&quot;) Use fill() to put two groups in the same plot with different colors. We need to set position = \"identity\"; otherwise, the bars will be stacked on top of each other vertically which is not what we want. ggplot(birthwt_mod, aes(x=bwt, fill=smoke)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) #+ # facet_grid(race~., scales=&quot;free&quot;) It is also possible to use both facet_grid and fill when we have want to group the data with two discrete variables. We will illustrate this with grouping according to the smoking status and the race. We also add scales = \"free\" so that the ranges of the y-axes will be adjusted according to the data in each histogram. # change the name so that the labels can be understood easily birthwt_mod$race[which(birthwt_mod$race==1)] = &quot;White&quot; birthwt_mod$race[which(birthwt_mod$race==2)] = &quot;Black&quot; birthwt_mod$race[which(birthwt_mod$race==3)] = &quot;Other&quot; ggplot(birthwt_mod, aes(x = bwt, fill = smoke)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.4) + facet_grid(race ~ ., scales = &quot;free&quot;) Note: we do not have a large dataset in this example so that grouping by two variables may not give us a very good understanding of the data. 8.5.2 Kernel Density Estimate Kernel density estimation is a nonparametric method to estimate the density of the samples. Nonparametric method means we do not impose a parametric model. A parametric model has a finite dimensional parameter \\(\\theta \\in \\mathbb{R}^d\\) for some finite \\(d\\). Let \\(X_1,\\ldots,X_n\\) be i.i.d. random variables from some distribution with density \\(f\\). The histogram for \\(f\\) at point \\(x_0\\) is \\[\\begin{equation*} \\hat{f}(x_0) = \\frac{\\text{number of $x_i$ in the bin containing $x_0$}}{n h}, \\end{equation*}\\] where the bin width is \\(h\\). As we already know, the histogram will not give a smooth estimate of the density. One may use another method called kernel density estimator, which could produce smooth estimate of the density. The kernel density estimator is \\[\\begin{equation*} \\hat{f}_n(x_0) = \\frac{1}{nh}\\sum^n_{i=1} K \\bigg( \\frac{x_0 - x_i}{h} \\bigg), \\end{equation*}\\] where \\(K\\) is a kernel and \\(h\\) is the bandwidth. For our purposes, a kernel is a non-negative symmetric function such that \\(\\int^\\infty_{-\\infty}K(x)dx = 1\\) and \\(\\int^\\infty_{-\\infty} x K(x)dx =0\\). For example, \\[\\begin{eqnarray*} \\text{the boxcar kernel:} &amp;&amp; K(x) = \\frac{1}{2}I(|x| \\leq 1)\\\\ \\text{the Gaussian kernel:} &amp;&amp; K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\\\ \\text{the Epanechnikov kernel:} &amp;&amp; K(x) = \\frac{3}{4}(1-x^2)I(|x| \\leq 1) \\\\ \\text{the tricube kernel:} &amp;&amp; K(x) = \\frac{70}{81}(1-|x|^3)^3I(|x| \\leq 1), \\end{eqnarray*}\\] where \\(I(|x| \\leq 1) = 1\\) if \\(|x| \\leq 1\\) and equals \\(0\\) otherwise. Since the kernel is symmetric around \\(0\\), the magnitude \\((x-x_i)/h\\) is the distance from \\(0\\). For the above kernels, the value of the kernels is smaller when we evaluate at a point further from \\(0\\). Therefore, data close to \\(x_0\\) will contribute larger weights in estimating \\(\\hat{f}(x_0)\\). The bandwidth will control the smoothness of the estimate: larger bandwidth will result in a smoother curve and smaller bandwidth will result in a noisy and rough curve. We can create a kernel density estimate of the distribution using geom_density(). ggplot(birthwt, aes(x = bwt)) + geom_density() + geom_density(adjust = 0.25, color = &quot;red&quot;) + # smaller bandwidth -&gt; noisy geom_density(adjust = 2, color = &quot;blue&quot;) # large bandwidth -&gt; smoother Overlaying a density curve with a histogram ggplot(birthwt, aes(x = bwt)) + geom_histogram(fill = &quot;cornsilk&quot;, aes(y = ..density..)) + geom_density() Displaying kernel density Estimates from grouped data To use geom_density() to display kernel density estimates from grouped data, the grouping variable must be a factor or a character vector. Recall that in birthwt_mod that we created earlier, the smoke variable is a character vector. With color: ggplot(birthwt_mod, aes(x = bwt, color = smoke)) + geom_density() With fill: ggplot(birthwt_mod, aes(x = bwt, fill = smoke)) + geom_density(alpha = 0.3) # to control the transparency With facet_grid(): ggplot(birthwt_mod, aes(x = bwt)) + geom_density() + facet_grid(smoke ~ .) 8.6 Saving your plots There are two types of image files: vector and raster (bitmap) Raster images are pixel-based. When you zoom in the image, you can see the individual pixels. Two examples are JPG and PNG files. JPG files quality is lower than that of the PNG files. Vector images are constructed using mathematical formulas. You can resize the image without a loss in image quality. When you zoom in the image, it is still smooth and clear. Two examples are AI and PDF files. 8.6.1 Outputting to pdf vector files Suppose you want to save the plot from the following code: ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() # first argument is the file name # width and height are in inches pdf(&quot;filename.pdf&quot;, width = 4, height = 4) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() dev.off() Outputting to a pdf file: usually the best option usually smaller than bitmap files such as PNG files. when you have overplotting (many points on the plot), a PDF file can be much larger than a PNG file. 8.6.2 Outputting to bitmap files # width and heights are in pixels png(&quot;png_plot.png&quot;, width = 600, height = 600) ggplot(mtcars, aes(x=wt,y=mpg)) + geom_point() dev.off() For high-quality print output, it is recommended to use at least 300 ppi (ppi = pixels per inch). Suppose you want to create a 4x4-inch PNG file with 300 ppi: ppi &lt;- 300 png(&quot;png_plot.png&quot;, width = 4*ppi, height = 4*ppi, res = ppi) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() dev.off() 8.7 Summary 8.7.1 Combining multiple operations with pipe %&gt;% 8.7.2 Bar charts examples of using pipe %&gt;% together with ggplot create bar charts of counts create bar charts of values change fill and outline of the bars create grouped bar charts create stacked bar charts convert a variable into factor in ggplot use different colour palette control the width of the bars 8.7.3 Line graphs create line graphs label the graph change the range of y-axis create line graphs with multiple lines use multiple geoms (geometric objects) (e.g. additing the points on top of the lines) change shape, size, fill, outline of points change line type 8.7.4 Scatter plot create scatter plots visualize an additional discrete variable visualize an additional continuous variable visualize two additional discrete variables overplotting (use smaller points, make points semitransparent, bin data into rectangels, jitter the points) label points in a scatter plot 8.7.5 Summarizing data distributions create histograms (frequency and density) compare two histograms (facet_grid(), fill()) create histograms with two additional discrete variables create kernel density estimates overlay a density curve with a histogram display kernel density estimates from grouped data (color, fill, facet_grid) 8.7.6 Saving your plots output to pdf vector files output to bitmap files "],["statistical-inference-in-r.html", "Chapter 9 Statistical Inference in R 9.1 Maximum Likelihood Estimation 9.2 Interval Estimation and Hypothesis Testing", " Chapter 9 Statistical Inference in R In this chapter, we discuss how to perform some estimations and hypothesis testings in R. You may have learned their theory in previous statistics courses. I do not intend to give a very comprehensive review to these methods due to time constraint. Optional Readings: You can find a few more statistical tests in Ch 9 of R Cookbook (https://rc2e.com/) You can review Ch 10-13 of John E. Freunds Mathematical Statistics with Applications by Irwin Miller and Marylees Miller (textbook for STAT 269) for some background and theory on statistical inference 9.1 Maximum Likelihood Estimation After you collect some data and formulate a statistical model, you have to estimate the parameters in your model. One of the most common methods is to use maximum likelihood estimation. Very often, there is no closed-form expression for your estimators. In general, suppose you have data \\(y_1,\\ldots,y_n\\). The likelihood function is a function of the parameter defined as \\[\\begin{equation*} L(\\theta|y_1,\\ldots,y_n) = f_{y_1,\\ldots,y_n}(y_1,\\ldots,y_n|\\theta), \\end{equation*}\\] where \\(f_{y_1,\\ldots,y_n}(\\cdot |\\theta)\\) is the joint pmf or pdf of \\(y_1,\\ldots,y_n\\) with parameter \\(\\theta\\). That is, the likelihood function evaluated at \\(\\theta\\) is simply the joint probability of observing \\(y_1,\\ldots,y_n\\) when the parameter value is \\(\\theta\\). Assuming \\(y_1,\\ldots,y_n\\) are i.i.d. with density \\(f(\\cdot|\\theta)\\), we have \\[\\begin{equation*} L(\\theta|y_1,\\ldots,y_n) = f_{y_1,\\ldots,y_n}(y_1,\\ldots,y_n|\\theta) = \\prod^n_{i=1} f(y_i|\\theta). \\end{equation*}\\] In maximum likelihood estimation, we estimate the parameter \\(\\theta\\) by maximizing \\(L\\). The maximizer is called the maximum likelihood estimator (MLE). Some theory Why do we want to maximize the likelihood? Informally, the likelihood is the chance of observing the data. Therefore, we want to find the parameters so that such a chance is maximized. MLE has good statistical properties. Under some regularity conditions, MLE is consistent: \\(\\hat{\\theta}_n\\) converges in probability to \\(\\theta_0\\) Asymptotically efficient: the estimator has the lowest variance asymptotically in some sense Asymptotically normality: can be used to find confidence intervals and perform hypothesis testings Example (Logistic Regression): Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) is a binary variable and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In logistic regression we assume that \\[\\begin{equation*} P(Y_i = 1|x_i, \\beta) = \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}} = \\frac{ e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}. \\end{equation*}\\] Since \\(Y_i\\) takes only two values, \\[\\begin{equation*} P(Y_i = 0|x_i, \\beta) = \\frac{1}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] We can use one single formula for \\(y = 0, 1\\): \\[\\begin{equation*} P(Y_i = y|x_i, \\beta) = \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The likelihood function (conditional on x) is \\[\\begin{equation*} L(\\beta|y_1,\\ldots,y_n, x_1,\\ldots,x_n) = \\prod^n_{i=1} P(Y_i = y_i|x_i, \\beta) = \\prod^n_{i=1} \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The MLE of \\(\\beta\\) is obtained by maximizing \\(L(\\beta|y,x)\\) with respect to \\(\\beta\\). We usually maximize the natural logarithm of the likelihood function instead of the likelihood function, which is easier. The log likelihood function is \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( (x^T_i \\beta) y_i - \\log(1+e^{x^T_i \\beta}) \\bigg). \\end{equation*}\\] In this case, there is no closed-form formula for finding the maximizer. Nevertheless, we can use numerical methods to find out the maximizer. Of course, there are existing functions to perform this task in R. However, we will illustrate how to perform an optimization using the function optim(). By default, optim() will find the minimum. Therefore, we will minimize the negative of the log likelihood function. Simulated Example This is also a good time to introduce how to perform simulation based on a model and check the validity of the estimation method and algorithm. We will first generate some covariates and binary variables based on the logistic regression model. # Setting set.seed(362) n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) Now, we will write a function to compute the negative of the log likelihood function (as a function of the parameter \\(\\beta\\) and the data \\(\\{y_i, x_{i1}, x_{i2}: i=1,\\ldots,n\\}\\)), which is the objective function to be minimized. neg_log_like &lt;- function(beta, y, x1, x2) { beta_X &lt;- beta[1] + beta[2] * x1 + beta[3] * x2 log_like &lt;- sum(beta_X * y) - sum(log(1 + exp(beta_X))) -log_like # return the negative log likelihood } After defining our objective function, we can now use optim() to perform the optimization. optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;) ## $par ## [1] 0.7830734 1.0067301 -1.4920231 ## ## $value ## [1] 632.7284 ## ## $counts ## function gradient ## 28 7 ## ## $convergence ## [1] 0 ## ## $message ## NULL par is the initial values for the optimization. We set some random numbers for the initial values by par = runif(3, 0, 1). Because the function neg_log_like have multiple arguments, we have to supply them inside optim(). method = \"BFGS\" is a quasi-Newton method. method = \"L-BFGS-B\" is also useful when you want to add box constraints to your variable. You can check ?optim to learn more about this. Output: par is the parameter values at which the minimum is obtained value is the minimum function value convergence = 0 indicates successful completion Compare with the built-in function glm() for estimating the parameters: # will discuss this in more detail later fit &lt;- glm(y ~ x1 + x2, family = &quot;binomial&quot;) fit ## ## Call: glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;) ## ## Coefficients: ## (Intercept) x1 x2 ## 0.7831 1.0067 -1.4920 ## ## Degrees of Freedom: 999 Total (i.e. Null); 997 Residual ## Null Deviance: 1324 ## Residual Deviance: 1265 AIC: 1271 You can see that both methods give the same estimates 0.783, 1.007, -1.492 for the regression coefficients. How do you know your method of estimation makes sense? How do you know if you simulate the data correctly? In the above example, the true parameters are 0.5, 1, -1. The estimates are not really that close to the true values. We do not know if the estimation method will give a good result in general. To tackle this problem, we could simulate many datasets, perform the estimation, and take a look at the distributions of the estimates. Perform the simulation and estimation \\(250\\) times: # Setting n &lt;- 1000 beta_0 &lt;- c(0.5, 1, -1) # true beta no_iter &lt;- 250 beta_est &lt;- matrix(0, nrow = no_iter, ncol = length(beta_0)) for (i in 1:no_iter) { # Simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Estimation beta_est[i, ] &lt;- optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;)$par } Displaying the results: library(tidyverse) # create the dataframe for plotting data &lt;- tibble(est = c(beta_est[, 1], beta_est[, 2], beta_est[, 3]), beta = c(rep(&quot;Beta1&quot;, no_iter), rep(&quot;Beta2&quot;, no_iter), rep(&quot;Beta3&quot;, no_iter))) # dataframe for adding the vertical lines for the true parameters vline_data &lt;- tibble(beta = c(&quot;Beta1&quot;, &quot;Beta2&quot;, &quot;Beta3&quot;), mean = beta_0) ggplot(data = data, mapping = aes(x = est)) + geom_histogram(fill = &quot;lightblue&quot;) + facet_grid(~ beta, scales = &quot;free&quot;) + geom_vline(data = vline_data, aes(xintercept = mean), color = &quot;blue&quot;) From the above plots, you can see the distributions of your estimators. The true parameters lie in the middle of the distributions. You can also add lines to visualize the mean of the distributions. In this case, the lines actually overlap with lines for the true parameters. Thus, the estimators are essentially unbiased. You can also see that there are times that \\(\\hat{\\beta}_0\\) can be as large as \\(1\\) or as small as \\(0.1\\) while the true value is \\(0.5\\). You can also use a larger sample size. # setting set.seed(362) n &lt;- 100000 beta_0 &lt;- c(0.5, 1, -1) # true beta # simulation x1 &lt;- runif(n, 0, 1) x2 &lt;- runif(n, 0, 1) prob_num &lt;- exp(beta_0[1] + beta_0[2] * x1 + beta_0[3] * x2) prob &lt;- prob_num / (1 + prob_num) y &lt;- rbinom(n, size = 1, prob = prob) # Estimation (est &lt;- optim(par = runif(3, 0, 1), f = neg_log_like, y = y, x1 = x1, x2 = x2, method = &quot;BFGS&quot;)$par) ## [1] 0.4803615 1.0275395 -0.9871783 The estimates are now 0.48, 1.03, -0.99, which are close to the true values 0.5, 1, -1. We will see some applications of the logistic regression later. 9.1.1 Exercises on MLE Exercise 1 (Gamma distribution) You observe a random sample \\(y_1,\\ldots,y_n\\) from a Gamma distribution with unknown parameters \\(\\alpha, \\beta\\). The likelihood function is \\[\\begin{equation*} L(\\alpha, \\beta |y_1,\\ldots,y_n) = \\prod^n_{i=1} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{\\alpha-1}_i e^{-\\beta y_i}. \\end{equation*}\\] The log likelihood, after simpliciation, is \\[\\begin{equation*} \\log L(\\alpha, \\beta) = n \\alpha \\log \\beta - n \\log \\Gamma(\\alpha) + (\\alpha - 1) \\sum^n_{i=1} \\log y_i - \\beta \\sum^n_{i=1} y_i. \\end{equation*}\\] # Setting set.seed(1) alpha &lt;- 1.5 beta &lt;- 2 n &lt;- 10000 # Simulation x &lt;- rgamma(n, alpha ,beta) # Optimization (Estimation) # Assignment 4 Exercise 2 (Poisson Regression) Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) only takes nonnegative integer values (count data) and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In Poisson regression, we assume that \\(Y_i\\) has a Poisson distribution and \\[\\begin{equation*} \\log (E(Y_i|x_i)) = \\beta^T x_i = \\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}, \\end{equation*}\\] where \\(\\beta \\in \\mathbb{R}^{p+1}\\). Alternatively, condition on \\(x_i\\), \\(Y_i|x_i \\sim \\text{Pois}(e^{\\beta^T x_i})\\). The likelihood is \\[\\begin{equation*} L(\\beta|y,x) = \\prod^n_{i=1} \\frac{e^{-e^{\\beta^T x_i}} e^{(\\beta^T x_i)y_i}}{y_i!}. \\end{equation*}\\] The log likelihood is \\[\\begin{equation*} \\log L(\\beta|y, x) = \\sum^n_{i=1} \\bigg( -e^{\\beta^T x_i} + (\\beta^T x_i) y_i - \\log (y_i!) \\bigg). \\end{equation*}\\] Clearly, the term \\(\\log (y_i !)\\) does not depend on \\(\\beta\\) and hence we do not need to consider it during our optimization. Therefore, it suffices to maximize (or minimize the negative of) the following objective function \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( -e^{\\beta^T x_i} + (\\beta^T x_i) y_i \\bigg). \\end{equation*}\\] # Setting set.seed(1) beta &lt;- c(0.3, 0.5, -0.5) n &lt;- 10000 p &lt;- length(beta) - 1 # Simulation X &lt;- cbind(1, matrix(runif(n * p), nrow = n, ncol = p)) y &lt;- rpois(n, exp(X %*% beta)) # Optimization (Estimation) # Assignment 4 9.1.2 Summary Review of the MLE Use optim() to minimize an objective function Simplify the objective function before you try to minimize it (take log and remove terms that do not depend on the parameters) how to simulate from a model how to check the validity of the simulation results 9.2 Interval Estimation and Hypothesis Testing Two types of estimation: point estimation (e.g. MLE) and interval estimation (e.g. confidence interval). 9.2.1 Examples of Hypothesis Testing Optional reading: Chapter 12 in John E. Freunds Mathematical Statistics with Applications. You have a die and you wonder if it is unbiased. If the die is unbiased, the (population) mean of the result from rolling the die is \\(3.5\\). Suppose you roll the die \\(10\\) times, the sample mean is \\(5\\). What is your decision on determining if the die is biased or not? How confident is your decision? What if you roll the die \\(100\\) times, and the sample mean is \\(5\\)? What is your decision now? Are you more confident in your decision? What if your roll the die \\(10\\) times but the sample mean is \\(4\\)? To answer these questions, we need to understand interval estimation and hypothesis testing. Some more examples: An engineer has to decide on the basis of sample data whether the true average lifetime of a certain kind of tire is at least \\(42,000\\) miles An agronomist has to decide on the basis of experiments whether one kind of fertilizer produces a higher yield of soybeans than another A manufacturer of pharmaceutical products has to decide on the basis of samples whether 90 percent of all patients given a new medication will recover from a certain disease These problems can all be translated into the language of statistical tests of hypotheses. the engineer has to test the hypothesis that \\(\\theta\\), the parameter of an exponential population, is at least \\(42,000\\) the agronomist has to decide whether \\(\\mu_1 &gt; \\mu_2\\), where \\(\\mu_1\\) and \\(\\mu_2\\) are the means of two normal populations the manufacturer has to decide whether \\(\\theta\\), the parameter of a binomial population, equals \\(0.90\\) In each case it must be assumed, of course, that the chosen distribution correctly describes the experimental conditions; that is, the distribution provides the correct statistical model. 9.2.2 Null Hypotheses, Alternative Hypotheses, and p-values An assertion or conjecture about the distribution of one or more random variables is called a statistical hypothesis. If a statistical hypothesis completely specifies the distribution, it is called a simple hypothesis; if not, it is referred to as a composite hypothesis. Null hypothesis In view of the assumptions of no difference, hypotheses such as these led to the term null hypothesis, but nowadays this term is applied to any hypothesis that we may want to test. p-value: the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct Steps in Hypothesis Testing Assume the null hypothesis is true Calculate a test statistic. E.g., sample mean Calculate a \\(p\\)-value (denoted by \\(p\\)) from the statistic and its distribution. For example, if the die is unbiased, what is the probability that we observe the sample mean to be larger than \\(5\\) after rolling it \\(100\\) times? small \\(p\\)-value small: we have strong evidence to reject the null hypothesis because it is unlikely to observe such a test statistic if the null hypothesis is true large \\(p\\)-value small: we do not have enough evidence to reject the null hypothesis Remark In this course, we will follow the common convention to reject the null hypothesis when \\(p &lt; 0.05\\) In real applications, how small is small depends on the problems. Being statistically significant (small \\(p\\)-value) does not mean the difference between the null and alternative hypotheses is large. One should also look at the confidence intervals or distributions of your estimates. 9.2.3 Type I error and Type II error Type I error: reject the null hypothesis when it is true. The probability of committing a type I error is denoted by \\(\\alpha\\) (level of significance of the test). Type II error: do not reject the null hypothesis when it is false. The probability of committing a type II error is denoted by \\(\\beta\\). \\(H_0\\) is true \\(H_1\\) is true Reject \\(H_0\\) Type I error No Error Do not reject \\(H_0\\) No Error Type II Error A good test procedure is one in which both \\(\\alpha\\) and \\(\\beta\\) are small, thereby giving us a good chance of making the correct decision. When the sample size \\(n\\) is held fixed, reducing \\(\\alpha\\) by changing the rejection region will increase \\(\\beta\\) and vice versa. The only way in which we can reduce the probabilities of both types of errors is to increase \\(n\\). As long as \\(n\\) is held fixed, this inverse relationship between the probabilities of type I and type II errors is typical of statistical decision procedures. Usually, we control \\(\\alpha\\) to be small (e.g. \\(0.05\\)). 9.2.4 Inference for Mean of One Sample Hypothesis Testing Problem You have a random sample \\(X_1,\\ldots,X_n\\) from a population. You want to know if the population mean \\(\\mu\\) is equal to \\(\\mu_0\\). That is, \\(H_0 : \\mu =\\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\). Solution You can use the \\(t\\)-test for this problem. It is appropriate when either Your data is normally distributed You have a large sample size \\(n\\). A rule of thumb is \\(n &gt; 30\\). The test statistic is \\[\\begin{equation*} \\frac{\\overline{X}_n - \\mu_0}{s/\\sqrt{n}}, \\end{equation*}\\] where \\(\\overline{X}_n\\) is the sample mean and \\(s\\) is the sample standard deviation. In R, use t.test() to perform the t-test. # Simulate the data set.seed(362) # so that you can replicate the result x &lt;- rnorm(75, mean = 100, sd = 15) # Perform t-test t.test(x, mu = 95) ## ## One Sample t-test ## ## data: x ## t = 4.7246, df = 74, p-value = 1.071e-05 ## alternative hypothesis: true mean is not equal to 95 ## 95 percent confidence interval: ## 99.36832 105.74018 ## sample estimates: ## mean of x ## 102.5543 The \\(p\\)-value is small, so it is unlikely that the mean of the population is \\(95\\). The \\(p\\)-value in this case is \\(2 \\times P_{H_0}(T &gt; |\\text{obs. T.S.|})\\), where \\(T\\) has a \\(t\\)-distribution with degrees of freedom \\(n-1\\) if the data from are from a normal distribution and obs. T.S. stands for the observed test statistic. The subscript \\(H_0\\) is to stress that the probability measure is under \\(H_0\\). [Optional] How are the test statistic and \\(p\\)-value calculated? # test statistic (obs_ts &lt;- (mean(x) - 95) / (sd(x) / sqrt(length(x)))) ## [1] 4.724574 # p-value 2 * (1 - pt(abs(obs_ts), df = length(x) - 1)) ## [1] 1.071253e-05 What do you expect when \\(x\\) is from a distribution with a much larger SD? set.seed(362) # so that you can replicate the result x2 &lt;- rnorm(75, mean = 100, sd = 200) (test_x2 &lt;- t.test(x2, mu = 95)) ## ## One Sample t-test ## ## data: x2 ## t = 1.832, df = 74, p-value = 0.07097 ## alternative hypothesis: true mean is not equal to 95 ## 95 percent confidence interval: ## 91.57758 176.53580 ## sample estimates: ## mean of x ## 134.0567 Even if the estimate of the population mean is 134, the test does not reject the null hypothesis that the mean is \\(95\\). This is because the sample has a very large variance. Interval Estimation The \\(100(1-\\alpha)\\%\\) confidence interval of \\(\\mu\\) is given by \\[\\begin{equation*} \\bigg[ \\overline{X}_n - t_{n-1; \\alpha/2} \\frac{s}{\\sqrt{n}}, \\overline{X}_n + t_{n-1; \\alpha/2} \\frac{s}{\\sqrt{n}} \\bigg], \\end{equation*}\\] where \\(P(t(n-1) &gt; t_{n-1;\\alpha/2}) = \\alpha/2\\) and \\(t(n-1)\\) is a \\(t\\)-distributed random variable with degrees of freedom \\(n-1\\). To find the confidence interval of \\(\\mu\\) in R, use t.test(). For example, the \\(99\\%\\) confidence interval of \\(\\mu\\) is t.test(x, conf.level = 0.99) ## ## One Sample t-test ## ## data: x ## t = 64.139, df = 74, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 99 percent confidence interval: ## 98.32683 106.78168 ## sample estimates: ## mean of x ## 102.5543 Remark By omitting mu = 0.95, the default value is mu = 0. Since we are interested in finding the confidence intervals, we do not need to care about the value of \\(\\mu_0\\). [Optional] Without using t.test(): # just an illustration of how CI can be computed alpha &lt;- 0.01 n &lt;- length(x) half_width &lt;- qt(1 - alpha / 2, n - 1) * sd(x) / sqrt(n) c(mean(x) - half_width, mean(x) + half_width) ## [1] 98.32683 106.78168 Interpretation If you can repeat the experiment many times, then about \\(95\\%\\) of the confidence intervals computed in those many times will contain the true mean. no_sim &lt;- 10000 set.seed(362) # so that you can replicate the result true_mean &lt;- 100 CI &lt;- matrix(0, nrow = no_sim, ncol = 2) for (i in 1:no_sim) { CI[i, ] &lt;- t.test(rnorm(75, mean = true_mean, sd = 200))$conf.int } # find out the proportion of CIs that contain 0 mean(CI[, 1] &lt; 100 &amp; 100 &lt; CI[, 2]) ## [1] 0.9503 Example Recall that we talked about how to use simulation to estimate \\(P(X &gt; Y)\\), where \\(X\\) and \\(Y\\) are some random variables. For example, if \\(X \\sim N(0, 1)\\), \\(Y \\sim \\text{Exp}(2)\\), and they are independent. R code to estimate \\(P(X &gt; Y)\\): set.seed(1) n &lt;- 10000 X &lt;- rnorm(n, 0, 1) Y &lt;- rexp(n, 2) mean(X &gt; Y) ## [1] 0.3299 We know the true value is not exactly 0.3299 because the law of large numbers only ensure that the sample mean converges to the true mean. We can use t.test() to find an confidence interval for \\(P(X &gt; Y)\\). t.test(X &gt; Y) ## ## One Sample t-test ## ## data: X &gt; Y ## t = 70.162, df = 9999, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3206831 0.3391169 ## sample estimates: ## mean of x ## 0.3299 We see that the \\(95\\%\\) CI is (0.321, 0.339). To make the CI narrower, we can increase the number of simulation. set.seed(1) n &lt;- 1000000 X &lt;- rnorm(n, 0, 1) Y &lt;- rexp(n, 2) t.test(X &gt; Y) ## ## One Sample t-test ## ## data: X &gt; Y ## t = 704.63, df = 1e+06, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.3308521 0.3326979 ## sample estimates: ## mean of x ## 0.331775 The \\(95\\%\\) CI becomes (0.331, 0.333). The width of the CI is equal to \\[\\begin{equation*} 2 \\times t_{n-1; \\alpha/2} \\frac{s}{\\sqrt{n}}. \\end{equation*}\\] From the above formula, you could determine the minimum number of simulations required to achieve a certain degree of accuracy, 9.2.5 Comparing the means of two samples Suppose you have one sample each from two populations. You want to test if the two populations have the same mean. There are two different \\(t\\)-tests for this task (assuming data are normally distributed or you have large samples): the observations are not paired the observations are paired To explain the meaning of paired data, consider two experiments to see if drinking coffee in the morning improves your test scores: Unpaired observations: Randomly select two groups of people. People in one group have a cup of morning coffee and take the test. The other group just takes the test. For each person, we have one test score. All the scores are independent. Paired observations: Randomly select one group of people. Give them the test twice, once with morning coffee and once without morning coffee. For each person, we have two test scores. Clearly, the two scores are not statistically independent. Example We illustrate the paired \\(t\\)-test using the dataset sleep (no package is required). The data contains the increase in hours of sleep when the subject took two soporific drugs compared to control on \\(10\\) subjects. Since each subject received two drugs, the observations are paired. Take a look at sleep: sleep ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 The sleep data is in a long format. Lets turn it into a wide format using spread, a function in the package tidyr, which is contained in tidyverse. (sleep_wide &lt;- spread(sleep, group, extra)) ## ID 1 2 ## 1 1 0.7 1.9 ## 2 2 -1.6 0.8 ## 3 3 -0.2 1.1 ## 4 4 -1.2 0.1 ## 5 5 -0.1 -0.1 ## 6 6 3.4 4.4 ## 7 7 3.7 5.5 ## 8 8 0.8 1.6 ## 9 9 0.0 4.6 ## 10 10 2.0 3.4 Paired \\(t\\)-test: t.test(sleep_wide[, 2], sleep_wide[, 3], paired = TRUE) ## ## Paired t-test ## ## data: sleep_wide[, 2] and sleep_wide[, 3] ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean of the differences ## -1.58 The \\(p\\)-value is \\(&lt; 0.05\\). We conclude that the effect of the two drugs are different. The \\(95\\%\\) CI of the difference between the two means is (-2.46, -0.70). Example Are the means of the birth weights in the smoking group and non-smoking group different? library(MASS) t.test(birthwt$bwt[birthwt$smoke == 1], birthwt$bwt[birthwt$smoke == 0]) ## ## Welch Two Sample t-test ## ## data: birthwt$bwt[birthwt$smoke == 1] and birthwt$bwt[birthwt$smoke == 0] ## t = -2.7299, df = 170.1, p-value = 0.007003 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -488.97860 -78.57486 ## sample estimates: ## mean of x mean of y ## 2771.919 3055.696 The \\(p\\)-value is \\(&lt; 0.05\\). We conclude that the means are different. The \\(95\\%\\) CI of the difference between the two means is (-489.0, -78.6). 9.2.6 Inference of a Sample Proportion You have a sample of values from a population consisting of successes and failures. The null hypothesis is the true proportion of success \\(p\\) is equal to some particular number \\(p_0\\). The alternative hypothesis is the \\(p \\neq p_0\\). Example You flip a coin \\(100\\) times independently. You want to test if the coin is fair \\((p_0 = 0.5)\\). # Simulate the coin flips set.seed(1) heads &lt;- rbinom(1, size = 100, prob = .4) # Test, p_0 = 0.5 (result &lt;- prop.test(heads, 100, p = 0.5)) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.1336 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.3233236 0.5228954 ## sample estimates: ## p ## 0.42 The point estimate is 0.42. Although the true probability of success used in the simulation is \\(0.4\\), for this particular data, we do not reject to null hypothesis that the true probability of success is \\(0.5\\) as the \\(p\\)-value equals 0.134, which is larger than \\(0.05\\). The 95% confidence interval is equal to (0.323, 0.523). You can change the alternative hypothesis to \\(p &gt; p_0\\) or \\(p &lt; p_0\\): prop.test(heads, 100, p = 0.5, alternative = &quot;greater&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.9332 ## alternative hypothesis: true p is greater than 0.5 ## 95 percent confidence interval: ## 0.3372368 1.0000000 ## sample estimates: ## p ## 0.42 prop.test(heads, 100, p = 0.5, alternative = &quot;less&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: heads out of 100, null probability 0.5 ## X-squared = 2.25, df = 1, p-value = 0.06681 ## alternative hypothesis: true p is less than 0.5 ## 95 percent confidence interval: ## 0.0000000 0.5072341 ## sample estimates: ## p ## 0.42 9.2.7 Testing groups for equal proportions You have samples from two or more groups. The data from each group are binary-valued: either success or failure. You want to test if the groups have equal proportions of success. Example # 3 groups no_success &lt;- c(48, 60, 50) # no. of successes in the 3 groups no_trial &lt;- c(100, 100, 100) # corresponding no. of trails in the 3 groups prop.test(no_success, no_trial) ## ## 3-sample test for equality of proportions without continuity correction ## ## data: no_success out of no_trial ## X-squared = 3.3161, df = 2, p-value = 0.1905 ## alternative hypothesis: two.sided ## sample estimates: ## prop 1 prop 2 prop 3 ## 0.48 0.60 0.50 \\(p\\)-value is \\(&gt; 0.05\\). We do not reject the null hypothesis that the three groups have the same proportion of success. Example In a class of \\(38\\) students, \\(14\\) of them got \\(A\\). In another class of \\(40\\) students, only \\(10\\) got \\(A\\). We want to know if the difference between the two proportions is statistically significant. prop.test(c(14, 10), c(38, 40)) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(14, 10) out of c(38, 40) ## X-squared = 0.7872, df = 1, p-value = 0.3749 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.1110245 0.3478666 ## sample estimates: ## prop 1 prop 2 ## 0.3684211 0.2500000 The \\(p\\)-value is \\(&gt; 0.05\\). We do not reject the null hypothesis that the students in the two groups have the same proportion of getting an A. 9.2.8 Testing if two samples have the same underlying distribution Problem You have two random samples \\(X_1,\\ldots,X_n\\) and \\(Y_1,\\ldots,Y_m\\). Let \\(F\\) and \\(G\\) be the distribution functions of \\(X_i\\)s and \\(Y_i\\)s respectively. You want to know if \\(F \\equiv G\\). Solution You may use the Kolmogorov-Smirnov test. \\(H_0: F = G\\) vs \\(H_1: F \\neq G\\). It does not require any assumptions. The test statistic is \\[ D := \\sup_x|F_n(x) - G_m(x)|,\\] where \\(F_n\\) and \\(G_m\\) are the empirical distribution functions of \\(X_i\\)s and \\(Y_i\\)s respectively. That is, \\[ F_n(x) := \\frac{1}{n} \\sum^n_{i=1} I(X_i \\leq x)\\] and \\[ G_m(x) := \\frac{1}{m} \\sum^m_{i=1} I(Y_i \\leq x).\\] Example set.seed(362) x &lt;- rnorm(100, 0, 1) y &lt;- rnorm(100, 0, 1) ks.test(x, y) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: x and y ## D = 0.07, p-value = 0.9671 ## alternative hypothesis: two-sided The \\(p\\)-value is not small. We do not have enough evidence to reject the null hypothesis that the two distributions are the same. Example z &lt;- rnorm(100, 2, 1) ks.test(y, z) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: y and z ## D = 0.77, p-value &lt; 2.2e-16 ## alternative hypothesis: two-sided The \\(p\\)-value is very small. We will reject the null hypothesis that the two distributions are the same. Example Recall the dataset birthwt from the package MASS. We created the following histograms to visualize the distributions of birth weights for the two groups (smoke and no smoke). We may want to ask if the two distributions are different ks.test(birthwt$bwt[birthwt$smoke == 0], birthwt$bwt[birthwt$smoke == 1]) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: birthwt$bwt[birthwt$smoke == 0] and birthwt$bwt[birthwt$smoke == 1] ## D = 0.21962, p-value = 0.02598 ## alternative hypothesis: two-sided The \\(p\\)-value is smaller than \\(0.05\\). Therefore, we conclude that the difference of the two distributions is statistically significant. "],["k-nearest-neighbors.html", "Chapter 10 k-nearest neighbors 10.1 Introduction 10.2 Feature Scaling 10.3 Example: Classifying Breast Cancers", " Chapter 10 k-nearest neighbors Optional Reading: Chapter 3 in Machine Learning with R by Brett Lantz 10.1 Introduction Setting: We have data \\(\\{(x_i, y_i) : i=1,\\ldots,n \\}\\), where \\(x_i\\) and \\(y_i\\) are the vector of the features the class label for the \\(i\\)th observation respectively. For example, in breast cancer diagnosis, \\(x_i\\) could be some summary statistics of the radius, texture, perimeter, area of the cell nuclei from a digitized image of a fine needle aspirate of a breast mass; \\(y_i\\) is the diagnosis (malignant or benign). We now have a new data point \\(x^*\\) (of course we do not have the class label \\(y^*\\)) and we want to assign a class label to it. For example, after a subject has a breast fine needle aspiration, can we use existing data to predict if the subject has breast cancer? In this chapter, we will study the \\(k\\)-nearest neighbors (\\(k\\)-NN) algorithm for classification. \\(k\\)-NN algorithm uses information about an examples \\(k\\) nearest neighbors to classify unlabeled examples. To measure how close the examples are, we need a distance measure. Traditionally, the \\(k\\)-NN algorithm uses Euclidean distance. Given two points \\(u = (u_1,\\ldots,u_p)\\) and \\(w = (w_1,\\ldots,w_p)\\), the Euclidean distance between them is \\[\\begin{equation*} d(u, w) = \\sqrt{ \\sum^p_{i=1}(u_i - w_i)^2}. \\end{equation*}\\] Algorithm: Compute the Euclidean distance \\(d(x_i, x^*)\\) for \\(i=1,\\ldots,n\\) Find the \\(k\\) training data with the smallest \\(d(x_i, x^*)\\) The predicted class for \\(x^*\\) is determined by the majority vote of the \\(k\\) training data in Step 2. The idea before the algorithm is simple. We expect that observations with similar features should have the same class label. In the following figure, we have some labeled data. Suppose the black dot is our new data without label, which group will you assign this data to? A natural choice is to assign the black dot to group A because the \\(5\\) nearest neighbors are all in group A. This is the idea of \\(k\\)-nearest neighbors algorithm. Remark: We can use the \\(k\\)-NN algorithm for regression. In that case, \\(y_i\\) is the numeric response. The corresponding algorithm replaces the majority vote by the mean of the responses in the \\(k\\) nearest training data. Other distance measures could be used. There are different methods to break ties. There is no learning phase. Applications: Computer vision applications, including optical character recognition and facial recognition in both still images and video Recommendation systems that predict whether a person will enjoy a movie or song Identifying patterns in genetic data to detect specific proteins or diseases Strengths of \\(k\\)-NN: Simple and effective Makes no assumptions about the underlying data distribution Weaknesses of \\(k\\)-NN: Does not produce a model, limiting the ability to understand how the features are related to the class Requires selection of an appropriate \\(k\\) Slow classification phase Nominal features and missing data require additional processing Choosing an appropriate \\(k\\): Large \\(k\\): reduce the impact or variance caused by noisy data, may underfit the traning data Small \\(k\\): may overfit the data Extreme case: \\(k = n\\), the most common class will always be the prediction Some people suggest using \\(\\sqrt{n}\\) as \\(k\\). One may also use a validation set or cross-validation to choose the appropriate \\(k\\) (will discuss these later). 10.2 Feature Scaling Clearly, if the features are in different scales, the distance measure computed will be dominated by some of the features and will not take into account of the importance of other features. Lets assume we have \\(n\\) data and \\(p\\) features. Two common methods for feature scaling: min-max normalization For each \\(i=1,\\ldots,p\\), \\(j=1,\\ldots,n\\), \\[\\begin{equation*} x^*_{ij} = \\frac{x_{ij} - \\min x_i}{\\max x_i -\\min x_i}, \\end{equation*}\\] where \\(\\min x_i = \\min_{j=1,\\ldots,n} x_{ij}\\) and \\(\\max x_i = \\max_{j=1,\\ldots,n} x_{ij}\\). The normalized features will have values between \\(0\\) and \\(1\\). \\(z\\)-score standardization For each \\(i=1,\\ldots,p\\), \\(j=1,\\ldots,n\\), \\[\\begin{equation*} x^*_{ij} = \\frac{x_{ij} - \\overline{x}_i}{s_i}, \\end{equation*}\\] where \\(\\overline{x}_i\\) and \\(s_i\\) are the sample mean and standard deviation of \\(\\{x_{i1},\\ldots,x_{in}\\}\\). For nominal features, we can convert them into a numeric feature using dummy coding. For example, if a nominal feature called temperature takes three values: hot, medium and cold. You can define two additional binary indicator variables: \\[\\begin{equation*} \\text{hot} = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if temperature = hot} \\\\ 0 &amp; \\text{otherwise} \\end{array} \\right. \\end{equation*}\\] and \\[\\begin{equation*} \\text{medium} = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if temperature = medium} \\\\ 0 &amp; \\text{otherwise.} \\end{array} \\right. \\end{equation*}\\] When both hot and medium are \\(0\\), we know the temperature is cold. In general, if we have \\(n\\) categories, we only need to create \\(n-1\\) additional binary indicators. 10.3 Example: Classifying Breast Cancers We will use the breast cancecr from UC Irvine Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29 Download the dataset from onQ. Read the data (change the path to where you save your data): wbcd &lt;- read.csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/wisc_bc_data.csv&quot;) Variables: names(wbcd) ## [1] &quot;id&quot; &quot;diagnosis&quot; &quot;radius_mean&quot; ## [4] &quot;texture_mean&quot; &quot;perimeter_mean&quot; &quot;area_mean&quot; ## [7] &quot;smoothness_mean&quot; &quot;compactness_mean&quot; &quot;concavity_mean&quot; ## [10] &quot;concave.points_mean&quot; &quot;symmetry_mean&quot; &quot;fractal_dimension_mean&quot; ## [13] &quot;radius_se&quot; &quot;texture_se&quot; &quot;perimeter_se&quot; ## [16] &quot;area_se&quot; &quot;smoothness_se&quot; &quot;compactness_se&quot; ## [19] &quot;concavity_se&quot; &quot;concave.points_se&quot; &quot;symmetry_se&quot; ## [22] &quot;fractal_dimension_se&quot; &quot;radius_worst&quot; &quot;texture_worst&quot; ## [25] &quot;perimeter_worst&quot; &quot;area_worst&quot; &quot;smoothness_worst&quot; ## [28] &quot;compactness_worst&quot; &quot;concavity_worst&quot; &quot;concave.points_worst&quot; ## [31] &quot;symmetry_worst&quot; &quot;fractal_dimension_worst&quot; Creating Training and Testing Datasets The first column is id, which should not be included in the classification. The second column is diagnosis. B means benign and M means malignant. In short, the meaning of malignant is cancerous and the meaning of benign is non-cancerous. We will separate the labels from the features. To evaluate the model performance, we always split our full dataset into a training dataset and a testing daatset. set.seed(6) # reproduce the result # create the random numbers for selecting the rows in the dataset random_index &lt;- sample(nrow(wbcd), 469) # our &quot;x&quot; wbcd_train &lt;- wbcd[random_index, -(1:2)] wbcd_test &lt;- wbcd[-random_index, -(1:2)] # our &quot;y&quot; wbcd_train_labels &lt;- wbcd[random_index, ]$diagnosis wbcd_test_labels &lt;- wbcd[-random_index, ]$diagnosis Note: In forming a training dataset and a testing dataset, you should not select the first \\(469\\) rows (unless you know the data have been randomly organized). Normalizing the data We have to normalize the features in both the training datasets and testing datasets We have to use the same normalizing methods for these two datasets Compute the min and max (or mean and sd) using only the training datasets wbcd_train_n &lt;- wbcd_train wbcd_test_n &lt;- wbcd_test train_min &lt;- apply(wbcd_train, 2, min) train_max &lt;- apply(wbcd_train, 2, max) for (i in 1:ncol(wbcd_train)) { wbcd_train_n[, i] &lt;- (wbcd_train[, i] - train_min[i]) / (train_max[i] - train_min[i]) # use the min and max from training data to normalize the testing data wbcd_test_n[, i] &lt;- (wbcd_test[, i] - train_min[i]) / (train_max[i] - train_min[i]) } We will use knn() in the package class to perform \\(k\\)-NN classification. knn() will return a factor of classifications of testing dataset. library(class) # install it if you haven&#39;t done so knn_predicted &lt;- knn(train = wbcd_train_n, test = wbcd_test_n, cl = wbcd_train_labels, k = 21) train : training dataset test: testing dataset cl: training labels k: \\(k\\)-nearest neighbors will be used 10.3.1 Evaluating Model Performance table(wbcd_test_labels, knn_predicted) ## knn_predicted ## wbcd_test_labels B M ## B 57 2 ## M 4 37 False positive: an error where the test result incorrectly indicates the presence of a condition such as a disease when the disease is not present. In this example, we have \\(2\\) false positives. False Negative: an error where the test result incorrectly fails to indicate the presence of a condition when it is present. In this example, we have \\(4\\) false negatives. Here the test result means our prediction. The accuracy is \\[\\begin{equation*} \\text{accuracy} = \\frac{57+37}{57+2+4+37} = 0.94. \\end{equation*}\\] The error rate is \\[\\begin{equation*} \\text{error rate} = \\frac{2 + 4}{57+2+4+37} = 0.06 = 1- \\text{accuracy}. \\end{equation*}\\] 10.3.2 Using \\(z\\)-score standardization Lets try the \\(z\\)-score standardization. wbcd_train_s &lt;- wbcd_train wbcd_test_s &lt;- wbcd_test train_mean &lt;- apply(wbcd_train, 2, mean) train_sd &lt;- apply(wbcd_train, 2, sd) for (i in 1:ncol(wbcd_train)) { wbcd_train_s[, i] &lt;- (wbcd_train[, i] - train_mean[i]) / train_sd[i] # use the mean and sd from training data to normalize the testing data wbcd_test_s[, i] &lt;- (wbcd_test[, i] - train_mean[i]) / train_sd[i] } Perform \\(k\\)-NN: knn_predicted &lt;- knn(train = wbcd_train_s, test = wbcd_test_s, cl = wbcd_train_labels, k = 21) Evaluate the performance: table(wbcd_test_labels, knn_predicted) ## knn_predicted ## wbcd_test_labels B M ## B 58 1 ## M 5 36 The performance using \\(z\\)-score standardization and min-max normalization is similar. 10.3.3 Testing alternative values of \\(k\\) k &lt;- c(1, 5, 11, 15, 21, 27) result &lt;- matrix(0, nrow = length(k), ncol = 4) result[, 1] &lt;- k colnames(result) = c(&quot;k value&quot;, &quot;False Negatives&quot;, &quot;False Positives&quot;, &quot;Percent Classified Correctly&quot;) for (i in 1:length(k)) { knn_predicted &lt;- knn(train = wbcd_train_n, test = wbcd_test_n, cl = wbcd_train_labels, k = k[i]) confusion_matrix &lt;- table(wbcd_test_labels, knn_predicted) result[i, 2] &lt;- confusion_matrix[2, 1] result[i, 3] &lt;- confusion_matrix[1, 2] result[i, 4] &lt;- sum(diag(confusion_matrix)) / length(wbcd_test_labels) } result ## k value False Negatives False Positives Percent Classified Correctly ## [1,] 1 4 3 0.93 ## [2,] 5 4 1 0.95 ## [3,] 11 3 1 0.96 ## [4,] 15 3 2 0.95 ## [5,] 21 4 2 0.94 ## [6,] 27 3 2 0.95 Important remark: while the accuracy is the highest when \\(k = 11\\), it does not mean the model is the best for this data. Note that We split the dataset into a training dataset and a testing dataset using one particular random partition. With another random partition, the results would be different. The more appropriate way to assess the accuracy is to use repeated \\(k\\)-fold cross validation (the \\(k\\) here is different from the \\(k\\) in our \\(k\\)-NN algorithm). We shall discuss this later. Having false Negatives could be a more severe problem than having false positives in this example. Although in this example, when \\(k=11\\), it also produces the lowest number of false negatives. "],["classification-trees.html", "Chapter 11 Classification Trees 11.1 Introduction 11.2 The C5.0 classification tree algorithm 11.3 Example: iris 11.4 Example: identifying risky bank loans", " Chapter 11 Classification Trees Optional Reading: Chapter 5 in Machine Learning with R by Brett Lantz. In this chapter, we will load the following libraries. library(tidyverse) library(GGally) # to use ggpairs library(C50) library(ggpubr) # to use ggarrange 11.1 Introduction A decision tree utilizes a tree structure to model the relationship between the features and the outcomes. In each branching node of the tree, a specific feature of the data is examined. According to the value of the feature, one of the branches will be followed. The leaf node represents the class label. Below is a simplified example of determining if you should accept a new job offer. In the above example, the leaf nodes are Decline offer and Accept offer. The paths from the root to leaf node represent classification rules. In data science, there are two types of decision trees: classification tree - for discrete outcomes (our focus in this chapter) regression tree - for continuous outcomes Classification trees are used for classification problems. Consider the iris dataset, which is available in base R. We have \\(150\\) observations and \\(5\\) variables named Sepal.Length, Sepal.Wdith, Petal.Length, Petal.Width, and Species. Lets take a look at the data. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... The first four variables are numeric features of the flowers. The last column in the dataset is the species of the flower. We will illustrate how to use the numeric features of the flowers to classify the flowers. First, we will split the dataset into a training dataset and a testing dataset. set.seed(6) random_index &lt;- sample(nrow(iris), 100) # select 100 examples for our training dataset iris_train &lt;- iris[random_index, ] iris_test &lt;- iris[-random_index, ] To create a scatterplot matrix, we can use pairs (base R graphics) or ggpairs (in the package GGally). With pairs(): # we only want to plot the numeric features # the 5th column is the species of the flowers pairs(iris_train[, -5]) With ggpairs(): library(GGally) # to use ggpairs, install it first if you haven&#39;t done so ggpairs(data = iris_train, aes(color = Species, alpha = 0.8), columns = 1:4) In ggpairs(): columns = 1:4 tells the function only uses the first \\(4\\) columns of the dataset for plotting color = Species indicates that we map the color to the variable Species alpha = 0.8 controls the level of transparency so that the density estimates do not overlap each other completely From the plots, it seems that the variable Petal.Width and Petal.Length will be useful in classifying the flowers. Lets focus on the scatterplot of these two variables. ggplot(iris_train, aes(x = Petal.Width, y = Petal.Length, color = Species)) + geom_point() From the above plot, we see that we can easily classify the flowers with high accuracy using these two features. For example, if we draw the horizontal line \\(y = 1.9\\), we could classify any iris lying on or below this line as setosa with \\(100\\%\\) accuracy. Next, we draw the vertical line \\(x = 1.7\\). We could classify any iris lying on or to the left of this line as versicolor. We get a pretty good classification with only 3 mistakes. The above classification is an example of classification trees (because the outcome is categorical) and can be visualized as In the chapter, we will learn how to perform such a task using the C5.0 algorithm in R. 11.2 The C5.0 classification tree algorithm There are different implementations of classification trees, we will use the C5.0 algorithm. Another popular algorithm is the CART (use the package rpart in R). Strengths of classification trees: an all-purpose classifier that does well on many types of problems highly automatic learning process, which can handle numeric or nominal features, as well as missing data excludes unimportant features can be used on both small and large datasets results in a model that can be interpreted without a mathematical background more efficient than other complex models Weaknesses of classification trees: decision tree models are often biased toward splits on features having a large number of levels it is easy to overfit or underfit the model can have trouble modeling some relationships due to reliance on axis-parallel splits small changes in training data can result in large changes to decision logic large trees can be difficult to interpret and the decisions they make may seem counterintuitive 11.2.1 Choosing the best split As we can see from the iris example, we first have to identify the feature to split upon. We choose the feature so that the resulting partitions contain examples primarily of a single class. In machine learning, examples mean observations. Definition: Purity = the degree to which a subset of examples contains only a single class One common measure of purity is entropy. For \\(c\\) classes (recall that we are considering classification problems), entropy ranges from \\(0\\) to \\(\\log_2(c)\\). Entropy is defined as \\[\\begin{equation*} \\text{Entropy}(S) = - \\sum^c_{i=1} p_i \\log_2 (p_i), \\end{equation*}\\] where \\(S\\) is a given set of data, \\(c\\) is the number of class levels, \\(p_i\\) is the proportion of data in class level \\(i\\), \\(\\log_2\\) is the logarithm to the base \\(2\\). By convention, \\(0 \\log_2 0 = 0\\). Example: Suppose we have \\(10\\) data where \\(6\\) of them are in group A and the rest are in group B. The entropy is \\(-0.6 \\log_2(0.6) - 0.4 \\log_2 (0.4) = 0.097\\). The smaller the entropy is, the purer the class is. If the Entropy is \\(0\\), it means the sample is completely homogeneous. That is, all the data belong to one group. If the entropy is \\(log_2(c)\\), it means the data are as diverse as possible. The following figure shows the entropy as a function of the proportion of one class when there are only two classes. The maximum entropy is obtained when we have \\(50\\%\\) of data in one group and another \\(50\\%\\) of data in another group. \\(C5.0\\) algorithm uses entropy to determine the optimal feature to split upon. The algorithm calculates the information gain that would result from a split on each possible feature, which is a measure of the change in homogeneity. The information gain for a particular feature \\(F\\) is calculated as the difference between the entropy before the split, denoted by \\(\\text{Entropy}(S)\\), and the entropy after the split, denoted by \\(\\text{Entropy}(S|F)\\): \\[\\begin{equation*} \\text{InfoGain}(F) = \\text{Entropy}(S) - \\text{Entropy}(S|F). \\end{equation*}\\] Suppose the feature \\(F\\) can take \\(m\\) values (WLOG, assume the \\(m\\) values are \\(1,\\ldots,m\\)), which splits the data into \\(m\\) partitions. Then, \\[\\begin{equation*} \\text{Entropy}(S|F) = \\sum^m_{i=1} w_i \\text{Entropy}(S_F(i)), \\end{equation*}\\] where \\(w_i\\) is the proportion of data in the \\(i\\)th partition and \\(S_F(i)\\) is the set of data in the \\(i\\)th partition. Example of Calculating Information Gain Consider the following data. animal ## # A tibble: 15 x 4 ## Animal TravelsBy HasFur Mammal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Bats Air Yes Yes ## 2 Bears Land Yes Yes ## 3 Birds Air No No ## 4 Cats Land Yes Yes ## 5 Dogs Land Yes Yes ## 6 Eels Sea No No ## 7 Elephants Land No Yes ## 8 Fish Sea No No ## 9 Frogs Land No No ## 10 Insects Air No No ## 11 Pigs Land No Yes ## 12 Rabbits Land Yes Yes ## 13 Rats Land Yes Yes ## 14 Rhinos Land No Yes ## 15 Sharks Sea No No Suppose we want to classify animals as mammal or non-mammal. Before any splitting, we have \\(15\\) animals and \\(9\\) of them are mammals. The entropy is \\[\\begin{equation*} - \\frac{9}{15} \\log_2 \\bigg( \\frac{9}{15} \\bigg) - \\frac{6}{15} \\log_2 \\bigg( \\frac{6}{15} \\bigg) = 0.971. \\end{equation*}\\] Remark: you can use log2(x) to compute \\(\\log_2(x)\\). If we split the node by TravelsBy, we have \\(3\\) partitions. # 1st partition filter(animal, TravelsBy == &quot;Air&quot;) ## # A tibble: 3 x 4 ## Animal TravelsBy HasFur Mammal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Bats Air Yes Yes ## 2 Birds Air No No ## 3 Insects Air No No \\[\\begin{equation*} \\text{Entropy}(S_\\text{TravelsBy}(\\text{Air})) = - \\frac{1}{3} \\log_2 \\frac{1}{3} - \\frac{2}{3} \\log_2 \\frac{2}{3} = 0.918. \\end{equation*}\\] # 2nd partition filter(animal, TravelsBy == &quot;Land&quot;) ## # A tibble: 9 x 4 ## Animal TravelsBy HasFur Mammal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Bears Land Yes Yes ## 2 Cats Land Yes Yes ## 3 Dogs Land Yes Yes ## 4 Elephants Land No Yes ## 5 Frogs Land No No ## 6 Pigs Land No Yes ## 7 Rabbits Land Yes Yes ## 8 Rats Land Yes Yes ## 9 Rhinos Land No Yes \\[\\begin{equation*} \\text{Entropy}(S_\\text{TravelsBy}(\\text{Land})) = - \\frac{8}{9} \\log_2 \\frac{8}{9} - \\frac{1}{9} \\log_2 \\frac{1}{9} = 0.503 \\end{equation*}\\] # 3rd partition filter(animal, TravelsBy == &quot;Sea&quot;) ## # A tibble: 3 x 4 ## Animal TravelsBy HasFur Mammal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Eels Sea No No ## 2 Fish Sea No No ## 3 Sharks Sea No No \\[\\begin{equation*} \\text{Entropy}(S_\\text{TravelsBy}(\\text{Sea})) = 0. \\end{equation*}\\] Entropy after splitting: \\[\\begin{eqnarray*} \\text{Entropy}(S|\\text{TravelsBy}) &amp;=&amp; w_{\\text{Air}} \\text{Entropy}(S_\\text{TravelsBy}(\\text{Air})) +\\\\ &amp;&amp; \\quad w_{\\text{Land}} \\text{Entropy}(S_\\text{TravelsBy}(\\text{Land})) + \\\\ &amp;&amp; \\quad w_{\\text{Sea}} \\text{Entropy}(S_\\text{TravelsBy}(\\text{Sea})), \\end{eqnarray*}\\] where \\(w_{\\text{Air}} = \\frac{3}{15}\\), \\(w_{\\text{Land}} = \\frac{9}{15}\\), \\(w_{\\text{Sea}} = \\frac{3}{15}\\). Therefore, \\(\\text{Entropy}(S|\\text{TravelsBy})\\) is \\(0.486\\). The information gain by splitting using TravelsBy is \\(0.971 - 0.486 = 0.485\\). Now, lets compute the information gain by splitting using HasFur. If we split the node by HasFur, then we have \\(2\\) partitions. # 1st partition filter(animal, HasFur == &quot;Yes&quot;) ## # A tibble: 6 x 4 ## Animal TravelsBy HasFur Mammal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Bats Air Yes Yes ## 2 Bears Land Yes Yes ## 3 Cats Land Yes Yes ## 4 Dogs Land Yes Yes ## 5 Rabbits Land Yes Yes ## 6 Rats Land Yes Yes \\[\\begin{equation*} \\text{Entropy}(S_\\text{HasFur}(\\text{Yes})) = 0. \\end{equation*}\\] # 2nd partition filter(animal, HasFur == &quot;No&quot;) ## # A tibble: 9 x 4 ## Animal TravelsBy HasFur Mammal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Birds Air No No ## 2 Eels Sea No No ## 3 Elephants Land No Yes ## 4 Fish Sea No No ## 5 Frogs Land No No ## 6 Insects Air No No ## 7 Pigs Land No Yes ## 8 Rhinos Land No Yes ## 9 Sharks Sea No No \\[\\begin{equation*} \\text{Entropy}(S_\\text{TravelsBy}(\\text{Land})) = - \\frac{3}{9} \\log_2 \\frac{3}{9} - \\frac{6}{9} \\log_2 \\frac{6}{9} = 0.918. \\end{equation*}\\] Entropy after splitting: \\[\\begin{eqnarray*} \\text{Entropy}(S|\\text{HasFur}) &amp;=&amp; w_{\\text{Yes}} \\text{Entropy}(S_\\text{HasFur}(\\text{Yes})) +\\\\ &amp;&amp; \\quad w_{\\text{No}} \\text{Entropy}(S_\\text{HasFur}(\\text{No})) \\\\ &amp;=&amp; 0 + \\frac{9}{15} (0.918) = 0.551. \\end{eqnarray*}\\] The information gain by splitting using HasFur is \\(0.971 - 0.551 = 0.42\\). Since splitting using TravelsBy leads to a larger information gain, we should use this to create the first branch. For Continuous Features The formula for information gain assumes nominal features. For continuous features, a common practice is to test various splits that divide the values into groups greater than or less than a threshold. This reduces the continuous feature into a two-level categorical feature that allows information gain to be calculated as above. The numeric cut point yielding the largest information gain is chosen for the split. For example, for the numeric feature Petal.Length, the split in the right plot results in a higher information gain. Remark: to arrange multiple ggplot objects in a single plot, you can use ggarrange() from the package ggpubr as follows: library(ggpubr) # install it first if you haven&#39;t done so g1 &lt;- ggplot(iris_train, aes(x = Petal.Width, y = Petal.Length, color = Species)) + geom_point() + geom_hline(yintercept = 1.5) g2 &lt;- ggplot(iris_train, aes(x = Petal.Width, y = Petal.Length, color = Species)) + geom_point() + geom_hline(yintercept = 2) ggarrange(g1, g2, common.legend = TRUE, legend = &quot;bottom&quot;) 11.2.2 Pruning the decision tree A decision tree can continue to grow indefinitely, choosing splitting features and dividing into smaller and smaller partitions. If the tree grows overly large, many of the decisions it makes will be overly specific and will not generalize to new data (i.e., overfitted to the training data). The process of pruning a decision tree involves reducing its size so that it generalizes better to unseen data. For C5.0 algorithm, it first grows a large tree that overfits the training data. Then, the nodes and branches that have little effect on the classification errors are removed. 11.3 Example: iris Install the package C50 and load the library. library(C50) Model Training Basic usage of C5.0() from C50: x: a data frame containing the training data without the class labels y: a factor vector of the class labels iris_ct &lt;- C5.0(x = iris_train[, -5], y = iris_train$Species) Classification of the training data table(iris_train$Species, predict(iris_ct, iris_train)) ## ## setosa versicolor virginica ## setosa 37 0 0 ## versicolor 0 33 0 ## virginica 0 3 27 Accuracy is \\(97/100\\). Visualize the decision tree plot(iris_ct) summary(iris_ct) From the summary, we see that if Petal.Length is less than or equal to \\(1.9\\), then the example is classified as setosa. The numbers in the bracket indicates how many examples are classified as setosa. if Petal.Length is greater than \\(1.9\\) andPetal.Width is less than or equal to \\(1.7\\), the example is classified as versicolor. This time there are two numbers in the bracket. The first number \\(36\\) indicates how many examples are classified as versicolor. The second number \\(3\\) indicates that out of \\(36\\) examples, \\(3\\) of them are classified incorrectly. if Petal.Length is greater than \\(1.9\\) and Petal.Width is greater than \\(1.7\\), the example is classified as virginica. \\(27\\) examples are classified as virginica. Evaluate the performance using testing data table(iris_test$Species, predict(iris_ct, iris_test)) ## ## setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 16 1 ## virginica 0 2 18 Accuracy is \\(47/50\\). In general, the training accuracy will be higher than the testing accuracy. 11.4 Example: identifying risky bank loans In this subsection, we will apply classification tree to identify factors that are linked to a higher risk of loan default. The dataset is from https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data) See also https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/credit.csv The dataset contains \\(1,000\\) examples of loans, together with \\(20\\) features about the loans and the loan applicants. credit &lt;- read.csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/credit.csv&quot;, stringsAsFactors = TRUE) credit$default &lt;- recode_factor(credit$default, &quot;1&quot; = &quot;no&quot;, &quot;2&quot; = &quot;yes&quot;) The default variable indicates whether the loan applicant was able to meet the agreed payment terms or if they went into default. table(credit$default) ## ## no yes ## 700 300 In this dataset, \\(30\\%\\) of the loans went into default. If the bank can successfully predict who is more likely to default, it can reject the loan application and reduce the losses due to default loans. We now split our data into a training dataset and a testing dataset. set.seed(6) random_index &lt;- sample(nrow(credit), 900) # 90% of data credit_train &lt;- credit[random_index, ] credit_test &lt;- credit[-random_index, ] Model training credit_ct &lt;- C5.0(x = credit_train[-21], y = credit_train$default) You can find the decision tree by using summary(credit_ct): The meaning of this is similar to that of the iris dataset. library(gmodels) credit_train_pred &lt;- predict(credit_ct, credit_train) CrossTable(credit_train$default, credit_train_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&quot;actual default&quot;, &quot;predicted default&quot;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 900 ## ## ## | predicted default ## actual default | no | yes | Row Total | ## ---------------|-----------|-----------|-----------| ## no | 605 | 25 | 630 | ## | 0.672 | 0.028 | | ## ---------------|-----------|-----------|-----------| ## yes | 83 | 187 | 270 | ## | 0.092 | 0.208 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 688 | 212 | 900 | ## ---------------|-----------|-----------|-----------| ## ## The accuracy for the training data is \\(0.672 + 0.208 = 0.88\\). Evaluating model performance credit_test_pred &lt;- predict(credit_ct, credit_test) CrossTable(credit_test$default, credit_test_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&quot;actual default&quot;, &quot;predicted default&quot;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | predicted default ## actual default | no | yes | Row Total | ## ---------------|-----------|-----------|-----------| ## no | 56 | 14 | 70 | ## | 0.560 | 0.140 | | ## ---------------|-----------|-----------|-----------| ## yes | 16 | 14 | 30 | ## | 0.160 | 0.140 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 72 | 28 | 100 | ## ---------------|-----------|-----------|-----------| ## ## The accuracy for the testing data is \\(0.56 + 0.14 = 0.7\\). Is this a good classifer? Lets take a look at the proportion of defaults in the testing data (it should be close to \\(0.7\\)): summary(credit_test$default) ## no yes ## 70 30 It turns out to be exactly \\(0.7\\) (recall that we randomly select \\(100\\) examples for the testing dataset so that this proportion may not exactly be \\(0.7\\)). If we predict no default for every loan, we would obtain the same accuracy as our classification tree. Remark: accuracy is not the only measure for classification problems. We will discuss other measures later. 11.4.1 Adaptive boosting One way the C5.0 algorithm improved upon the C4.5 algorithm was through the addition of adaptive boosting. This is a process in which many classification trees are built and the trees vote on the best class for each example. To add boosting to our classification tree, we only need to include the additional parameter trials in the function C5.0. This indicates the number of separate classification trees to use in the boosted team. credit_ct_boost10 &lt;- C5.0(x = credit_train[-21], y = credit_train$default, trials = 10) Training error: credit_train_pred_boost10 &lt;- predict(credit_ct_boost10, credit_train) CrossTable(credit_train$default, credit_train_pred_boost10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&quot;actual default&quot;, &quot;predicted default&quot;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 900 ## ## ## | predicted default ## actual default | no | yes | Row Total | ## ---------------|-----------|-----------|-----------| ## no | 630 | 0 | 630 | ## | 0.700 | 0.000 | | ## ---------------|-----------|-----------|-----------| ## yes | 15 | 255 | 270 | ## | 0.017 | 0.283 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 645 | 255 | 900 | ## ---------------|-----------|-----------|-----------| ## ## The accuracy becomes \\(0.7 + 0.283 = 0.983\\). However, we are only interested in the testing accuracy. Evaluating performance: credit_test_pred_boost10 &lt;- predict(credit_ct_boost10, credit_test) CrossTable(credit_test$default, credit_test_pred_boost10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&quot;actual default&quot;, &quot;predicted default&quot;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 100 ## ## ## | predicted default ## actual default | no | yes | Row Total | ## ---------------|-----------|-----------|-----------| ## no | 59 | 11 | 70 | ## | 0.590 | 0.110 | | ## ---------------|-----------|-----------|-----------| ## yes | 17 | 13 | 30 | ## | 0.170 | 0.130 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 76 | 24 | 100 | ## ---------------|-----------|-----------|-----------| ## ## The testing accuracy only improves a little bit to \\(0.72\\). The results are not very satisfactory. Here are some possible reasons: this classification problem is intrinsically difficult for this dataset. For example, we do not have a large sample size and do not have enough information about the background of the loan applicants. Other methods and algorithms may perform better. We shall talk about a better way of measuring the testing accuracy and other ways to improve the classification later. At this point, only the most basic usages of classification tree are discussed. "],["linear-regression-models.html", "Chapter 12 Linear Regression Models 12.1 Simple Linear Regression 12.2 Smoothed Conditional Means 12.3 Multiple Linear Regression 12.4 Example: diamonds 12.5 Categorical Predictors 12.6 Compare models using ANOVA 12.7 Prediction 12.8 Interaction Terms 12.9 Variable Transformation 12.10 Polynomial Regression 12.11 Stepwise regression 12.12 Best subset", " Chapter 12 Linear Regression Models Reference: R cookbook https://rc2e.com/linearregressionandanova, any linear regression textbooks. Packages used in this chapter: library(tidyverse) # contains ggplot2 and dplyr library(nycflights13) # contains the dataset &quot;flights&quot; library(bestglm) # find the best subset model Regression analysis is the study of the relationship between the responses and the covariates. Linear regression is one of the most used statistical techniques when the response is continuous. It can be used for prediction and explain variation in the response variable. Two types of variables: response = dependent variable, usually denoted by \\(Y\\) explanatory variable = independent variable = covariate = predictor = feature, usually denoted by \\(X_1,\\ldots,X_p\\) 12.1 Simple Linear Regression Suppose we have data \\((x_1,y_1),\\ldots,(x_n,y_n)\\). A simple linear regression specifies that \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\end{equation*}\\] where \\(E(\\varepsilon_i) = 0\\), \\(Var(\\varepsilon_i) = \\sigma^2 &gt; 0\\), and \\(\\varepsilon_i\\)s are i.i.d. Taking expectation on both sides of the above equation, we see that \\[\\begin{equation*} E(Y|X=x) = \\beta_0 + \\beta_1 x. \\end{equation*}\\] Hence, the conditional mean of \\(Y\\) given \\(x\\) is linear in \\(x\\). In the model: \\(\\beta_0\\) is the intercept (mean of \\(Y\\) when \\(x=0\\)) \\(\\beta_1\\) is the slope (which is the change in the mean of \\(Y\\) for \\(1\\) unit increase in \\(x\\)) \\(\\varepsilon\\) is the error term (anything that is not explained by \\(x\\)). It is the vertical distance between \\(y\\) and the conditional mean \\(E(Y|X=x)\\). \\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters to be estimated from data How to estimate \\(\\beta_0\\) and \\(\\beta_1\\)? Recall that a line can be specified by the intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)). Therefore, intuitively, we want to find a line that best fits the points. How to define the best fit? Answer: Method of least square We can minimize the residual sum of squares, which is defined by \\[\\begin{equation*} \\sum^n_{i=1} [ y_i - (\\beta_0 + \\beta_1 x_i)]^2. \\end{equation*}\\] The least square estimator for \\((\\beta_0, \\beta_1)\\) is defined as the minimizer of the residual sum of squares. That is, \\[\\begin{equation*} (\\hat{\\beta}_0, \\hat{\\beta}_1) := \\text{argmin}_{\\beta_0, \\beta_1} \\sum^n_{i=1} [ y_i - (\\beta_0 + \\beta_1 x_i)]^2. \\end{equation*}\\] The minimizer can be found by differentiating the objection function with respect to \\(\\beta_0\\) and \\(\\beta_1\\) setting the resulting expressions to \\(0\\) solving the simultaneous equations The above steps lead to a closed-form formula for \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) in terms of \\(x\\)s and \\(y\\)s, which is a special case of the formula given in Section 11.3 Multiple Linear Regression. The residual is \\(\\hat{\\varepsilon}_i = y_i - \\hat{y}_i\\), where \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\). To perform linear regression in R, we use lm(). Examples in R Recall the dataset flights in the package nycflights13. # y = arr_delay, x = dep_delay lm(arr_delay ~ dep_delay, data = flights) ## ## Call: ## lm(formula = arr_delay ~ dep_delay, data = flights) ## ## Coefficients: ## (Intercept) dep_delay ## -5.899 1.019 The regression equation is \\[\\begin{equation*} \\text{arr delay} = -5.899 + 1.019 \\text{dep delay} + \\varepsilon. \\end{equation*}\\] From this equation, we can see that for \\(1\\) minute increase in departure delay, the arrival delay will increase by \\(1.019\\) minute on average. When there is no departure delay, the flights arrive earlier on average (a negative arrival delay means there was an early arrival). Perform a simple linear regression without a dataset # Example 1 lm(flights$arr_delay ~ flights$dep_delay) ## ## Call: ## lm(formula = flights$arr_delay ~ flights$dep_delay) ## ## Coefficients: ## (Intercept) flights$dep_delay ## -5.899 1.019 Another example # Example 2 set.seed(1) x &lt;- rnorm(100, 0, 1) y &lt;- 1 + x + rnorm(100, 0, 1) lm(y ~ x) ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## 0.9623 0.9989 To visualize the regression line using ggplot(), use geom_smooth() and set method = lm. ggplot(mapping = aes(x = x, y= y)) + geom_point() + geom_smooth(method = lm) To remove the confidence interval, set se = FALSE in geom_smooth(). ggplot(mapping = aes(x = x, y= y)) + geom_point() + geom_smooth(method = lm, se = FALSE) Remark: You have seen geom_smooth() in Assignment 3. 12.2 Smoothed Conditional Means In simple linear regression, a linear relationship between the response and predictor is assumed. In many cases, this assumption may not hold. In those situations, we may fit a curve to the data. For example, we can use geom_smooth() (without setting method = lm) to visualize the fitting of a function through the points of a scatterplot that best represents the relationship bewteen the response and the predictor without assuming the linear relationship. cars %&gt;% ggplot(mapping = aes(x = speed, y = dist)) + geom_point() + geom_smooth() You can see the message saying geom_smooth() is using method = \"loess\". loess stands for locally estimated scatterplot smoothing. You may also see lowess, which stands for locally weighted scatterplot smoothing. Basically, a locally weighted regression solves a separate weighted least squares problem at each target point \\(x_0\\): \\[\\begin{equation*} \\min_{\\alpha(x_0), \\beta(x_0)} \\sum^n_{i=1} K_\\lambda(x_0, x_i)[y_i - \\alpha(x_0) - \\beta(x_0) x_i]^2, \\end{equation*}\\] where \\(K_\\lambda(x_0, x_i) = K(|x_i-x_0|/\\lambda)\\) for some kernel function \\(K\\) and \\(\\lambda\\) is a positive number. The estimate is \\(\\hat{f}(x_0) = \\hat{\\alpha}(x_0) + \\hat{\\beta}(x_0)x_0\\). The idea is that data points that are close to \\(x_0\\) will have larger weights \\(K_\\lambda(x_0, x_i)\\) so that the points near \\(x_0\\) are more important in estimating \\(\\alpha(x_0)\\) and \\(\\beta(x_0)\\). This is the meaning of locally weighted. 12.3 Multiple Linear Regression We can add more predictors to explain the response variable better: \\[\\begin{equation*} y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_p x_{ip} + \\varepsilon_i. \\end{equation*}\\] Suppose we have \\(n\\) data, then we can use the matrix notation to represent our model: \\[\\begin{equation*} Y = X \\beta + \\varepsilon, \\end{equation*}\\] where \\[\\begin{equation*} y = \\left( \\begin{array}{c} y_1 \\\\ \\vdots \\\\ y_n \\end{array} \\right), \\quad X = \\left( \\begin{array}{cccc} 1 &amp; x_{11} &amp; \\ldots &amp; x_{1p}\\\\ 1 &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\ldots &amp; x_{np} \\end{array} \\right), \\quad \\text{and } \\varepsilon = \\left( \\begin{array}{c} \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array} \\right). \\end{equation*}\\] As in simple linear regression, we estimate \\(\\beta\\) by minimizing the residual sum of squares: \\[\\begin{equation*} \\sum^n_{i=1}(y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2} - \\ldots - \\beta_p x_{ip})^2. \\end{equation*}\\] The least square estimator for \\(\\beta\\) is \\[\\begin{equation*} \\hat{\\beta} = (X^T X)^{-1}X^T Y. \\end{equation*}\\] Examples in R Without a dataframe: set.seed(1) n &lt;- 100 x1 &lt;- rnorm(n, 0, 1) x2 &lt;- rnorm(n, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + rnorm(n, 0, 1) lm(y ~ x1 + x2) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Coefficients: ## (Intercept) x1 x2 ## 1.025 2.021 2.947 With a dataframe: new_data &lt;- tibble(response = y, cov1 = x1, cov2 = x2) # use the column names of your dataframe lm(response ~ cov1 + cov2, data = new_data) ## ## Call: ## lm(formula = response ~ cov1 + cov2, data = new_data) ## ## Coefficients: ## (Intercept) cov1 cov2 ## 1.025 2.021 2.947 To obtain more information about the model: # assign the model object to a variable fit &lt;- lm(y ~ x1 + x2) # summary is one of the most important functions for linear regression summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.94359 -0.43645 0.00202 0.63692 2.63941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0254 0.1052 9.747 4.71e-16 *** ## x1 2.0211 0.1168 17.311 &lt; 2e-16 *** ## x2 2.9465 0.1095 26.914 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.043 on 97 degrees of freedom ## Multiple R-squared: 0.9134, Adjusted R-squared: 0.9116 ## F-statistic: 511.6 on 2 and 97 DF, p-value: &lt; 2.2e-16 From the summary, you can find Estimates of the regression coefficients. The \\(p\\)-value indicates if the regression coefficient is significantly different from \\(0\\). If the \\(p\\)-value is smaller than \\(0.05\\), then we reject the null hypothesis that the regression coefficient is equal to \\(0\\) at \\(0.05\\) significance level. \\(R^2\\) is a measure of the variance of \\(y\\) that is explained by the model. The higher the \\(R^2\\) is, the better is your model. However, adding additional variables will always increase \\(R^2\\) while this may not improve the model in the sense that it may not improve your prediction for new data. The adjusted \\(R^2\\) accounts for the number of variables in the model and is a better measure of the models quality. \\(F\\)-statistic tells you whether your model is statistically significant or not. The null hypothesis is that all coefficients are zero and the alternative hypothesis is that not all coefficients are zero. Objects in fit names(fit) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; &quot;fitted.values&quot; &quot;assign&quot; ## [7] &quot;qr&quot; &quot;df.residual&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; Extract the coefficients fit$coefficients # or coef(fit) ## (Intercept) x1 x2 ## 1.025353 2.021110 2.946533 Extract the residuals fit$residuals # or resid(fit) Confidence intervals for regression coefficients confint(fit) ## 2.5 % 97.5 % ## (Intercept) 0.8165723 1.234135 ## x1 1.7893884 2.252832 ## x2 2.7292486 3.163818 12.4 Example: diamonds Consider the diamonds dataset in ggplot2. Lets try to predict the price of an diamond based on its characteristics. In diamonds, cut, color and clarity are categorical features. To use them in regression, a standard way is to use the dummy coding discussed in the kNN chapter. The lm() function can handle this automatically as long as the variable is of a factor type. # When the variable is an ordered factor, the names in the output of # lm() are weird. So I turn them into an unordered factor first. diamonds2 &lt;- diamonds diamonds2$cut &lt;- factor(diamonds2$cut, order = FALSE) diamonds2$clarity &lt;- factor(diamonds2$clarity, order = FALSE) diamonds2$color &lt;- factor(diamonds2$color, order = FALSE) Lets try price ~ cut. The variable cut takes \\(5\\) values: Fair, Good, Very Good, Premium and Ideal. Therefore, we expect to have 5 regresion coefficients (1 for intercept, 4 for cut). fit &lt;- lm(price ~ cut, data = diamonds2) summary(fit) ## ## Call: ## lm(formula = price ~ cut, data = diamonds2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4258 -2741 -1494 1360 15348 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4358.76 98.79 44.122 &lt; 2e-16 *** ## cutGood -429.89 113.85 -3.776 0.000160 *** ## cutVery Good -377.00 105.16 -3.585 0.000338 *** ## cutPremium 225.50 104.40 2.160 0.030772 * ## cutIdeal -901.22 102.41 -8.800 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3964 on 53935 degrees of freedom ## Multiple R-squared: 0.01286, Adjusted R-squared: 0.01279 ## F-statistic: 175.7 on 4 and 53935 DF, p-value: &lt; 2.2e-16 From the output, we see that we have \\(4\\) additional predictors named cutGood, cutVery Good, cutPremium and cutIdeal corresponding to the values cut can take. We do not see cutFair because the value fair has been used as the baseline for the regression model. That means if cut is fair, then the mean price is just the intercept value. If cut is Good, then the mean price is \\(4358.76-429.89.\\) The results for the regression coefficients do not make sense because they imply that diamonds with ideal cut are the cheapest on average. The problem is that we have not controlled for other variables. The meaning of control for other variables is that we should also put in other relevant variables so that the interpretation of the regression coefficients will make sense. For example, if we include carat, then the interpretation of the regression coefficient for cutGood is that fixing the weight of the diamond, diamonds with good cut is how much more expensive than diamonds with fair cut on average (because the baseline value is fair). Now, lets try to add carat. fit &lt;- lm(price ~ cut + carat, data = diamonds2) summary(fit) ## ## Call: ## lm(formula = price ~ cut + carat, data = diamonds2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17540.7 -791.6 -37.6 522.1 12721.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3875.47 40.41 -95.91 &lt;2e-16 *** ## cutGood 1120.33 43.50 25.75 &lt;2e-16 *** ## cutVery Good 1510.14 40.24 37.53 &lt;2e-16 *** ## cutPremium 1439.08 39.87 36.10 &lt;2e-16 *** ## cutIdeal 1800.92 39.34 45.77 &lt;2e-16 *** ## carat 7871.08 13.98 563.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1511 on 53934 degrees of freedom ## Multiple R-squared: 0.8565, Adjusted R-squared: 0.8565 ## F-statistic: 6.437e+04 on 5 and 53934 DF, p-value: &lt; 2.2e-16 It makes more sense now although the coefficient of cutVery good is still higher than that of cutPremium. We now add clarity and color. fit &lt;- lm(price ~ cut + carat + clarity + color, data = diamonds2) summary(fit) ## ## Call: ## lm(formula = price ~ cut + carat + clarity + color, data = diamonds2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16813.5 -680.4 -197.6 466.4 10394.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7362.80 51.68 -142.46 &lt;2e-16 *** ## cutGood 655.77 33.63 19.50 &lt;2e-16 *** ## cutVery Good 848.72 31.28 27.14 &lt;2e-16 *** ## cutPremium 869.40 30.93 28.11 &lt;2e-16 *** ## cutIdeal 998.25 30.66 32.56 &lt;2e-16 *** ## carat 8886.13 12.03 738.44 &lt;2e-16 *** ## claritySI2 2625.95 44.79 58.63 &lt;2e-16 *** ## claritySI1 3573.69 44.60 80.13 &lt;2e-16 *** ## clarityVS2 4217.83 44.84 94.06 &lt;2e-16 *** ## clarityVS1 4534.88 45.54 99.59 &lt;2e-16 *** ## clarityVVS2 4967.20 46.89 105.93 &lt;2e-16 *** ## clarityVVS1 5072.03 48.21 105.20 &lt;2e-16 *** ## clarityIF 5419.65 52.14 103.95 &lt;2e-16 *** ## colorE -211.68 18.32 -11.56 &lt;2e-16 *** ## colorF -303.31 18.51 -16.39 &lt;2e-16 *** ## colorG -506.20 18.12 -27.93 &lt;2e-16 *** ## colorH -978.70 19.27 -50.78 &lt;2e-16 *** ## colorI -1440.30 21.65 -66.54 &lt;2e-16 *** ## colorJ -2325.22 26.72 -87.01 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1157 on 53921 degrees of freedom ## Multiple R-squared: 0.9159, Adjusted R-squared: 0.9159 ## F-statistic: 3.264e+04 on 18 and 53921 DF, p-value: &lt; 2.2e-16 Now, in terms of the relative magnitude, all the regression coefficients make sense. Note that the baseline value of color is D (which is the best) so that any other colors should have negative coefficients. For cut and clarity, the baseline values are the worst so that the other variables should have positive coefficients. 12.5 Categorical Predictors See the diamonds example. 12.6 Compare models using ANOVA If you want to compare if the difference between two models are statistically significant or not, you can use anova(). When you compare two models using anova(), one model must be contained within the other. set.seed(100) n &lt;- 100 x1 &lt;- rnorm(n, 0, 1) x2 &lt;- rnorm(n, 0, 1) x3 &lt;- rnorm(n, 0, 1) x4 &lt;- rnorm(n, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + rnorm(n, 0, 1) m1 &lt;- lm(y ~ x1) m2 &lt;- lm(y ~ x1 + x2) m3 &lt;- lm(y ~ x1 + x2 + x3) m4 &lt;- lm(y ~ x1 + x2 + x3 + x4) m1 is contained within m2, m2 is contained within m3, and m3 is contained within m4. Compare m1 and m2: anova(m1, m2) ## Analysis of Variance Table ## ## Model 1: y ~ x1 ## Model 2: y ~ x1 + x2 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 98 618.26 ## 2 97 112.56 1 505.7 435.79 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The difference is statistically significant, implying x2 should be necessary. Compare m2 and m4: anova(m2, m4) ## Analysis of Variance Table ## ## Model 1: y ~ x1 + x2 ## Model 2: y ~ x1 + x2 + x3 + x4 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 112.56 ## 2 95 110.16 2 2.4024 1.0359 0.3589 The difference is not statistically significant, implying x3 and x4 may not be necessary. Compare m3 and m4: anova(m3, m4) ## Analysis of Variance Table ## ## Model 1: y ~ x1 + x2 + x3 ## Model 2: y ~ x1 + x2 + x3 + x4 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 96 110.78 ## 2 95 110.16 1 0.61992 0.5346 0.4665 The difference is not statistically significant, implying x4 may not be necessary. 12.7 Prediction Lets use the flights dataset again. set.seed(1) random_index &lt;- sample(nrow(flights), size = nrow(flights) * 0.7) flights_train &lt;- flights[random_index, ] flights_test &lt;- flights[-random_index, ] fit &lt;- lm(arr_delay ~ dep_delay, data = flights_train) Prediction # prediction predict(fit, flights_test) Since the response is continuous, we cannot use accuracy to measure the performance of our prediction. One possible measure is to use correlation. The higher the correlation, the better the prediction. cor(predict(fit, flights_test), flights_test$arr_delay, use = &quot;complete.obs&quot;) ## [1] 0.9171425 Since there are missing values, we set use = \"complete.obs\". 12.8 Interaction Terms When the effect on the response of one predictor variable depends the levels or values of the other predictor variables, we can include interaction terms. For example, if we have two predictors \\(X_1\\) and \\(X_2\\), we can include an additional interaction term \\(X_1 X_2\\): \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\varepsilon_i. \\end{equation*}\\] To perform the following regression \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\varepsilon_i, \\end{equation*}\\] use lm(y ~ x1 * x2). Example: # simulation set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + 4 * x1 * x2 + rnorm(100, 0, 1) # estimation lm(y ~ x1 * x2) ## ## Call: ## lm(formula = y ~ x1 * x2) ## ## Coefficients: ## (Intercept) x1 x2 x1:x2 ## 1.031 1.966 2.972 3.760 To perform the following regression \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\beta_4 X_{i1} X_{i2} + \\beta_5 X_{i1}X_{i3} + \\beta_6 X_{i2} X_{i3} + \\beta_7 X_{i1} X_{i2} X_{i3} + \\varepsilon_i, \\end{equation*}\\] use lm(y ~ x1 * x2 * x3). Example: # simulation set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) x3 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + 4 * x1 * x2 + 2 * x1 * x2 * x3 + rnorm(100, 0, 1) # estimation fit &lt;- lm(y ~ x1 * x2 * x3) summary(fit) ## ## Call: ## lm(formula = y ~ x1 * x2 * x3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.31655 -0.51032 -0.00288 0.76844 2.01342 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.050754 0.098109 10.710 &lt;2e-16 *** ## x1 1.862611 0.113359 16.431 &lt;2e-16 *** ## x2 2.942130 0.104931 28.039 &lt;2e-16 *** ## x3 0.034585 0.096500 0.358 0.7209 ## x1:x2 3.636972 0.143910 25.272 &lt;2e-16 *** ## x1:x3 -0.246646 0.118356 -2.084 0.0399 * ## x2:x3 0.002958 0.113553 0.026 0.9793 ## x1:x2:x3 1.873330 0.140592 13.325 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9552 on 92 degrees of freedom ## Multiple R-squared: 0.9625, Adjusted R-squared: 0.9597 ## F-statistic: 337.6 on 7 and 92 DF, p-value: &lt; 2.2e-16 We notice that the coefficients of x3 and x2:x3 are not significantly different from \\(0\\). This makes sense because our simulation model does not include these two predictors. However, we see that x1:x3 is significantly different from \\(0\\) (at the \\(0.05\\) level) even though we did not include this predictor in the simulation model. This is because we will commit Type I error. We can also include a specific interaction by using : instead of *. For example, to fit \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\beta_4 X_{i1} X_{i2} X_{i3} + \\varepsilon_i, \\end{equation*}\\] we can use lm(y ~ x1 + x2 + x1:x2 + x1:x2:x3). Example: set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) x3 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x2 + 4 * x1 * x2 + 2 * x1 * x2 * x3 + rnorm(100, 0, 1) lm(y ~ x1 + x2 + x1:x2 + x1:x2:x3) ## ## Call: ## lm(formula = y ~ x1 + x2 + x1:x2 + x1:x2:x3) ## ## Coefficients: ## (Intercept) x1 x2 x1:x2 x1:x2:x3 ## 1.061 1.853 2.981 3.598 1.967 12.9 Variable Transformation We can transform our response or predictors. For example, the model \\[\\begin{equation*} \\log Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\varepsilon_i \\end{equation*}\\] is a special case of the genearl linear regression model because we can define \\(Y_i&#39;\\) as \\(\\log Y_i\\). Then, we obtain \\[\\begin{equation*} Y_i&#39; = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\varepsilon_i. \\end{equation*}\\] Lets generate some data in which \\(x\\) and \\(y\\) do not have a linear relationship. set.seed(1) x &lt;- rnorm(100, 0, 0.5) y &lt;- exp(1 + 2 * x + rnorm(100, 0, 1)) To perform regression for the transformed variable: lm(log(y) ~ x) ## ## Call: ## lm(formula = log(y) ~ x) ## ## Coefficients: ## (Intercept) x ## 0.9623 1.9979 12.10 Polynomial Regression Polynomial regression models are special cases of the general linear regression model. They contain higher-order terms of the predictor variables, making the response function nonlinear. Suppose we only have one predictor \\(X_{1}\\). An example of a polynomial regression model is \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i1}^2 + \\varepsilon_i. \\end{equation*}\\] It is a special case of the general linear regression model because we can define \\(X_{i2}\\) as \\(X_{i1}^2\\) and write \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\varepsilon_i. \\end{equation*}\\] Example: set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x1^2 - 2 * x2 + rnorm(100, 0, 1) lm(y ~ poly(x1, 2, raw = TRUE) + x2) ## ## Call: ## lm(formula = y ~ poly(x1, 2, raw = TRUE) + x2) ## ## Coefficients: ## (Intercept) poly(x1, 2, raw = TRUE)1 poly(x1, 2, raw = TRUE)2 ## 1.044 2.025 2.976 ## x2 ## -2.058 We have to set raw = TRUE in ploy(). Alternatively, include I(x1^2) not x1^2: set.seed(1) x1 &lt;- rnorm(100, 0, 1) x2 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x1^2 - 2 * x2 + rnorm(100, 0, 1) lm(y ~ x1 + I(x1^2) + x2) ## ## Call: ## lm(formula = y ~ x1 + I(x1^2) + x2) ## ## Coefficients: ## (Intercept) x1 I(x1^2) x2 ## 1.044 2.025 2.976 -2.058 Note that lm(y ~ x1 + x1^2 + x2) does not do what you think. If you want to perform \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i1}^3 + \\varepsilon, \\end{equation*}\\] you can use lm(y ~ x1 + I(x1^3)). Example: set.seed(1) x1 &lt;- rnorm(100, 0, 1) y &lt;- 1 + 2 * x1 + 3 * x1^3 + rnorm(100, 0, 1) lm(y ~ x1 + I(x1^3)) ## ## Call: ## lm(formula = y ~ x1 + I(x1^3)) ## ## Coefficients: ## (Intercept) x1 I(x1^3) ## 0.9623 2.0014 2.9990 12.11 Stepwise regression Suppose that there are \\(p\\) predictors, \\(x_1,\\ldots,x_p\\). In many cases, not all of them are useful to model and predict the response. That is, we may only want to use a subset of them to form a model. Stepwise regression is one of the methods to achieve this. To perform stepwise regression, you can use step() in R. This function uses AIC as a criterion. AIC is defined as \\[\\begin{equation*} AIC = -2\\log L(\\hat{\\beta}) + 2k, \\end{equation*}\\] where \\(k\\) is the number of parameters and \\(L(\\hat{\\beta})\\) is the likelihood function evaluated at the MLE \\(\\hat{\\beta}\\). As you can see, there are two components in the formula. We want a large likelihood while keeping a model simple (= fewer parameters = smaller \\(k\\)). Therefore, a model with a smaller AIC is preferred. You can think of the term \\(2k\\) as a penalty that discourages us to use too many predictors. This is because while using more predictors can always increase the value of the likelihood function, it will result in overfitting. We will illustrate step() using the prostate cancer dataset zprostate in the package bestglm. Details of the dataset: a study of 97 men with prostate cancer examined the correlation between PSA (prostate specific antigen) and a number of clinical measurements: lcavol, lweight, lbph, svi, lcp, gleason, pgg45. Backward stepwise regression: starts with the full model (or a model with many variables) and removes the variable that results in the largest AIC until no variables can be removed. full_model &lt;- lm(lpsa ~ . , data = zprostate[, -10]) step(full_model, direction = &quot;backward&quot;) ## Start: AIC=-60.78 ## lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + ## pgg45 ## ## Df Sum of Sq RSS AIC ## - gleason 1 0.0491 43.108 -62.668 ## - pgg45 1 0.5102 43.569 -61.636 ## - lcp 1 0.6814 43.740 -61.256 ## &lt;none&gt; 43.058 -60.779 ## - lbph 1 1.3646 44.423 -59.753 ## - age 1 1.7981 44.857 -58.810 ## - lweight 1 4.6907 47.749 -52.749 ## - svi 1 4.8803 47.939 -52.364 ## - lcavol 1 20.1994 63.258 -25.467 ## ## Step: AIC=-62.67 ## lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45 ## ## Df Sum of Sq RSS AIC ## - lcp 1 0.6684 43.776 -63.176 ## &lt;none&gt; 43.108 -62.668 ## - pgg45 1 1.1987 44.306 -62.008 ## - lbph 1 1.3844 44.492 -61.602 ## - age 1 1.7579 44.865 -60.791 ## - lweight 1 4.6429 47.751 -54.746 ## - svi 1 4.8333 47.941 -54.360 ## - lcavol 1 21.3191 64.427 -25.691 ## ## Step: AIC=-63.18 ## lpsa ~ lcavol + lweight + age + lbph + svi + pgg45 ## ## Df Sum of Sq RSS AIC ## - pgg45 1 0.6607 44.437 -63.723 ## &lt;none&gt; 43.776 -63.176 ## - lbph 1 1.3329 45.109 -62.266 ## - age 1 1.4878 45.264 -61.934 ## - svi 1 4.1766 47.953 -56.336 ## - lweight 1 4.6553 48.431 -55.373 ## - lcavol 1 22.7555 66.531 -24.572 ## ## Step: AIC=-63.72 ## lpsa ~ lcavol + lweight + age + lbph + svi ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 44.437 -63.723 ## - age 1 1.1588 45.595 -63.226 ## - lbph 1 1.5087 45.945 -62.484 ## - lweight 1 4.3140 48.751 -56.735 ## - svi 1 5.8509 50.288 -53.724 ## - lcavol 1 25.9427 70.379 -21.119 ## ## Call: ## lm(formula = lpsa ~ lcavol + lweight + age + lbph + svi, data = zprostate[, ## -10]) ## ## Coefficients: ## (Intercept) lcavol lweight age lbph svi ## 2.4784 0.6412 0.2520 -0.1224 0.1469 0.2960 From the full model, removing gleason results in a model with the smallest AIC. Therefore, the algorithm first removes gleason. Next, removing lcp results in a model with the smallest AIC. Therefore, lcp is removed in the next step. pgg45 is removed because removing it will continue to reduce the AIC. Finally, removing any variable will not reduce the AIC. Therefore, the algorithm stops and the best model is lpsa ~ lcavol + lweight + age + lbph + svi. Forward stepwise regression: starts with the intercept (or a few variables) and adds new ones until the model cannot be improved (in the sense that the AIC will not decrease). null_model &lt;- lm(lpsa ~ 1, data = zprostate[, -10]) # the model with intercept only step(null_model, direction = &quot;forward&quot;, scope = formula(full_model)) ## Start: AIC=28.84 ## lpsa ~ 1 ## ## Df Sum of Sq RSS AIC ## + lcavol 1 69.003 58.915 -44.366 ## + svi 1 41.011 86.907 -6.658 ## + lcp 1 38.528 89.389 -3.926 ## + lweight 1 24.019 103.899 10.665 ## + pgg45 1 22.814 105.103 11.783 ## + gleason 1 17.416 110.502 16.641 ## + lbph 1 4.136 123.782 27.650 ## + age 1 3.679 124.239 28.007 ## &lt;none&gt; 127.918 28.838 ## ## Step: AIC=-44.37 ## lpsa ~ lcavol ## ## Df Sum of Sq RSS AIC ## + lweight 1 7.1726 51.742 -54.958 ## + svi 1 5.2375 53.677 -51.397 ## + lbph 1 3.2658 55.649 -47.898 ## + pgg45 1 1.6980 57.217 -45.203 ## &lt;none&gt; 58.915 -44.366 ## + lcp 1 0.6562 58.259 -43.452 ## + gleason 1 0.4156 58.499 -43.053 ## + age 1 0.0025 58.912 -42.370 ## ## Step: AIC=-54.96 ## lpsa ~ lcavol + lweight ## ## Df Sum of Sq RSS AIC ## + svi 1 5.1737 46.568 -63.177 ## + pgg45 1 1.8158 49.926 -56.424 ## &lt;none&gt; 51.742 -54.958 ## + lcp 1 0.8187 50.923 -54.506 ## + gleason 1 0.7163 51.026 -54.311 ## + age 1 0.6456 51.097 -54.176 ## + lbph 1 0.4440 51.298 -53.794 ## ## Step: AIC=-63.18 ## lpsa ~ lcavol + lweight + svi ## ## Df Sum of Sq RSS AIC ## + lbph 1 0.97296 45.595 -63.226 ## &lt;none&gt; 46.568 -63.177 ## + age 1 0.62301 45.945 -62.484 ## + pgg45 1 0.50069 46.068 -62.226 ## + gleason 1 0.34449 46.224 -61.898 ## + lcp 1 0.06937 46.499 -61.322 ## ## Step: AIC=-63.23 ## lpsa ~ lcavol + lweight + svi + lbph ## ## Df Sum of Sq RSS AIC ## + age 1 1.15879 44.437 -63.723 ## &lt;none&gt; 45.595 -63.226 ## + pgg45 1 0.33173 45.264 -61.934 ## + gleason 1 0.20691 45.389 -61.667 ## + lcp 1 0.10115 45.494 -61.441 ## ## Step: AIC=-63.72 ## lpsa ~ lcavol + lweight + svi + lbph + age ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 44.437 -63.723 ## + pgg45 1 0.66071 43.776 -63.176 ## + gleason 1 0.47674 43.960 -62.769 ## + lcp 1 0.13040 44.306 -62.008 ## ## Call: ## lm(formula = lpsa ~ lcavol + lweight + svi + lbph + age, data = zprostate[, ## -10]) ## ## Coefficients: ## (Intercept) lcavol lweight svi lbph age ## 2.4784 0.6412 0.2520 0.2960 0.1469 -0.1224 Remark: there is also an option direction = \"both\". 12.12 Best subset Given \\(p\\) predictors, the best subset problem is to find out of all the \\(2^p\\) subsets, the best subset according to some goodness-of-fit criterion. Some examples of goodness-of-fit criteria are AIC and BIC. We will use the package bestglm. The function to find out the best subset is also called bestglm(). BIC is defined as \\[\\begin{equation*} BIC = -2\\log L(\\hat{\\beta}) + k \\log (n), \\end{equation*}\\] where \\(k\\) is the number of parameters, \\(n\\) is the number of observations, and \\(L(\\hat{\\beta})\\) is the likelihood function evaluated at the MLE. You can think of \\(k \\log (n)\\) as a penalty to discourage us to use too many predictors to avoid overfitting. library(bestglm) # the last column ind bestglm(zprostate[, -10], IC = &quot;BIC&quot;) ## BIC ## BICq equivalent for q in (0.0561398731346802, 0.759703855996616) ## Best Model: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4783869 0.07184863 34.494560 8.637858e-55 ## lcavol 0.6197821 0.08823567 7.024168 3.493564e-10 ## lweight 0.2835097 0.07524408 3.767867 2.887126e-04 ## svi 0.2755825 0.08573414 3.214385 1.797619e-03 "],["root-finding-and-optimization.html", "Chapter 13 Root finding and optimization 13.1 Root Finding 13.2 Newton-Raphson Method 13.3 Minimization and Maximization 13.4 optim", " Chapter 13 Root finding and optimization Reference: Computational Methods for Numerical Analysis with R by James P. Howard, II Library used: library(Deriv) # for symbolic differentiation library(tidyverse) Since some of you have some questions about optim(). I will briefly introduce some basic algorithms for root finding and optimization. A root of a function means the \\(x\\) such that \\(f(x) = 0\\). Some common questions: what is the use of par in optim() why you get different minimizers with different initial values 13.1 Root Finding 13.1.1 Bisection Method Game: Before we introduce the bisection method, consider a game where Player \\(A\\) picks an integer from \\(1\\) to \\(100\\), and Player \\(B\\) has to guess this number. Player \\(A\\) will tell Player \\(B\\) if the guess is too low or too high. For example, Player \\(A\\) picks \\(10\\). Player \\(B\\) guesses \\(40\\). Player \\(A\\) tells Player \\(B\\) the number is too high. Player \\(B\\) will then pick a number from \\(1\\) to \\(40\\), say, \\(28\\). Play \\(A\\) tells Player \\(B\\) the number is also too high. Eventually, Player \\(B\\) can guess the number correctly. You may realize that the best approach is to halve the range with each turn. Idea of Bisection Method: Recall that for any continuous function \\(f\\), if it has values of opposite sign in an interval, then it has a root in the interval. For example, if \\(f(a) &lt; 0\\) and \\(f(b) &gt; 0\\), then there exists \\(c \\in (a, b)\\) such that \\(f(c) = 0\\). Now, suppose that \\(f(a) f(b) &lt; 0\\) (so that \\(f\\) has values of opposite sign at \\(a\\) and \\(b\\)). Let \\(\\varepsilon\\) denote a tolerance level. Algorithm: Let \\(c = \\frac{a + b}{2}\\). If \\(f(c) = 0\\), stop and return \\(c\\). If \\(\\text{sign}(f(a)) \\neq \\text{sign}(f(c))\\); set \\(b = c\\); else set \\(a = c\\). Repeat Step 1 to 3 until \\(|b-a| &lt; \\varepsilon\\). Implementation in R: This is a good example to see when a while loop is useful. bisection &lt;- function(f, a, b, tol = 1e-5, max_iter = 100) { iter &lt;- 0 # Step 4 while (abs(b - a) &gt; tol) { # Step 1 c &lt;- (a + b) / 2 # Step 2: if f(c) = 0, stop and return c if (f(c) == 0) { return(c) } iter &lt;- iter + 1 if (iter &gt; max_iter) { warning(&quot;Maximum number of iterations reached&quot;) return(c) } # Step 3 if (f(a) * f(c) &lt; 0) { b &lt;- c } else { a &lt;- c } # print(round(c(c, f(c)), digits = 3)) } return((a + b) / 2) } Remark: 1e-3 = 0.001. We set the default value of \\(\\varepsilon\\) to \\(0.001\\) and the maximum number of iterations to \\(100\\). Example f &lt;- function(x) { x^2 - 1 } Without providing the values of tol and max_iter, the function will use tol = 1e-3 and max_iter = 100. bisection(f, 0.5, 1.25) ## [1] 1.000001 To change tol: bisection(f, 0.5, 1.25, tol = 0.1) ## [1] 1.015625 bisection(f, 0.5, 1.25, tol = 0.0000001) ## [1] 1 To change max_iter: bisection(f, 0.5, 1.25, tol = 0.000000000001, max_iter = 10) ## Warning in bisection(f, 0.5, 1.25, tol = 1e-12, max_iter = 10): Maximum number of iterations reached ## [1] 0.9998779 13.2 Newton-Raphson Method Newton-Raphson method can be used to find a root of a function. Idea of Netwon-Raphson Method (or Netwons Method): Given an initial estimate of the root \\(x_0\\), approximate your function by its tangent line at \\(x_0\\) and find the root (you can find the root of a line easily). To find the root, we equate the slope at \\(x_0\\) found by \\(f&#39;(x_0)\\) and using the two points \\((x_1, 0)\\) and \\((x_0, f(x_0))\\), where \\(x_1\\) denotes the root of the tangent line at \\(x_0\\). That is, \\[\\begin{equation*} \\frac{f(x_0) - 0}{x_1 - x_0} = f&#39;(x_0). \\end{equation*}\\] Rearranging the terms give \\[\\begin{equation*} x_1 = x_0 - \\frac{f(x_0)}{f&#39;(x_0)}. \\end{equation*}\\] Iterate the above procedure until convergence. Here is an gif animination from wikipedia: https://en.wikipedia.org/wiki/Newton%27s_method#/media/File:NewtonIteration_Ani.gif Algorithm of Netwon-Raphson Method: Given an initial estimate of the root \\(x_0\\), set \\[\\begin{equation*} x_1 = x_0 - \\frac{f(x_0)}{f&#39;(x_0)}. \\end{equation*}\\] Iterate the following equation until convergence \\[\\begin{equation*} x_n = x_{n-1} - \\frac{f(x_{n-1})}{f&#39;(x_{n-1})}. \\end{equation*}\\] Implementation in R: f is the function you want to find the root fp is the first derivative of f x0 is the initial value. Since we cannot iterate indefinitely, we have to set a tolerance (tol) and a maximum number of iteration (max_iter). NR &lt;- function(f, fp, x0, tol = 1e-3, max_iter = 100) { iter &lt;- 0 old_x &lt;- x0 x &lt;- old_x + 10 * tol # any number such that abs(x - old_x) &gt; tol while(abs(x - old_x) &gt; tol) { iter &lt;- iter + 1 if (iter &gt; max_iter) { print(&quot;Maximum number of iterations reached&quot;) return(x) } old_x &lt;- x x &lt;- x - f(x) / fp(x) } return(x) } Symbolic differentiation To perform symbolic differentiation in R (instead of numerical differentiation), we can use Deriv() in the package Deriv. f &lt;- function(x) { x^2 - 2 * x + 1 } # Symbolic differentiation fp &lt;- Deriv(f) fp # we know it is 2x - 2 ## function (x) ## 2 * x - 2 Apply our NR function Lets plot the function first. Using our function: NR(f, fp, 1.25, tol = 1e-3) ## [1] 1.000508 13.3 Minimization and Maximization 13.3.1 Newton-Raphson Method Recall that at local extrema, \\(f&#39;(x) = 0\\), therefore we can use the Netwon-Raphson method for minimization and maximization. Algorithm of Netwon-Raphson Method: Given an initial estimate of the root \\(x_0\\), set \\[\\begin{equation*} x_1 = x_0 - \\frac{f&#39;(x_0)}{f&#39;&#39;(x_0)}. \\end{equation*}\\] Iterate the following equation until convergence \\[\\begin{equation*} x_n = x_{n-1} - \\frac{f&#39;(x_{n-1})}{f&#39;&#39;(x_{n-1})}. \\end{equation*}\\] 13.3.2 Gradient Descent Gradient Descent Iterative method for finding a local minimum Requires an initial value and a step size \\(h\\) Idea of Gradient Descent: The gradient descent method uses the derivative at \\(x\\) and takes a step down, of size \\(h\\), in the direction of the slope. The process is repeated using the new point as \\(x\\). As the function slides down a slope, the derivative will start shrinking resulting in smaller changes in \\(x\\). As the change in \\(x\\) decreases below the tolerance value, we have reached a local minimum. Algorithm of Gradient Descent: Set an initial point \\(x_0\\) and a step size \\(h &gt; 0\\) Iterate until convergence: \\[\\begin{equation*} x_{n+1} = x_{n} - h f&#39;(x_{n}). \\end{equation*}\\] Implementation in R: grad_des &lt;- function(fp, x0, h = 1e-3, tol = 1e-4, max_iter = 1000) { iter &lt;- 0 old_x &lt;- x0 x &lt;- x0 + 2 * tol while(abs(x - old_x) &gt; tol) { iter &lt;- iter + 1 if (iter &gt; max_iter) { stop(&quot;Maximum number of iterations reached&quot;) } old_x &lt;- x x &lt;- x - h * fp(x) } return(x) } Example: Find the local minima of \\(f(x) = \\frac{1}{4} x^4 + x^3 - x - 1\\). Plot: Gradient descent: f &lt;- function(x) {1/4 * x^4 + x^3 - x - 1} fp &lt;- Deriv(f) grad_des(fp, x0 = -2, h = 0.01) ## [1] -2.878224 grad_des(fp, x0 = 2, h = 0.01) ## [1] 0.5344022 Displaying error message: grad_des(fp, x0 = -2, h = 0.01, max_iter = 2) ## Error in grad_des(fp, x0 = -2, h = 0.01, max_iter = 2): Maximum number of iterations reached Remark: to find a local maximum, use gradient ascent. Algorithm: Set an initial point \\(x_0\\) and a step size \\(h &gt; 0\\) Iterate until convergence: \\[\\begin{equation*} x_{n+1} = x_{n} + h f&#39;(x_{n}). \\end{equation*}\\] Implementation: grad_asc &lt;- function(fp, x0, h = 1e-3, tol = 1e-4, max_iter = 1000) { grad_des(fp, x0, -h, tol, max_iter) } Example: grad_asc(fp, x0 = 0, h = 0.01) ## [1] -0.6490877 13.4 optim Going back to optim(), the par plays the same role as the x_0 in the Netwon-Raphson method and the gradient descent method. Since we have some tolerance level and the iteration will stop when the tolerance level is reached, if you start at different initial values, you may get slightly different results I used some random numbers for the initial values in optim() because we do not know a good estimate of the minimizer. In general, you will perform the optimization with several set of different initial values and see which one results in a smaller function value. Sometimes, the algorithm will get stuck in a local minimum and using several set of random initial values can alleviate this problem. "],["logistic-regression-model.html", "Chapter 14 Logistic Regression Model", " Chapter 14 Logistic Regression Model library(tidyverse) library(mlbench) # for some datasets We have already seen logistic regression model in Chapter 9. Logistic regression is one of the most common methods for classification. Suppose we observe \\(\\{(x_i, y_i):i=1,\\ldots,n\\}\\), where \\(y_i\\) is a binary variable and \\(x_i\\) is a vector of covariates (including the intercept \\(1\\)). In logistic regression we assume that \\[\\begin{equation*} P(Y_i = 1|x_i, \\beta) = \\frac{e^{x^T_i \\beta}}{1+e^{x^T_i \\beta}} = \\frac{ e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} +\\ldots +\\beta_p x_{ip}}}. \\end{equation*}\\] Since \\(Y_i\\) takes only two values, \\[\\begin{equation*} P(Y_i = 0|x_i, \\beta) = \\frac{1}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] We can use one single formula for \\(y = 0, 1\\): \\[\\begin{equation*} P(Y_i = y|x_i, \\beta) = \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The likelihood function (conditional on x) is \\[\\begin{equation*} L(\\beta|y_1,\\ldots,y_n, x_1,\\ldots,x_n) = \\prod^n_{i=1} P(Y_i = y_i|x_i, \\beta) = \\prod^n_{i=1} \\frac{e^{(x^T_i \\beta)y_i}}{1+e^{x^T_i \\beta}}. \\end{equation*}\\] The MLE of \\(\\beta\\) is obtained by maximizing \\(L(\\beta|y,x)\\) with respect to \\(\\beta\\). We usually maximize the natural logarithm of the likelihood function instead of the likelihood function, which is easier. The log likelihood function is \\[\\begin{equation*} \\sum^n_{i=1} \\bigg( (x^T_i \\beta) y_i - \\log(1+e^{x^T_i \\beta}) \\bigg), \\end{equation*}\\] which can be maximized numerically in computer. Odds In statistics, odds of an event are defined as the ratio of the probability that the event will occur to the probability that the event will not occur. For example, if the event of interest is getting a \\(6\\) when you roll a die. The odds are \\((1/6):(5/6) = 1:5\\). In the logistic regression model, the odds of \\(Y=1\\) given \\(x\\) are \\[\\begin{equation*} \\frac{P(Y=1|x)}{P(Y=0|x)} = e^{\\beta^T x}. \\end{equation*}\\] Interpretation of \\(\\beta_j\\): changing the value of \\(x_j\\) by one unit, while keeping other predictors fixed, multiplies the odds by \\(e^{\\beta_j}\\). Log odds The log odds is the log of the odds. In the logistic regression model, the log odds of \\(Y=1\\) given \\(x\\) are \\[\\begin{equation*} \\log \\frac{P(Y=1|x)}{P(Y=0|x)} = \\beta^T x. \\end{equation*}\\] Interpretation of \\(\\beta_j\\): changing the value of \\(x_j\\) by one unit, while keeping other predictors fixed, changes the log odds by \\(\\beta_j\\). logit function The logit function, \\(\\text{logit}:(0, 1) \\rightarrow \\mathbb{R}\\), is defined as \\[\\begin{equation*} \\text{logit}(p) = \\log \\frac{p}{1-p}. \\end{equation*}\\] logistic function The standard logistic function is defined as \\[\\begin{equation*} f(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}. \\end{equation*}\\] It is the inverse of the logit function. Example The package mlbench contains some artificial and real-world machine learning benchmark problems (datasets). See https://cran.r-project.org/web/packages/mlbench/mlbench.pdf We will use the dataset PimaIndiansDiabetes2 from mlbench. Details: pregnant: Number of times pregnant glucose: Plasma glucose concentration (glucose tolerance test) pressure: Diastolic blood pressure (mm Hg) triceps: Triceps skin fold thickness (mm) insulin: 2-Hour serum insulin (mu U/ml) mass: Body mass index (weight in kg/(height in m)^2) pedigree: Diabetes pedigree function age: Age (years) diabetes: Class variable (test for diabetes) data(PimaIndiansDiabetes2) PimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2) # Split the data into training and test set set.seed(1) random_index &lt;- sample(nrow(PimaIndiansDiabetes2), size = nrow(PimaIndiansDiabetes2) * 0.7) train_data &lt;- PimaIndiansDiabetes2[random_index, ] test_data &lt;- PimaIndiansDiabetes2[-random_index, ] Simple model: Fitting a logistic regression model is similar to fitting a linear regression model. In addition to the formula and the data, we use glm() and specify family = binomial. predict(fit_simple, test_data, type = \"response\"): give us the estimated probability ifelse(prob &gt; 0.5, \"pos\", \"neg\"): classify the case as pos when the probability is greater than \\(0.5\\) # Estimation fit_simple &lt;- glm(diabetes ~ glucose, data = train_data, family = binomial) # Summary summary(fit_simple) ## ## Call: ## glm(formula = diabetes ~ glucose, family = binomial, data = train_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2089 -0.7182 -0.4459 0.7004 2.4145 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.419871 0.774032 -8.294 &lt; 2e-16 *** ## glucose 0.044509 0.005775 7.707 1.29e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 348.35 on 273 degrees of freedom ## Residual deviance: 262.14 on 272 degrees of freedom ## AIC: 266.14 ## ## Number of Fisher Scoring iterations: 4 # Prediction prob &lt;- predict(fit_simple, test_data, type = &quot;response&quot;) predicted_class &lt;- ifelse(prob &gt; 0.5, &quot;pos&quot;, &quot;neg&quot;) # Confusion Matrix table(predicted_class, test_data$diabetes) ## ## predicted_class neg pos ## neg 71 23 ## pos 8 16 # Accuracy mean(predicted_class == test_data$diabetes) ## [1] 0.7372881 For this simple model, the accuracy is \\(73.7\\%\\). Now, we will use all the covariates. # Estimation fit &lt;- glm(diabetes ~ ., data = train_data, family = binomial) # Summary summary(fit) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial, data = train_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6139 -0.6178 -0.3523 0.5804 2.1311 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.130e+01 1.558e+00 -7.250 4.18e-13 *** ## pregnant 2.511e-02 6.582e-02 0.381 0.7028 ## glucose 4.038e-02 7.028e-03 5.745 9.17e-09 *** ## pressure 1.368e-02 1.562e-02 0.876 0.3811 ## triceps 1.381e-02 2.151e-02 0.642 0.5209 ## insulin -9.559e-04 1.808e-03 -0.529 0.5970 ## mass 7.334e-02 3.586e-02 2.045 0.0408 * ## pedigree 8.706e-01 5.093e-01 1.709 0.0874 . ## age 3.612e-02 2.323e-02 1.555 0.1200 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 348.35 on 273 degrees of freedom ## Residual deviance: 233.54 on 265 degrees of freedom ## AIC: 251.54 ## ## Number of Fisher Scoring iterations: 5 # Prediction prob &lt;- predict(fit, test_data, type = &quot;response&quot;) predicted_class &lt;- ifelse(prob &gt; 0.5, &quot;pos&quot;, &quot;neg&quot;) # Confusion Matrix table(predicted_class, test_data$diabetes) ## ## predicted_class neg pos ## neg 69 15 ## pos 10 24 # Accuracy mean(predicted_class == test_data$diabetes) ## [1] 0.7881356 The accuracy is \\(78.8\\%\\). "],["k-means-clustering.html", "Chapter 15 k-means Clustering 15.1 Introduction 15.2 Applications", " Chapter 15 k-means Clustering Reference: Ch9 of Machine Learning with R, Ch6 of R Graphics Cookbook Packages used: library(tidyverse) # for read_csv library(factoextra) # visualize clusering results library(jpeg) # readJPEG reads an image from a JPEG file/content into a raster array library(ggpubr) 15.1 Introduction Clustering Clustering is an unsupervised learning task that divides data into clusters, or groups of similar items while data points in different clusters are very different. It is an unsupervised learning method as we do not have labels of the data. We do not have the ground truth to compare the results. Clustering will only tell you which groups of data are similar. One may get some meaningful interpretation of the groups by studying the members in each group, e.g., by calculating some summary statistics and making use of some visualization tools. \\(k\\)-means clustering \\(k\\)-means clustering is one of the most popular clustering methods. It is intended for situations in which all variables are of the numeric type. In \\(k\\)-means clustering, all the examples are assigned to one of the \\(k\\) clusters, where \\(k\\) is a positive integer that has been specified before performing the clustering. The goal is to assign similar data to the same cluster. Some Applications Cluster analysis: Interesting groups may be discovered, such as the groups of motor insurance policy holders with a high average claim cost, or the groups of clients in a banking database having a heavy investment in real estate. In marketing segmentation, we segment customers into groups with similar demographics or buying patterns for targeted marketing campaigns Vector quantization: \\(k\\)-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors \\(k\\). The \\(k\\)-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. See https://en.wikipedia.org/wiki/Image_segmentation Notation and Setting We have \\(n\\) data points \\(x_i = (x_{i1},\\ldots,x_{ip})&#39; \\in \\mathbb{R}^p\\), \\(i=1,\\ldots,n\\). Each observation is uniquely labeled by an integer \\(i \\in \\{1,\\ldots,n\\}\\). We use the notation \\(C_i\\) to denote which cluster the \\(i\\)th observation is assigned to. For example, \\(C_1 = 2\\) means the \\(1\\)st observation is assigned to the \\(2\\)nd cluster, and \\(C_2 = 3\\) means the \\(2\\)nd observation is assigned to the \\(3\\)rd cluster. We use \\(i\\) to index data \\(j\\) to index cluster \\(l\\) to index feature Dissimilarity measure In clustering, one have to define a dissimilarity measure \\(d\\). In \\(k\\)-means clustering, the squared Euclidean distance is used. Given two data points \\(x_i = (x_{i1},\\ldots,x_{ip})\\) and \\(x_{i&#39;} = (x_{i&#39;1},\\ldots,x_{i&#39;p})\\), the squared Euclidean distance is given by \\[\\begin{equation*} d(x_i, x_{i&#39;}) = \\sum^p_{l=1} (x_{il} - x_{i&#39;l})^2 = ||x_i - x_{i&#39;}||^2. \\end{equation*}\\] k-means clustering algorithm (Lloyds algorithm) Specify the number of clusters \\(k\\) Randomly select \\(k\\) data points as the centroids, denoted by \\(\\mu_1,\\ldots,\\mu_k\\). For each \\(i=1,\\ldots,n\\), set \\[ C_i = \\text{argmin}_j ||x_i - \\mu_j||^2. \\] In words, assign the closest cluster to the \\(i\\)th observation. For each \\(j=1,\\ldots,k\\), \\(l=1,\\ldots,p\\), set \\[ \\mu_{jl} = \\frac{\\sum^n_{i=1}I(C_i =j) x_{il}}{\\sum^n_{i=1}I(C_i =j)}. \\] In words, \\(\\mu_j\\) is simply the mean of all the points assigned to \\(j\\)th cluster. Note that \\(\\mu_j = (\\mu_{j1},\\ldots,\\mu_{jp})\\). Repeat Steps 3 - 4 until the assignments do not change. Remarks \\(\\text{argmin}\\) is the argument of the minimum. \\(\\text{argmin}_j ||x_i - \\mu_j||^2\\) means the value of \\(j\\) such that \\(||x_i - \\mu_j||^2\\) is minimum. \\(I(C_i = j)\\) equals \\(1\\) if \\(C_i = j\\) and equals \\(0\\) otherwise. The \\(I\\) is the indicator function. \\(k\\)-means is sensitive to the number of clusters. See the elbow method. \\(k\\)-means is sensitive to the randomly-chosen cluster centers. Different initial centers may result in different clustering results. As a result, we should use multiple set of initial cluster centers and choose the best result (smallest within-group sum of squared errors, see the end of this chapter). Scale your data before applying \\(k\\)-means clustering. For categorical data: one possible solution is to use \\(k\\)-modes. The \\(k\\)-modes algorithm uses a matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimize the clustering cost functions For mixed data (numeric + categorical features): can use \\(k\\)-prototypes. It integrates the \\(k\\)-means and \\(k\\)-modes algorithms using a combined dissimilarity measure. R package: clustMixType See also Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values by Z. Huang (1998). 15.2 Applications 15.2.1 Cluster Analysis The Mall_Customers.csv (in onQ) contains the gender, age, annual income and spending score of some customers. mall &lt;- read_csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/Mall_Customers.csv&quot;) mall ## # A tibble: 200 x 5 ## CustomerID Gender Age `Annual Income (k$)` `Spending Score (1-100)` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Male 19 15 39 ## 2 2 Male 21 15 81 ## 3 3 Female 20 16 6 ## 4 4 Female 23 16 77 ## 5 5 Female 31 17 40 ## 6 6 Female 22 17 76 ## 7 7 Female 35 18 6 ## 8 8 Female 23 18 94 ## 9 9 Male 64 19 3 ## 10 10 Female 30 19 72 ## # ... with 190 more rows We will use the kmeans() function in R (contained in base R) to perform \\(k\\)-means clustering. x: your dataframe/ matrix centers: the number of clusters nstart: how many random sets should be chosen (the best result will be reported) Lets perform a \\(k\\)-means clustering on the customers using Annual Income (k$) and Spending Score (1-100) with \\(k=3\\). mall_kmeans &lt;- kmeans(x = scale(mall[, 4:5]), centers = 3, nstart = 25) # To view the cluster mall_kmeans$cluster ## [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [49] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [97] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ## [145] 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ## [193] 2 1 2 1 2 1 2 1 # To view the centers mall_kmeans$centers ## Annual Income (k$) Spending Score (1-100) ## 1 0.9891010 1.23640011 ## 2 1.0066735 -1.22246770 ## 3 -0.6246222 -0.01435636 Visualize the clusters: library(factoextra) fviz_cluster(mall_kmeans, data = mall[, 4:5], geom = &quot;point&quot;) By looking at the plot, we see the 3 clusters are high annual income + low spending score high annual income + high spending score low annual income Lets try \\(k=5\\): mall_kmeans &lt;- kmeans(x = scale(mall[, 4:5]), centers = 5, nstart = 25) Visualizing the clusters: fviz_cluster(mall_kmeans, data = mall[, 4:5], geom = &quot;point&quot;) The \\(5\\) clusters are high annual income + low spending score high annual income + high spending score medium annual income + medium spending score low annual income + high spending score low annual income + low spending score Now, lets use one more variable for clustering: mall_kmeans &lt;- kmeans(x = scale(mall[, 3:5]), centers = 5, nstart = 25) To visualize more than 2 variables, we can use a dimension reduction technique called principal component analysis. The function fviz_cluster will automatically perform that and give you a \\(2D\\)-plot using the first two principal comonents. fviz_cluster(mall_kmeans, data = mall[, 3:5], geom = &quot;point&quot;) To understand more about the clusters, we should take a look at some plots and some summary statistics. An example is to use violin plots. Violin Plots A violin plot is a kernel density estimate, mirrored so that it forms a symmetrical shape. It is a helpful tool to compare multiple data distributions when we put several plots side by side. To provide additional information, we can also have box plots overlaid, with a white dot at the median. We first add the cluster information to our dataset. mall_cluster &lt;- mutate(mall, Cluster = factor(mall_kmeans$cluster)) Create violin plots outlier.colour = NA: do not display outliers mall_cluster %&gt;% ggplot(aes(x = Cluster, y = `Annual Income (k$)`)) + geom_violin(trim = FALSE) + geom_boxplot(width = .1, fill = &quot;black&quot;, outlier.colour = NA) + stat_summary(fun = median, geom = &quot;point&quot;, fill = &quot;white&quot;, shape = 21, size = 2.5) The other two variables: Determine the optimal \\(k\\) The elbow method is a heuristic method to determine the number of clusters. We plot the total within-group sum of squared errors against the number of clusters. We pick the elbow of the curve as the number of clusters to use. The elbow (or knee) of a curve is a point where the curve visibly bends. WSS &lt;- rep(0, 10) for (k in 1:10) { # extract the total within-group sum of squared errors WSS[k] = kmeans(x = scale(mall[, 3:5]), centers = k, nstart = 25)$tot.withinss } ggplot(mapping = aes(x = 1:10, y = WSS)) + geom_line() + geom_point() + geom_vline(xintercept = 4) + scale_x_discrete(name = &quot;k&quot;, limits = factor(1:10)) + labs(title = &quot;Elbow Method&quot;) It seems to me that the curve bends at \\(k=4\\). 15.2.2 Image Segementation Image segementation using \\(k\\)-means clustering: Each pixel is a data point. Group the pixels into \\(k\\) different clusters We will use the readJPEG() function from the package jpeg to reads an image from a jpeg file into a raster array. library(jpeg) # read the image into a raster array image &lt;- readJPEG(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/image/flower_s.jpg&quot;) # 3-way array (matrix = 2-way array) (image_dim &lt;- dim(image)) ## [1] 200 300 3 # Assign RGB channels to each pixel image_RGB &lt;- tibble( x = rep(1:image_dim[2], each = image_dim[1]), y = rep(image_dim[1]:1, image_dim[2]), R = as.vector(image[, , 1]), G = as.vector(image[, , 2]), B = as.vector(image[, , 3]) ) # use view(image_RGB) to view the resulting tibble Original Image Apply \\(k\\)-means clustering: Code for creating the above images: cluster_plot &lt;- list() i &lt;- 0 for (k in c(2, 3, 5, 8)) { i &lt;- i + 1 # perform k-means image_kmeans &lt;- kmeans(image_RGB[, c(&quot;R&quot;, &quot;G&quot;, &quot;B&quot;)], centers = k, nstart = 25) # for each pixel, use the colour of the center of the corresponding cluster cluster_color &lt;- rgb(image_kmeans$centers[image_kmeans$cluster, ]) cluster_plot[[i]] &lt;- ggplot(data = image_RGB, aes(x = x, y = y)) + geom_point(colour = cluster_color) + labs(title = paste(&quot;k = &quot;, k)) + xlab(&quot;&quot;) + ylab(&quot;&quot;) } # save the plots to your computer png(paste0(&quot;image/flower_cluster.png&quot;), width = 900, height = 600) ggarrange(cluster_plot[[1]], cluster_plot[[2]], cluster_plot[[3]], cluster_plot[[4]]) dev.off() Another example Original Image: Apply \\(k\\)-means clustering: Explanation of the algorithm The \\(k\\)-means algorithm searches for a partition of the data into \\(k\\) clusters . It tries to minimize the within-group sum of squared errors (WSS): \\[\\begin{equation*} WSS(C, \\mu) = \\sum^k_{j=1} \\sum^n_{i=1} I(C_i = j) ||x_i - \\mu_j||^2, \\end{equation*}\\] where \\(C = (C_1,\\ldots,C_n)\\) and \\(\\mu = \\{\\mu_1,\\ldots,\\mu_k\\}\\). To minimize WGSS, the algorithm iteratively solves two problems: Problem 1. Fix \\(\\mu\\) and minimize \\(WGSS(C, \\mu)\\) with respect to \\(C\\) (Step 3 in the algorithm) Problem 2. Fix \\(C\\) and minimize \\(WGSS(C, \\mu)\\) with respect to \\(\\mu\\) (Step 4 in the algorithm) For Problem 1, the solution is \\[\\begin{equation*} C_i = \\text{argmin}_j ||x_i - \\mu_j||^2. \\end{equation*}\\] For Problem 2, the solution is \\[\\begin{equation*} \\mu_{jl} = \\frac{\\sum^n_{i=1}I(C_i = j) x_{il}}{\\sum^n_{i=1} I(C_i=j)}. \\end{equation*}\\] "],["neural-networks.html", "Chapter 16 Neural Networks 16.1 Introduction 16.2 Regression: Predict the Strength of Concrete 16.3 Classification", " Chapter 16 Neural Networks Optional Reading: Chapter 7 in Machine Learning with R by Brett Lantz Packages used: library(tidyverse) library(neuralnet) library(ggpubr) 16.1 Introduction An artificial neural network (ANN) (or simply neural network) models the relationship between a set of input signals and output signals using a model derived from our understanding of how a biological brain responds to stimuli from sensory inputs. The human brain is made up of about \\(85\\) billion neurons. A neuron is an electrically excitable cell that communicates with other cells via specialized connections called synapses. ANNs contain far fewer artificial neurons (units). A neural network is simply a nested nonlinear model. Neural network can be used for regression and classification. For regression, we can have more than one output unit (response). For classification, we have \\(K\\) output units. Each unit models the probability of class \\(k\\) for \\(k=1,\\ldots,K\\). We can code our class label as \\(K\\) \\(0-1\\) variables. For example, if you have \\(3\\) classes a, b and c. Then, we \\((Y_1, Y_2, Y_3) = (1, 0, 0)\\) if the class label is a, \\((Y_1, Y_2, Y_3) = (0, 1, 0)\\) if the class label is b, \\((Y_1, Y_2, Y_3) = (0, 0, 1)\\) if the class label is c. The following figure shows a single hidden layer feedforward neural network. It is also an example of a 2-layer neural network (one hidden layer + one output layer, the input layer is not counted as a layer). Suppose we have \\(p\\) features \\(X_1,\\ldots,X_p\\), one hidden layer with \\(J\\) hidden units and \\(K\\) outputs \\(Y_1,\\ldots,Y_K\\). Input layer: the units are the features \\(X_1, X_2,\\ldots, X_p\\). Hidden layer: for \\(j=1,\\ldots,J\\), \\[\\begin{equation*} h_j(X) = f \\bigg(v_{j0} + \\sum^p_{i=1} v_{ji} X_i\\bigg). \\end{equation*}\\] The units in the hidden layer are called hidden units because the values are not directly observed. Output layer: for \\(k=1,\\ldots,K\\), \\[\\begin{equation*} o_k(X) = g\\bigg(w_{k0} + \\sum^J_{j=1} w_{kj} h_j(X) \\bigg). \\end{equation*}\\] The \\(o_K(X)\\)s are the predictions of \\(Y_k\\)s. \\(v_{ji}\\)s and \\(w_{kj}\\)s are the unknown weights to be estimated. \\(f\\) and \\(g\\) are the activation functions. Each unit computes its value based on linear combination of values of units that point into it, and an activation function. Deep neural network = neural network with multiple hidden layers Deep learning = the practice of training deep neural networks \\(3\\) characteristics of neural networks: Activation function: transforms a neurons net input signal into a single output signal to be broadcasted further in the network. Activation function allows the neural network to approximate nonlinear function. Without nonlinearities, the neural network would be linear, no matter how many layers it has. Network topology (or architecture): describes the number of neurons in the model as well as the number of layers and manner in which they are connected. Training algorithm: specifies how connection weights are set in order to inhibit or excite neurons in proportion to the input signal Activation functions For regression, we typically choose the identity function for the output function. That is, \\(g(x) = x\\). For classification, we can use the logistic function or the softmax function for the output function which returns a value between \\(0\\) and \\(1\\). Different activation function may fit certain types of data more appropriately. Some common activation functions: Logistic sigmoid \\(f(x) = \\frac{1}{1+e^{-x}}\\). Range = \\((0, 1)\\) tanh \\(f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\). Range = \\((-1, 1)\\). Rectified Linear Unit (ReLU) \\(f(x) = x I(x \\geq 0)\\). Range = \\([0, \\infty)\\). Network typology \\(3\\) key characteristics: The number of layers Single-layer network Multilayer network: one or more hidden layers Whether information in the network is allowed to travel backward Feedforward network: input signal is fed in one direction from the input layer to the output layer Recurrent network (or feedback network): allows signals to travel backward using loops The number of nodes within each layer of the network Number of input nodes = number of features Number of hidden nodes = user decide prior to training the model Number of output nodes = number of outcomes to be modeled or the number of class levels in the outcome In general, using a larger number of neurons will fit the training data better, but this runs a risk of overfitting (generalize poorly to future data). It can also take more time to train the model. Training the model For regression, the loss function is the sum-of-squared errors: \\[\\begin{equation*} L(\\theta) = \\frac{1}{2} \\sum^K_{k=1} \\sum^N_{i=1} (y_{ik} - o_k(x_i))^2. \\end{equation*}\\] Here \\(x_i\\) is a vector of features for the \\(i\\)th example and \\((y_{i1},\\ldots,y_{iK})\\) is the vector of outputs. \\(o_k(x_i)\\) is the prediction of \\(y_{ik}\\). \\(\\theta\\) is the collection of all the weights. For classification, we use either sum-of-squared errors or cross-entropy. The cross-entropy is defined as \\[\\begin{equation*} L(\\theta) = - \\sum^N_{i=1} \\sum^K_{k=1} y_{ik} \\log o_k(x_i). \\end{equation*}\\] The generic approach to minimizing \\(L(\\theta)\\) is by gradient descent, called back-propagation in this setting using the chain rule for differentiation. We will not go into the detail of this method. 16.2 Regression: Predict the Strength of Concrete In engineering, it is crucial to have accurate estimates of the performance of building materials. These estimates are required in order to develop safety guidelines governing the materials used in the construction of buildings, bridges, and roadways. Estimating the strength of concrete is a challenge of particular interest. Although it is used in nearly every construction project, concrete performance varies greatly due to a wide variety of ingredients that interact in complex ways. As a result, it is difficult to accurately predict concrete strength of the final product. A model that could reliably predict concrete strength given a listing of the composition of the input materials could result in safer construction practices. 16.2.1 Data We will use the data donated to the UCI Machine Learning Repository by I-Cheng Yeh. https://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength Description of the data: Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate. The actual concrete compressive strength (MPa) for a given mixture under a specific age (days) was determined from laboratory. Data is in raw form (not scaled). Import the Data concrete &lt;- read_csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/concrete.csv&quot;) concrete ## # A tibble: 1,030 x 9 ## cement slag ash water superplastic coarseagg fineagg age strength ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 540 0 0 162 2.5 1040 676 28 80.0 ## 2 540 0 0 162 2.5 1055 676 28 61.9 ## 3 332. 142. 0 228 0 932 594 270 40.3 ## 4 332. 142. 0 228 0 932 594 365 41.0 ## 5 199. 132. 0 192 0 978. 826. 360 44.3 ## 6 266 114 0 228 0 932 670 90 47.0 ## 7 380 95 0 228 0 932 594 365 43.7 ## 8 380 95 0 228 0 932 594 28 36.4 ## 9 266 114 0 228 0 932 670 28 45.8 ## 10 475 0 0 228 0 932 594 28 39.3 ## # ... with 1,020 more rows We have \\(8\\) features and \\(1\\) outcome. Neural networks work best when the input data are scaled to a narrow range around zero. We discussed feature scaling when we discuss \\(k\\)-NN. Here, we will normalize the data. Before we normalize the data, we will separate the our data into a training dataset and a testing dataset. We will use \\(75\\%\\) of the data to form our training dataset and the rest to form our testing dataset. set.seed(362) random_index &lt;- sample(nrow(concrete), size = nrow(concrete) * 0.75) concrete_train &lt;- concrete[random_index, ] concrete_test &lt;- concrete[-random_index, ] Normalize the data Define a function to normalize our training data and testing data. normalize &lt;- function(train, test) { train_n &lt;- train test_n &lt;- test train_min &lt;- apply(train, 2, min) train_max &lt;- apply(train, 2, max) for (i in 1:ncol(train)) { train_n[, i] &lt;- (train[, i] - train_min[i]) / (train_max[i] - train_min[i]) # use the min and max from training data to normalize the testing data test_n[, i] &lt;- (test_n[, i] - train_min[i]) / (train_max[i] - train_min[i]) } return(list(train = train_n, test = test_n)) } Apply the normalize function: train_test_n &lt;- normalize(concrete_train, concrete_test) concrete_train_n &lt;- train_test_n$train concrete_test_n &lt;- train_test_n$test 16.2.2 Training a model We will train a multilayer feedforward neural network using the function neuralnet() in the package neuralnet. library(neuralnet) Usage of the function: neuralnet(formula, data, hidden = 1, act.fct = &quot;logistic&quot;) formula: same as the formula used in lm(), e.g., strength ~ . and strange ~ cement + slag. hidden: the number of neurons in the hidden layer (by default, \\(1\\) hidden layer with \\(1\\) neuron). To use \\(2\\) hidden layers with \\(3\\) and \\(4\\) neurons, set hidden = c(3, 4). act.fct: the activation function. The default is to use the logistic function. # The default is to use one hidden layer with one unit # you get slightly different results each time because of the random initial values # set the seed to reproduce the result set.seed(1) concrete_ANN &lt;- neuralnet(strength ~ ., data = concrete_train_n) Visualize the network (for learning and teaching purpose): plot(concrete_ANN) The numbers on the arrows are the weights. The additional \\(1\\) is the bias term (recall that in linear regression we always include the intercept too). 16.2.3 Understanding the model Now, lets understand the model by computing the prediction manually. We will first define the logistic function. logistic &lt;- function(x) { 1 / (1 + exp(-x)) } Predicted values: fitted_values &lt;- concrete_ANN$net.result[[1]][, 1] fitted_values[1] ## [1] 0.658175 Step-by-step calculation: # Extract the weights v_{ji} weights &lt;- concrete_ANN$weights[[1]][[1]][, 1] # (1, X_1, X_2, ..., X_p) features &lt;- c(1, as.numeric(concrete_train_n[1, 1:8])) # v_{j0} + sum v_{ji} X_i linear_combination &lt;- sum(weights * features) # h_j(X) = f(v_{j0} + sum v_{ji} X_i) (J = 1 as we only have 1 hidden unit) h_X &lt;- logistic(linear_combination) # Extract the weights w_{kj} weights2 &lt;- concrete_ANN$weights[[1]][[2]][, 1] # o_K(X) = g(w_{k0} + sum w_{kj} h_j(X)), with g(x) = x sum(c(1, h_X) * weights2) ## [1] 0.658175 16.2.4 Evaluating the Performance Prediction Use the compute function: # I used neuralnet::compute instead of compute because dplyr also contains a function called compute prediction &lt;- neuralnet::compute(concrete_ANN, concrete_test_n[, 1:8]) predicted_strength &lt;- prediction$net.result[, 1] Scatterplot of predictions versus the true values. plot_predict_1 &lt;- ggplot(mapping = aes(x = concrete_test_n$strength, y = predicted_strength)) + geom_point() + labs(title = &quot;Single Hidden Layer with 1 unit&quot;) plot_predict_1 Since this is a numeric prediction problem rather than a classification problem, we cannot use a confusion matrix to examine model accuracy. Instead, we can use correlation to measure the performance of the prediction. The higher the correlation, the better the prediction is. cor(concrete_test_n$strength, predicted_strength) ## [1] 0.8222051 16.2.5 Improving the Model One hidden layer with \\(5\\) units: concrete_ANN_5 &lt;- neuralnet(strength ~ ., data = concrete_train_n, hidden = 5) prediction_5 &lt;- neuralnet::compute(concrete_ANN_5, concrete_test_n[, 1:8]) predicted_strength_5 &lt;- prediction_5$net.result[, 1] cor(concrete_test_n$strength, predicted_strength_5) ## [1] 0.9092216 Two hidden layers, each with \\(5\\) units: concrete_ANN_55 &lt;- neuralnet(strength ~ ., data = concrete_train_n, hidden = c(5, 5)) prediction_55 &lt;- neuralnet::compute(concrete_ANN_55, concrete_test_n[, 1:8]) predicted_strength_55 &lt;- prediction_55$net.result[, 1] cor(concrete_test_n$strength, predicted_strength_55) ## [1] 0.9274489 plot_predict_55 &lt;- ggplot(mapping = aes(x = concrete_test_n$strength, y = predicted_strength_55)) + geom_point() + labs(title = &quot;Two Hidden Layers, each with 5 units&quot;) ggarrange(plot_predict_1, plot_predict_55) 16.3 Classification We will use the breast cancer dataset to illustrate how to perform classification using neural networks. The following datasets are normalized already with the output being (B, M). Download them in onQ. wbcd_train_n &lt;- read.csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/wisc_bc_train_normalize_01.csv&quot;) wbcd_test_n &lt;- read.csv(&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/wisc_bc_test_normalize_01.csv&quot;) Train the model: Set linear.output = FALSE to use the logistic function to get a predicted number between \\(0\\) and \\(1\\). set.seed(1) fit &lt;- neuralnet(B + M ~ ., data = wbcd_train_n, hidden = c(4, 4), linear.output = FALSE) 3-layer feedforward neural network: Evaluate the performance Training Data: Use max.col to find the column correspond to the maximum number. # Training Data # result head(neuralnet::compute(fit, wbcd_train_n)$net.result) ## [,1] [,2] ## [1,] 5.310829e-14 1.000000e+00 ## [2,] 1.000000e+00 3.221476e-12 ## [3,] 1.000000e+00 2.943722e-12 ## [4,] 4.879723e-15 1.000000e+00 ## [5,] 2.869652e-07 9.999998e-01 ## [6,] 1.000000e+00 1.947128e-12 # assign the class label according to the one with a larger number predict &lt;- max.col(neuralnet::compute(fit, wbcd_train_n)$net.result) # first column is for B, second column is for M predict &lt;- recode(predict, &quot;1&quot; = &quot;B&quot;, &quot;2&quot; = &quot;M&quot;) train_diagnosis &lt;- recode(wbcd_train_n$B, &quot;0&quot; = &quot;M&quot;, &quot;1&quot; = &quot;B&quot;) # Confusion Matrix for Training Data table(train_diagnosis, predict) ## predict ## train_diagnosis B M ## B 298 0 ## M 4 167 Testing Data: predict &lt;- max.col(neuralnet::compute(fit, wbcd_test_n)$net.result) predict &lt;- recode(predict, &quot;1&quot; = &quot;B&quot;, &quot;2&quot; = &quot;M&quot;) test_diagnosis &lt;- recode(wbcd_test_n$B, &quot;0&quot; = &quot;M&quot;, &quot;1&quot; = &quot;B&quot;) # Confusion Matrix for Testing Data table(test_diagnosis, predict) ## predict ## test_diagnosis B M ## B 57 2 ## M 1 40 "],["evaluating-model-performance-1.html", "Chapter 17 Evaluating Model Performance 17.1 Accuracy and Confusion Matrix 17.2 Other Measures of Performance 17.3 Estimating future performances", " Chapter 17 Evaluating Model Performance Reference: Chapter 10 in Machine Learning with R by Brett Lantz Packages used in this chapter: library(caret) library(gmodels) library(pROC) library(mlbench) 17.1 Accuracy and Confusion Matrix We have defined Accuracy as \\[\\begin{equation*} \\text{Accuracy} = \\frac{\\text{number of correct predictions}}{\\text{total number of predictions}}. \\end{equation*}\\] However, the class imbalance problem below will show that this measure may not be a good measure of model performance in many cases. Class imbalance problem If you have a model with \\(99\\%\\) accuracy, does that mean the model is useful? Imagine a disease that is found 11 out of every 1000 people. A model that predicts no disease will have an accuracy of \\(989/1000 = 98.9\\%\\), which is just slightly lower than the accuracy of your model. This shows that accuracy alone may not be a particular useful measure of performance in many applications. Class imbalance problem = the trouble associated with data having a large majority of records belonging to a single class Another problem is that accuracy does not tell us whether the model is good at identifying the positive cases or the negative cases. Confusion matrices We have already seen some confusion matrices in previous chapters. A confusion matrix is a table that categorizes predictions according to whether they match the actual value. Correct predictions fall on the diagonal in the confusion matrix Recall the logistic regression example: library(mlbench) data(PimaIndiansDiabetes2) PimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2) # Split the data into training and test set set.seed(1) random_index &lt;- sample(nrow(PimaIndiansDiabetes2), size = nrow(PimaIndiansDiabetes2) * 0.7) train_data &lt;- PimaIndiansDiabetes2[random_index, ] test_data &lt;- PimaIndiansDiabetes2[-random_index, ] fit_simple &lt;- glm(diabetes ~ glucose, data = train_data, family = binomial) # Prediction prob &lt;- predict(fit_simple, test_data, type = &quot;response&quot;) predicted_class &lt;- ifelse(prob &gt; 0.5, &quot;pos&quot;, &quot;neg&quot;) We can use the function CrossTable() from the package gmodels to create a confusion matrix with some additional information. library(gmodels) CrossTable(predicted_class, test_data$diabetes, prop.chisq = FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 118 ## ## ## | test_data$diabetes ## predicted_class | neg | pos | Row Total | ## ----------------|-----------|-----------|-----------| ## neg | 71 | 23 | 94 | ## | 0.755 | 0.245 | 0.797 | ## | 0.899 | 0.590 | | ## | 0.602 | 0.195 | | ## ----------------|-----------|-----------|-----------| ## pos | 8 | 16 | 24 | ## | 0.333 | 0.667 | 0.203 | ## | 0.101 | 0.410 | | ## | 0.068 | 0.136 | | ## ----------------|-----------|-----------|-----------| ## Column Total | 79 | 39 | 118 | ## | 0.669 | 0.331 | | ## ----------------|-----------|-----------|-----------| ## ## The \\(3\\) proportions are N / Row Total (e.g. \\(71/94 = 0.755\\)) N / Col Total (e.g. \\(71/79 = 0.899\\)) N / Table Total (e.g. \\(71/118 = 0.602\\)) In a confusion matrix, predictions fall into one of the four categories: True positive (TP): Correctly classified as the class of interest True negative (TN): Correctly classified as not the class of interest False positive (FP): Incorrectly classified as the class of interest False negative (FN): Incorrectly classified as not the class of interest Positive = the class of interest The notations TP, TN, FP and FN are also used to refer to the number of times the predictions fell into each of these categories. In the logistic regression example, suppose our class of interest is pos. TP = 16 TN = 71 FP = 8 FN = 23 17.2 Other Measures of Performance The package caret contains a function confusionMatrix() to output several useful measures of classification performance. We will talk about some of them in the next section. library(caret) # requires factor inputs confusionMatrix(data = factor(predicted_class), reference = factor(test_data$diabetes), positive = &quot;pos&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 71 23 ## pos 8 16 ## ## Accuracy : 0.7373 ## 95% CI : (0.6483, 0.814) ## No Information Rate : 0.6695 ## P-Value [Acc &gt; NIR] : 0.06908 ## ## Kappa : 0.3423 ## ## Mcnemar&#39;s Test P-Value : 0.01192 ## ## Sensitivity : 0.4103 ## Specificity : 0.8987 ## Pos Pred Value : 0.6667 ## Neg Pred Value : 0.7553 ## Prevalence : 0.3305 ## Detection Rate : 0.1356 ## Detection Prevalence : 0.2034 ## Balanced Accuracy : 0.6545 ## ## &#39;Positive&#39; Class : pos ## 17.2.1 The kappa statistic Idea: we want to adjust accuracy by accounting for the possibility of a correct prediction by chance alone. For example, if the disease is found \\(11\\) out of every \\(1000\\) people, we can predict each case randomly by assigning the positive case with probability \\(0.989\\). In this way, our accuracy will be \\(98.9\\%\\) on average. Definition of the kappa statistic: \\[\\begin{equation*} \\kappa = \\frac{p_a - p_e}{1 - p_e}, \\end{equation*}\\] where \\(p_a\\) is the proportion of actual agreement. That is, \\(p_a\\) is the proportion of all instances where the predicted type and actual type agree. Mathematically, \\[\\begin{equation*} p_a = \\frac{TP + TN}{TP + TN + FP + FN} = \\text{Accuracy}. \\end{equation*}\\] \\(p_e\\) is the expected agreement between the classifier and the true values, under the assumption that they were chosen at random. Let \\(p_{\\text{pos, actual}}\\) denote the proportion of the positive cases in the actual data \\(p_{\\text{neg, actual}}\\) denote the proportion of the negative cases in the actual data \\(p_{\\text{pos, pred}}\\) denote the proportion of the positive cases in the predictions \\(p_{\\text{neg, pred}}\\) denote the proportion of negative cases in the predictions Mathematically, \\[\\begin{equation*} p_e = p_{\\text{pos, actual}} \\times p_{\\text{pos, pred}} + p_{\\text{neg, actual}} \\times p_{\\text{neg, pred}}. \\end{equation*}\\] This is because there are only two ways to get agreement (both are positive and both are negative) and the predictions are chosen at random (so that the predictions and the actual data are independent -&gt; independence means multiplication in probability). In the above example, \\(p_a = \\frac{16 + 71}{118} = 0.7372881\\) \\(p_e = \\frac{79}{118} \\frac{94}{118} + \\frac{39}{118} \\frac{24}{118} = 0.6005458\\). \\(\\kappa = \\frac{p_a - p_e}{1 - p_e} = 0.342\\). Common interpretation of \\(\\kappa\\): less than \\(0.2\\) = poor agreement \\(0.2\\) to \\(0.4\\) = fair agreement \\(0.4\\) to \\(0.8\\) = moderate agreement \\(0.6\\) to \\(0.8\\) = good agreement \\(0.8\\) to \\(1\\) = very good agreement 17.2.2 Sensitivity and specificity Most of you have probably received some spam (junk email) in your email. Many email providers filter the spam automatically. You may also experience that sometimes some legitimate emails (ham) are classified as spam. Finding a useful classifier often involves a balance between predictions that are overly conservative and overly aggressive. (Overly Aggressive) For example, you can classify an email as spam if you have never sent an email to that address. (Overly Conservative) To guarantee that no ham messages will be inadvertently filtered might require us to allow an unacceptable amount of spam to pass through the filter. A pair of performance measures captures this trade-off: sensitivity and specificity. Sensitivity (also called the true positive rate) is defined as \\[\\begin{equation*} \\text{Sensitivity} = \\frac{TP}{TP + FN}. \\end{equation*}\\] It is the proportion of positive examples that are correctly classified. E.g., the proportion of spam filtered. Specificity (also called the true negative rate) is defined as \\[\\begin{equation*} \\text{Specificity} = \\frac{TN}{TN + FP}. \\end{equation*}\\] It is the proportion of negative examples that are correctly classified. E.g., the proportion of ham not filtered. Ideally, we want both of them to be close to \\(1\\). There is usually a tradeoff between the two measures. 17.2.3 ROC and AUC ROC = receiver operating characteristic curve AUC = area under the ROC curve Recall that in the logistic regression example, we chose \\(0.5\\) as the threshold to decide our predictions. Changing this threshold will change the sensitivity and the specificity of your model. ROC is a tool to visualize the trade-off between the sensitivity and specificity. In ROC, sensitivity is plot against 1 - specificity at different levels of the threshold. It tells you how well the model perform. A classifier that is closer to the perfect classifier is considered to be better A classifier that results in a higher AUC is considered to be better The diagonal line from the bottom-left to the top-right corner of the diagram represents a classifier with no predictive value. To understand how ROC is constructed, we use our logistic regression example. The package caret contains the two functions sensitivity and specificity to compute the sensitivity and specificity, respectively. library(caret) threshold &lt;- seq(0, 1, by = 0.001) roc_sensitivity &lt;- rep(0, length(threshold)) roc_specificity &lt;- rep(0, length(threshold)) for (i in 1:length(threshold)) { # for each value of threshold, find our predictions predicted_class &lt;- factor(ifelse(prob &gt; threshold[i], &quot;pos&quot;, &quot;neg&quot;)) # compute the corresponding sensitivity and specificity roc_sensitivity[i] &lt;- sensitivity(predicted_class, test_data$diabetes, positive = &quot;pos&quot;) roc_specificity[i] &lt;- specificity(predicted_class, test_data$diabetes, negative = &quot;neg&quot;) } # ROC, use geom_path instead of geom_line to retain the order when joining the points ggplot(mapping = aes(x = 1 - roc_specificity, y = roc_sensitivity)) + geom_path() + labs(x = &quot;1 - Specificity&quot;, y = &quot;Sensitivity&quot;) There is a function called roc() in the package pROC to construct the ROC. library(pROC) roc_logistic &lt;- roc(predictor = as.numeric(prob), response = test_data$diabetes) plot(roc_logistic) # find the AUC auc(roc_logistic) ## Area under the curve: 0.7592 Note that the x-axis label from this function is Specificity starting from 1. 17.3 Estimating future performances Recall that in evaluating the model performance, we have partition our data into training and testing datasets. This is known as the holdout method. To estimate the future performance, we should not use any part of the testing data when we train our model using our training dataset. Otherwise, our result will be biased. One problem with the above random partition is that the proportions of the class of interest are very different in the training dataset and testing datase even if we have randomly selected our observations. Example set.seed(2) random_index &lt;- sample(nrow(PimaIndiansDiabetes2), size = nrow(PimaIndiansDiabetes2) * 0.7) train_data &lt;- PimaIndiansDiabetes2[random_index, ] test_data &lt;- PimaIndiansDiabetes2[-random_index, ] table(train_data$diabetes) / nrow(train_data) ## ## neg pos ## 0.649635 0.350365 table(test_data$diabetes) / nrow(test_data) ## ## neg pos ## 0.7118644 0.2881356 You can see that the proportion of pos in the training dataset is \\(0.350\\) while that in the testing dataset is \\(0.288\\). This may affect the estimation of the model performance. To avoid this issue, we can use stratified random sampling to ensure the training dataset and testing dataset have nearly the same proportion in the class of interest. To do that in R, we can use the function createDataPartition from the package caret. stratified_random_index &lt;- createDataPartition(PimaIndiansDiabetes2$diabetes, p = 0.7, list = FALSE) train_data &lt;- PimaIndiansDiabetes2[stratified_random_index, ] test_data &lt;- PimaIndiansDiabetes2[-stratified_random_index, ] table(train_data$diabetes) / nrow(train_data) ## ## neg pos ## 0.6690909 0.3309091 table(test_data$diabetes) / nrow(test_data) ## ## neg pos ## 0.6666667 0.3333333 table(PimaIndiansDiabetes2$diabetes) / nrow(PimaIndiansDiabetes2) ## ## neg pos ## 0.6683673 0.3316327 Now, both proportions are similar to the one in the whole dataset. While stratified sampling distribute the classes evenly, it does not guarantee other variables are distributed evenly. To mitigate the problem, we can use repeated holdout. That is, we repeat the whole process several times with several random holdout samples and take the average of the performance measures to evaluate the model performance. As multiple holdout samples are used, it is less likely that the model is trained or tested on non-representative data. 17.3.1 Cross-validation \\(k\\)-fold cross-validation (\\(k\\)-fold CV) is a procedure in which instead of randomly partition the data many times, we divide the data into \\(k\\) random non-overlapping partitions. Then, we combine \\(k-1\\) partitions to form a training dataset and the remaining one to form the testing dataset. We repeat this procedure \\(k\\) times using different partitions and obtain \\(k\\) evaluations. It is common to use \\(10\\)-fold CV. For example, when \\(k=3\\): Randomly split your dataset \\(S\\) into \\(3\\) partitions \\(S_1,S_2,S_3\\). Use \\(S_1, S_2\\) to train your model. Evaluate the performance using \\(S_3\\). Use \\(S_1, S_3\\) to train your model. Evaluate the performance using \\(S_2\\). Use \\(S_2, S_3\\) to train your model. Evaluate the performance using \\(S_1\\). Report the average of the performance measures obtained in Steps 2-4. See https://en.wikipedia.org/wiki/File:KfoldCV.gif for a gif animation showing the \\(k\\)-fold CV. Example: k &lt;- 10 set.seed(1) # the result is a list folds &lt;- createFolds(PimaIndiansDiabetes2$diabetes, k = k) accuracy &lt;- rep(0, k) AUC &lt;- rep(0, k) for (i in 1:k) { train_data &lt;- PimaIndiansDiabetes2[folds[[i]], ] test_data &lt;- PimaIndiansDiabetes2[-folds[[i]], ] fit_simple &lt;- glm(diabetes ~ glucose, data = train_data, family = binomial) # Prediction prob &lt;- predict(fit_simple, test_data, type = &quot;response&quot;) predicted_class &lt;- ifelse(prob &gt; 0.5, &quot;pos&quot;, &quot;neg&quot;) # Compute accuracy and AUC accuracy[i] &lt;- mean(predicted_class == test_data$diabetes) AUC[i] &lt;- auc(roc(predictor = as.numeric(prob), response = test_data$diabetes)) } Results: accuracy ## [1] 0.7733711 0.7535411 0.7705382 0.7698864 0.7535411 0.7535411 0.7563739 0.7620397 0.7443182 ## [10] 0.7705382 mean(accuracy) ## [1] 0.7607689 AUC ## [1] 0.7954331 0.7999783 0.8259271 0.8071468 0.8094126 0.7986021 0.8012821 0.8102455 0.7917803 ## [10] 0.8177061 mean(AUC) ## [1] 0.8057514 "]]
