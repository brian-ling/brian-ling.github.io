<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 18 Decision Trees | STAT 362 R for Data Science</title>
  <meta name="description" content="Notes for STAT 362" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 18 Decision Trees | STAT 362 R for Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes for STAT 362" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 18 Decision Trees | STAT 362 R for Data Science" />
  
  <meta name="twitter:description" content="Notes for STAT 362" />
  

<meta name="author" content="Brian Ling" />


<meta name="date" content="2022-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regularization.html"/>
<link rel="next" href="ensemble-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Data Science</a></li>

<li class="divider"></li>
<li><a href="index.html#syllabus">Syllabus<span></span></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> What is R and RStudio?<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-will-you-learn-in-this-course"><i class="fa fa-check"></i><b>1.2</b> What will you learn in this course?<span></span></a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#r-and-r-as-a-programming-language"><i class="fa fa-check"></i><b>1.2.1</b> R and R as a programming language<span></span></a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#data-wrangling"><i class="fa fa-check"></i><b>1.2.2</b> Data Wrangling<span></span></a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#data-visualization"><i class="fa fa-check"></i><b>1.2.3</b> Data Visualization<span></span></a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#statistical-inference"><i class="fa fa-check"></i><b>1.2.4</b> Statistical Inference<span></span></a></li>
<li class="chapter" data-level="1.2.5" data-path="introduction.html"><a href="introduction.html#machine-learning"><i class="fa fa-check"></i><b>1.2.5</b> Machine Learning<span></span></a></li>
<li class="chapter" data-level="1.2.6" data-path="introduction.html"><a href="introduction.html#some-numerical-methods"><i class="fa fa-check"></i><b>1.2.6</b> Some Numerical Methods<span></span></a></li>
<li class="chapter" data-level="1.2.7" data-path="introduction.html"><a href="introduction.html#lastly"><i class="fa fa-check"></i><b>1.2.7</b> Lastly<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#lets-get-started"><i class="fa fa-check"></i><b>1.3</b> Let’s Get Started<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#r-data-structures"><i class="fa fa-check"></i><b>1.4</b> R Data Structures<span></span></a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#vectors"><i class="fa fa-check"></i><b>1.4.1</b> Vectors<span></span></a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#factors"><i class="fa fa-check"></i><b>1.4.2</b> Factors<span></span></a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction.html"><a href="introduction.html#matrix"><i class="fa fa-check"></i><b>1.4.3</b> Matrix<span></span></a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction.html"><a href="introduction.html#lists"><i class="fa fa-check"></i><b>1.4.4</b> Lists<span></span></a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction.html"><a href="introduction.html#data-frames"><i class="fa fa-check"></i><b>1.4.5</b> Data frames<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#operators"><i class="fa fa-check"></i><b>1.5</b> Operators<span></span></a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#vectorized-operators"><i class="fa fa-check"></i><b>1.5.1</b> Vectorized Operators<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#built-in-functions"><i class="fa fa-check"></i><b>1.6</b> Built-in Functions<span></span></a><ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#sort"><i class="fa fa-check"></i><b>1.6.1</b> <code>sort()</code><span></span></a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction.html"><a href="introduction.html#seq"><i class="fa fa-check"></i><b>1.6.2</b> <code>seq()</code><span></span></a></li>
<li class="chapter" data-level="1.6.3" data-path="introduction.html"><a href="introduction.html#rep"><i class="fa fa-check"></i><b>1.6.3</b> <code>rep()</code><span></span></a></li>
<li class="chapter" data-level="1.6.4" data-path="introduction.html"><a href="introduction.html#pmax-pmin"><i class="fa fa-check"></i><b>1.6.4</b> <code>pmax</code>, <code>pmin</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#some-useful-rstudio-shortcuts"><i class="fa fa-check"></i><b>1.7</b> Some Useful RStudio Shortcuts<span></span></a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#comments-to-exercises"><i class="fa fa-check"></i><b>1.9</b> Comments to Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>2</b> Probability<span></span></a><ul>
<li class="chapter" data-level="2.1" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Probability Distributions<span></span></a><ul>
<li class="chapter" data-level="2.1.1" data-path="probability.html"><a href="probability.html#common-distributions"><i class="fa fa-check"></i><b>2.1.1</b> Common Distributions<span></span></a></li>
<li class="chapter" data-level="2.1.2" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>2.1.2</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability.html"><a href="probability.html#simulation"><i class="fa fa-check"></i><b>2.2</b> Simulation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>3</b> Programming in R<span></span></a><ul>
<li class="chapter" data-level="3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#writing-functions-in-r"><i class="fa fa-check"></i><b>3.1</b> Writing functions in R<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="programming-in-r.html"><a href="programming-in-r.html#control-flow"><i class="fa fa-check"></i><b>3.2</b> Control Flow<span></span></a><ul>
<li class="chapter" data-level="3.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#for-loop"><i class="fa fa-check"></i><b>3.2.1</b> for loop<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#while-loop"><i class="fa fa-check"></i><b>3.2.2</b> while loop<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond"><i class="fa fa-check"></i><b>3.2.3</b> if (cond)<span></span></a></li>
<li class="chapter" data-level="3.2.4" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond-else-expr"><i class="fa fa-check"></i><b>3.2.4</b> if (cond) else expr<span></span></a></li>
<li class="chapter" data-level="3.2.5" data-path="programming-in-r.html"><a href="programming-in-r.html#if-else-ladder"><i class="fa fa-check"></i><b>3.2.5</b> If else ladder<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="programming-in-r.html"><a href="programming-in-r.html#automatically-reindent-code"><i class="fa fa-check"></i><b>3.3</b> Automatically Reindent Code<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="programming-in-r.html"><a href="programming-in-r.html#speed-consideration"><i class="fa fa-check"></i><b>3.4</b> Speed Consideration<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="programming-in-r.html"><a href="programming-in-r.html#another-simulation-example"><i class="fa fa-check"></i><b>3.5</b> Another Simulation Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html"><i class="fa fa-check"></i><b>4</b> Creating Some Basic Plots<span></span></a><ul>
<li class="chapter" data-level="4.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#scatter-plot"><i class="fa fa-check"></i><b>4.1</b> Scatter Plot<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#line-graph"><i class="fa fa-check"></i><b>4.2</b> Line Graph<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#bar-chart"><i class="fa fa-check"></i><b>4.3</b> Bar Chart<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#histogram"><i class="fa fa-check"></i><b>4.4</b> Histogram<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#box-plot"><i class="fa fa-check"></i><b>4.5</b> Box Plot<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#plotting-a-function-curve"><i class="fa fa-check"></i><b>4.6</b> Plotting a function curve<span></span></a></li>
<li class="chapter" data-level="4.7" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#more-on-plots-with-base-r"><i class="fa fa-check"></i><b>4.7</b> More on plots with base R<span></span></a><ul>
<li class="chapter" data-level="4.7.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#multi-frame-plot"><i class="fa fa-check"></i><b>4.7.1</b> Multi-frame plot<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#subsubsec:type_of_plot"><i class="fa fa-check"></i><b>4.7.2</b> Type of Plot<span></span></a></li>
<li class="chapter" data-level="4.7.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#parameters-of-a-plot"><i class="fa fa-check"></i><b>4.7.3</b> Parameters of a plot<span></span></a></li>
<li class="chapter" data-level="4.7.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#elements-on-plot"><i class="fa fa-check"></i><b>4.7.4</b> Elements on plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#summary-of-ggplot"><i class="fa fa-check"></i><b>4.8</b> Summary of <code>ggplot</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html"><i class="fa fa-check"></i><b>5</b> Managing Data with R<span></span></a><ul>
<li class="chapter" data-level="5.1" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#saving-loading-and-removing-r-data-structures"><i class="fa fa-check"></i><b>5.2</b> Saving, loading, and removing R data structures<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#importing-and-saving-data-from-csv-files"><i class="fa fa-check"></i><b>5.3</b> Importing and saving data from CSV files<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html"><i class="fa fa-check"></i><b>6</b> Review (Chapter 1-5)<span></span></a><ul>
<li class="chapter" data-level="6.1" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#simulation-1"><i class="fa fa-check"></i><b>6.1</b> Simulation<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#matrix-1"><i class="fa fa-check"></i><b>6.2</b> Matrix<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#basic-operation"><i class="fa fa-check"></i><b>6.3</b> Basic Operation<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#some-basic-plots-in-r"><i class="fa fa-check"></i><b>6.4</b> Some basic plots in R<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html"><i class="fa fa-check"></i><b>7</b> Data Transformation with <code>dplyr</code><span></span></a><ul>
<li class="chapter" data-level="7.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#arrange"><i class="fa fa-check"></i><b>7.2</b> <code>arrange()</code><span></span></a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-2"><i class="fa fa-check"></i><b>7.2.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#filter"><i class="fa fa-check"></i><b>7.3</b> <code>filter()</code><span></span></a><ul>
<li class="chapter" data-level="7.3.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-3"><i class="fa fa-check"></i><b>7.3.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#select"><i class="fa fa-check"></i><b>7.4</b> <code>select()</code><span></span></a><ul>
<li class="chapter" data-level="7.4.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-4"><i class="fa fa-check"></i><b>7.4.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#mutate"><i class="fa fa-check"></i><b>7.5</b> <code>mutate()</code><span></span></a><ul>
<li class="chapter" data-level="7.5.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-5"><i class="fa fa-check"></i><b>7.5.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summarize-group_by"><i class="fa fa-check"></i><b>7.6</b> <code>summarize(), group_by()</code><span></span></a></li>
<li class="chapter" data-level="7.7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#combining-multiple-operations-with-pipe"><i class="fa fa-check"></i><b>7.7</b> Combining Multiple Operations with Pipe <code>%&gt;%</code><span></span></a></li>
<li class="chapter" data-level="7.8" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summary"><i class="fa fa-check"></i><b>7.8</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html"><i class="fa fa-check"></i><b>8</b> Data Visualization with ggplot2<span></span></a><ul>
<li class="chapter" data-level="8.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts"><i class="fa fa-check"></i><b>8.1</b> Bar charts<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graph-1"><i class="fa fa-check"></i><b>8.2</b> Line Graph<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plots"><i class="fa fa-check"></i><b>8.3</b> Scatter Plots<span></span></a><ul>
<li class="chapter" data-level="8.3.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#overplotting"><i class="fa fa-check"></i><b>8.3.1</b> Overplotting<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#labelling-points-in-a-scatter-plot"><i class="fa fa-check"></i><b>8.3.2</b> Labelling points in a scatter plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions"><i class="fa fa-check"></i><b>8.4</b> Summarizing Data Distributions<span></span></a><ul>
<li class="chapter" data-level="8.4.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#histogram-1"><i class="fa fa-check"></i><b>8.4.1</b> Histogram<span></span></a></li>
<li class="chapter" data-level="8.4.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#kernel-density-estimate"><i class="fa fa-check"></i><b>8.4.2</b> Kernel Density Estimate<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots"><i class="fa fa-check"></i><b>8.5</b> Saving your plots<span></span></a><ul>
<li class="chapter" data-level="8.5.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-pdf-vector-files"><i class="fa fa-check"></i><b>8.5.1</b> Outputting to pdf vector files<span></span></a></li>
<li class="chapter" data-level="8.5.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-bitmap-files"><i class="fa fa-check"></i><b>8.5.2</b> Outputting to bitmap files<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summary-1"><i class="fa fa-check"></i><b>8.6</b> Summary<span></span></a><ul>
<li class="chapter" data-level="8.6.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#combining-multiple-operations-with-pipe-1"><i class="fa fa-check"></i><b>8.6.1</b> Combining multiple operations with pipe <code>%&gt;%</code><span></span></a></li>
<li class="chapter" data-level="8.6.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts-1"><i class="fa fa-check"></i><b>8.6.2</b> Bar charts<span></span></a></li>
<li class="chapter" data-level="8.6.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graphs"><i class="fa fa-check"></i><b>8.6.3</b> Line graphs<span></span></a></li>
<li class="chapter" data-level="8.6.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plot-1"><i class="fa fa-check"></i><b>8.6.4</b> Scatter plot<span></span></a></li>
<li class="chapter" data-level="8.6.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions-1"><i class="fa fa-check"></i><b>8.6.5</b> Summarizing data distributions<span></span></a></li>
<li class="chapter" data-level="8.6.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots-1"><i class="fa fa-check"></i><b>8.6.6</b> Saving your plots<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference in R<span></span></a><ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.1</b> Maximum Likelihood Estimation<span></span></a><ul>
<li class="chapter" data-level="9.1.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#exercises-on-mle"><i class="fa fa-check"></i><b>9.1.1</b> Exercises on MLE<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#summary-2"><i class="fa fa-check"></i><b>9.1.2</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#interval-estimation-and-hypothesis-testing"><i class="fa fa-check"></i><b>9.2</b> Interval Estimation and Hypothesis Testing<span></span></a><ul>
<li class="chapter" data-level="9.2.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.2.1</b> Examples of Hypothesis Testing<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#null-hypotheses-alternative-hypotheses-and-p-values"><i class="fa fa-check"></i><b>9.2.2</b> Null Hypotheses, Alternative Hypotheses, and p-values<span></span></a></li>
<li class="chapter" data-level="9.2.3" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#type-i-error-and-type-ii-error"><i class="fa fa-check"></i><b>9.2.3</b> Type I error and Type II error<span></span></a></li>
<li class="chapter" data-level="9.2.4" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-for-mean-of-one-sample"><i class="fa fa-check"></i><b>9.2.4</b> Inference for Mean of One Sample<span></span></a></li>
<li class="chapter" data-level="9.2.5" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#comparing-the-means-of-two-samples"><i class="fa fa-check"></i><b>9.2.5</b> Comparing the means of two samples<span></span></a></li>
<li class="chapter" data-level="9.2.6" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-of-a-sample-proportion"><i class="fa fa-check"></i><b>9.2.6</b> Inference of a Sample Proportion<span></span></a></li>
<li class="chapter" data-level="9.2.7" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-groups-for-equal-proportions"><i class="fa fa-check"></i><b>9.2.7</b> Testing groups for equal proportions<span></span></a></li>
<li class="chapter" data-level="9.2.8" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-if-two-samples-have-the-same-underlying-distribution"><i class="fa fa-check"></i><b>9.2.8</b> Testing if two samples have the same underlying distribution<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html"><i class="fa fa-check"></i><b>10</b> Root finding and optimization<span></span></a><ul>
<li class="chapter" data-level="10.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#root-finding"><i class="fa fa-check"></i><b>10.1</b> Root Finding<span></span></a><ul>
<li class="chapter" data-level="10.1.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#bisection-method"><i class="fa fa-check"></i><b>10.1.1</b> Bisection Method<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method"><i class="fa fa-check"></i><b>10.2</b> Newton-Raphson Method<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#minimization-and-maximization"><i class="fa fa-check"></i><b>10.3</b> Minimization and Maximization<span></span></a><ul>
<li class="chapter" data-level="10.3.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method-1"><i class="fa fa-check"></i><b>10.3.1</b> Newton-Raphson Method<span></span></a></li>
<li class="chapter" data-level="10.3.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#gradient-descent"><i class="fa fa-check"></i><b>10.3.2</b> Gradient Descent<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#optim"><i class="fa fa-check"></i><b>10.4</b> <code>optim</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>11</b> k-nearest neighbors<span></span></a><ul>
<li class="chapter" data-level="11.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#feature-scaling"><i class="fa fa-check"></i><b>11.2</b> Feature Scaling<span></span></a></li>
<li class="chapter" data-level="11.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-classifying-breast-cancers"><i class="fa fa-check"></i><b>11.3</b> Example: Classifying Breast Cancers<span></span></a><ul>
<li class="chapter" data-level="11.3.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#evaluating-model-performance"><i class="fa fa-check"></i><b>11.3.1</b> Evaluating Model Performance<span></span></a></li>
<li class="chapter" data-level="11.3.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#using-z-score-standardization"><i class="fa fa-check"></i><b>11.3.2</b> Using <span class="math inline">\(z\)</span>-score standardization<span></span></a></li>
<li class="chapter" data-level="11.3.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#testing-alternative-values-of-k"><i class="fa fa-check"></i><b>11.3.3</b> Testing alternative values of <span class="math inline">\(k\)</span><span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>12</b> Linear Regression Models<span></span></a><ul>
<li class="chapter" data-level="12.1" data-path="linear-regression-models.html"><a href="linear-regression-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Simple Linear Regression<span></span></a></li>
<li class="chapter" data-level="12.2" data-path="linear-regression-models.html"><a href="linear-regression-models.html#smoothed-conditional-means"><i class="fa fa-check"></i><b>12.2</b> Smoothed Conditional Means<span></span></a></li>
<li class="chapter" data-level="12.3" data-path="linear-regression-models.html"><a href="linear-regression-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>12.3</b> Multiple Linear Regression<span></span></a></li>
<li class="chapter" data-level="12.4" data-path="linear-regression-models.html"><a href="linear-regression-models.html#example-diamonds"><i class="fa fa-check"></i><b>12.4</b> Example: <code>diamonds</code><span></span></a></li>
<li class="chapter" data-level="12.5" data-path="linear-regression-models.html"><a href="linear-regression-models.html#categorical-predictors"><i class="fa fa-check"></i><b>12.5</b> Categorical Predictors<span></span></a></li>
<li class="chapter" data-level="12.6" data-path="linear-regression-models.html"><a href="linear-regression-models.html#compare-models-using-anova"><i class="fa fa-check"></i><b>12.6</b> Compare models using ANOVA<span></span></a></li>
<li class="chapter" data-level="12.7" data-path="linear-regression-models.html"><a href="linear-regression-models.html#prediction"><i class="fa fa-check"></i><b>12.7</b> Prediction<span></span></a></li>
<li class="chapter" data-level="12.8" data-path="linear-regression-models.html"><a href="linear-regression-models.html#interaction-terms"><i class="fa fa-check"></i><b>12.8</b> Interaction Terms<span></span></a></li>
<li class="chapter" data-level="12.9" data-path="linear-regression-models.html"><a href="linear-regression-models.html#variable-transformation"><i class="fa fa-check"></i><b>12.9</b> Variable Transformation<span></span></a></li>
<li class="chapter" data-level="12.10" data-path="linear-regression-models.html"><a href="linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>12.10</b> Polynomial Regression<span></span></a></li>
<li class="chapter" data-level="12.11" data-path="linear-regression-models.html"><a href="linear-regression-models.html#stepwise-regression"><i class="fa fa-check"></i><b>12.11</b> Stepwise regression<span></span></a></li>
<li class="chapter" data-level="12.12" data-path="linear-regression-models.html"><a href="linear-regression-models.html#best-subset"><i class="fa fa-check"></i><b>12.12</b> Best subset<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logistic-regression-model.html"><a href="logistic-regression-model.html"><i class="fa fa-check"></i><b>13</b> Logistic Regression Model<span></span></a></li>
<li class="chapter" data-level="14" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>14</b> k-means Clustering<span></span></a><ul>
<li class="chapter" data-level="14.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-3"><i class="fa fa-check"></i><b>14.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="14.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications"><i class="fa fa-check"></i><b>14.2</b> Applications<span></span></a><ul>
<li class="chapter" data-level="14.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#cluster-analysis"><i class="fa fa-check"></i><b>14.2.1</b> Cluster Analysis<span></span></a></li>
<li class="chapter" data-level="14.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#image-segementation-and-image-compression"><i class="fa fa-check"></i><b>14.2.2</b> Image Segementation and Image Compression<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>15</b> Hierarchical Clustering<span></span></a><ul>
<li class="chapter" data-level="15.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#dissimilarity-measure-and-linkage"><i class="fa fa-check"></i><b>15.1</b> Dissimilarity measure and Linkage<span></span></a></li>
<li class="chapter" data-level="15.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#alogrithm"><i class="fa fa-check"></i><b>15.2</b> Alogrithm<span></span></a></li>
<li class="chapter" data-level="15.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#applications-1"><i class="fa fa-check"></i><b>15.3</b> Applications<span></span></a><ul>
<li class="chapter" data-level="15.3.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#nci60-data"><i class="fa fa-check"></i><b>15.3.1</b> NCI60 Data<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>16</b> Resampling Methods<span></span></a><ul>
<li class="chapter" data-level="16.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>16.1</b> Cross-validation<span></span></a><ul>
<li class="chapter" data-level="16.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cv-in-glm"><i class="fa fa-check"></i><b>16.1.1</b> CV in GLM<span></span></a></li>
<li class="chapter" data-level="16.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#general-implementation"><i class="fa fa-check"></i><b>16.1.2</b> General Implementation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="resampling-methods.html"><a href="resampling-methods.html#bootstrap"><i class="fa fa-check"></i><b>16.2</b> Bootstrap<span></span></a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>17</b> Regularization<span></span></a><ul>
<li class="chapter" data-level="17.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>17.1</b> Ridge Regression<span></span></a></li>
<li class="chapter" data-level="17.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>17.2</b> LASSO<span></span></a></li>
<li class="chapter" data-level="17.3" data-path="regularization.html"><a href="regularization.html#selecting-the-tuning-parameter"><i class="fa fa-check"></i><b>17.3</b> Selecting the tuning parameter<span></span></a></li>
<li class="chapter" data-level="17.4" data-path="regularization.html"><a href="regularization.html#glmnet"><i class="fa fa-check"></i><b>17.4</b> <code>glmnet</code><span></span></a><ul>
<li class="chapter" data-level="17.4.1" data-path="regularization.html"><a href="regularization.html#ridge-regression-1"><i class="fa fa-check"></i><b>17.4.1</b> Ridge Regression<span></span></a></li>
<li class="chapter" data-level="17.4.2" data-path="regularization.html"><a href="regularization.html#lasso-1"><i class="fa fa-check"></i><b>17.4.2</b> LASSO<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>18</b> Decision Trees<span></span></a><ul>
<li class="chapter" data-level="18.1" data-path="decision-trees.html"><a href="decision-trees.html#introduction-to-classification-tree"><i class="fa fa-check"></i><b>18.1</b> Introduction to Classification Tree<span></span></a></li>
<li class="chapter" data-level="18.2" data-path="decision-trees.html"><a href="decision-trees.html#introduction-to-regression-tree"><i class="fa fa-check"></i><b>18.2</b> Introduction to regression tree<span></span></a></li>
<li class="chapter" data-level="18.3" data-path="decision-trees.html"><a href="decision-trees.html#mathematical-formulation"><i class="fa fa-check"></i><b>18.3</b> Mathematical Formulation<span></span></a></li>
<li class="chapter" data-level="18.4" data-path="decision-trees.html"><a href="decision-trees.html#examples"><i class="fa fa-check"></i><b>18.4</b> Examples<span></span></a><ul>
<li class="chapter" data-level="18.4.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-tree"><i class="fa fa-check"></i><b>18.4.1</b> Classification Tree<span></span></a></li>
<li class="chapter" data-level="18.4.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-tree"><i class="fa fa-check"></i><b>18.4.2</b> Regression Tree<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>19</b> Ensemble Methods<span></span></a><ul>
<li class="chapter" data-level="19.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>19.1</b> Bagging<span></span></a></li>
<li class="chapter" data-level="19.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>19.2</b> Random Forest<span></span></a></li>
<li class="chapter" data-level="19.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#example"><i class="fa fa-check"></i><b>19.3</b> Example<span></span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 362 R for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decision-trees" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 18</span> Decision Trees<a href="decision-trees.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Reference: Ch8 in An introduction to Statistical Leraning with applications in R by James, Witten, Hastie and Tibshirani.</p>
<p>In this chapter, we will load the following packages:</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="decision-trees.html#cb466-1"></a><span class="kw">library</span>(tidyverse) </span>
<span id="cb466-2"><a href="decision-trees.html#cb466-2"></a><span class="kw">library</span>(GGally) <span class="co"># to use ggpairs</span></span>
<span id="cb466-3"><a href="decision-trees.html#cb466-3"></a><span class="kw">library</span>(ISLR2)</span></code></pre></div>
<ul>
<li><p>Decision trees involve <strong>stratifying</strong> or <strong>segmenting</strong> the predictor space into a number of simple regions</p></li>
<li><p>To make a prediction for a new observation, we typically use the <strong>mean</strong> (for continuous response) or <strong>mode</strong> (for categorical response) response value of the training observations in the region to which the new observation belongs</p></li>
<li><p>Since the set of splitting rules used to segment the predictor space can be <strong>summarized in a tree</strong>, these types of approaches are known as decision tree methods.</p></li>
<li><p>Decision trees can be applied to both regression and classification problems</p>
<ul>
<li><strong>classification</strong> tree - for categorical outcomes</li>
<li><strong>regression</strong> tree - for continuous outcomes</li>
</ul></li>
<li><p>A key advantage of the decision tree is its <strong>interpretability</strong> (compared with other “black box” models, see Ch20).</p></li>
</ul>
<div id="introduction-to-classification-tree" class="section level2 hasAnchor">
<h2><span class="header-section-number">18.1</span> Introduction to Classification Tree<a href="decision-trees.html#introduction-to-classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below is a simplified example of determining if you should accept a new job offer using a tree representation.</p>
<p><img src="image/decision_tree_job_offer.jpg" width="60%" style="display: block; margin: auto;" /></p>
<ul>
<li><p>In each branching node of the tree, a specific feature of the data is examined. According to the value of the feature, one of the branches will be followed.</p></li>
<li><p>The leaf node represents the class label.</p></li>
<li><p>In the above example, the leaf nodes are “Decline offer” and “Accept offer”.</p></li>
<li><p>The paths from the root to leaf node represent classification rules.</p></li>
</ul>
<p>Consider the <code>iris</code> dataset, which is available in base R. We have <span class="math inline">\(150\)</span> observations and <span class="math inline">\(5\)</span> variables named <code>Sepal.Length</code>, <code>Sepal.Wdith</code>, <code>Petal.Length</code>, <code>Petal.Width</code>, and <code>Species</code>.</p>
<p>Let’s take a look at the data.</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="decision-trees.html#cb467-1"></a><span class="kw">str</span>(iris) </span>
<span id="cb467-2"><a href="decision-trees.html#cb467-2"></a><span class="co">## &#39;data.frame&#39;:    150 obs. of  5 variables:</span></span>
<span id="cb467-3"><a href="decision-trees.html#cb467-3"></a><span class="co">##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...</span></span>
<span id="cb467-4"><a href="decision-trees.html#cb467-4"></a><span class="co">##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...</span></span>
<span id="cb467-5"><a href="decision-trees.html#cb467-5"></a><span class="co">##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...</span></span>
<span id="cb467-6"><a href="decision-trees.html#cb467-6"></a><span class="co">##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...</span></span>
<span id="cb467-7"><a href="decision-trees.html#cb467-7"></a><span class="co">##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</span></span></code></pre></div>
<p>The first four variables are numeric features of the flowers. The last column in the dataset is the species of the flower. We will illustrate how to use the numeric features of the flowers to classify the flowers.</p>
<p>First, we will split the dataset into a training dataset and a testing dataset. For the moment, we will only use the training dataset.</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="decision-trees.html#cb468-1"></a><span class="kw">set.seed</span>(<span class="dv">6</span>)</span>
<span id="cb468-2"><a href="decision-trees.html#cb468-2"></a>random_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(iris), <span class="dv">100</span>) <span class="co"># select 100 examples for our training dataset</span></span>
<span id="cb468-3"><a href="decision-trees.html#cb468-3"></a>iris_train &lt;-<span class="st"> </span>iris[random_index, ]</span>
<span id="cb468-4"><a href="decision-trees.html#cb468-4"></a>iris_test &lt;-<span class="st"> </span>iris[<span class="op">-</span>random_index, ]</span></code></pre></div>
<p>It can be helpful to look at the scatterplot matrix when you wish to look at the relationship between each pair of variables. To create a scatterplot matrix, we can use <code>pairs</code> (base R graphics) or <code>ggpairs</code> (in the package <code>GGally</code>).</p>
<p>With <code>pairs()</code>:</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="decision-trees.html#cb469-1"></a><span class="co"># we only want to plot the numeric features</span></span>
<span id="cb469-2"><a href="decision-trees.html#cb469-2"></a><span class="co"># the 5th column is the species of the flowers</span></span>
<span id="cb469-3"><a href="decision-trees.html#cb469-3"></a><span class="kw">pairs</span>(iris_train[, <span class="dv">-5</span>])</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-522-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>With <code>ggpairs()</code>, you can also visualize an additional category by specifying <code>color =</code> with your variable describing the category.</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="decision-trees.html#cb470-1"></a><span class="kw">library</span>(GGally) <span class="co"># to use ggpairs, install it first if you haven&#39;t done so</span></span>
<span id="cb470-2"><a href="decision-trees.html#cb470-2"></a><span class="kw">ggpairs</span>(<span class="dt">data =</span> iris_train, <span class="kw">aes</span>(<span class="dt">color =</span> Species, <span class="dt">alpha =</span> <span class="fl">0.8</span>), <span class="dt">columns =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-523-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>In the above code:</p>
<ul>
<li><code>columns = 1:4</code> tells the function only uses the first <span class="math inline">\(4\)</span> columns of the dataset for plotting.</li>
<li><code>color = Species</code> indicates that we map the color to the variable <code>Species</code></li>
<li><code>alpha = 0.8</code> controls the level of transparency so that the density estimates do not overlap each other completely</li>
</ul>
<p>From the plots, it seems that the variable <code>Petal.Width</code> and <code>Petal.Length</code> will be useful in classifying the flowers. Let’s focus on the scatterplot of these two variables.</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="decision-trees.html#cb471-1"></a><span class="kw">ggplot</span>(iris_train, <span class="kw">aes</span>(<span class="dt">x =</span> Petal.Width, <span class="dt">y =</span> Petal.Length, <span class="dt">color =</span> Species)) <span class="op">+</span></span>
<span id="cb471-2"><a href="decision-trees.html#cb471-2"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-524-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>From the above plot, we see that we can easily classify the flowers with high accuracy using these two features. For example, if we draw the horizontal line <span class="math inline">\(y = 1.9\)</span>, we could classify any iris lying on or below this line as setosa with <span class="math inline">\(100\%\)</span> accuracy.</p>
<p><img src="Book_files/figure-html/unnamed-chunk-525-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Next, we draw the vertical line <span class="math inline">\(x = 1.7\)</span>. We could classify any iris lying on or to the left of this line as versicolor.</p>
<p><img src="Book_files/figure-html/unnamed-chunk-526-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>We get a pretty good classification with only 3 mistakes.</p>
<p>The above classification is an example of classification trees (because the outcome is categorical) and can be visualized as</p>
<p><img src="Book_files/figure-html/unnamed-chunk-528-1.png" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="introduction-to-regression-tree" class="section level2 hasAnchor">
<h2><span class="header-section-number">18.2</span> Introduction to regression tree<a href="decision-trees.html#introduction-to-regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To describe the regression trees, we consider the dataset <code>Hitters</code> from the package <code>ISLR2</code>, a baseball player dataset with <span class="math inline">\(322\)</span> observations of major league players from the 1986 and 1987 seasons. In this dataset, there are <span class="math inline">\(20\)</span> variables.</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="decision-trees.html#cb472-1"></a><span class="kw">library</span>(ISLR2)</span>
<span id="cb472-2"><a href="decision-trees.html#cb472-2"></a><span class="kw">str</span>(Hitters)</span>
<span id="cb472-3"><a href="decision-trees.html#cb472-3"></a><span class="co">## &#39;data.frame&#39;:    263 obs. of  20 variables:</span></span>
<span id="cb472-4"><a href="decision-trees.html#cb472-4"></a><span class="co">##  $ AtBat    : int  315 479 496 321 594 185 298 323 401 574 ...</span></span>
<span id="cb472-5"><a href="decision-trees.html#cb472-5"></a><span class="co">##  $ Hits     : int  81 130 141 87 169 37 73 81 92 159 ...</span></span>
<span id="cb472-6"><a href="decision-trees.html#cb472-6"></a><span class="co">##  $ HmRun    : int  7 18 20 10 4 1 0 6 17 21 ...</span></span>
<span id="cb472-7"><a href="decision-trees.html#cb472-7"></a><span class="co">##  $ Runs     : int  24 66 65 39 74 23 24 26 49 107 ...</span></span>
<span id="cb472-8"><a href="decision-trees.html#cb472-8"></a><span class="co">##  $ RBI      : int  38 72 78 42 51 8 24 32 66 75 ...</span></span>
<span id="cb472-9"><a href="decision-trees.html#cb472-9"></a><span class="co">##  $ Walks    : int  39 76 37 30 35 21 7 8 65 59 ...</span></span>
<span id="cb472-10"><a href="decision-trees.html#cb472-10"></a><span class="co">##  $ Years    : int  14 3 11 2 11 2 3 2 13 10 ...</span></span>
<span id="cb472-11"><a href="decision-trees.html#cb472-11"></a><span class="co">##  $ CAtBat   : int  3449 1624 5628 396 4408 214 509 341 5206 4631 ...</span></span>
<span id="cb472-12"><a href="decision-trees.html#cb472-12"></a><span class="co">##  $ CHits    : int  835 457 1575 101 1133 42 108 86 1332 1300 ...</span></span>
<span id="cb472-13"><a href="decision-trees.html#cb472-13"></a><span class="co">##  $ CHmRun   : int  69 63 225 12 19 1 0 6 253 90 ...</span></span>
<span id="cb472-14"><a href="decision-trees.html#cb472-14"></a><span class="co">##  $ CRuns    : int  321 224 828 48 501 30 41 32 784 702 ...</span></span>
<span id="cb472-15"><a href="decision-trees.html#cb472-15"></a><span class="co">##  $ CRBI     : int  414 266 838 46 336 9 37 34 890 504 ...</span></span>
<span id="cb472-16"><a href="decision-trees.html#cb472-16"></a><span class="co">##  $ CWalks   : int  375 263 354 33 194 24 12 8 866 488 ...</span></span>
<span id="cb472-17"><a href="decision-trees.html#cb472-17"></a><span class="co">##  $ League   : Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 2 1 2 2 1 2 1 2 1 1 ...</span></span>
<span id="cb472-18"><a href="decision-trees.html#cb472-18"></a><span class="co">##  $ Division : Factor w/ 2 levels &quot;E&quot;,&quot;W&quot;: 2 2 1 1 2 1 2 2 1 1 ...</span></span>
<span id="cb472-19"><a href="decision-trees.html#cb472-19"></a><span class="co">##  $ PutOuts  : int  632 880 200 805 282 76 121 143 0 238 ...</span></span>
<span id="cb472-20"><a href="decision-trees.html#cb472-20"></a><span class="co">##  $ Assists  : int  43 82 11 40 421 127 283 290 0 445 ...</span></span>
<span id="cb472-21"><a href="decision-trees.html#cb472-21"></a><span class="co">##  $ Errors   : int  10 14 3 4 25 7 9 19 0 22 ...</span></span>
<span id="cb472-22"><a href="decision-trees.html#cb472-22"></a><span class="co">##  $ Salary   : num  475 480 500 91.5 750 ...</span></span>
<span id="cb472-23"><a href="decision-trees.html#cb472-23"></a><span class="co">##  $ NewLeague: Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 2 1 2 2 1 1 1 2 1 1 ...</span></span>
<span id="cb472-24"><a href="decision-trees.html#cb472-24"></a><span class="co">##  - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:59] 1 16 19 23 31 33 37 39 40 42 ...</span></span>
<span id="cb472-25"><a href="decision-trees.html#cb472-25"></a><span class="co">##   ..- attr(*, &quot;names&quot;)= chr [1:59] &quot;-Andy Allanson&quot; &quot;-Billy Beane&quot; &quot;-Bruce Bochte&quot; &quot;-Bob Boone&quot; ...</span></span></code></pre></div>
<p>We will consider the problem of predicting player’s <code>Salary</code> (in the log scale so that its distribution has more of a typical bell-shape) based on the other variables. In particular, to illustrate the regression trees, we shall first use only two variables <code>Years</code> and <code>Hits</code>.</p>
<p><code>Years</code>: the number of years that the player has played in the major leagues</p>
<p><code>Hits</code>: the number of hits that he made in the previous year</p>
<p>The following figure shows an example of regression tree using <code>Years</code> and <code>Hits</code>. The split at the top of the tree results in two large branches. The left-hand branch corresponds to <code>Years &lt; 4.5</code>, and the right-hand branch corresponds to <code>Years &gt;= 4.5</code>. The tree has two internal nodes and three terminal nodes, or leaves. The number in each leaf is the mean of the response for the observations that fall there.</p>
<p><img src="image/8_1_regression_tree.PNG" width="40%" style="display: block; margin: auto;" /></p>
<p>For players with <code>Years &lt; 4.5</code>, the mean log salary is <span class="math inline">\(5.11\)</span> and so we make a prediction of <span class="math inline">\(e^{5.11}\)</span> (thousands of dollars). Players with <code>Years &gt;= 4.5</code> are assigned to the right branch, and that group is further subdidived by <code>Hits</code>. The prediction of salary for players with <code>Years &gt;= 4.5</code> and <code>Hits &lt; 117.5</code> is <span class="math inline">\(e^{6}\)</span> and the prediction of salary for players with <code>Years &gt;= 4.5</code> and <code>Hits &gt;= 117.5</code> is <span class="math inline">\(e^{6.74}\)</span>.</p>
<p>The following figure shows that partition of the predictor space:</p>
<p><img src="image/8_2_regression_tree.PNG" width="40%" style="display: block; margin: auto;" /></p>
</div>
<div id="mathematical-formulation" class="section level2 hasAnchor">
<h2><span class="header-section-number">18.3</span> Mathematical Formulation<a href="decision-trees.html#mathematical-formulation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first describe the regression tree. The idea behind a <strong>regression tree</strong> consists of two steps:</p>
<ol style="list-style-type: decimal">
<li><p>Divide the <strong>predictor space</strong>, which is the set of possible values for the <span class="math inline">\(p\)</span> features <span class="math inline">\(X_1,\ldots,X_p\)</span>, into <span class="math inline">\(J\)</span> distinct and non-overlapping regions <span class="math inline">\(R_1,\ldots,R_J\)</span></p></li>
<li><p>For each observation that falls into the region <span class="math inline">\(R_j\)</span>, we make the same prediction, usually using the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span> (because it minimizes the sum of squares error).</p></li>
</ol>
<p>The first step is however computationally untractable if we allow arbitrary shapes of non-overlapping regions.</p>
<p><strong>First simplification</strong>: divide the predictor space into high-dimensional rectangles or boxes only</p>
<p>The goal then is to find boxes <span class="math inline">\(R_1,\ldots,R_J\)</span> that minimize the RSS
<span class="math display">\[\begin{equation*}
\sum^J_{j=1} \sum_{i : x_i \in R_j} (y_i - \hat{y}_{R_j})^2,
\end{equation*}\]</span>
where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean of the response for the training observation in <span class="math inline">\(R_j\)</span>.</p>
<p><strong>Second simplification</strong>: since it is also computationally infeasible to consider every possible partition of the feature space into <span class="math inline">\(J\)</span> boxes, we have to take a top-down, greedy approach that is known as <strong>recursive binary splitting</strong>.</p>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li><p>For any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, define the pair of half-planes
<span class="math display">\[\begin{equation*}
R_1(j, s) := \{X| X_j &lt; s\} \text{ and } R_2(j, s) := \{X|X_j \geq s \},
\end{equation*}\]</span>
and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that minimize the expression
<span class="math display">\[\begin{equation*}
\sum_{i: x_i \in R_1(j, s)} (y_i - \hat{y}_{R_1})^2 + \sum_{i : x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2,
\end{equation*}\]</span>
where <span class="math inline">\(\hat{y}_{R_k}\)</span> is the mean response for the training observations in <span class="math inline">\(R_k(j, s)\)</span> for <span class="math inline">\(k=1,2\)</span>.</p></li>
<li><p>Next, we repeat the above process within each of the resulting regions.</p></li>
<li><p>The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than <span class="math inline">\(5\)</span> observations.</p></li>
<li><p>Finally, tree pruning is performed to avoid the model becomes too complex, which overfits the training data and leads to poor test performance.</p></li>
</ol>
<p>Remarks:</p>
<ol style="list-style-type: decimal">
<li><p>The idea of tree pruning is similar to regularization in Ch17. We add an additional tuning parameter that penalizes a tree with many terminal nodes (large tree) and find the optimal tuning parameter using cross-validation. The details are omitted.</p></li>
<li><p>We can grow a classification tree similar to a regression tree except that</p></li>
</ol>
<ul>
<li>our prediction of each new observation is now the most commonly occurring class of training observations in the region to which it belongs</li>
<li>we replace the RSS by classification error rate/ Gini index/ entropy (see the reference for details)</li>
</ul>
</div>
<div id="examples" class="section level2 hasAnchor">
<h2><span class="header-section-number">18.4</span> Examples<a href="decision-trees.html#examples" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="classification-tree" class="section level3 hasAnchor">
<h3><span class="header-section-number">18.4.1</span> Classification Tree<a href="decision-trees.html#classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s begin with the iris dataset. To build a classification tree, we can use the function <code>tree()</code> from the package <code>tree</code>. The syntax of <code>tree()</code> is similar to that of <code>lm()</code>.</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="decision-trees.html#cb473-1"></a><span class="kw">library</span>(tree)</span>
<span id="cb473-2"><a href="decision-trees.html#cb473-2"></a><span class="co"># split the data into training data and testing data</span></span>
<span id="cb473-3"><a href="decision-trees.html#cb473-3"></a><span class="kw">set.seed</span>(<span class="dv">10</span>)</span>
<span id="cb473-4"><a href="decision-trees.html#cb473-4"></a>index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(iris), <span class="dv">100</span>)</span>
<span id="cb473-5"><a href="decision-trees.html#cb473-5"></a>tree_iris &lt;-<span class="st"> </span><span class="kw">tree</span>(Species <span class="op">~</span>. , <span class="dt">data =</span> iris[index, ])</span>
<span id="cb473-6"><a href="decision-trees.html#cb473-6"></a></span>
<span id="cb473-7"><a href="decision-trees.html#cb473-7"></a><span class="co"># display the result</span></span>
<span id="cb473-8"><a href="decision-trees.html#cb473-8"></a>tree_iris</span>
<span id="cb473-9"><a href="decision-trees.html#cb473-9"></a><span class="co">## node), split, n, deviance, yval, (yprob)</span></span>
<span id="cb473-10"><a href="decision-trees.html#cb473-10"></a><span class="co">##       * denotes terminal node</span></span>
<span id="cb473-11"><a href="decision-trees.html#cb473-11"></a><span class="co">## </span></span>
<span id="cb473-12"><a href="decision-trees.html#cb473-12"></a><span class="co">## 1) root 100 217.70 virginica ( 0.29000 0.31000 0.40000 )  </span></span>
<span id="cb473-13"><a href="decision-trees.html#cb473-13"></a><span class="co">##   2) Petal.Length &lt; 4.75 58  80.41 setosa ( 0.50000 0.50000 0.00000 )  </span></span>
<span id="cb473-14"><a href="decision-trees.html#cb473-14"></a><span class="co">##     4) Petal.Length &lt; 2.5 29   0.00 setosa ( 1.00000 0.00000 0.00000 ) *</span></span>
<span id="cb473-15"><a href="decision-trees.html#cb473-15"></a><span class="co">##     5) Petal.Length &gt; 2.5 29   0.00 versicolor ( 0.00000 1.00000 0.00000 ) *</span></span>
<span id="cb473-16"><a href="decision-trees.html#cb473-16"></a><span class="co">##   3) Petal.Length &gt; 4.75 42  16.08 virginica ( 0.00000 0.04762 0.95238 )  </span></span>
<span id="cb473-17"><a href="decision-trees.html#cb473-17"></a><span class="co">##     6) Petal.Width &lt; 1.75 5   6.73 virginica ( 0.00000 0.40000 0.60000 ) *</span></span>
<span id="cb473-18"><a href="decision-trees.html#cb473-18"></a><span class="co">##     7) Petal.Width &gt; 1.75 37   0.00 virginica ( 0.00000 0.00000 1.00000 ) *</span></span></code></pre></div>
<p>The <code>summary()</code> function lsts the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.</p>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="decision-trees.html#cb474-1"></a><span class="kw">summary</span>(tree_iris)</span>
<span id="cb474-2"><a href="decision-trees.html#cb474-2"></a><span class="co">## </span></span>
<span id="cb474-3"><a href="decision-trees.html#cb474-3"></a><span class="co">## Classification tree:</span></span>
<span id="cb474-4"><a href="decision-trees.html#cb474-4"></a><span class="co">## tree(formula = Species ~ ., data = iris[index, ])</span></span>
<span id="cb474-5"><a href="decision-trees.html#cb474-5"></a><span class="co">## Variables actually used in tree construction:</span></span>
<span id="cb474-6"><a href="decision-trees.html#cb474-6"></a><span class="co">## [1] &quot;Petal.Length&quot; &quot;Petal.Width&quot; </span></span>
<span id="cb474-7"><a href="decision-trees.html#cb474-7"></a><span class="co">## Number of terminal nodes:  4 </span></span>
<span id="cb474-8"><a href="decision-trees.html#cb474-8"></a><span class="co">## Residual mean deviance:  0.07011 = 6.73 / 96 </span></span>
<span id="cb474-9"><a href="decision-trees.html#cb474-9"></a><span class="co">## Misclassification error rate: 0.02 = 2 / 100</span></span></code></pre></div>
<p>Plot the tree:</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="decision-trees.html#cb475-1"></a><span class="kw">plot</span>(tree_iris)</span>
<span id="cb475-2"><a href="decision-trees.html#cb475-2"></a><span class="kw">text</span>(tree_iris, <span class="dt">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-534-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Prediction and accuracy:</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="decision-trees.html#cb476-1"></a><span class="co"># prediction</span></span>
<span id="cb476-2"><a href="decision-trees.html#cb476-2"></a>predict_iris &lt;-<span class="st"> </span><span class="kw">predict</span>(tree_iris, iris[<span class="op">-</span>index, ], <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb476-3"><a href="decision-trees.html#cb476-3"></a></span>
<span id="cb476-4"><a href="decision-trees.html#cb476-4"></a>(result &lt;-<span class="st"> </span><span class="kw">table</span>(iris[<span class="op">-</span>index, <span class="dv">5</span>], predict_iris))</span>
<span id="cb476-5"><a href="decision-trees.html#cb476-5"></a><span class="co">##             predict_iris</span></span>
<span id="cb476-6"><a href="decision-trees.html#cb476-6"></a><span class="co">##              setosa versicolor virginica</span></span>
<span id="cb476-7"><a href="decision-trees.html#cb476-7"></a><span class="co">##   setosa         21          0         0</span></span>
<span id="cb476-8"><a href="decision-trees.html#cb476-8"></a><span class="co">##   versicolor      0         15         4</span></span>
<span id="cb476-9"><a href="decision-trees.html#cb476-9"></a><span class="co">##   virginica       0          1         9</span></span>
<span id="cb476-10"><a href="decision-trees.html#cb476-10"></a></span>
<span id="cb476-11"><a href="decision-trees.html#cb476-11"></a><span class="co"># accuracy</span></span>
<span id="cb476-12"><a href="decision-trees.html#cb476-12"></a><span class="kw">sum</span>(<span class="kw">diag</span>(result)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(result)</span>
<span id="cb476-13"><a href="decision-trees.html#cb476-13"></a><span class="co">## [1] 0.9</span></span></code></pre></div>
<p>Now, let’s consider <code>Hitters</code> dataset from the package <code>ISLR2</code>. We have used this dataset in Ch17. We will create an additional categorical variable indicating if <code>Salary</code> is higher than the upper quartile of <code>Salary</code></p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="decision-trees.html#cb477-1"></a><span class="kw">library</span>(ISLR2)</span>
<span id="cb477-2"><a href="decision-trees.html#cb477-2"></a><span class="co"># remove rows with missing values</span></span>
<span id="cb477-3"><a href="decision-trees.html#cb477-3"></a>Hitters &lt;-<span class="st"> </span><span class="kw">na.omit</span>(Hitters)</span>
<span id="cb477-4"><a href="decision-trees.html#cb477-4"></a></span>
<span id="cb477-5"><a href="decision-trees.html#cb477-5"></a><span class="co"># create an additional variable</span></span>
<span id="cb477-6"><a href="decision-trees.html#cb477-6"></a>Hitters<span class="op">$</span>High &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(Hitters<span class="op">$</span>Salary <span class="op">&gt;=</span><span class="st"> </span></span>
<span id="cb477-7"><a href="decision-trees.html#cb477-7"></a><span class="st">                                </span><span class="kw">quantile</span>(Hitters<span class="op">$</span>Salary, <span class="fl">0.75</span>), <span class="st">&quot;High&quot;</span>, <span class="st">&quot;Low&quot;</span>))</span>
<span id="cb477-8"><a href="decision-trees.html#cb477-8"></a></span>
<span id="cb477-9"><a href="decision-trees.html#cb477-9"></a><span class="co"># split the dataset</span></span>
<span id="cb477-10"><a href="decision-trees.html#cb477-10"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb477-11"><a href="decision-trees.html#cb477-11"></a>index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(Hitters), <span class="kw">nrow</span>(Hitters) <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span>)</span>
<span id="cb477-12"><a href="decision-trees.html#cb477-12"></a>Hitters_train &lt;-<span class="st"> </span>Hitters[index, ]</span>
<span id="cb477-13"><a href="decision-trees.html#cb477-13"></a>Hitters_test &lt;-<span class="st"> </span>Hitters[<span class="op">-</span>index, ]</span></code></pre></div>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="decision-trees.html#cb478-1"></a><span class="co"># fit classification tree</span></span>
<span id="cb478-2"><a href="decision-trees.html#cb478-2"></a>fit &lt;-<span class="st"> </span><span class="kw">tree</span>(High <span class="op">~</span>. <span class="op">-</span><span class="st"> </span>Salary, <span class="dt">data =</span> Hitters_train)</span>
<span id="cb478-3"><a href="decision-trees.html#cb478-3"></a><span class="kw">summary</span>(fit)</span>
<span id="cb478-4"><a href="decision-trees.html#cb478-4"></a><span class="co">## </span></span>
<span id="cb478-5"><a href="decision-trees.html#cb478-5"></a><span class="co">## Classification tree:</span></span>
<span id="cb478-6"><a href="decision-trees.html#cb478-6"></a><span class="co">## tree(formula = High ~ . - Salary, data = Hitters_train)</span></span>
<span id="cb478-7"><a href="decision-trees.html#cb478-7"></a><span class="co">## Variables actually used in tree construction:</span></span>
<span id="cb478-8"><a href="decision-trees.html#cb478-8"></a><span class="co">## [1] &quot;CAtBat&quot;  &quot;CWalks&quot;  &quot;AtBat&quot;   &quot;Hits&quot;    &quot;CRuns&quot;   &quot;PutOuts&quot; &quot;Assists&quot;</span></span>
<span id="cb478-9"><a href="decision-trees.html#cb478-9"></a><span class="co">## Number of terminal nodes:  8 </span></span>
<span id="cb478-10"><a href="decision-trees.html#cb478-10"></a><span class="co">## Residual mean deviance:  0.3054 = 37.56 / 123 </span></span>
<span id="cb478-11"><a href="decision-trees.html#cb478-11"></a><span class="co">## Misclassification error rate: 0.08397 = 11 / 131</span></span>
<span id="cb478-12"><a href="decision-trees.html#cb478-12"></a>(result &lt;-<span class="st"> </span><span class="kw">table</span>(Hitters_test<span class="op">$</span>High, <span class="kw">predict</span>(fit, Hitters_test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)))</span>
<span id="cb478-13"><a href="decision-trees.html#cb478-13"></a><span class="co">##       </span></span>
<span id="cb478-14"><a href="decision-trees.html#cb478-14"></a><span class="co">##        High Low</span></span>
<span id="cb478-15"><a href="decision-trees.html#cb478-15"></a><span class="co">##   High   20  12</span></span>
<span id="cb478-16"><a href="decision-trees.html#cb478-16"></a><span class="co">##   Low    13  87</span></span>
<span id="cb478-17"><a href="decision-trees.html#cb478-17"></a><span class="kw">sum</span>(<span class="kw">diag</span>(result)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(result)</span>
<span id="cb478-18"><a href="decision-trees.html#cb478-18"></a><span class="co">## [1] 0.8106061</span></span></code></pre></div>
<p>The classification accuracy in the training data is <span class="math inline">\((131 - 11)/131 = 0.916\)</span>. The classification accuracy in the testing data is <span class="math inline">\(0.811\)</span>.</p>
<p>Visualize the classification rules using tree:</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="decision-trees.html#cb479-1"></a><span class="kw">plot</span>(fit)</span>
<span id="cb479-2"><a href="decision-trees.html#cb479-2"></a><span class="kw">text</span>(fit)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-538-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Let’s try to see if pruning the tree will improve the testing accuracy. Set <code>FUN = prune.misclass</code> to tell the function to use misclassification error as the evaluation metric in CV.</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="decision-trees.html#cb480-1"></a>cv_fit &lt;-<span class="st"> </span><span class="kw">cv.tree</span>(fit, <span class="dt">FUN =</span> prune.misclass) </span>
<span id="cb480-2"><a href="decision-trees.html#cb480-2"></a>cv_fit</span>
<span id="cb480-3"><a href="decision-trees.html#cb480-3"></a><span class="co">## $size</span></span>
<span id="cb480-4"><a href="decision-trees.html#cb480-4"></a><span class="co">## [1] 8 4 3 1</span></span>
<span id="cb480-5"><a href="decision-trees.html#cb480-5"></a><span class="co">## </span></span>
<span id="cb480-6"><a href="decision-trees.html#cb480-6"></a><span class="co">## $dev</span></span>
<span id="cb480-7"><a href="decision-trees.html#cb480-7"></a><span class="co">## [1] 27 27 20 40</span></span>
<span id="cb480-8"><a href="decision-trees.html#cb480-8"></a><span class="co">## </span></span>
<span id="cb480-9"><a href="decision-trees.html#cb480-9"></a><span class="co">## $k</span></span>
<span id="cb480-10"><a href="decision-trees.html#cb480-10"></a><span class="co">## [1] -Inf    0    1   13</span></span>
<span id="cb480-11"><a href="decision-trees.html#cb480-11"></a><span class="co">## </span></span>
<span id="cb480-12"><a href="decision-trees.html#cb480-12"></a><span class="co">## $method</span></span>
<span id="cb480-13"><a href="decision-trees.html#cb480-13"></a><span class="co">## [1] &quot;misclass&quot;</span></span>
<span id="cb480-14"><a href="decision-trees.html#cb480-14"></a><span class="co">## </span></span>
<span id="cb480-15"><a href="decision-trees.html#cb480-15"></a><span class="co">## attr(,&quot;class&quot;)</span></span>
<span id="cb480-16"><a href="decision-trees.html#cb480-16"></a><span class="co">## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</span></span>
<span id="cb480-17"><a href="decision-trees.html#cb480-17"></a><span class="kw">plot</span>(cv_fit<span class="op">$</span>size, cv_fit<span class="op">$</span>dev, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>) </span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-539-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p><code>size</code>: number of terminal nodes of the tree</p>
<p><code>dev</code>: cross-validation errors</p>
<p>We will choose the <code>size</code> with the minimum <code>dev</code>.</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="decision-trees.html#cb481-1"></a>prune_fit &lt;-<span class="st"> </span><span class="kw">prune.misclass</span>(fit, <span class="dt">best =</span> <span class="dv">3</span>)</span>
<span id="cb481-2"><a href="decision-trees.html#cb481-2"></a>(result &lt;-<span class="st"> </span><span class="kw">table</span>(Hitters_test<span class="op">$</span>High, <span class="kw">predict</span>(prune_fit, Hitters_test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)))</span>
<span id="cb481-3"><a href="decision-trees.html#cb481-3"></a><span class="co">##       </span></span>
<span id="cb481-4"><a href="decision-trees.html#cb481-4"></a><span class="co">##        High Low</span></span>
<span id="cb481-5"><a href="decision-trees.html#cb481-5"></a><span class="co">##   High   21  11</span></span>
<span id="cb481-6"><a href="decision-trees.html#cb481-6"></a><span class="co">##   Low    16  84</span></span>
<span id="cb481-7"><a href="decision-trees.html#cb481-7"></a><span class="kw">sum</span>(<span class="kw">diag</span>(result)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(result)</span>
<span id="cb481-8"><a href="decision-trees.html#cb481-8"></a><span class="co">## [1] 0.7954545</span></span></code></pre></div>
<p>For this particular example, the classification accuracy actually becomes lower when we prune the tree. However, the pruned tree has only <span class="math inline">\(3\)</span> terminal nodes, which can be easier for interpretation.</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="decision-trees.html#cb482-1"></a><span class="kw">plot</span>(prune_fit)</span>
<span id="cb482-2"><a href="decision-trees.html#cb482-2"></a><span class="kw">text</span>(prune_fit)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-541-1.png" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="regression-tree" class="section level3 hasAnchor">
<h3><span class="header-section-number">18.4.2</span> Regression Tree<a href="decision-trees.html#regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fitting regression tree and finding the MSE in test data</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="decision-trees.html#cb483-1"></a><span class="co"># Fit regression tree</span></span>
<span id="cb483-2"><a href="decision-trees.html#cb483-2"></a>reg_tree &lt;-<span class="st"> </span><span class="kw">tree</span>(Salary <span class="op">~</span>. <span class="op">-</span><span class="st"> </span>High, Hitters_train)</span>
<span id="cb483-3"><a href="decision-trees.html#cb483-3"></a></span>
<span id="cb483-4"><a href="decision-trees.html#cb483-4"></a><span class="co"># MSE in test data</span></span>
<span id="cb483-5"><a href="decision-trees.html#cb483-5"></a><span class="kw">mean</span>((Hitters_test<span class="op">$</span>Salary <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(reg_tree, Hitters_test))<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb483-6"><a href="decision-trees.html#cb483-6"></a><span class="co">## [1] 122872.5</span></span></code></pre></div>
<p>Tree without pruning</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="decision-trees.html#cb484-1"></a><span class="kw">plot</span>(reg_tree)</span>
<span id="cb484-2"><a href="decision-trees.html#cb484-2"></a><span class="kw">text</span>(reg_tree)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-543-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Use cross validation to determine the optimal choice of size (the number of terminal nodes)</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="decision-trees.html#cb485-1"></a><span class="kw">set.seed</span>(<span class="dv">3</span>) <span class="co"># the result of CV is also random because we partition the data randomly</span></span>
<span id="cb485-2"><a href="decision-trees.html#cb485-2"></a>cv &lt;-<span class="st"> </span><span class="kw">cv.tree</span>(reg_tree)</span>
<span id="cb485-3"><a href="decision-trees.html#cb485-3"></a><span class="kw">plot</span>(cv<span class="op">$</span>size, cv<span class="op">$</span>dev, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-544-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Prune the tree:</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="decision-trees.html#cb486-1"></a><span class="co"># best size</span></span>
<span id="cb486-2"><a href="decision-trees.html#cb486-2"></a>cv<span class="op">$</span>size[<span class="kw">which.min</span>(cv<span class="op">$</span>dev)] </span>
<span id="cb486-3"><a href="decision-trees.html#cb486-3"></a><span class="co">## [1] 5</span></span>
<span id="cb486-4"><a href="decision-trees.html#cb486-4"></a></span>
<span id="cb486-5"><a href="decision-trees.html#cb486-5"></a>prune_reg_tree &lt;-<span class="st"> </span><span class="kw">prune.tree</span>(reg_tree, <span class="dt">best =</span> cv<span class="op">$</span>size[<span class="kw">which.min</span>(cv<span class="op">$</span>dev)])</span>
<span id="cb486-6"><a href="decision-trees.html#cb486-6"></a></span>
<span id="cb486-7"><a href="decision-trees.html#cb486-7"></a><span class="co"># MSE in test data</span></span>
<span id="cb486-8"><a href="decision-trees.html#cb486-8"></a><span class="kw">mean</span>((Hitters_test<span class="op">$</span>Salary <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(prune_reg_tree, Hitters_test))<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb486-9"><a href="decision-trees.html#cb486-9"></a><span class="co">## [1] 122267.2</span></span></code></pre></div>
<p>Pruned tree:</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="decision-trees.html#cb487-1"></a><span class="kw">plot</span>(prune_reg_tree)</span>
<span id="cb487-2"><a href="decision-trees.html#cb487-2"></a><span class="kw">text</span>(prune_reg_tree)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-546-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>The performance (in terms of MSE) of the pruned tree in test data is similar to the tree without pruning.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regularization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ensemble-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Book.pdf", "Book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
