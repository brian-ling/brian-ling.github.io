<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 k-means Clustering | STAT 362 R for Data Science</title>
  <meta name="description" content="Notes for STAT 362" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 k-means Clustering | STAT 362 R for Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes for STAT 362" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 k-means Clustering | STAT 362 R for Data Science" />
  
  <meta name="twitter:description" content="Notes for STAT 362" />
  

<meta name="author" content="Brian Ling" />


<meta name="date" content="2022-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic-regression-model.html"/>
<link rel="next" href="neural-networks.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> What is R and RStudio?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-will-you-learn-in-this-course"><i class="fa fa-check"></i><b>1.2</b> What will you learn in this course?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#r-and-r-as-a-programming-language"><i class="fa fa-check"></i><b>1.2.1</b> R and R as a programming language</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#data-wrangling"><i class="fa fa-check"></i><b>1.2.2</b> Data Wrangling</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#data-visualization"><i class="fa fa-check"></i><b>1.2.3</b> Data Visualization</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#statistical-inference"><i class="fa fa-check"></i><b>1.2.4</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2.5" data-path="introduction.html"><a href="introduction.html#machine-learning"><i class="fa fa-check"></i><b>1.2.5</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2.6" data-path="introduction.html"><a href="introduction.html#some-numerical-methods"><i class="fa fa-check"></i><b>1.2.6</b> Some Numerical Methods</a></li>
<li class="chapter" data-level="1.2.7" data-path="introduction.html"><a href="introduction.html#lastly"><i class="fa fa-check"></i><b>1.2.7</b> Lastly</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#lets-get-started"><i class="fa fa-check"></i><b>1.3</b> Let’s Get Started</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#r-data-structures"><i class="fa fa-check"></i><b>1.4</b> R Data Structures</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#vectors"><i class="fa fa-check"></i><b>1.4.1</b> Vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#factors"><i class="fa fa-check"></i><b>1.4.2</b> Factors</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction.html"><a href="introduction.html#matrix"><i class="fa fa-check"></i><b>1.4.3</b> Matrix</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction.html"><a href="introduction.html#lists"><i class="fa fa-check"></i><b>1.4.4</b> Lists</a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction.html"><a href="introduction.html#data-frames"><i class="fa fa-check"></i><b>1.4.5</b> Data frames</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#operators"><i class="fa fa-check"></i><b>1.5</b> Operators</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#vectorized-operators"><i class="fa fa-check"></i><b>1.5.1</b> Vectorized Operators</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#built-in-functions"><i class="fa fa-check"></i><b>1.6</b> Built-in Functions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#sort"><i class="fa fa-check"></i><b>1.6.1</b> <code>sort()</code></a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction.html"><a href="introduction.html#seq"><i class="fa fa-check"></i><b>1.6.2</b> <code>seq()</code></a></li>
<li class="chapter" data-level="1.6.3" data-path="introduction.html"><a href="introduction.html#rep"><i class="fa fa-check"></i><b>1.6.3</b> <code>rep()</code></a></li>
<li class="chapter" data-level="1.6.4" data-path="introduction.html"><a href="introduction.html#pmax-pmin"><i class="fa fa-check"></i><b>1.6.4</b> <code>pmax</code>, <code>pmin</code></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#some-useful-rstudio-shortcuts"><i class="fa fa-check"></i><b>1.7</b> Some Useful RStudio Shortcuts</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#comments-to-exercises"><i class="fa fa-check"></i><b>1.9</b> Comments to Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Probability Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="probability.html"><a href="probability.html#common-distributions"><i class="fa fa-check"></i><b>2.1.1</b> Common Distributions</a></li>
<li class="chapter" data-level="2.1.2" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>2.1.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability.html"><a href="probability.html#simulation"><i class="fa fa-check"></i><b>2.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>3</b> Programming in R</a><ul>
<li class="chapter" data-level="3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#writing-functions-in-r"><i class="fa fa-check"></i><b>3.1</b> Writing functions in R</a></li>
<li class="chapter" data-level="3.2" data-path="programming-in-r.html"><a href="programming-in-r.html#control-flow"><i class="fa fa-check"></i><b>3.2</b> Control Flow</a><ul>
<li class="chapter" data-level="3.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#for-loop"><i class="fa fa-check"></i><b>3.2.1</b> for loop</a></li>
<li class="chapter" data-level="3.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#while-loop"><i class="fa fa-check"></i><b>3.2.2</b> while loop</a></li>
<li class="chapter" data-level="3.2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond"><i class="fa fa-check"></i><b>3.2.3</b> if (cond)</a></li>
<li class="chapter" data-level="3.2.4" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond-else-expr"><i class="fa fa-check"></i><b>3.2.4</b> if (cond) else expr</a></li>
<li class="chapter" data-level="3.2.5" data-path="programming-in-r.html"><a href="programming-in-r.html#if-else-ladder"><i class="fa fa-check"></i><b>3.2.5</b> If else ladder</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="programming-in-r.html"><a href="programming-in-r.html#automatically-reindent-code"><i class="fa fa-check"></i><b>3.3</b> Automatically Reindent Code</a></li>
<li class="chapter" data-level="3.4" data-path="programming-in-r.html"><a href="programming-in-r.html#speed-consideration"><i class="fa fa-check"></i><b>3.4</b> Speed Consideration</a></li>
<li class="chapter" data-level="3.5" data-path="programming-in-r.html"><a href="programming-in-r.html#another-simulation-example"><i class="fa fa-check"></i><b>3.5</b> Another Simulation Example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html"><i class="fa fa-check"></i><b>4</b> Creating Some Basic Plots</a><ul>
<li class="chapter" data-level="4.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#scatter-plot"><i class="fa fa-check"></i><b>4.1</b> Scatter Plot</a></li>
<li class="chapter" data-level="4.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#line-graph"><i class="fa fa-check"></i><b>4.2</b> Line Graph</a></li>
<li class="chapter" data-level="4.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#bar-chart"><i class="fa fa-check"></i><b>4.3</b> Bar Chart</a></li>
<li class="chapter" data-level="4.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#histogram"><i class="fa fa-check"></i><b>4.4</b> Histogram</a></li>
<li class="chapter" data-level="4.5" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#box-plot"><i class="fa fa-check"></i><b>4.5</b> Box Plot</a></li>
<li class="chapter" data-level="4.6" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#plotting-a-function-curve"><i class="fa fa-check"></i><b>4.6</b> Plotting a function curve</a></li>
<li class="chapter" data-level="4.7" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#more-on-plots-with-base-r"><i class="fa fa-check"></i><b>4.7</b> More on plots with base R</a><ul>
<li class="chapter" data-level="4.7.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#multi-frame-plot"><i class="fa fa-check"></i><b>4.7.1</b> Multi-frame plot</a></li>
<li class="chapter" data-level="4.7.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#subsubsec:type_of_plot"><i class="fa fa-check"></i><b>4.7.2</b> Type of Plot</a></li>
<li class="chapter" data-level="4.7.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#parameters-of-a-plot"><i class="fa fa-check"></i><b>4.7.3</b> Parameters of a plot</a></li>
<li class="chapter" data-level="4.7.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#elements-on-plot"><i class="fa fa-check"></i><b>4.7.4</b> Elements on plot</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#summary-of-ggplot"><i class="fa fa-check"></i><b>4.8</b> Summary of <code>ggplot</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html"><i class="fa fa-check"></i><b>5</b> Managing Data with R</a><ul>
<li class="chapter" data-level="5.1" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a></li>
<li class="chapter" data-level="5.2" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#saving-loading-and-removing-r-data-structures"><i class="fa fa-check"></i><b>5.2</b> Saving, loading, and removing R data structures</a></li>
<li class="chapter" data-level="5.3" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#importing-and-saving-data-from-csv-files"><i class="fa fa-check"></i><b>5.3</b> Importing and saving data from CSV files</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html"><i class="fa fa-check"></i><b>6</b> Review (Chapter 1-5)</a><ul>
<li class="chapter" data-level="6.1" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#simulation-1"><i class="fa fa-check"></i><b>6.1</b> Simulation</a></li>
<li class="chapter" data-level="6.2" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#matrix-1"><i class="fa fa-check"></i><b>6.2</b> Matrix</a></li>
<li class="chapter" data-level="6.3" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#basic-operation"><i class="fa fa-check"></i><b>6.3</b> Basic Operation</a></li>
<li class="chapter" data-level="6.4" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#some-basic-plots-in-r"><i class="fa fa-check"></i><b>6.4</b> Some basic plots in R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html"><i class="fa fa-check"></i><b>7</b> Data Transformation with <code>dplyr</code></a><ul>
<li class="chapter" data-level="7.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#arrange"><i class="fa fa-check"></i><b>7.2</b> <code>arrange()</code></a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-2"><i class="fa fa-check"></i><b>7.2.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#filter"><i class="fa fa-check"></i><b>7.3</b> <code>filter()</code></a><ul>
<li class="chapter" data-level="7.3.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-3"><i class="fa fa-check"></i><b>7.3.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#select"><i class="fa fa-check"></i><b>7.4</b> <code>select()</code></a><ul>
<li class="chapter" data-level="7.4.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-4"><i class="fa fa-check"></i><b>7.4.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#mutate"><i class="fa fa-check"></i><b>7.5</b> <code>mutate()</code></a><ul>
<li class="chapter" data-level="7.5.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-5"><i class="fa fa-check"></i><b>7.5.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summarize-group_by"><i class="fa fa-check"></i><b>7.6</b> <code>summarize(), group_by()</code></a></li>
<li class="chapter" data-level="7.7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#combining-multiple-operations-with-pipe"><i class="fa fa-check"></i><b>7.7</b> Combining Multiple Operations with Pipe <code>%&gt;%</code></a></li>
<li class="chapter" data-level="7.8" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summary"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html"><i class="fa fa-check"></i><b>8</b> Data Visualization with ggplot2</a><ul>
<li class="chapter" data-level="8.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts"><i class="fa fa-check"></i><b>8.1</b> Bar charts</a></li>
<li class="chapter" data-level="8.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graph-1"><i class="fa fa-check"></i><b>8.2</b> Line Graph</a></li>
<li class="chapter" data-level="8.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plots"><i class="fa fa-check"></i><b>8.3</b> Scatter Plots</a><ul>
<li class="chapter" data-level="8.3.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#overplotting"><i class="fa fa-check"></i><b>8.3.1</b> Overplotting</a></li>
<li class="chapter" data-level="8.3.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#labelling-points-in-a-scatter-plot"><i class="fa fa-check"></i><b>8.3.2</b> Labelling points in a scatter plot</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions"><i class="fa fa-check"></i><b>8.4</b> Summarizing Data Distributions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#histogram-1"><i class="fa fa-check"></i><b>8.4.1</b> Histogram</a></li>
<li class="chapter" data-level="8.4.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#kernel-density-estimate"><i class="fa fa-check"></i><b>8.4.2</b> Kernel Density Estimate</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots"><i class="fa fa-check"></i><b>8.5</b> Saving your plots</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-pdf-vector-files"><i class="fa fa-check"></i><b>8.5.1</b> Outputting to pdf vector files</a></li>
<li class="chapter" data-level="8.5.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-bitmap-files"><i class="fa fa-check"></i><b>8.5.2</b> Outputting to bitmap files</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summary-1"><i class="fa fa-check"></i><b>8.6</b> Summary</a><ul>
<li class="chapter" data-level="8.6.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#combining-multiple-operations-with-pipe-1"><i class="fa fa-check"></i><b>8.6.1</b> Combining multiple operations with pipe <code>%&gt;%</code></a></li>
<li class="chapter" data-level="8.6.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts-1"><i class="fa fa-check"></i><b>8.6.2</b> Bar charts</a></li>
<li class="chapter" data-level="8.6.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graphs"><i class="fa fa-check"></i><b>8.6.3</b> Line graphs</a></li>
<li class="chapter" data-level="8.6.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plot-1"><i class="fa fa-check"></i><b>8.6.4</b> Scatter plot</a></li>
<li class="chapter" data-level="8.6.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions-1"><i class="fa fa-check"></i><b>8.6.5</b> Summarizing data distributions</a></li>
<li class="chapter" data-level="8.6.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots-1"><i class="fa fa-check"></i><b>8.6.6</b> Saving your plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference in R</a><ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.1</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#exercises-on-mle"><i class="fa fa-check"></i><b>9.1.1</b> Exercises on MLE</a></li>
<li class="chapter" data-level="9.1.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#summary-2"><i class="fa fa-check"></i><b>9.1.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#interval-estimation-and-hypothesis-testing"><i class="fa fa-check"></i><b>9.2</b> Interval Estimation and Hypothesis Testing</a><ul>
<li class="chapter" data-level="9.2.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.2.1</b> Examples of Hypothesis Testing</a></li>
<li class="chapter" data-level="9.2.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#null-hypotheses-alternative-hypotheses-and-p-values"><i class="fa fa-check"></i><b>9.2.2</b> Null Hypotheses, Alternative Hypotheses, and p-values</a></li>
<li class="chapter" data-level="9.2.3" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#type-i-error-and-type-ii-error"><i class="fa fa-check"></i><b>9.2.3</b> Type I error and Type II error</a></li>
<li class="chapter" data-level="9.2.4" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-for-mean-of-one-sample"><i class="fa fa-check"></i><b>9.2.4</b> Inference for Mean of One Sample</a></li>
<li class="chapter" data-level="9.2.5" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#comparing-the-means-of-two-samples"><i class="fa fa-check"></i><b>9.2.5</b> Comparing the means of two samples</a></li>
<li class="chapter" data-level="9.2.6" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-of-a-sample-proportion"><i class="fa fa-check"></i><b>9.2.6</b> Inference of a Sample Proportion</a></li>
<li class="chapter" data-level="9.2.7" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-groups-for-equal-proportions"><i class="fa fa-check"></i><b>9.2.7</b> Testing groups for equal proportions</a></li>
<li class="chapter" data-level="9.2.8" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-if-two-samples-have-the-same-underlying-distribution"><i class="fa fa-check"></i><b>9.2.8</b> Testing if two samples have the same underlying distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html"><i class="fa fa-check"></i><b>10</b> Root finding and optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#root-finding"><i class="fa fa-check"></i><b>10.1</b> Root Finding</a><ul>
<li class="chapter" data-level="10.1.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#bisection-method"><i class="fa fa-check"></i><b>10.1.1</b> Bisection Method</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method"><i class="fa fa-check"></i><b>10.2</b> Newton-Raphson Method</a></li>
<li class="chapter" data-level="10.3" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#minimization-and-maximization"><i class="fa fa-check"></i><b>10.3</b> Minimization and Maximization</a><ul>
<li class="chapter" data-level="10.3.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method-1"><i class="fa fa-check"></i><b>10.3.1</b> Newton-Raphson Method</a></li>
<li class="chapter" data-level="10.3.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#gradient-descent"><i class="fa fa-check"></i><b>10.3.2</b> Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#optim"><i class="fa fa-check"></i><b>10.4</b> <code>optim</code></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>11</b> k-nearest neighbors</a><ul>
<li class="chapter" data-level="11.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#feature-scaling"><i class="fa fa-check"></i><b>11.2</b> Feature Scaling</a></li>
<li class="chapter" data-level="11.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-classifying-breast-cancers"><i class="fa fa-check"></i><b>11.3</b> Example: Classifying Breast Cancers</a><ul>
<li class="chapter" data-level="11.3.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#evaluating-model-performance"><i class="fa fa-check"></i><b>11.3.1</b> Evaluating Model Performance</a></li>
<li class="chapter" data-level="11.3.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#using-z-score-standardization"><i class="fa fa-check"></i><b>11.3.2</b> Using <span class="math inline">\(z\)</span>-score standardization</a></li>
<li class="chapter" data-level="11.3.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#testing-alternative-values-of-k"><i class="fa fa-check"></i><b>11.3.3</b> Testing alternative values of <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="classification-trees.html"><a href="classification-trees.html"><i class="fa fa-check"></i><b>12</b> Classification Trees</a><ul>
<li class="chapter" data-level="12.1" data-path="classification-trees.html"><a href="classification-trees.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="classification-trees.html"><a href="classification-trees.html#the-c5.0-classification-tree-algorithm"><i class="fa fa-check"></i><b>12.2</b> The C5.0 classification tree algorithm</a><ul>
<li class="chapter" data-level="12.2.1" data-path="classification-trees.html"><a href="classification-trees.html#choosing-the-best-split"><i class="fa fa-check"></i><b>12.2.1</b> Choosing the best split</a></li>
<li class="chapter" data-level="12.2.2" data-path="classification-trees.html"><a href="classification-trees.html#pruning-the-decision-tree"><i class="fa fa-check"></i><b>12.2.2</b> Pruning the decision tree</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="classification-trees.html"><a href="classification-trees.html#example-iris"><i class="fa fa-check"></i><b>12.3</b> Example: <code>iris</code></a></li>
<li class="chapter" data-level="12.4" data-path="classification-trees.html"><a href="classification-trees.html#example-identifying-risky-bank-loans"><i class="fa fa-check"></i><b>12.4</b> Example: identifying risky bank loans</a><ul>
<li class="chapter" data-level="12.4.1" data-path="classification-trees.html"><a href="classification-trees.html#adaptive-boosting"><i class="fa fa-check"></i><b>12.4.1</b> Adaptive boosting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>13</b> Linear Regression Models</a><ul>
<li class="chapter" data-level="13.1" data-path="linear-regression-models.html"><a href="linear-regression-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>13.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="13.2" data-path="linear-regression-models.html"><a href="linear-regression-models.html#smoothed-conditional-means"><i class="fa fa-check"></i><b>13.2</b> Smoothed Conditional Means</a></li>
<li class="chapter" data-level="13.3" data-path="linear-regression-models.html"><a href="linear-regression-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>13.3</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="13.4" data-path="linear-regression-models.html"><a href="linear-regression-models.html#example-diamonds"><i class="fa fa-check"></i><b>13.4</b> Example: <code>diamonds</code></a></li>
<li class="chapter" data-level="13.5" data-path="linear-regression-models.html"><a href="linear-regression-models.html#categorical-predictors"><i class="fa fa-check"></i><b>13.5</b> Categorical Predictors</a></li>
<li class="chapter" data-level="13.6" data-path="linear-regression-models.html"><a href="linear-regression-models.html#compare-models-using-anova"><i class="fa fa-check"></i><b>13.6</b> Compare models using ANOVA</a></li>
<li class="chapter" data-level="13.7" data-path="linear-regression-models.html"><a href="linear-regression-models.html#prediction"><i class="fa fa-check"></i><b>13.7</b> Prediction</a></li>
<li class="chapter" data-level="13.8" data-path="linear-regression-models.html"><a href="linear-regression-models.html#interaction-terms"><i class="fa fa-check"></i><b>13.8</b> Interaction Terms</a></li>
<li class="chapter" data-level="13.9" data-path="linear-regression-models.html"><a href="linear-regression-models.html#variable-transformation"><i class="fa fa-check"></i><b>13.9</b> Variable Transformation</a></li>
<li class="chapter" data-level="13.10" data-path="linear-regression-models.html"><a href="linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>13.10</b> Polynomial Regression</a></li>
<li class="chapter" data-level="13.11" data-path="linear-regression-models.html"><a href="linear-regression-models.html#stepwise-regression"><i class="fa fa-check"></i><b>13.11</b> Stepwise regression</a></li>
<li class="chapter" data-level="13.12" data-path="linear-regression-models.html"><a href="linear-regression-models.html#best-subset"><i class="fa fa-check"></i><b>13.12</b> Best subset</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="logistic-regression-model.html"><a href="logistic-regression-model.html"><i class="fa fa-check"></i><b>14</b> Logistic Regression Model</a></li>
<li class="chapter" data-level="15" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>15</b> k-means Clustering</a><ul>
<li class="chapter" data-level="15.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-4"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications"><i class="fa fa-check"></i><b>15.2</b> Applications</a><ul>
<li class="chapter" data-level="15.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#cluster-analysis"><i class="fa fa-check"></i><b>15.2.1</b> Cluster Analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#image-segementation"><i class="fa fa-check"></i><b>15.2.2</b> Image Segementation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>16</b> Neural Networks</a><ul>
<li class="chapter" data-level="16.1" data-path="neural-networks.html"><a href="neural-networks.html#introduction-5"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="neural-networks.html"><a href="neural-networks.html#regression-predict-the-strength-of-concrete"><i class="fa fa-check"></i><b>16.2</b> Regression: Predict the Strength of Concrete</a><ul>
<li class="chapter" data-level="16.2.1" data-path="neural-networks.html"><a href="neural-networks.html#data"><i class="fa fa-check"></i><b>16.2.1</b> Data</a></li>
<li class="chapter" data-level="16.2.2" data-path="neural-networks.html"><a href="neural-networks.html#training-a-model"><i class="fa fa-check"></i><b>16.2.2</b> Training a model</a></li>
<li class="chapter" data-level="16.2.3" data-path="neural-networks.html"><a href="neural-networks.html#understanding-the-model"><i class="fa fa-check"></i><b>16.2.3</b> Understanding the model</a></li>
<li class="chapter" data-level="16.2.4" data-path="neural-networks.html"><a href="neural-networks.html#evaluating-the-performance"><i class="fa fa-check"></i><b>16.2.4</b> Evaluating the Performance</a></li>
<li class="chapter" data-level="16.2.5" data-path="neural-networks.html"><a href="neural-networks.html#improving-the-model"><i class="fa fa-check"></i><b>16.2.5</b> Improving the Model</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="neural-networks.html"><a href="neural-networks.html#classification"><i class="fa fa-check"></i><b>16.3</b> Classification</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html"><i class="fa fa-check"></i><b>17</b> Evaluating Model Performance</a><ul>
<li class="chapter" data-level="17.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#accuracy-and-confusion-matrix"><i class="fa fa-check"></i><b>17.1</b> Accuracy and Confusion Matrix</a></li>
<li class="chapter" data-level="17.2" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#other-measures-of-performance"><i class="fa fa-check"></i><b>17.2</b> Other Measures of Performance</a><ul>
<li class="chapter" data-level="17.2.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#the-kappa-statistic"><i class="fa fa-check"></i><b>17.2.1</b> The kappa statistic</a></li>
<li class="chapter" data-level="17.2.2" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>17.2.2</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="17.2.3" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#roc-and-auc"><i class="fa fa-check"></i><b>17.2.3</b> ROC and AUC</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#estimating-future-performances"><i class="fa fa-check"></i><b>17.3</b> Estimating future performances</a><ul>
<li class="chapter" data-level="17.3.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#cross-validation"><i class="fa fa-check"></i><b>17.3.1</b> Cross-validation</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 362 R for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="k-means-clustering" class="section level1">
<h1><span class="header-section-number">Chapter 15</span> k-means Clustering</h1>
<p>Reference: Ch9 of Machine Learning with R, Ch6 of R Graphics Cookbook</p>
<p>Packages used:</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="k-means-clustering.html#cb446-1"></a><span class="kw">library</span>(tidyverse) <span class="co"># for read_csv</span></span>
<span id="cb446-2"><a href="k-means-clustering.html#cb446-2"></a><span class="kw">library</span>(factoextra) <span class="co"># visualize clusering results</span></span>
<span id="cb446-3"><a href="k-means-clustering.html#cb446-3"></a><span class="kw">library</span>(jpeg) <span class="co"># readJPEG reads an image from a JPEG file/content into a raster array</span></span>
<span id="cb446-4"><a href="k-means-clustering.html#cb446-4"></a><span class="kw">library</span>(ggpubr)</span></code></pre></div>
<div id="introduction-4" class="section level2">
<h2><span class="header-section-number">15.1</span> Introduction</h2>
<p><strong>Clustering</strong></p>
<p>Clustering is an unsupervised learning task that divides data into clusters, or groups of “similar” items while data points in different clusters are very different. It is an unsupervised learning method as we do not have “labels” of the data. We do not have the ground truth to compare the results. Clustering will only tell you which groups of data are similar. One may get some meaningful interpretation of the groups by studying the members in each group, e.g., by calculating some summary statistics and making use of some visualization tools.</p>
<p><strong><span class="math inline">\(k\)</span>-means clustering</strong></p>
<p><span class="math inline">\(k\)</span>-means clustering is one of the most popular clustering methods. It is intended for situations in which all variables are of the <strong>numeric type</strong>.</p>
<p>In <span class="math inline">\(k\)</span>-means clustering, all the examples are assigned to one of the <span class="math inline">\(k\)</span> clusters, where <span class="math inline">\(k\)</span> is a positive integer that has been specified before performing the clustering. The goal is to assign similar data to the same cluster.</p>
<p><strong>Some Applications</strong></p>
<ol style="list-style-type: decimal">
<li><p>Cluster analysis: Interesting groups may be discovered, such as the groups of motor insurance policy holders with a high average claim cost, or the groups of clients in a banking database having a heavy investment in real estate. In marketing segmentation, we segment customers into groups with similar demographics or buying patterns for targeted marketing campaigns</p></li>
<li><p>Vector quantization: “<span class="math inline">\(k\)</span>-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors <span class="math inline">\(k\)</span>. The <span class="math inline">\(k\)</span>-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation.” See <a href="https://en.wikipedia.org/wiki/Image_segmentation" class="uri">https://en.wikipedia.org/wiki/Image_segmentation</a></p></li>
</ol>
<p><strong>Notation and Setting</strong></p>
<p>We have <span class="math inline">\(n\)</span> data points <span class="math inline">\(x_i = (x_{i1},\ldots,x_{ip})&#39; \in \mathbb{R}^p\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>. Each observation is uniquely labeled by an integer <span class="math inline">\(i \in \{1,\ldots,n\}\)</span>. We use the notation <span class="math inline">\(C_i\)</span> to denote which cluster the <span class="math inline">\(i\)</span>th observation is assigned to. For example, <span class="math inline">\(C_1 = 2\)</span> means the <span class="math inline">\(1\)</span>st observation is assigned to the <span class="math inline">\(2\)</span>nd cluster, and <span class="math inline">\(C_2 = 3\)</span> means the <span class="math inline">\(2\)</span>nd observation is assigned to the <span class="math inline">\(3\)</span>rd cluster.</p>
<p>We use</p>
<ul>
<li><span class="math inline">\(i\)</span> to index data</li>
<li><span class="math inline">\(j\)</span> to index cluster</li>
<li><span class="math inline">\(l\)</span> to index feature</li>
</ul>
<p><strong>Dissimilarity measure</strong></p>
<p>In clustering, one have to define a dissimilarity measure <span class="math inline">\(d\)</span>. In <span class="math inline">\(k\)</span>-means clustering, the <strong>squared</strong> Euclidean distance is used. Given two data points <span class="math inline">\(x_i = (x_{i1},\ldots,x_{ip})\)</span> and <span class="math inline">\(x_{i&#39;} = (x_{i&#39;1},\ldots,x_{i&#39;p})\)</span>, the squared Euclidean distance is given by
<span class="math display">\[\begin{equation*}
d(x_i, x_{i&#39;}) = \sum^p_{l=1} (x_{il} - x_{i&#39;l})^2 = ||x_i - x_{i&#39;}||^2.
\end{equation*}\]</span></p>
<p><strong>k-means clustering algorithm (Lloyd’s algorithm)</strong></p>
<ol style="list-style-type: decimal">
<li><p>Specify the number of clusters <span class="math inline">\(k\)</span></p></li>
<li><p>Randomly select <span class="math inline">\(k\)</span> data points as the centroids, denoted by <span class="math inline">\(\mu_1,\ldots,\mu_k\)</span>.</p></li>
<li><p>For each <span class="math inline">\(i=1,\ldots,n\)</span>, set
<span class="math display">\[
C_i = \text{argmin}_j ||x_i - \mu_j||^2.
\]</span>
In words, assign the closest cluster to the <span class="math inline">\(i\)</span>th observation.</p></li>
<li><p>For each <span class="math inline">\(j=1,\ldots,k\)</span>, <span class="math inline">\(l=1,\ldots,p\)</span>, set
<span class="math display">\[
\mu_{jl} = \frac{\sum^n_{i=1}I(C_i =j) x_{il}}{\sum^n_{i=1}I(C_i =j)}.
\]</span>
In words, <span class="math inline">\(\mu_j\)</span> is simply the mean of all the points assigned to <span class="math inline">\(j\)</span>th cluster. Note that <span class="math inline">\(\mu_j = (\mu_{j1},\ldots,\mu_{jp})\)</span>.</p></li>
<li><p>Repeat Steps 3 - 4 until the assignments do not change.</p></li>
</ol>
<p><strong>Remarks</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\text{argmin}\)</span> is the argument of the minimum. <span class="math inline">\(\text{argmin}_j ||x_i - \mu_j||^2\)</span> means the value of <span class="math inline">\(j\)</span> such that <span class="math inline">\(||x_i - \mu_j||^2\)</span> is minimum.</p></li>
<li><p><span class="math inline">\(I(C_i = j)\)</span> equals <span class="math inline">\(1\)</span> if <span class="math inline">\(C_i = j\)</span> and equals <span class="math inline">\(0\)</span> otherwise. The <span class="math inline">\(I\)</span> is the indicator function.</p></li>
<li><p><span class="math inline">\(k\)</span>-means is sensitive to the number of clusters. See the elbow method.</p></li>
<li><p><span class="math inline">\(k\)</span>-means is sensitive to the randomly-chosen cluster centers. Different initial centers may result in different clustering results. As a result, we should use multiple set of initial cluster centers and choose the best result (smallest within-group sum of squared errors, see the end of this chapter).</p></li>
<li><p>Scale your data before applying <span class="math inline">\(k\)</span>-means clustering.</p></li>
</ol>
<p><strong>For categorical data</strong>: one possible solution is to use <span class="math inline">\(k\)</span>-modes. The <span class="math inline">\(k\)</span>-modes algorithm uses a matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimize the clustering cost functions</p>
<p><strong>For mixed data (numeric + categorical features)</strong>: can use <span class="math inline">\(k\)</span>-prototypes. It integrates the <span class="math inline">\(k\)</span>-means and <span class="math inline">\(k\)</span>-modes algorithms using a combined dissimilarity measure.</p>
<p>R package: <code>clustMixType</code></p>
<p>See also Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values by Z. Huang (1998).</p>
</div>
<div id="applications" class="section level2">
<h2><span class="header-section-number">15.2</span> Applications</h2>
<div id="cluster-analysis" class="section level3">
<h3><span class="header-section-number">15.2.1</span> Cluster Analysis</h3>
<p>The <code>Mall_Customers.csv</code> (in onQ) contains the gender, age, annual income and spending score of some customers.</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="k-means-clustering.html#cb447-1"></a>mall &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/Mall_Customers.csv&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="k-means-clustering.html#cb448-1"></a>mall</span>
<span id="cb448-2"><a href="k-means-clustering.html#cb448-2"></a><span class="co">## # A tibble: 200 x 5</span></span>
<span id="cb448-3"><a href="k-means-clustering.html#cb448-3"></a><span class="co">##    CustomerID Gender   Age `Annual Income (k$)` `Spending Score (1-100)`</span></span>
<span id="cb448-4"><a href="k-means-clustering.html#cb448-4"></a><span class="co">##         &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;                &lt;dbl&gt;                    &lt;dbl&gt;</span></span>
<span id="cb448-5"><a href="k-means-clustering.html#cb448-5"></a><span class="co">##  1          1 Male      19                   15                       39</span></span>
<span id="cb448-6"><a href="k-means-clustering.html#cb448-6"></a><span class="co">##  2          2 Male      21                   15                       81</span></span>
<span id="cb448-7"><a href="k-means-clustering.html#cb448-7"></a><span class="co">##  3          3 Female    20                   16                        6</span></span>
<span id="cb448-8"><a href="k-means-clustering.html#cb448-8"></a><span class="co">##  4          4 Female    23                   16                       77</span></span>
<span id="cb448-9"><a href="k-means-clustering.html#cb448-9"></a><span class="co">##  5          5 Female    31                   17                       40</span></span>
<span id="cb448-10"><a href="k-means-clustering.html#cb448-10"></a><span class="co">##  6          6 Female    22                   17                       76</span></span>
<span id="cb448-11"><a href="k-means-clustering.html#cb448-11"></a><span class="co">##  7          7 Female    35                   18                        6</span></span>
<span id="cb448-12"><a href="k-means-clustering.html#cb448-12"></a><span class="co">##  8          8 Female    23                   18                       94</span></span>
<span id="cb448-13"><a href="k-means-clustering.html#cb448-13"></a><span class="co">##  9          9 Male      64                   19                        3</span></span>
<span id="cb448-14"><a href="k-means-clustering.html#cb448-14"></a><span class="co">## 10         10 Female    30                   19                       72</span></span>
<span id="cb448-15"><a href="k-means-clustering.html#cb448-15"></a><span class="co">## # ... with 190 more rows</span></span></code></pre></div>
<p>We will use the <code>kmeans()</code> function in R (contained in base R) to perform <span class="math inline">\(k\)</span>-means clustering.</p>
<ul>
<li><p><code>x</code>: your dataframe/ matrix</p></li>
<li><p><code>centers</code>: the number of clusters</p></li>
<li><p><code>nstart</code>: how many random sets should be chosen (the best result will be reported)</p></li>
</ul>
<p>Let’s perform a <span class="math inline">\(k\)</span>-means clustering on the customers using <code>Annual Income (k$)</code> and <code>Spending Score (1-100)</code> with <span class="math inline">\(k=3\)</span>.</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="k-means-clustering.html#cb449-1"></a>mall_kmeans &lt;-<span class="st"> </span><span class="kw">kmeans</span>(<span class="dt">x =</span> <span class="kw">scale</span>(mall[, <span class="dv">4</span><span class="op">:</span><span class="dv">5</span>]), <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</span></code></pre></div>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="k-means-clustering.html#cb450-1"></a><span class="co"># To view the cluster</span></span>
<span id="cb450-2"><a href="k-means-clustering.html#cb450-2"></a>mall_kmeans<span class="op">$</span>cluster</span>
<span id="cb450-3"><a href="k-means-clustering.html#cb450-3"></a><span class="co">##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</span></span>
<span id="cb450-4"><a href="k-means-clustering.html#cb450-4"></a><span class="co">##  [48] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</span></span>
<span id="cb450-5"><a href="k-means-clustering.html#cb450-5"></a><span class="co">##  [95] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2</span></span>
<span id="cb450-6"><a href="k-means-clustering.html#cb450-6"></a><span class="co">## [142] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1</span></span>
<span id="cb450-7"><a href="k-means-clustering.html#cb450-7"></a><span class="co">## [189] 2 1 2 1 2 1 2 1 2 1 2 1</span></span>
<span id="cb450-8"><a href="k-means-clustering.html#cb450-8"></a></span>
<span id="cb450-9"><a href="k-means-clustering.html#cb450-9"></a><span class="co"># To view the centers</span></span>
<span id="cb450-10"><a href="k-means-clustering.html#cb450-10"></a>mall_kmeans<span class="op">$</span>centers</span>
<span id="cb450-11"><a href="k-means-clustering.html#cb450-11"></a><span class="co">##   Annual Income (k$) Spending Score (1-100)</span></span>
<span id="cb450-12"><a href="k-means-clustering.html#cb450-12"></a><span class="co">## 1          0.9891010             1.23640011</span></span>
<span id="cb450-13"><a href="k-means-clustering.html#cb450-13"></a><span class="co">## 2          1.0066735            -1.22246770</span></span>
<span id="cb450-14"><a href="k-means-clustering.html#cb450-14"></a><span class="co">## 3         -0.6246222            -0.01435636</span></span></code></pre></div>
<p>Visualize the clusters:</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="k-means-clustering.html#cb451-1"></a><span class="kw">library</span>(factoextra)</span>
<span id="cb451-2"><a href="k-means-clustering.html#cb451-2"></a><span class="kw">fviz_cluster</span>(mall_kmeans,  <span class="dt">data =</span> mall[, <span class="dv">4</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-497-1.png" width="75%" style="display: block; margin: auto;" />
By looking at the plot, we see the 3 clusters are</p>
<ul>
<li>high annual income + low spending score</li>
<li>high annual income + high spending score</li>
<li>low annual income</li>
</ul>
<p>Let’s try <span class="math inline">\(k=5\)</span>:</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="k-means-clustering.html#cb452-1"></a>mall_kmeans &lt;-<span class="st"> </span><span class="kw">kmeans</span>(<span class="dt">x =</span> <span class="kw">scale</span>(mall[, <span class="dv">4</span><span class="op">:</span><span class="dv">5</span>]), <span class="dt">centers =</span> <span class="dv">5</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</span></code></pre></div>
<p>Visualizing the clusters:</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="k-means-clustering.html#cb453-1"></a><span class="kw">fviz_cluster</span>(mall_kmeans,  <span class="dt">data =</span> mall[, <span class="dv">4</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-499-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>The <span class="math inline">\(5\)</span> clusters are</p>
<ul>
<li>high annual income + low spending score</li>
<li>high annual income + high spending score</li>
<li>medium annual income + medium spending score</li>
<li>low annual income + high spending score</li>
<li>low annual income + low spending score</li>
</ul>
<p>Now, let’s use one more variable for clustering:</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="k-means-clustering.html#cb454-1"></a>mall_kmeans &lt;-<span class="st"> </span><span class="kw">kmeans</span>(<span class="dt">x =</span> <span class="kw">scale</span>(mall[, <span class="dv">3</span><span class="op">:</span><span class="dv">5</span>]), <span class="dt">centers =</span> <span class="dv">5</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</span></code></pre></div>
<p>To visualize more than 2 variables, we can use a dimension reduction technique called principal component analysis. The function <code>fviz_cluster</code> will automatically perform that and give you a <span class="math inline">\(2D\)</span>-plot using the first two principal comonents.</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="k-means-clustering.html#cb455-1"></a><span class="kw">fviz_cluster</span>(mall_kmeans,  <span class="dt">data =</span> mall[, <span class="dv">3</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-501-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>To understand more about the clusters, we should take a look at some plots and some summary statistics. An example is to use violin plots.</p>
<p><strong>Violin Plots</strong></p>
<p>A violin plot is a kernel density estimate, mirrored so that it forms a symmetrical shape. It is a helpful tool to compare multiple data distributions when we put several plots side by side. To provide additional information, we can also have box plots overlaid, with a white dot at the median.</p>
<p>We first add the cluster information to our dataset.</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="k-means-clustering.html#cb456-1"></a>mall_cluster &lt;-<span class="st"> </span><span class="kw">mutate</span>(mall, <span class="dt">Cluster =</span> <span class="kw">factor</span>(mall_kmeans<span class="op">$</span>cluster))</span></code></pre></div>
<p><strong>Create violin plots</strong></p>
<p><code>outlier.colour = NA</code>: do not display outliers</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="k-means-clustering.html#cb457-1"></a>mall_cluster <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb457-2"><a href="k-means-clustering.html#cb457-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Cluster, <span class="dt">y =</span> <span class="st">`</span><span class="dt">Annual Income (k$)</span><span class="st">`</span>)) <span class="op">+</span></span>
<span id="cb457-3"><a href="k-means-clustering.html#cb457-3"></a><span class="st">  </span><span class="kw">geom_violin</span>(<span class="dt">trim =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb457-4"><a href="k-means-clustering.html#cb457-4"></a><span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">width =</span> <span class="fl">.1</span>, <span class="dt">fill =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">outlier.colour =</span> <span class="ot">NA</span>) <span class="op">+</span></span>
<span id="cb457-5"><a href="k-means-clustering.html#cb457-5"></a><span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun =</span> median, <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">shape =</span> <span class="dv">21</span>, <span class="dt">size =</span> <span class="fl">2.5</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-503-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>The other two variables:</p>
<p><img src="Book_files/figure-html/unnamed-chunk-504-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p><img src="Book_files/figure-html/unnamed-chunk-505-1.png" width="75%" style="display: block; margin: auto;" />
<strong>Determine the optimal <span class="math inline">\(k\)</span></strong></p>
<p>The elbow method is a heuristic method to determine the number of clusters. We plot the total within-group sum of squared errors against the number of clusters. We pick the elbow of the curve as the number of clusters to use. The elbow (or knee) of a curve is a point where the curve visibly bends.</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="k-means-clustering.html#cb458-1"></a>WSS &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb458-2"><a href="k-means-clustering.html#cb458-2"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {</span>
<span id="cb458-3"><a href="k-means-clustering.html#cb458-3"></a>  <span class="co"># extract the total within-group sum of squared errors</span></span>
<span id="cb458-4"><a href="k-means-clustering.html#cb458-4"></a>  WSS[k] =<span class="st"> </span><span class="kw">kmeans</span>(<span class="dt">x =</span> <span class="kw">scale</span>(mall[, <span class="dv">3</span><span class="op">:</span><span class="dv">5</span>]), <span class="dt">centers =</span> k, <span class="dt">nstart =</span> <span class="dv">25</span>)<span class="op">$</span>tot.withinss</span>
<span id="cb458-5"><a href="k-means-clustering.html#cb458-5"></a>}</span>
<span id="cb458-6"><a href="k-means-clustering.html#cb458-6"></a></span>
<span id="cb458-7"><a href="k-means-clustering.html#cb458-7"></a><span class="kw">ggplot</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">y =</span> WSS)) <span class="op">+</span></span>
<span id="cb458-8"><a href="k-means-clustering.html#cb458-8"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb458-9"><a href="k-means-clustering.html#cb458-9"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb458-10"><a href="k-means-clustering.html#cb458-10"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb458-11"><a href="k-means-clustering.html#cb458-11"></a><span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;k&quot;</span>, <span class="dt">limits =</span> <span class="kw">factor</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)) <span class="op">+</span></span>
<span id="cb458-12"><a href="k-means-clustering.html#cb458-12"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Elbow Method&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-506-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>It seems to me that the curve bends at <span class="math inline">\(k=4\)</span>.</p>
</div>
<div id="image-segementation" class="section level3">
<h3><span class="header-section-number">15.2.2</span> Image Segementation</h3>
<p>Image segementation using <span class="math inline">\(k\)</span>-means clustering:</p>
<ul>
<li>Each pixel is a data point.</li>
<li>Group the pixels into <span class="math inline">\(k\)</span> different clusters</li>
</ul>
<p>We will use the <code>readJPEG()</code> function from the package <code>jpeg</code> to reads an image from a jpeg file into a raster array.</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="k-means-clustering.html#cb459-1"></a><span class="kw">library</span>(jpeg)</span>
<span id="cb459-2"><a href="k-means-clustering.html#cb459-2"></a></span>
<span id="cb459-3"><a href="k-means-clustering.html#cb459-3"></a><span class="co"># read the image into a raster array</span></span>
<span id="cb459-4"><a href="k-means-clustering.html#cb459-4"></a>image &lt;-<span class="st"> </span><span class="kw">readJPEG</span>(<span class="st">&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/image/flower_s.jpg&quot;</span>)</span>
<span id="cb459-5"><a href="k-means-clustering.html#cb459-5"></a></span>
<span id="cb459-6"><a href="k-means-clustering.html#cb459-6"></a><span class="co"># 3-way array (matrix = 2-way array)</span></span>
<span id="cb459-7"><a href="k-means-clustering.html#cb459-7"></a>(image_dim &lt;-<span class="st"> </span><span class="kw">dim</span>(image))</span>
<span id="cb459-8"><a href="k-means-clustering.html#cb459-8"></a><span class="co">## [1] 200 300   3</span></span>
<span id="cb459-9"><a href="k-means-clustering.html#cb459-9"></a></span>
<span id="cb459-10"><a href="k-means-clustering.html#cb459-10"></a><span class="co"># Assign RGB channels to each pixel </span></span>
<span id="cb459-11"><a href="k-means-clustering.html#cb459-11"></a>image_RGB &lt;-<span class="st"> </span><span class="kw">tibble</span>(</span>
<span id="cb459-12"><a href="k-means-clustering.html#cb459-12"></a>  <span class="dt">x =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>image_dim[<span class="dv">2</span>], <span class="dt">each =</span> image_dim[<span class="dv">1</span>]),</span>
<span id="cb459-13"><a href="k-means-clustering.html#cb459-13"></a>  <span class="dt">y =</span> <span class="kw">rep</span>(image_dim[<span class="dv">1</span>]<span class="op">:</span><span class="dv">1</span>, image_dim[<span class="dv">2</span>]),</span>
<span id="cb459-14"><a href="k-means-clustering.html#cb459-14"></a>  <span class="dt">R =</span> <span class="kw">as.vector</span>(image[, , <span class="dv">1</span>]),</span>
<span id="cb459-15"><a href="k-means-clustering.html#cb459-15"></a>  <span class="dt">G =</span> <span class="kw">as.vector</span>(image[, , <span class="dv">2</span>]),</span>
<span id="cb459-16"><a href="k-means-clustering.html#cb459-16"></a>  <span class="dt">B =</span> <span class="kw">as.vector</span>(image[, , <span class="dv">3</span>])</span>
<span id="cb459-17"><a href="k-means-clustering.html#cb459-17"></a>  )</span>
<span id="cb459-18"><a href="k-means-clustering.html#cb459-18"></a></span>
<span id="cb459-19"><a href="k-means-clustering.html#cb459-19"></a><span class="co"># use view(image_RGB) to view the resulting tibble</span></span></code></pre></div>
<p>Original Image</p>
<p><img src="image/flower_s.jpg" width="50%" style="display: block; margin: auto;" /></p>
<!-- ```{r eval = FALSE} -->
<!-- # Plot the image -->
<!-- ggplot(data = image, aes(x = x, y = y)) +  -->
<!--   geom_point(colour = rgb(image[c("R", "G", "B")])) + -->
<!--   labs(title = "Original Image") + -->
<!--   xlab("") + -->
<!--   ylab("")  -->
<!-- ``` -->
<p>Apply <span class="math inline">\(k\)</span>-means clustering:</p>
<p><img src="image/flower_cluster.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Code for creating the above images:</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="k-means-clustering.html#cb460-1"></a>cluster_plot &lt;-<span class="st"> </span><span class="kw">list</span>()</span>
<span id="cb460-2"><a href="k-means-clustering.html#cb460-2"></a>i &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb460-3"><a href="k-means-clustering.html#cb460-3"></a></span>
<span id="cb460-4"><a href="k-means-clustering.html#cb460-4"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">8</span>)) {</span>
<span id="cb460-5"><a href="k-means-clustering.html#cb460-5"></a>  i &lt;-<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb460-6"><a href="k-means-clustering.html#cb460-6"></a></span>
<span id="cb460-7"><a href="k-means-clustering.html#cb460-7"></a>  <span class="co"># perform k-means</span></span>
<span id="cb460-8"><a href="k-means-clustering.html#cb460-8"></a>    image_kmeans &lt;-<span class="st"> </span><span class="kw">kmeans</span>(image_RGB[, <span class="kw">c</span>(<span class="st">&quot;R&quot;</span>, <span class="st">&quot;G&quot;</span>, <span class="st">&quot;B&quot;</span>)], <span class="dt">centers =</span> k, <span class="dt">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb460-9"><a href="k-means-clustering.html#cb460-9"></a>    </span>
<span id="cb460-10"><a href="k-means-clustering.html#cb460-10"></a>  <span class="co"># for each pixel, use the colour of the center of the corresponding cluster</span></span>
<span id="cb460-11"><a href="k-means-clustering.html#cb460-11"></a>  cluster_color &lt;-<span class="st"> </span><span class="kw">rgb</span>(image_kmeans<span class="op">$</span>centers[image_kmeans<span class="op">$</span>cluster, ])</span>
<span id="cb460-12"><a href="k-means-clustering.html#cb460-12"></a>  </span>
<span id="cb460-13"><a href="k-means-clustering.html#cb460-13"></a>  cluster_plot[[i]] &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> image_RGB, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb460-14"><a href="k-means-clustering.html#cb460-14"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">colour =</span> cluster_color) <span class="op">+</span></span>
<span id="cb460-15"><a href="k-means-clustering.html#cb460-15"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste</span>(<span class="st">&quot;k = &quot;</span>, k)) <span class="op">+</span></span>
<span id="cb460-16"><a href="k-means-clustering.html#cb460-16"></a><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>) <span class="op">+</span></span>
<span id="cb460-17"><a href="k-means-clustering.html#cb460-17"></a><span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb460-18"><a href="k-means-clustering.html#cb460-18"></a>}</span>
<span id="cb460-19"><a href="k-means-clustering.html#cb460-19"></a></span>
<span id="cb460-20"><a href="k-means-clustering.html#cb460-20"></a><span class="co"># save the plots to your computer</span></span>
<span id="cb460-21"><a href="k-means-clustering.html#cb460-21"></a><span class="kw">png</span>(<span class="kw">paste0</span>(<span class="st">&quot;image/flower_cluster.png&quot;</span>), <span class="dt">width =</span> <span class="dv">900</span>, <span class="dt">height =</span> <span class="dv">600</span>)</span>
<span id="cb460-22"><a href="k-means-clustering.html#cb460-22"></a><span class="kw">ggarrange</span>(cluster_plot[[<span class="dv">1</span>]], cluster_plot[[<span class="dv">2</span>]], cluster_plot[[<span class="dv">3</span>]], cluster_plot[[<span class="dv">4</span>]])</span>
<span id="cb460-23"><a href="k-means-clustering.html#cb460-23"></a><span class="kw">dev.off</span>()</span></code></pre></div>
<p><strong>Another example</strong></p>
<p>Original Image:</p>
<p><img src="image/swing_s.jpg" width="50%" style="display: block; margin: auto;" /></p>
<p>Apply <span class="math inline">\(k\)</span>-means clustering:</p>
<p><img src="image/swing_cluster.png" width="80%" style="display: block; margin: auto;" /></p>
<p><strong>Explanation of the algorithm</strong></p>
<p>The <span class="math inline">\(k\)</span>-means algorithm searches for a partition of the data into <span class="math inline">\(k\)</span> clusters . It tries to minimize the within-group sum of squared errors (WSS):
<span class="math display">\[\begin{equation*}
WSS(C, \mu) = \sum^k_{j=1} \sum^n_{i=1} I(C_i = j) ||x_i - \mu_j||^2,
\end{equation*}\]</span>
where <span class="math inline">\(C = (C_1,\ldots,C_n)\)</span> and <span class="math inline">\(\mu = \{\mu_1,\ldots,\mu_k\}\)</span>. To minimize WGSS, the algorithm iteratively solves two problems:</p>
<ul>
<li><p>Problem 1. Fix <span class="math inline">\(\mu\)</span> and minimize <span class="math inline">\(WGSS(C, \mu)\)</span> with respect to <span class="math inline">\(C\)</span> (Step 3 in the algorithm)</p></li>
<li><p>Problem 2. Fix <span class="math inline">\(C\)</span> and minimize <span class="math inline">\(WGSS(C, \mu)\)</span> with respect to <span class="math inline">\(\mu\)</span> (Step 4 in the algorithm)</p></li>
</ul>
<p>For Problem 1, the solution is
<span class="math display">\[\begin{equation*}
C_i = \text{argmin}_j ||x_i - \mu_j||^2.
\end{equation*}\]</span></p>
<p>For Problem 2, the solution is
<span class="math display">\[\begin{equation*}
\mu_{jl} = \frac{\sum^n_{i=1}I(C_i = j) x_{il}}{\sum^n_{i=1} I(C_i=j)}.
\end{equation*}\]</span></p>
<!-- **Explanation of the algorithm** -->
<!-- In $k$-means clustering, our goal is to find $C$ which assigns close points to the same cluster. Therefore, it makes sense to minimize the following loss function -->
<!-- \begin{equation*} -->
<!-- W(C) = \frac{1}{2} \sum^k_{j=1} \sum_{i:C(i)=j} \sum_{i':C(i')=j} ||x_i - x_i'||^2. -->
<!-- \end{equation*} -->
<!-- Te above loss function can be rewritten as -->
<!-- \begin{equation*} -->
<!-- W(C) = \sum^k_{j=1} N_j \sum_{i:C(i)=j} ||x_i - \overline{x}_j||^2, -->
<!-- \end{equation*} -->
<!-- where $\overline{x}_j$ is the mean vector associated with the $j$th cluster and $N_j = \sum^n_{i=1} I(C(i)=j)$ is the size of the $j$th cluster. -->
<!-- Because the number of possible assignments of the $n$ data points to $k$ clusters are too many, instead of trying to get a global minimizer by evaluating the loss function $W$ at every possible assignments, one will use an iterative greedy descent, which is much faster but usually converge to local optima. An iterative descent algorithm for finding -->
<!-- \begin{equation*} -->
<!-- C^* = \text{argmin}_C \sum^k_{j=1} N_j \sum_{i:C(i)=j} ||x_i - \overline{x}_j||^2 -->
<!-- \end{equation*} -->
<!-- can be obtained by noting that for any set of observations $S$ (in particular, it also holds in our case), -->
<!-- \begin{equation*} -->
<!-- \overline{x}_S = \text{argmin}_\mu \sum_{i \in S}||x_i - \mu||^2, -->
<!-- \end{equation*} -->
<!-- where $\overline{x}_S$ is the average of all the observations in $S$. Therefore, we can enlarge the optimization problem to -->
<!-- \begin{equation*} -->
<!-- \min_{C, \mu_1,\ldots,\mu_k} \sum^k_{j=1}N_j \sum_{i:C(i) =j}||x_i - \mu_j||^2. -->
<!-- \end{equation*} -->
<!-- To solve the above enlarged optimization problem, one may apply the alternating algorithm described above: Step 3 is to minimize over $C$ by fixing $\mu_1,\ldots,\mu_k$ and Step 4 is to minimize over $\mu_1,\ldots,\mu_k$ by fixing $C$. -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="neural-networks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Book.pdf", "Book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
