<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 14 k-means Clustering | STAT 362 R for Data Science</title>
  <meta name="description" content="Notes for STAT 362" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 14 k-means Clustering | STAT 362 R for Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes for STAT 362" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 14 k-means Clustering | STAT 362 R for Data Science" />
  
  <meta name="twitter:description" content="Notes for STAT 362" />
  

<meta name="author" content="Brian Ling" />


<meta name="date" content="2022-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic-regression-model.html"/>
<link rel="next" href="hierarchical-clustering.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Data Science</a></li>

<li class="divider"></li>
<li><a href="index.html#syllabus">Syllabus<span></span></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> What is R and RStudio?<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-will-you-learn-in-this-course"><i class="fa fa-check"></i><b>1.2</b> What will you learn in this course?<span></span></a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#r-and-r-as-a-programming-language"><i class="fa fa-check"></i><b>1.2.1</b> R and R as a programming language<span></span></a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#data-wrangling"><i class="fa fa-check"></i><b>1.2.2</b> Data Wrangling<span></span></a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#data-visualization"><i class="fa fa-check"></i><b>1.2.3</b> Data Visualization<span></span></a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#statistical-inference"><i class="fa fa-check"></i><b>1.2.4</b> Statistical Inference<span></span></a></li>
<li class="chapter" data-level="1.2.5" data-path="introduction.html"><a href="introduction.html#machine-learning"><i class="fa fa-check"></i><b>1.2.5</b> Machine Learning<span></span></a></li>
<li class="chapter" data-level="1.2.6" data-path="introduction.html"><a href="introduction.html#some-numerical-methods"><i class="fa fa-check"></i><b>1.2.6</b> Some Numerical Methods<span></span></a></li>
<li class="chapter" data-level="1.2.7" data-path="introduction.html"><a href="introduction.html#lastly"><i class="fa fa-check"></i><b>1.2.7</b> Lastly<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#lets-get-started"><i class="fa fa-check"></i><b>1.3</b> Let’s Get Started<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#r-data-structures"><i class="fa fa-check"></i><b>1.4</b> R Data Structures<span></span></a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#vectors"><i class="fa fa-check"></i><b>1.4.1</b> Vectors<span></span></a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#factors"><i class="fa fa-check"></i><b>1.4.2</b> Factors<span></span></a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction.html"><a href="introduction.html#matrix"><i class="fa fa-check"></i><b>1.4.3</b> Matrix<span></span></a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction.html"><a href="introduction.html#lists"><i class="fa fa-check"></i><b>1.4.4</b> Lists<span></span></a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction.html"><a href="introduction.html#data-frames"><i class="fa fa-check"></i><b>1.4.5</b> Data frames<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#operators"><i class="fa fa-check"></i><b>1.5</b> Operators<span></span></a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#vectorized-operators"><i class="fa fa-check"></i><b>1.5.1</b> Vectorized Operators<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#built-in-functions"><i class="fa fa-check"></i><b>1.6</b> Built-in Functions<span></span></a><ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#sort"><i class="fa fa-check"></i><b>1.6.1</b> <code>sort()</code><span></span></a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction.html"><a href="introduction.html#seq"><i class="fa fa-check"></i><b>1.6.2</b> <code>seq()</code><span></span></a></li>
<li class="chapter" data-level="1.6.3" data-path="introduction.html"><a href="introduction.html#rep"><i class="fa fa-check"></i><b>1.6.3</b> <code>rep()</code><span></span></a></li>
<li class="chapter" data-level="1.6.4" data-path="introduction.html"><a href="introduction.html#pmax-pmin"><i class="fa fa-check"></i><b>1.6.4</b> <code>pmax</code>, <code>pmin</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#some-useful-rstudio-shortcuts"><i class="fa fa-check"></i><b>1.7</b> Some Useful RStudio Shortcuts<span></span></a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#comments-to-exercises"><i class="fa fa-check"></i><b>1.9</b> Comments to Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>2</b> Probability<span></span></a><ul>
<li class="chapter" data-level="2.1" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Probability Distributions<span></span></a><ul>
<li class="chapter" data-level="2.1.1" data-path="probability.html"><a href="probability.html#common-distributions"><i class="fa fa-check"></i><b>2.1.1</b> Common Distributions<span></span></a></li>
<li class="chapter" data-level="2.1.2" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>2.1.2</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability.html"><a href="probability.html#simulation"><i class="fa fa-check"></i><b>2.2</b> Simulation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>3</b> Programming in R<span></span></a><ul>
<li class="chapter" data-level="3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#writing-functions-in-r"><i class="fa fa-check"></i><b>3.1</b> Writing functions in R<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="programming-in-r.html"><a href="programming-in-r.html#control-flow"><i class="fa fa-check"></i><b>3.2</b> Control Flow<span></span></a><ul>
<li class="chapter" data-level="3.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#for-loop"><i class="fa fa-check"></i><b>3.2.1</b> for loop<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#while-loop"><i class="fa fa-check"></i><b>3.2.2</b> while loop<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond"><i class="fa fa-check"></i><b>3.2.3</b> if (cond)<span></span></a></li>
<li class="chapter" data-level="3.2.4" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond-else-expr"><i class="fa fa-check"></i><b>3.2.4</b> if (cond) else expr<span></span></a></li>
<li class="chapter" data-level="3.2.5" data-path="programming-in-r.html"><a href="programming-in-r.html#if-else-ladder"><i class="fa fa-check"></i><b>3.2.5</b> If else ladder<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="programming-in-r.html"><a href="programming-in-r.html#automatically-reindent-code"><i class="fa fa-check"></i><b>3.3</b> Automatically Reindent Code<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="programming-in-r.html"><a href="programming-in-r.html#speed-consideration"><i class="fa fa-check"></i><b>3.4</b> Speed Consideration<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="programming-in-r.html"><a href="programming-in-r.html#another-simulation-example"><i class="fa fa-check"></i><b>3.5</b> Another Simulation Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html"><i class="fa fa-check"></i><b>4</b> Creating Some Basic Plots<span></span></a><ul>
<li class="chapter" data-level="4.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#scatter-plot"><i class="fa fa-check"></i><b>4.1</b> Scatter Plot<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#line-graph"><i class="fa fa-check"></i><b>4.2</b> Line Graph<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#bar-chart"><i class="fa fa-check"></i><b>4.3</b> Bar Chart<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#histogram"><i class="fa fa-check"></i><b>4.4</b> Histogram<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#box-plot"><i class="fa fa-check"></i><b>4.5</b> Box Plot<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#plotting-a-function-curve"><i class="fa fa-check"></i><b>4.6</b> Plotting a function curve<span></span></a></li>
<li class="chapter" data-level="4.7" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#more-on-plots-with-base-r"><i class="fa fa-check"></i><b>4.7</b> More on plots with base R<span></span></a><ul>
<li class="chapter" data-level="4.7.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#multi-frame-plot"><i class="fa fa-check"></i><b>4.7.1</b> Multi-frame plot<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#subsubsec:type_of_plot"><i class="fa fa-check"></i><b>4.7.2</b> Type of Plot<span></span></a></li>
<li class="chapter" data-level="4.7.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#parameters-of-a-plot"><i class="fa fa-check"></i><b>4.7.3</b> Parameters of a plot<span></span></a></li>
<li class="chapter" data-level="4.7.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#elements-on-plot"><i class="fa fa-check"></i><b>4.7.4</b> Elements on plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#summary-of-ggplot"><i class="fa fa-check"></i><b>4.8</b> Summary of <code>ggplot</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html"><i class="fa fa-check"></i><b>5</b> Managing Data with R<span></span></a><ul>
<li class="chapter" data-level="5.1" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#saving-loading-and-removing-r-data-structures"><i class="fa fa-check"></i><b>5.2</b> Saving, loading, and removing R data structures<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#importing-and-saving-data-from-csv-files"><i class="fa fa-check"></i><b>5.3</b> Importing and saving data from CSV files<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html"><i class="fa fa-check"></i><b>6</b> Review (Chapter 1-5)<span></span></a><ul>
<li class="chapter" data-level="6.1" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#simulation-1"><i class="fa fa-check"></i><b>6.1</b> Simulation<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#matrix-1"><i class="fa fa-check"></i><b>6.2</b> Matrix<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#basic-operation"><i class="fa fa-check"></i><b>6.3</b> Basic Operation<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#some-basic-plots-in-r"><i class="fa fa-check"></i><b>6.4</b> Some basic plots in R<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html"><i class="fa fa-check"></i><b>7</b> Data Transformation with <code>dplyr</code><span></span></a><ul>
<li class="chapter" data-level="7.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#arrange"><i class="fa fa-check"></i><b>7.2</b> <code>arrange()</code><span></span></a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-2"><i class="fa fa-check"></i><b>7.2.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#filter"><i class="fa fa-check"></i><b>7.3</b> <code>filter()</code><span></span></a><ul>
<li class="chapter" data-level="7.3.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-3"><i class="fa fa-check"></i><b>7.3.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#select"><i class="fa fa-check"></i><b>7.4</b> <code>select()</code><span></span></a><ul>
<li class="chapter" data-level="7.4.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-4"><i class="fa fa-check"></i><b>7.4.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#mutate"><i class="fa fa-check"></i><b>7.5</b> <code>mutate()</code><span></span></a><ul>
<li class="chapter" data-level="7.5.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-5"><i class="fa fa-check"></i><b>7.5.1</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summarize-group_by"><i class="fa fa-check"></i><b>7.6</b> <code>summarize(), group_by()</code><span></span></a></li>
<li class="chapter" data-level="7.7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#combining-multiple-operations-with-pipe"><i class="fa fa-check"></i><b>7.7</b> Combining Multiple Operations with Pipe <code>%&gt;%</code><span></span></a></li>
<li class="chapter" data-level="7.8" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summary"><i class="fa fa-check"></i><b>7.8</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html"><i class="fa fa-check"></i><b>8</b> Data Visualization with ggplot2<span></span></a><ul>
<li class="chapter" data-level="8.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts"><i class="fa fa-check"></i><b>8.1</b> Bar charts<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graph-1"><i class="fa fa-check"></i><b>8.2</b> Line Graph<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plots"><i class="fa fa-check"></i><b>8.3</b> Scatter Plots<span></span></a><ul>
<li class="chapter" data-level="8.3.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#overplotting"><i class="fa fa-check"></i><b>8.3.1</b> Overplotting<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#labelling-points-in-a-scatter-plot"><i class="fa fa-check"></i><b>8.3.2</b> Labelling points in a scatter plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions"><i class="fa fa-check"></i><b>8.4</b> Summarizing Data Distributions<span></span></a><ul>
<li class="chapter" data-level="8.4.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#histogram-1"><i class="fa fa-check"></i><b>8.4.1</b> Histogram<span></span></a></li>
<li class="chapter" data-level="8.4.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#kernel-density-estimate"><i class="fa fa-check"></i><b>8.4.2</b> Kernel Density Estimate<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots"><i class="fa fa-check"></i><b>8.5</b> Saving your plots<span></span></a><ul>
<li class="chapter" data-level="8.5.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-pdf-vector-files"><i class="fa fa-check"></i><b>8.5.1</b> Outputting to pdf vector files<span></span></a></li>
<li class="chapter" data-level="8.5.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-bitmap-files"><i class="fa fa-check"></i><b>8.5.2</b> Outputting to bitmap files<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summary-1"><i class="fa fa-check"></i><b>8.6</b> Summary<span></span></a><ul>
<li class="chapter" data-level="8.6.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#combining-multiple-operations-with-pipe-1"><i class="fa fa-check"></i><b>8.6.1</b> Combining multiple operations with pipe <code>%&gt;%</code><span></span></a></li>
<li class="chapter" data-level="8.6.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts-1"><i class="fa fa-check"></i><b>8.6.2</b> Bar charts<span></span></a></li>
<li class="chapter" data-level="8.6.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graphs"><i class="fa fa-check"></i><b>8.6.3</b> Line graphs<span></span></a></li>
<li class="chapter" data-level="8.6.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plot-1"><i class="fa fa-check"></i><b>8.6.4</b> Scatter plot<span></span></a></li>
<li class="chapter" data-level="8.6.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions-1"><i class="fa fa-check"></i><b>8.6.5</b> Summarizing data distributions<span></span></a></li>
<li class="chapter" data-level="8.6.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots-1"><i class="fa fa-check"></i><b>8.6.6</b> Saving your plots<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference in R<span></span></a><ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.1</b> Maximum Likelihood Estimation<span></span></a><ul>
<li class="chapter" data-level="9.1.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#exercises-on-mle"><i class="fa fa-check"></i><b>9.1.1</b> Exercises on MLE<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#summary-2"><i class="fa fa-check"></i><b>9.1.2</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#interval-estimation-and-hypothesis-testing"><i class="fa fa-check"></i><b>9.2</b> Interval Estimation and Hypothesis Testing<span></span></a><ul>
<li class="chapter" data-level="9.2.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.2.1</b> Examples of Hypothesis Testing<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#null-hypotheses-alternative-hypotheses-and-p-values"><i class="fa fa-check"></i><b>9.2.2</b> Null Hypotheses, Alternative Hypotheses, and p-values<span></span></a></li>
<li class="chapter" data-level="9.2.3" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#type-i-error-and-type-ii-error"><i class="fa fa-check"></i><b>9.2.3</b> Type I error and Type II error<span></span></a></li>
<li class="chapter" data-level="9.2.4" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-for-mean-of-one-sample"><i class="fa fa-check"></i><b>9.2.4</b> Inference for Mean of One Sample<span></span></a></li>
<li class="chapter" data-level="9.2.5" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#comparing-the-means-of-two-samples"><i class="fa fa-check"></i><b>9.2.5</b> Comparing the means of two samples<span></span></a></li>
<li class="chapter" data-level="9.2.6" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-of-a-sample-proportion"><i class="fa fa-check"></i><b>9.2.6</b> Inference of a Sample Proportion<span></span></a></li>
<li class="chapter" data-level="9.2.7" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-groups-for-equal-proportions"><i class="fa fa-check"></i><b>9.2.7</b> Testing groups for equal proportions<span></span></a></li>
<li class="chapter" data-level="9.2.8" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-if-two-samples-have-the-same-underlying-distribution"><i class="fa fa-check"></i><b>9.2.8</b> Testing if two samples have the same underlying distribution<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html"><i class="fa fa-check"></i><b>10</b> Root finding and optimization<span></span></a><ul>
<li class="chapter" data-level="10.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#root-finding"><i class="fa fa-check"></i><b>10.1</b> Root Finding<span></span></a><ul>
<li class="chapter" data-level="10.1.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#bisection-method"><i class="fa fa-check"></i><b>10.1.1</b> Bisection Method<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method"><i class="fa fa-check"></i><b>10.2</b> Newton-Raphson Method<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#minimization-and-maximization"><i class="fa fa-check"></i><b>10.3</b> Minimization and Maximization<span></span></a><ul>
<li class="chapter" data-level="10.3.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method-1"><i class="fa fa-check"></i><b>10.3.1</b> Newton-Raphson Method<span></span></a></li>
<li class="chapter" data-level="10.3.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#gradient-descent"><i class="fa fa-check"></i><b>10.3.2</b> Gradient Descent<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#optim"><i class="fa fa-check"></i><b>10.4</b> <code>optim</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>11</b> k-nearest neighbors<span></span></a><ul>
<li class="chapter" data-level="11.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#feature-scaling"><i class="fa fa-check"></i><b>11.2</b> Feature Scaling<span></span></a></li>
<li class="chapter" data-level="11.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-classifying-breast-cancers"><i class="fa fa-check"></i><b>11.3</b> Example: Classifying Breast Cancers<span></span></a><ul>
<li class="chapter" data-level="11.3.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#evaluating-model-performance"><i class="fa fa-check"></i><b>11.3.1</b> Evaluating Model Performance<span></span></a></li>
<li class="chapter" data-level="11.3.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#using-z-score-standardization"><i class="fa fa-check"></i><b>11.3.2</b> Using <span class="math inline">\(z\)</span>-score standardization<span></span></a></li>
<li class="chapter" data-level="11.3.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#testing-alternative-values-of-k"><i class="fa fa-check"></i><b>11.3.3</b> Testing alternative values of <span class="math inline">\(k\)</span><span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>12</b> Linear Regression Models<span></span></a><ul>
<li class="chapter" data-level="12.1" data-path="linear-regression-models.html"><a href="linear-regression-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Simple Linear Regression<span></span></a></li>
<li class="chapter" data-level="12.2" data-path="linear-regression-models.html"><a href="linear-regression-models.html#smoothed-conditional-means"><i class="fa fa-check"></i><b>12.2</b> Smoothed Conditional Means<span></span></a></li>
<li class="chapter" data-level="12.3" data-path="linear-regression-models.html"><a href="linear-regression-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>12.3</b> Multiple Linear Regression<span></span></a></li>
<li class="chapter" data-level="12.4" data-path="linear-regression-models.html"><a href="linear-regression-models.html#example-diamonds"><i class="fa fa-check"></i><b>12.4</b> Example: <code>diamonds</code><span></span></a></li>
<li class="chapter" data-level="12.5" data-path="linear-regression-models.html"><a href="linear-regression-models.html#categorical-predictors"><i class="fa fa-check"></i><b>12.5</b> Categorical Predictors<span></span></a></li>
<li class="chapter" data-level="12.6" data-path="linear-regression-models.html"><a href="linear-regression-models.html#compare-models-using-anova"><i class="fa fa-check"></i><b>12.6</b> Compare models using ANOVA<span></span></a></li>
<li class="chapter" data-level="12.7" data-path="linear-regression-models.html"><a href="linear-regression-models.html#prediction"><i class="fa fa-check"></i><b>12.7</b> Prediction<span></span></a></li>
<li class="chapter" data-level="12.8" data-path="linear-regression-models.html"><a href="linear-regression-models.html#interaction-terms"><i class="fa fa-check"></i><b>12.8</b> Interaction Terms<span></span></a></li>
<li class="chapter" data-level="12.9" data-path="linear-regression-models.html"><a href="linear-regression-models.html#variable-transformation"><i class="fa fa-check"></i><b>12.9</b> Variable Transformation<span></span></a></li>
<li class="chapter" data-level="12.10" data-path="linear-regression-models.html"><a href="linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>12.10</b> Polynomial Regression<span></span></a></li>
<li class="chapter" data-level="12.11" data-path="linear-regression-models.html"><a href="linear-regression-models.html#stepwise-regression"><i class="fa fa-check"></i><b>12.11</b> Stepwise regression<span></span></a></li>
<li class="chapter" data-level="12.12" data-path="linear-regression-models.html"><a href="linear-regression-models.html#best-subset"><i class="fa fa-check"></i><b>12.12</b> Best subset<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logistic-regression-model.html"><a href="logistic-regression-model.html"><i class="fa fa-check"></i><b>13</b> Logistic Regression Model<span></span></a></li>
<li class="chapter" data-level="14" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>14</b> k-means Clustering<span></span></a><ul>
<li class="chapter" data-level="14.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-3"><i class="fa fa-check"></i><b>14.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="14.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications"><i class="fa fa-check"></i><b>14.2</b> Applications<span></span></a><ul>
<li class="chapter" data-level="14.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#cluster-analysis"><i class="fa fa-check"></i><b>14.2.1</b> Cluster Analysis<span></span></a></li>
<li class="chapter" data-level="14.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#image-segementation-and-image-compression"><i class="fa fa-check"></i><b>14.2.2</b> Image Segementation and Image Compression<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>15</b> Hierarchical Clustering<span></span></a><ul>
<li class="chapter" data-level="15.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#dissimilarity-measure-and-linkage"><i class="fa fa-check"></i><b>15.1</b> Dissimilarity measure and Linkage<span></span></a></li>
<li class="chapter" data-level="15.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#alogrithm"><i class="fa fa-check"></i><b>15.2</b> Alogrithm<span></span></a></li>
<li class="chapter" data-level="15.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#applications-1"><i class="fa fa-check"></i><b>15.3</b> Applications<span></span></a><ul>
<li class="chapter" data-level="15.3.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#nci60-data"><i class="fa fa-check"></i><b>15.3.1</b> NCI60 Data<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>16</b> Resampling Methods<span></span></a><ul>
<li class="chapter" data-level="16.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cross-validation"><i class="fa fa-check"></i><b>16.1</b> Cross-validation<span></span></a><ul>
<li class="chapter" data-level="16.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#cv-in-glm"><i class="fa fa-check"></i><b>16.1.1</b> CV in GLM<span></span></a></li>
<li class="chapter" data-level="16.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#general-implementation"><i class="fa fa-check"></i><b>16.1.2</b> General Implementation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="resampling-methods.html"><a href="resampling-methods.html#bootstrap"><i class="fa fa-check"></i><b>16.2</b> Bootstrap<span></span></a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>17</b> Regularization<span></span></a><ul>
<li class="chapter" data-level="17.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>17.1</b> Ridge Regression<span></span></a></li>
<li class="chapter" data-level="17.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>17.2</b> LASSO<span></span></a></li>
<li class="chapter" data-level="17.3" data-path="regularization.html"><a href="regularization.html#selecting-the-tuning-parameter"><i class="fa fa-check"></i><b>17.3</b> Selecting the tuning parameter<span></span></a></li>
<li class="chapter" data-level="17.4" data-path="regularization.html"><a href="regularization.html#glmnet"><i class="fa fa-check"></i><b>17.4</b> <code>glmnet</code><span></span></a><ul>
<li class="chapter" data-level="17.4.1" data-path="regularization.html"><a href="regularization.html#ridge-regression-1"><i class="fa fa-check"></i><b>17.4.1</b> Ridge Regression<span></span></a></li>
<li class="chapter" data-level="17.4.2" data-path="regularization.html"><a href="regularization.html#lasso-1"><i class="fa fa-check"></i><b>17.4.2</b> LASSO<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>18</b> Decision Trees<span></span></a><ul>
<li class="chapter" data-level="18.1" data-path="decision-trees.html"><a href="decision-trees.html#introduction-to-classification-tree"><i class="fa fa-check"></i><b>18.1</b> Introduction to Classification Tree<span></span></a></li>
<li class="chapter" data-level="18.2" data-path="decision-trees.html"><a href="decision-trees.html#introduction-to-regression-tree"><i class="fa fa-check"></i><b>18.2</b> Introduction to regression tree<span></span></a></li>
<li class="chapter" data-level="18.3" data-path="decision-trees.html"><a href="decision-trees.html#mathematical-formulation"><i class="fa fa-check"></i><b>18.3</b> Mathematical Formulation<span></span></a></li>
<li class="chapter" data-level="18.4" data-path="decision-trees.html"><a href="decision-trees.html#examples"><i class="fa fa-check"></i><b>18.4</b> Examples<span></span></a><ul>
<li class="chapter" data-level="18.4.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-tree"><i class="fa fa-check"></i><b>18.4.1</b> Classification Tree<span></span></a></li>
<li class="chapter" data-level="18.4.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-tree"><i class="fa fa-check"></i><b>18.4.2</b> Regression Tree<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>19</b> Ensemble Methods<span></span></a><ul>
<li class="chapter" data-level="19.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>19.1</b> Bagging<span></span></a></li>
<li class="chapter" data-level="19.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>19.2</b> Random Forest<span></span></a></li>
<li class="chapter" data-level="19.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#example"><i class="fa fa-check"></i><b>19.3</b> Example<span></span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 362 R for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="k-means-clustering" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 14</span> k-means Clustering<a href="k-means-clustering.html#k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Reference: Ch9 of Machine Learning with R, Ch6 of R Graphics Cookbook</p>
<p>Packages used:</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="k-means-clustering.html#cb417-1"></a><span class="kw">library</span>(tidyverse) <span class="co"># for read_csv</span></span>
<span id="cb417-2"><a href="k-means-clustering.html#cb417-2"></a><span class="kw">library</span>(factoextra) <span class="co"># visualize clusering results</span></span>
<span id="cb417-3"><a href="k-means-clustering.html#cb417-3"></a><span class="kw">library</span>(jpeg) <span class="co"># readJPEG reads an image from a JPEG file/content into a raster array</span></span>
<span id="cb417-4"><a href="k-means-clustering.html#cb417-4"></a><span class="kw">library</span>(ggpubr)</span></code></pre></div>
<div id="introduction-3" class="section level2 hasAnchor">
<h2><span class="header-section-number">14.1</span> Introduction<a href="k-means-clustering.html#introduction-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Clustering</strong></p>
<p>Clustering is an unsupervised learning task that divides data into clusters, or groups of “similar” items while data points in different clusters are very different. It is an unsupervised learning method as we do not have “labels” of the data. We do not have the ground truth to compare the results. Clustering will only tell you which groups of data are similar. One may get some meaningful interpretation of the groups by studying the members in each group, e.g., by calculating some summary statistics and making use of some visualization tools.</p>
<p><strong><span class="math inline">\(k\)</span>-means clustering</strong></p>
<p><span class="math inline">\(k\)</span>-means clustering is one of the most popular clustering methods. It is intended for situations in which all variables are of the <strong>numeric type</strong>.</p>
<p>In <span class="math inline">\(k\)</span>-means clustering, all the examples are assigned to one of the <span class="math inline">\(k\)</span> clusters, where <span class="math inline">\(k\)</span> is a positive integer that has been specified before performing the clustering. The goal is to assign similar data to the same cluster.</p>
<p><strong>Some Applications</strong></p>
<ol style="list-style-type: decimal">
<li><p>Cluster analysis: Interesting groups may be discovered, such as the groups of motor insurance policy holders with a high average claim cost, or the groups of clients in a banking database having a heavy investment in real estate. In marketing segmentation, we segment customers into groups with similar demographics or buying patterns for targeted marketing campaigns</p></li>
<li><p>Image segmentation: The goal of segmentation is to partition an image into regions each of which has a reasonably homogeneous visual appearance or which corresponds to objects or parts of objects.</p></li>
</ol>
<p><strong>Notation and Setting</strong></p>
<p>We have <span class="math inline">\(n\)</span> data points <span class="math inline">\(x_i = (x_{i1},\ldots,x_{ip})&#39; \in \mathbb{R}^p\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>. Each observation is uniquely labeled by an integer <span class="math inline">\(i \in \{1,\ldots,n\}\)</span>. We use the notation <span class="math inline">\(C_i\)</span> to denote which cluster the <span class="math inline">\(i\)</span>th observation is assigned to. For example, <span class="math inline">\(C_1 = 2\)</span> means the <span class="math inline">\(1\)</span>st observation is assigned to the <span class="math inline">\(2\)</span>nd cluster, and <span class="math inline">\(C_2 = 3\)</span> means the <span class="math inline">\(2\)</span>nd observation is assigned to the <span class="math inline">\(3\)</span>rd cluster.</p>
<p>We use</p>
<ul>
<li><span class="math inline">\(i\)</span> to index data</li>
<li><span class="math inline">\(j\)</span> to index cluster</li>
<li><span class="math inline">\(l\)</span> to index feature</li>
</ul>
<p><strong>Dissimilarity measure</strong></p>
<p>In clustering, one have to define a dissimilarity measure <span class="math inline">\(d\)</span>. In <span class="math inline">\(k\)</span>-means clustering, the <strong>squared</strong> Euclidean distance is used. Given two data points <span class="math inline">\(x_i = (x_{i1},\ldots,x_{ip})\)</span> and <span class="math inline">\(x_{i&#39;} = (x_{i&#39;1},\ldots,x_{i&#39;p})\)</span>, the squared Euclidean distance is given by
<span class="math display">\[\begin{equation*}
d(x_i, x_{i&#39;}) = \sum^p_{l=1} (x_{il} - x_{i&#39;l})^2 = ||x_i - x_{i&#39;}||^2.
\end{equation*}\]</span></p>
<p><strong>k-means clustering algorithm (Lloyd’s algorithm)</strong></p>
<ol style="list-style-type: decimal">
<li><p>Specify the number of clusters <span class="math inline">\(k\)</span></p></li>
<li><p>Randomly select <span class="math inline">\(k\)</span> data points as the centroids, denoted by <span class="math inline">\(\mu_1,\ldots,\mu_k\)</span>.</p></li>
<li><p>For each <span class="math inline">\(i=1,\ldots,n\)</span>, set
<span class="math display">\[
C_i = \text{argmin}_j ||x_i - \mu_j||^2.
\]</span>
In words, assign the closest cluster to the <span class="math inline">\(i\)</span>th observation.</p></li>
<li><p>For each <span class="math inline">\(j=1,\ldots,k\)</span>, <span class="math inline">\(l=1,\ldots,p\)</span>, set
<span class="math display">\[
\mu_{jl} = \frac{\sum^n_{i=1}I(C_i =j) x_{il}}{\sum^n_{i=1}I(C_i =j)}.
\]</span>
In words, <span class="math inline">\(\mu_j\)</span> is simply the mean of all the points assigned to <span class="math inline">\(j\)</span>th cluster. Note that <span class="math inline">\(\mu_j = (\mu_{j1},\ldots,\mu_{jp})\)</span>.</p></li>
<li><p>Repeat Steps 3 - 4 until the assignments do not change.</p></li>
</ol>
<p><strong>Illustration of the algorithm:</strong></p>
<ol style="list-style-type: decimal">
<li><p>Set <span class="math inline">\(k=2\)</span>.</p></li>
<li><p>Randomly select 2 data points as the initial centroids (Brown crosses).</p></li>
</ol>
<p><img src="Book_files/figure-html/unnamed-chunk-452-1.png" width="75%" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: decimal">
<li>For each observation, find the nearest centroid.</li>
</ol>
<p><img src="Book_files/figure-html/unnamed-chunk-453-1.png" width="75%" style="display: block; margin: auto;" /></p>
<ol start="4" style="list-style-type: decimal">
<li>Compute the mean of all the points assigned to the two cluster to find the new centroids.</li>
</ol>
<p><img src="Book_files/figure-html/unnamed-chunk-454-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Repeat Step 3:</p>
<p><img src="Book_files/figure-html/unnamed-chunk-455-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Repeat Step 4:</p>
<p><img src="Book_files/figure-html/unnamed-chunk-456-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p><strong>Remarks</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\text{argmin}\)</span> is the argument of the minimum. <span class="math inline">\(\text{argmin}_j ||x_i - \mu_j||^2\)</span> means the value of <span class="math inline">\(j\)</span> such that <span class="math inline">\(||x_i - \mu_j||^2\)</span> is minimum.</p></li>
<li><p><span class="math inline">\(I(C_i = j)\)</span> equals <span class="math inline">\(1\)</span> if <span class="math inline">\(C_i = j\)</span> and equals <span class="math inline">\(0\)</span> otherwise. The <span class="math inline">\(I\)</span> is the indicator function.</p></li>
<li><p><span class="math inline">\(k\)</span>-means is sensitive to the number of clusters. See the elbow method.</p></li>
<li><p><span class="math inline">\(k\)</span>-means is sensitive to the randomly-chosen cluster centers. Different initial centers may result in different clustering results. As a result, we should use multiple set of initial cluster centers and choose the best result (smallest within-group sum of squared errors, see the end of this chapter).</p></li>
<li><p>Scale your data before applying <span class="math inline">\(k\)</span>-means clustering.</p></li>
</ol>
<p><strong>For categorical data</strong>: one possible solution is to use <span class="math inline">\(k\)</span>-modes. The <span class="math inline">\(k\)</span>-modes algorithm uses a matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimize the clustering cost functions</p>
<p><strong>For mixed data (numeric + categorical features)</strong>: can use <span class="math inline">\(k\)</span>-prototypes. It integrates the <span class="math inline">\(k\)</span>-means and <span class="math inline">\(k\)</span>-modes algorithms using a combined dissimilarity measure.</p>
<p>R package: <code>clustMixType</code></p>
<p>See also Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values by Z. Huang (1998).</p>
</div>
<div id="applications" class="section level2 hasAnchor">
<h2><span class="header-section-number">14.2</span> Applications<a href="k-means-clustering.html#applications" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="cluster-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">14.2.1</span> Cluster Analysis<a href="k-means-clustering.html#cluster-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>Mall_Customers.csv</code> (in onQ) contains the gender, age, annual income and spending score of some customers.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="k-means-clustering.html#cb418-1"></a>mall &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/Mall_Customers.csv&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="k-means-clustering.html#cb419-1"></a>mall</span>
<span id="cb419-2"><a href="k-means-clustering.html#cb419-2"></a><span class="co">## # A tibble: 200 x 5</span></span>
<span id="cb419-3"><a href="k-means-clustering.html#cb419-3"></a><span class="co">##    CustomerID Gender   Age `Annual Income (k$)` `Spending Score (1-100)`</span></span>
<span id="cb419-4"><a href="k-means-clustering.html#cb419-4"></a><span class="co">##         &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;                &lt;dbl&gt;                    &lt;dbl&gt;</span></span>
<span id="cb419-5"><a href="k-means-clustering.html#cb419-5"></a><span class="co">##  1          1 Male      19                   15                       39</span></span>
<span id="cb419-6"><a href="k-means-clustering.html#cb419-6"></a><span class="co">##  2          2 Male      21                   15                       81</span></span>
<span id="cb419-7"><a href="k-means-clustering.html#cb419-7"></a><span class="co">##  3          3 Female    20                   16                        6</span></span>
<span id="cb419-8"><a href="k-means-clustering.html#cb419-8"></a><span class="co">##  4          4 Female    23                   16                       77</span></span>
<span id="cb419-9"><a href="k-means-clustering.html#cb419-9"></a><span class="co">##  5          5 Female    31                   17                       40</span></span>
<span id="cb419-10"><a href="k-means-clustering.html#cb419-10"></a><span class="co">##  6          6 Female    22                   17                       76</span></span>
<span id="cb419-11"><a href="k-means-clustering.html#cb419-11"></a><span class="co">##  7          7 Female    35                   18                        6</span></span>
<span id="cb419-12"><a href="k-means-clustering.html#cb419-12"></a><span class="co">##  8          8 Female    23                   18                       94</span></span>
<span id="cb419-13"><a href="k-means-clustering.html#cb419-13"></a><span class="co">##  9          9 Male      64                   19                        3</span></span>
<span id="cb419-14"><a href="k-means-clustering.html#cb419-14"></a><span class="co">## 10         10 Female    30                   19                       72</span></span>
<span id="cb419-15"><a href="k-means-clustering.html#cb419-15"></a><span class="co">## # ... with 190 more rows</span></span></code></pre></div>
<p>We will use the <code>kmeans()</code> function in R (contained in base R) to perform <span class="math inline">\(k\)</span>-means clustering.</p>
<ul>
<li><p><code>x</code>: your dataframe/ matrix</p></li>
<li><p><code>centers</code>: the number of clusters</p></li>
<li><p><code>nstart</code>: how many random sets should be chosen (the best result will be reported). Recommended setting: always run <span class="math inline">\(k\)</span>-means clustering with a large value of <code>nstart</code>, such as <span class="math inline">\(20\)</span> or <span class="math inline">\(50\)</span>, to reduce the chance of obtaining an undesirable local optimum.</p></li>
</ul>
<p>To be able to reproduce the <span class="math inline">\(k\)</span>-means output, you can also set a random seed using the <code>set.seed()</code> function. Otherwise, the cluster labels can be different each time you run the code.</p>
<p>Let’s perform a <span class="math inline">\(k\)</span>-means clustering on the customers using <code>Annual Income (k$)</code> and <code>Spending Score (1-100)</code> with <span class="math inline">\(k=3\)</span>.</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="k-means-clustering.html#cb420-1"></a>mall_kmeans &lt;-<span class="st"> </span><span class="kw">kmeans</span>(<span class="dt">x =</span> <span class="kw">scale</span>(mall[, <span class="dv">4</span><span class="op">:</span><span class="dv">5</span>]), <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</span></code></pre></div>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="k-means-clustering.html#cb421-1"></a><span class="co"># To view the cluster</span></span>
<span id="cb421-2"><a href="k-means-clustering.html#cb421-2"></a>mall_kmeans<span class="op">$</span>cluster</span>
<span id="cb421-3"><a href="k-means-clustering.html#cb421-3"></a><span class="co">##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</span></span>
<span id="cb421-4"><a href="k-means-clustering.html#cb421-4"></a><span class="co">##  [45] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</span></span>
<span id="cb421-5"><a href="k-means-clustering.html#cb421-5"></a><span class="co">##  [89] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 2 1 2 1 2 1 2</span></span>
<span id="cb421-6"><a href="k-means-clustering.html#cb421-6"></a><span class="co">## [133] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2</span></span>
<span id="cb421-7"><a href="k-means-clustering.html#cb421-7"></a><span class="co">## [177] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2</span></span>
<span id="cb421-8"><a href="k-means-clustering.html#cb421-8"></a></span>
<span id="cb421-9"><a href="k-means-clustering.html#cb421-9"></a><span class="co"># To view the centers</span></span>
<span id="cb421-10"><a href="k-means-clustering.html#cb421-10"></a>mall_kmeans<span class="op">$</span>centers</span>
<span id="cb421-11"><a href="k-means-clustering.html#cb421-11"></a><span class="co">##   Annual Income (k$) Spending Score (1-100)</span></span>
<span id="cb421-12"><a href="k-means-clustering.html#cb421-12"></a><span class="co">## 1          1.0066735            -1.22246770</span></span>
<span id="cb421-13"><a href="k-means-clustering.html#cb421-13"></a><span class="co">## 2          0.9891010             1.23640011</span></span>
<span id="cb421-14"><a href="k-means-clustering.html#cb421-14"></a><span class="co">## 3         -0.6246222            -0.01435636</span></span></code></pre></div>
<p>Visualize the clusters:</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="k-means-clustering.html#cb422-1"></a><span class="kw">library</span>(factoextra)</span>
<span id="cb422-2"><a href="k-means-clustering.html#cb422-2"></a><span class="kw">fviz_cluster</span>(mall_kmeans,  <span class="dt">data =</span> mall[, <span class="dv">4</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-462-1.png" width="75%" style="display: block; margin: auto;" />
By looking at the plot, we see the 3 clusters are</p>
<ul>
<li>high annual income + low spending score</li>
<li>high annual income + high spending score</li>
<li>low annual income</li>
</ul>
<p>Let’s try <span class="math inline">\(k=5\)</span>:</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="k-means-clustering.html#cb423-1"></a>mall_kmeans &lt;-<span class="st"> </span><span class="kw">kmeans</span>(<span class="dt">x =</span> <span class="kw">scale</span>(mall[, <span class="dv">4</span><span class="op">:</span><span class="dv">5</span>]), <span class="dt">centers =</span> <span class="dv">5</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</span></code></pre></div>
<p>Visualizing the clusters:</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="k-means-clustering.html#cb424-1"></a><span class="kw">fviz_cluster</span>(mall_kmeans,  <span class="dt">data =</span> mall[, <span class="dv">4</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-464-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>The <span class="math inline">\(5\)</span> clusters are</p>
<ul>
<li>high annual income + low spending score</li>
<li>high annual income + high spending score</li>
<li>medium annual income + medium spending score</li>
<li>low annual income + high spending score</li>
<li>low annual income + low spending score</li>
</ul>
<p>Now, let’s use one more variable for clustering:</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="k-means-clustering.html#cb425-1"></a>mall_kmeans &lt;-<span class="st"> </span><span class="kw">kmeans</span>(<span class="dt">x =</span> <span class="kw">scale</span>(mall[, <span class="dv">3</span><span class="op">:</span><span class="dv">5</span>]), <span class="dt">centers =</span> <span class="dv">5</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</span></code></pre></div>
<p>To visualize more than 2 variables, we can use a dimension reduction technique called principal component analysis. The function <code>fviz_cluster</code> will automatically perform that and give you a <span class="math inline">\(2D\)</span>-plot using the first two principal comonents.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="k-means-clustering.html#cb426-1"></a><span class="kw">fviz_cluster</span>(mall_kmeans,  <span class="dt">data =</span> mall[, <span class="dv">3</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-466-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>To understand more about the clusters, we should take a look at some plots and some summary statistics. An example is to use violin plots.</p>
<p><strong>Violin Plots</strong></p>
<p>A violin plot is a kernel density estimate, mirrored so that it forms a symmetrical shape. It is a helpful tool to compare multiple data distributions when we put several plots side by side. To provide additional information, we can also have box plots overlaid, with a white dot at the median.</p>
<p>We first add the cluster information to our dataset.</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="k-means-clustering.html#cb427-1"></a>mall_cluster &lt;-<span class="st"> </span><span class="kw">mutate</span>(mall, <span class="dt">Cluster =</span> <span class="kw">factor</span>(mall_kmeans<span class="op">$</span>cluster))</span></code></pre></div>
<p><strong>Create violin plots</strong></p>
<p><code>outlier.colour = NA</code>: do not display outliers</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="k-means-clustering.html#cb428-1"></a>mall_cluster <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb428-2"><a href="k-means-clustering.html#cb428-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Cluster, <span class="dt">y =</span> <span class="st">`</span><span class="dt">Annual Income (k$)</span><span class="st">`</span>)) <span class="op">+</span></span>
<span id="cb428-3"><a href="k-means-clustering.html#cb428-3"></a><span class="st">  </span><span class="kw">geom_violin</span>(<span class="dt">trim =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb428-4"><a href="k-means-clustering.html#cb428-4"></a><span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">width =</span> <span class="fl">.1</span>, <span class="dt">fill =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">outlier.colour =</span> <span class="ot">NA</span>) <span class="op">+</span></span>
<span id="cb428-5"><a href="k-means-clustering.html#cb428-5"></a><span class="st">  </span><span class="kw">stat_summary</span>(<span class="dt">fun =</span> median, <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">shape =</span> <span class="dv">21</span>, <span class="dt">size =</span> <span class="fl">2.5</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-468-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>The other two variables:</p>
<p><img src="Book_files/figure-html/unnamed-chunk-469-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p><img src="Book_files/figure-html/unnamed-chunk-470-1.png" width="75%" style="display: block; margin: auto;" />
<strong>Determine the optimal <span class="math inline">\(k\)</span></strong></p>
<p>The elbow method is a heuristic method to determine the number of clusters. We plot the total within-group sum of squared errors against the number of clusters. We pick the elbow of the curve as the number of clusters to use. The elbow (or knee) of a curve is a point where the curve visibly bends.</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="k-means-clustering.html#cb429-1"></a>WSS &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb429-2"><a href="k-means-clustering.html#cb429-2"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {</span>
<span id="cb429-3"><a href="k-means-clustering.html#cb429-3"></a>  <span class="co"># extract the total within-group sum of squared errors</span></span>
<span id="cb429-4"><a href="k-means-clustering.html#cb429-4"></a>  WSS[k] =<span class="st"> </span><span class="kw">kmeans</span>(<span class="dt">x =</span> <span class="kw">scale</span>(mall[, <span class="dv">3</span><span class="op">:</span><span class="dv">5</span>]), <span class="dt">centers =</span> k, <span class="dt">nstart =</span> <span class="dv">25</span>)<span class="op">$</span>tot.withinss</span>
<span id="cb429-5"><a href="k-means-clustering.html#cb429-5"></a>}</span>
<span id="cb429-6"><a href="k-means-clustering.html#cb429-6"></a></span>
<span id="cb429-7"><a href="k-means-clustering.html#cb429-7"></a><span class="kw">ggplot</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">y =</span> WSS)) <span class="op">+</span></span>
<span id="cb429-8"><a href="k-means-clustering.html#cb429-8"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb429-9"><a href="k-means-clustering.html#cb429-9"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb429-10"><a href="k-means-clustering.html#cb429-10"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb429-11"><a href="k-means-clustering.html#cb429-11"></a><span class="st">  </span><span class="kw">scale_x_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;k&quot;</span>, <span class="dt">limits =</span> <span class="kw">factor</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>)) <span class="op">+</span></span>
<span id="cb429-12"><a href="k-means-clustering.html#cb429-12"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Elbow Method&quot;</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-471-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>It seems to me that the curve bends at <span class="math inline">\(k=4\)</span>.</p>
</div>
<div id="image-segementation-and-image-compression" class="section level3 hasAnchor">
<h3><span class="header-section-number">14.2.2</span> Image Segementation and Image Compression<a href="k-means-clustering.html#image-segementation-and-image-compression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Image segementation using <span class="math inline">\(k\)</span>-means clustering:</p>
<ul>
<li><p>Each pixel is a data point in a 3-dimensioal space comprising the intensities of the red, blue, and green channels.</p></li>
<li><p>Group the pixels into <span class="math inline">\(k\)</span> different clusters</p></li>
<li><p>For a given value of <span class="math inline">\(k\)</span>. the result from the <span class="math inline">\(k\)</span>-means clustering is an image using a paletter of only <span class="math inline">\(k\)</span> colors.</p></li>
</ul>
<p>Note: a direct use of <span class="math inline">\(k\)</span>-means clustering is not a particular sophisticated approach to image segmentation (e.g., we haven’t taken into account of the information from spatial proximity)</p>
<p>We will use the <code>readJPEG()</code> function from the package <code>jpeg</code> to reads an image from a jpeg file into a raster array.</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="k-means-clustering.html#cb430-1"></a><span class="kw">library</span>(jpeg)</span>
<span id="cb430-2"><a href="k-means-clustering.html#cb430-2"></a></span>
<span id="cb430-3"><a href="k-means-clustering.html#cb430-3"></a><span class="co"># read the image into a raster array</span></span>
<span id="cb430-4"><a href="k-means-clustering.html#cb430-4"></a>image &lt;-<span class="st"> </span><span class="kw">readJPEG</span>(<span class="st">&quot;C:/Queens Teaching/Teaching/STAT 362/08_datasets/image/flower_s.jpg&quot;</span>)</span>
<span id="cb430-5"><a href="k-means-clustering.html#cb430-5"></a></span>
<span id="cb430-6"><a href="k-means-clustering.html#cb430-6"></a><span class="co"># 3-way array (matrix = 2-way array)</span></span>
<span id="cb430-7"><a href="k-means-clustering.html#cb430-7"></a>(image_dim &lt;-<span class="st"> </span><span class="kw">dim</span>(image))</span>
<span id="cb430-8"><a href="k-means-clustering.html#cb430-8"></a><span class="co">## [1] 200 300   3</span></span>
<span id="cb430-9"><a href="k-means-clustering.html#cb430-9"></a><span class="co">## [1] 200 300   3</span></span>
<span id="cb430-10"><a href="k-means-clustering.html#cb430-10"></a></span>
<span id="cb430-11"><a href="k-means-clustering.html#cb430-11"></a><span class="co"># Assign RGB channels to each pixel </span></span>
<span id="cb430-12"><a href="k-means-clustering.html#cb430-12"></a>image_RGB &lt;-<span class="st"> </span><span class="kw">tibble</span>(</span>
<span id="cb430-13"><a href="k-means-clustering.html#cb430-13"></a>  <span class="dt">x =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>image_dim[<span class="dv">2</span>], <span class="dt">each =</span> image_dim[<span class="dv">1</span>]),</span>
<span id="cb430-14"><a href="k-means-clustering.html#cb430-14"></a>  <span class="dt">y =</span> <span class="kw">rep</span>(image_dim[<span class="dv">1</span>]<span class="op">:</span><span class="dv">1</span>, image_dim[<span class="dv">2</span>]),</span>
<span id="cb430-15"><a href="k-means-clustering.html#cb430-15"></a>  <span class="dt">R =</span> <span class="kw">as.vector</span>(image[, , <span class="dv">1</span>]),</span>
<span id="cb430-16"><a href="k-means-clustering.html#cb430-16"></a>  <span class="dt">G =</span> <span class="kw">as.vector</span>(image[, , <span class="dv">2</span>]),</span>
<span id="cb430-17"><a href="k-means-clustering.html#cb430-17"></a>  <span class="dt">B =</span> <span class="kw">as.vector</span>(image[, , <span class="dv">3</span>])</span>
<span id="cb430-18"><a href="k-means-clustering.html#cb430-18"></a>  )</span>
<span id="cb430-19"><a href="k-means-clustering.html#cb430-19"></a></span>
<span id="cb430-20"><a href="k-means-clustering.html#cb430-20"></a><span class="co"># use view(image_RGB) to view the resulting tibble</span></span></code></pre></div>
<p>Original Image</p>
<p><img src="image/flower_s.jpg" width="50%" style="display: block; margin: auto;" /></p>
<!-- ```{r eval = FALSE} -->
<!-- # Plot the image -->
<!-- ggplot(data = image, aes(x = x, y = y)) +  -->
<!--   geom_point(colour = rgb(image[c("R", "G", "B")])) + -->
<!--   labs(title = "Original Image") + -->
<!--   xlab("") + -->
<!--   ylab("")  -->
<!-- ``` -->
<p>Apply <span class="math inline">\(k\)</span>-means clustering:</p>
<p><img src="image/flower_cluster.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Code for creating the above images:</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="k-means-clustering.html#cb431-1"></a>cluster_plot &lt;-<span class="st"> </span><span class="kw">list</span>()</span>
<span id="cb431-2"><a href="k-means-clustering.html#cb431-2"></a>i &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb431-3"><a href="k-means-clustering.html#cb431-3"></a></span>
<span id="cb431-4"><a href="k-means-clustering.html#cb431-4"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">8</span>)) {</span>
<span id="cb431-5"><a href="k-means-clustering.html#cb431-5"></a>  i &lt;-<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb431-6"><a href="k-means-clustering.html#cb431-6"></a></span>
<span id="cb431-7"><a href="k-means-clustering.html#cb431-7"></a>  <span class="co"># perform k-means</span></span>
<span id="cb431-8"><a href="k-means-clustering.html#cb431-8"></a>    image_kmeans &lt;-<span class="st"> </span><span class="kw">kmeans</span>(image_RGB[, <span class="kw">c</span>(<span class="st">&quot;R&quot;</span>, <span class="st">&quot;G&quot;</span>, <span class="st">&quot;B&quot;</span>)], <span class="dt">centers =</span> k, <span class="dt">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb431-9"><a href="k-means-clustering.html#cb431-9"></a>    </span>
<span id="cb431-10"><a href="k-means-clustering.html#cb431-10"></a>  <span class="co"># for each pixel, use the colour of the center of the corresponding cluster</span></span>
<span id="cb431-11"><a href="k-means-clustering.html#cb431-11"></a>  cluster_color &lt;-<span class="st"> </span><span class="kw">rgb</span>(image_kmeans<span class="op">$</span>centers[image_kmeans<span class="op">$</span>cluster, ])</span>
<span id="cb431-12"><a href="k-means-clustering.html#cb431-12"></a>  </span>
<span id="cb431-13"><a href="k-means-clustering.html#cb431-13"></a>  cluster_plot[[i]] &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> image_RGB, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb431-14"><a href="k-means-clustering.html#cb431-14"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">colour =</span> cluster_color) <span class="op">+</span></span>
<span id="cb431-15"><a href="k-means-clustering.html#cb431-15"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste</span>(<span class="st">&quot;k = &quot;</span>, k)) <span class="op">+</span></span>
<span id="cb431-16"><a href="k-means-clustering.html#cb431-16"></a><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>) <span class="op">+</span></span>
<span id="cb431-17"><a href="k-means-clustering.html#cb431-17"></a><span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb431-18"><a href="k-means-clustering.html#cb431-18"></a>}</span>
<span id="cb431-19"><a href="k-means-clustering.html#cb431-19"></a></span>
<span id="cb431-20"><a href="k-means-clustering.html#cb431-20"></a><span class="co"># save the plots to your computer</span></span>
<span id="cb431-21"><a href="k-means-clustering.html#cb431-21"></a><span class="kw">png</span>(<span class="kw">paste0</span>(<span class="st">&quot;image/flower_cluster.png&quot;</span>), <span class="dt">width =</span> <span class="dv">900</span>, <span class="dt">height =</span> <span class="dv">600</span>)</span>
<span id="cb431-22"><a href="k-means-clustering.html#cb431-22"></a><span class="kw">ggarrange</span>(cluster_plot[[<span class="dv">1</span>]], cluster_plot[[<span class="dv">2</span>]], cluster_plot[[<span class="dv">3</span>]], cluster_plot[[<span class="dv">4</span>]])</span>
<span id="cb431-23"><a href="k-means-clustering.html#cb431-23"></a><span class="kw">dev.off</span>()</span></code></pre></div>
<p><strong>Another example</strong></p>
<p>Original Image:</p>
<p><img src="image/swing_s.jpg" width="50%" style="display: block; margin: auto;" /></p>
<p>Apply <span class="math inline">\(k\)</span>-means clustering:</p>
<p><img src="image/swing_cluster.png" width="80%" style="display: block; margin: auto;" /></p>
<p><strong>Image Compression</strong></p>
<p>Two types of data (image is a special case of data) compression:</p>
<ul>
<li><p>lossless data compression: reconstruct the original data exactly from the compressed representation</p></li>
<li><p>lossy data compression: accept some errors in the reconstruction in return for higher levels of compression than can be achieved in the lossless case</p></li>
</ul>
<p>To apply <span class="math inline">\(k\)</span>-means clustering to lossy data compression:</p>
<ol style="list-style-type: decimal">
<li><p>For each pixel, store only the identity of the cluster to which it is assigned</p></li>
<li><p>Store the values of the <span class="math inline">\(k\)</span> cluster centers</p></li>
</ol>
<p><strong>Bit:</strong> The bit is the most basic unit of information in computing and digital communications. The bit represents a logical state with one of two possible values. These values are most commonly represented as either “1” or “0”. <span class="math inline">\(8\)</span> bits can represent at most <span class="math inline">\(2^8 = 256\)</span> different values.</p>
<ol style="list-style-type: decimal">
<li><p>Suppose the original image has <span class="math inline">\(n\)</span> pixels comprising <span class="math inline">\(R, G, B\)</span> values each of which is stored with <span class="math inline">\(8\)</span> bits of precision.</p></li>
<li><p>To store the whole image, we needs <span class="math inline">\(24 n\)</span> bits.</p></li>
<li><p>Using <span class="math inline">\(k\)</span>-means clustering, we needs <span class="math inline">\(\log_2 K\)</span> bits per pixel to store the cluster identity and <span class="math inline">\(24\)</span> bits for each cluster. Total = <span class="math inline">\(24K + N \log_2 K\)</span>.</p></li>
</ol>
<p><strong>Example:</strong></p>
<ul>
<li><p>For an image with <span class="math inline">\(240 \times 180 = 43,200\)</span> pixels, we need <span class="math inline">\(1,036,800\)</span> bits to transmit directly.</p></li>
<li><p>For the same image, using <span class="math inline">\(k\)</span>-means clustering, we need</p>
<ul>
<li><span class="math inline">\(43,248\)</span> bits (<span class="math inline">\(k=2\)</span>), compression ratios: <span class="math inline">\(4.2\%\)</span></li>
<li><span class="math inline">\(86,472\)</span> bits (<span class="math inline">\(k=3\)</span>), compression ratios: <span class="math inline">\(8.3\%\)</span></li>
<li><span class="math inline">\(173,040\)</span> bits (<span class="math inline">\(k=10\)</span>), compression ratios: <span class="math inline">\(16.7\%\)</span></li>
</ul></li>
</ul>
<p><strong>Explanation of the algorithm</strong></p>
<p>The <span class="math inline">\(k\)</span>-means algorithm searches for a partition of the data into <span class="math inline">\(k\)</span> clusters . It tries to minimize the within-group sum of squared errors (WSS):
<span class="math display">\[\begin{equation*}
WSS(C, \mu) = \sum^k_{j=1} \sum^n_{i=1} I(C_i = j) ||x_i - \mu_j||^2,
\end{equation*}\]</span>
where <span class="math inline">\(C = (C_1,\ldots,C_n)\)</span> and <span class="math inline">\(\mu = \{\mu_1,\ldots,\mu_k\}\)</span>. To minimize WGSS, the algorithm iteratively solves two problems:</p>
<ul>
<li><p>Problem 1. Fix <span class="math inline">\(\mu\)</span> and minimize <span class="math inline">\(WGSS(C, \mu)\)</span> with respect to <span class="math inline">\(C\)</span> (Step 3 in the algorithm)</p></li>
<li><p>Problem 2. Fix <span class="math inline">\(C\)</span> and minimize <span class="math inline">\(WGSS(C, \mu)\)</span> with respect to <span class="math inline">\(\mu\)</span> (Step 4 in the algorithm)</p></li>
</ul>
<p>For Problem 1, the solution is
<span class="math display">\[\begin{equation*}
C_i = \text{argmin}_j ||x_i - \mu_j||^2.
\end{equation*}\]</span></p>
<p>For Problem 2, the solution is
<span class="math display">\[\begin{equation*}
\mu_{jl} = \frac{\sum^n_{i=1}I(C_i = j) x_{il}}{\sum^n_{i=1} I(C_i=j)}.
\end{equation*}\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hierarchical-clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Book.pdf", "Book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
