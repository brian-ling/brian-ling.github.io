<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Linear Regression Models | STAT 362 R for Data Science</title>
  <meta name="description" content="Notes for STAT 362" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Linear Regression Models | STAT 362 R for Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes for STAT 362" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Linear Regression Models | STAT 362 R for Data Science" />
  
  <meta name="twitter:description" content="Notes for STAT 362" />
  

<meta name="author" content="Brian Ling" />


<meta name="date" content="2022-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-trees.html"/>
<link rel="next" href="logistic-regression-model.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> What is R and RStudio?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-will-you-learn-in-this-course"><i class="fa fa-check"></i><b>1.2</b> What will you learn in this course?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#r-and-r-as-a-programming-language"><i class="fa fa-check"></i><b>1.2.1</b> R and R as a programming language</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#data-wrangling"><i class="fa fa-check"></i><b>1.2.2</b> Data Wrangling</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#data-visualization"><i class="fa fa-check"></i><b>1.2.3</b> Data Visualization</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#statistical-inference"><i class="fa fa-check"></i><b>1.2.4</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2.5" data-path="introduction.html"><a href="introduction.html#machine-learning"><i class="fa fa-check"></i><b>1.2.5</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2.6" data-path="introduction.html"><a href="introduction.html#some-numerical-methods"><i class="fa fa-check"></i><b>1.2.6</b> Some Numerical Methods</a></li>
<li class="chapter" data-level="1.2.7" data-path="introduction.html"><a href="introduction.html#lastly"><i class="fa fa-check"></i><b>1.2.7</b> Lastly</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#lets-get-started"><i class="fa fa-check"></i><b>1.3</b> Let’s Get Started</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#r-data-structures"><i class="fa fa-check"></i><b>1.4</b> R Data Structures</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#vectors"><i class="fa fa-check"></i><b>1.4.1</b> Vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#factors"><i class="fa fa-check"></i><b>1.4.2</b> Factors</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction.html"><a href="introduction.html#matrix"><i class="fa fa-check"></i><b>1.4.3</b> Matrix</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction.html"><a href="introduction.html#lists"><i class="fa fa-check"></i><b>1.4.4</b> Lists</a></li>
<li class="chapter" data-level="1.4.5" data-path="introduction.html"><a href="introduction.html#data-frames"><i class="fa fa-check"></i><b>1.4.5</b> Data frames</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#operators"><i class="fa fa-check"></i><b>1.5</b> Operators</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introduction.html"><a href="introduction.html#vectorized-operators"><i class="fa fa-check"></i><b>1.5.1</b> Vectorized Operators</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#built-in-functions"><i class="fa fa-check"></i><b>1.6</b> Built-in Functions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="introduction.html"><a href="introduction.html#sort"><i class="fa fa-check"></i><b>1.6.1</b> <code>sort()</code></a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction.html"><a href="introduction.html#seq"><i class="fa fa-check"></i><b>1.6.2</b> <code>seq()</code></a></li>
<li class="chapter" data-level="1.6.3" data-path="introduction.html"><a href="introduction.html#rep"><i class="fa fa-check"></i><b>1.6.3</b> <code>rep()</code></a></li>
<li class="chapter" data-level="1.6.4" data-path="introduction.html"><a href="introduction.html#pmax-pmin"><i class="fa fa-check"></i><b>1.6.4</b> <code>pmax</code>, <code>pmin</code></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#some-useful-rstudio-shortcuts"><i class="fa fa-check"></i><b>1.7</b> Some Useful RStudio Shortcuts</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#comments-to-exercises"><i class="fa fa-check"></i><b>1.9</b> Comments to Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Probability Distributions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="probability.html"><a href="probability.html#common-distributions"><i class="fa fa-check"></i><b>2.1.1</b> Common Distributions</a></li>
<li class="chapter" data-level="2.1.2" data-path="probability.html"><a href="probability.html#exercises-1"><i class="fa fa-check"></i><b>2.1.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability.html"><a href="probability.html#simulation"><i class="fa fa-check"></i><b>2.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>3</b> Programming in R</a><ul>
<li class="chapter" data-level="3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#writing-functions-in-r"><i class="fa fa-check"></i><b>3.1</b> Writing functions in R</a></li>
<li class="chapter" data-level="3.2" data-path="programming-in-r.html"><a href="programming-in-r.html#control-flow"><i class="fa fa-check"></i><b>3.2</b> Control Flow</a><ul>
<li class="chapter" data-level="3.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#for-loop"><i class="fa fa-check"></i><b>3.2.1</b> for loop</a></li>
<li class="chapter" data-level="3.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#while-loop"><i class="fa fa-check"></i><b>3.2.2</b> while loop</a></li>
<li class="chapter" data-level="3.2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond"><i class="fa fa-check"></i><b>3.2.3</b> if (cond)</a></li>
<li class="chapter" data-level="3.2.4" data-path="programming-in-r.html"><a href="programming-in-r.html#if-cond-else-expr"><i class="fa fa-check"></i><b>3.2.4</b> if (cond) else expr</a></li>
<li class="chapter" data-level="3.2.5" data-path="programming-in-r.html"><a href="programming-in-r.html#if-else-ladder"><i class="fa fa-check"></i><b>3.2.5</b> If else ladder</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="programming-in-r.html"><a href="programming-in-r.html#automatically-reindent-code"><i class="fa fa-check"></i><b>3.3</b> Automatically Reindent Code</a></li>
<li class="chapter" data-level="3.4" data-path="programming-in-r.html"><a href="programming-in-r.html#speed-consideration"><i class="fa fa-check"></i><b>3.4</b> Speed Consideration</a></li>
<li class="chapter" data-level="3.5" data-path="programming-in-r.html"><a href="programming-in-r.html#another-simulation-example"><i class="fa fa-check"></i><b>3.5</b> Another Simulation Example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html"><i class="fa fa-check"></i><b>4</b> Creating Some Basic Plots</a><ul>
<li class="chapter" data-level="4.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#scatter-plot"><i class="fa fa-check"></i><b>4.1</b> Scatter Plot</a></li>
<li class="chapter" data-level="4.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#line-graph"><i class="fa fa-check"></i><b>4.2</b> Line Graph</a></li>
<li class="chapter" data-level="4.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#bar-chart"><i class="fa fa-check"></i><b>4.3</b> Bar Chart</a></li>
<li class="chapter" data-level="4.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#histogram"><i class="fa fa-check"></i><b>4.4</b> Histogram</a></li>
<li class="chapter" data-level="4.5" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#box-plot"><i class="fa fa-check"></i><b>4.5</b> Box Plot</a></li>
<li class="chapter" data-level="4.6" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#plotting-a-function-curve"><i class="fa fa-check"></i><b>4.6</b> Plotting a function curve</a></li>
<li class="chapter" data-level="4.7" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#more-on-plots-with-base-r"><i class="fa fa-check"></i><b>4.7</b> More on plots with base R</a><ul>
<li class="chapter" data-level="4.7.1" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#multi-frame-plot"><i class="fa fa-check"></i><b>4.7.1</b> Multi-frame plot</a></li>
<li class="chapter" data-level="4.7.2" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#subsubsec:type_of_plot"><i class="fa fa-check"></i><b>4.7.2</b> Type of Plot</a></li>
<li class="chapter" data-level="4.7.3" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#parameters-of-a-plot"><i class="fa fa-check"></i><b>4.7.3</b> Parameters of a plot</a></li>
<li class="chapter" data-level="4.7.4" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#elements-on-plot"><i class="fa fa-check"></i><b>4.7.4</b> Elements on plot</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="creating-some-basic-plots.html"><a href="creating-some-basic-plots.html#summary-of-ggplot"><i class="fa fa-check"></i><b>4.8</b> Summary of <code>ggplot</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html"><i class="fa fa-check"></i><b>5</b> Managing Data with R</a><ul>
<li class="chapter" data-level="5.1" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a></li>
<li class="chapter" data-level="5.2" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#saving-loading-and-removing-r-data-structures"><i class="fa fa-check"></i><b>5.2</b> Saving, loading, and removing R data structures</a></li>
<li class="chapter" data-level="5.3" data-path="managing-data-with-r.html"><a href="managing-data-with-r.html#importing-and-saving-data-from-csv-files"><i class="fa fa-check"></i><b>5.3</b> Importing and saving data from CSV files</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html"><i class="fa fa-check"></i><b>6</b> Review (Chapter 1-5)</a><ul>
<li class="chapter" data-level="6.1" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#simulation-1"><i class="fa fa-check"></i><b>6.1</b> Simulation</a></li>
<li class="chapter" data-level="6.2" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#matrix-1"><i class="fa fa-check"></i><b>6.2</b> Matrix</a></li>
<li class="chapter" data-level="6.3" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#basic-operation"><i class="fa fa-check"></i><b>6.3</b> Basic Operation</a></li>
<li class="chapter" data-level="6.4" data-path="review-chapter-1-5.html"><a href="review-chapter-1-5.html#some-basic-plots-in-r"><i class="fa fa-check"></i><b>6.4</b> Some basic plots in R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html"><i class="fa fa-check"></i><b>7</b> Data Transformation with <code>dplyr</code></a><ul>
<li class="chapter" data-level="7.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#arrange"><i class="fa fa-check"></i><b>7.2</b> <code>arrange()</code></a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-2"><i class="fa fa-check"></i><b>7.2.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#filter"><i class="fa fa-check"></i><b>7.3</b> <code>filter()</code></a><ul>
<li class="chapter" data-level="7.3.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-3"><i class="fa fa-check"></i><b>7.3.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#select"><i class="fa fa-check"></i><b>7.4</b> <code>select()</code></a><ul>
<li class="chapter" data-level="7.4.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-4"><i class="fa fa-check"></i><b>7.4.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#mutate"><i class="fa fa-check"></i><b>7.5</b> <code>mutate()</code></a><ul>
<li class="chapter" data-level="7.5.1" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#exercises-5"><i class="fa fa-check"></i><b>7.5.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summarize-group_by"><i class="fa fa-check"></i><b>7.6</b> <code>summarize(), group_by()</code></a></li>
<li class="chapter" data-level="7.7" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#combining-multiple-operations-with-pipe"><i class="fa fa-check"></i><b>7.7</b> Combining Multiple Operations with Pipe <code>%&gt;%</code></a></li>
<li class="chapter" data-level="7.8" data-path="data-transformation-with-dplyr.html"><a href="data-transformation-with-dplyr.html#summary"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html"><i class="fa fa-check"></i><b>8</b> Data Visualization with ggplot2</a><ul>
<li class="chapter" data-level="8.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts"><i class="fa fa-check"></i><b>8.1</b> Bar charts</a></li>
<li class="chapter" data-level="8.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graph-1"><i class="fa fa-check"></i><b>8.2</b> Line Graph</a></li>
<li class="chapter" data-level="8.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plots"><i class="fa fa-check"></i><b>8.3</b> Scatter Plots</a><ul>
<li class="chapter" data-level="8.3.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#overplotting"><i class="fa fa-check"></i><b>8.3.1</b> Overplotting</a></li>
<li class="chapter" data-level="8.3.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#labelling-points-in-a-scatter-plot"><i class="fa fa-check"></i><b>8.3.2</b> Labelling points in a scatter plot</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions"><i class="fa fa-check"></i><b>8.4</b> Summarizing Data Distributions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#histogram-1"><i class="fa fa-check"></i><b>8.4.1</b> Histogram</a></li>
<li class="chapter" data-level="8.4.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#kernel-density-estimate"><i class="fa fa-check"></i><b>8.4.2</b> Kernel Density Estimate</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots"><i class="fa fa-check"></i><b>8.5</b> Saving your plots</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-pdf-vector-files"><i class="fa fa-check"></i><b>8.5.1</b> Outputting to pdf vector files</a></li>
<li class="chapter" data-level="8.5.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#outputting-to-bitmap-files"><i class="fa fa-check"></i><b>8.5.2</b> Outputting to bitmap files</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summary-1"><i class="fa fa-check"></i><b>8.6</b> Summary</a><ul>
<li class="chapter" data-level="8.6.1" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#combining-multiple-operations-with-pipe-1"><i class="fa fa-check"></i><b>8.6.1</b> Combining multiple operations with pipe <code>%&gt;%</code></a></li>
<li class="chapter" data-level="8.6.2" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#bar-charts-1"><i class="fa fa-check"></i><b>8.6.2</b> Bar charts</a></li>
<li class="chapter" data-level="8.6.3" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#line-graphs"><i class="fa fa-check"></i><b>8.6.3</b> Line graphs</a></li>
<li class="chapter" data-level="8.6.4" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#scatter-plot-1"><i class="fa fa-check"></i><b>8.6.4</b> Scatter plot</a></li>
<li class="chapter" data-level="8.6.5" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#summarizing-data-distributions-1"><i class="fa fa-check"></i><b>8.6.5</b> Summarizing data distributions</a></li>
<li class="chapter" data-level="8.6.6" data-path="data-visualization-with-ggplot2.html"><a href="data-visualization-with-ggplot2.html#saving-your-plots-1"><i class="fa fa-check"></i><b>8.6.6</b> Saving your plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html"><i class="fa fa-check"></i><b>9</b> Statistical Inference in R</a><ul>
<li class="chapter" data-level="9.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.1</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="9.1.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#exercises-on-mle"><i class="fa fa-check"></i><b>9.1.1</b> Exercises on MLE</a></li>
<li class="chapter" data-level="9.1.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#summary-2"><i class="fa fa-check"></i><b>9.1.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#interval-estimation-and-hypothesis-testing"><i class="fa fa-check"></i><b>9.2</b> Interval Estimation and Hypothesis Testing</a><ul>
<li class="chapter" data-level="9.2.1" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#examples-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.2.1</b> Examples of Hypothesis Testing</a></li>
<li class="chapter" data-level="9.2.2" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#null-hypotheses-alternative-hypotheses-and-p-values"><i class="fa fa-check"></i><b>9.2.2</b> Null Hypotheses, Alternative Hypotheses, and p-values</a></li>
<li class="chapter" data-level="9.2.3" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#type-i-error-and-type-ii-error"><i class="fa fa-check"></i><b>9.2.3</b> Type I error and Type II error</a></li>
<li class="chapter" data-level="9.2.4" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-for-mean-of-one-sample"><i class="fa fa-check"></i><b>9.2.4</b> Inference for Mean of One Sample</a></li>
<li class="chapter" data-level="9.2.5" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#comparing-the-means-of-two-samples"><i class="fa fa-check"></i><b>9.2.5</b> Comparing the means of two samples</a></li>
<li class="chapter" data-level="9.2.6" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#inference-of-a-sample-proportion"><i class="fa fa-check"></i><b>9.2.6</b> Inference of a Sample Proportion</a></li>
<li class="chapter" data-level="9.2.7" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-groups-for-equal-proportions"><i class="fa fa-check"></i><b>9.2.7</b> Testing groups for equal proportions</a></li>
<li class="chapter" data-level="9.2.8" data-path="statistical-inference-in-r.html"><a href="statistical-inference-in-r.html#testing-if-two-samples-have-the-same-underlying-distribution"><i class="fa fa-check"></i><b>9.2.8</b> Testing if two samples have the same underlying distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html"><i class="fa fa-check"></i><b>10</b> Root finding and optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#root-finding"><i class="fa fa-check"></i><b>10.1</b> Root Finding</a><ul>
<li class="chapter" data-level="10.1.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#bisection-method"><i class="fa fa-check"></i><b>10.1.1</b> Bisection Method</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method"><i class="fa fa-check"></i><b>10.2</b> Newton-Raphson Method</a></li>
<li class="chapter" data-level="10.3" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#minimization-and-maximization"><i class="fa fa-check"></i><b>10.3</b> Minimization and Maximization</a><ul>
<li class="chapter" data-level="10.3.1" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#newton-raphson-method-1"><i class="fa fa-check"></i><b>10.3.1</b> Newton-Raphson Method</a></li>
<li class="chapter" data-level="10.3.2" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#gradient-descent"><i class="fa fa-check"></i><b>10.3.2</b> Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="root-finding-and-optimization.html"><a href="root-finding-and-optimization.html#optim"><i class="fa fa-check"></i><b>10.4</b> <code>optim</code></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>11</b> k-nearest neighbors</a><ul>
<li class="chapter" data-level="11.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#feature-scaling"><i class="fa fa-check"></i><b>11.2</b> Feature Scaling</a></li>
<li class="chapter" data-level="11.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#example-classifying-breast-cancers"><i class="fa fa-check"></i><b>11.3</b> Example: Classifying Breast Cancers</a><ul>
<li class="chapter" data-level="11.3.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#evaluating-model-performance"><i class="fa fa-check"></i><b>11.3.1</b> Evaluating Model Performance</a></li>
<li class="chapter" data-level="11.3.2" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#using-z-score-standardization"><i class="fa fa-check"></i><b>11.3.2</b> Using <span class="math inline">\(z\)</span>-score standardization</a></li>
<li class="chapter" data-level="11.3.3" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#testing-alternative-values-of-k"><i class="fa fa-check"></i><b>11.3.3</b> Testing alternative values of <span class="math inline">\(k\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="classification-trees.html"><a href="classification-trees.html"><i class="fa fa-check"></i><b>12</b> Classification Trees</a><ul>
<li class="chapter" data-level="12.1" data-path="classification-trees.html"><a href="classification-trees.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="classification-trees.html"><a href="classification-trees.html#the-c5.0-classification-tree-algorithm"><i class="fa fa-check"></i><b>12.2</b> The C5.0 classification tree algorithm</a><ul>
<li class="chapter" data-level="12.2.1" data-path="classification-trees.html"><a href="classification-trees.html#choosing-the-best-split"><i class="fa fa-check"></i><b>12.2.1</b> Choosing the best split</a></li>
<li class="chapter" data-level="12.2.2" data-path="classification-trees.html"><a href="classification-trees.html#pruning-the-decision-tree"><i class="fa fa-check"></i><b>12.2.2</b> Pruning the decision tree</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="classification-trees.html"><a href="classification-trees.html#example-iris"><i class="fa fa-check"></i><b>12.3</b> Example: <code>iris</code></a></li>
<li class="chapter" data-level="12.4" data-path="classification-trees.html"><a href="classification-trees.html#example-identifying-risky-bank-loans"><i class="fa fa-check"></i><b>12.4</b> Example: identifying risky bank loans</a><ul>
<li class="chapter" data-level="12.4.1" data-path="classification-trees.html"><a href="classification-trees.html#adaptive-boosting"><i class="fa fa-check"></i><b>12.4.1</b> Adaptive boosting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>13</b> Linear Regression Models</a><ul>
<li class="chapter" data-level="13.1" data-path="linear-regression-models.html"><a href="linear-regression-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>13.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="13.2" data-path="linear-regression-models.html"><a href="linear-regression-models.html#smoothed-conditional-means"><i class="fa fa-check"></i><b>13.2</b> Smoothed Conditional Means</a></li>
<li class="chapter" data-level="13.3" data-path="linear-regression-models.html"><a href="linear-regression-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>13.3</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="13.4" data-path="linear-regression-models.html"><a href="linear-regression-models.html#example-diamonds"><i class="fa fa-check"></i><b>13.4</b> Example: <code>diamonds</code></a></li>
<li class="chapter" data-level="13.5" data-path="linear-regression-models.html"><a href="linear-regression-models.html#categorical-predictors"><i class="fa fa-check"></i><b>13.5</b> Categorical Predictors</a></li>
<li class="chapter" data-level="13.6" data-path="linear-regression-models.html"><a href="linear-regression-models.html#compare-models-using-anova"><i class="fa fa-check"></i><b>13.6</b> Compare models using ANOVA</a></li>
<li class="chapter" data-level="13.7" data-path="linear-regression-models.html"><a href="linear-regression-models.html#prediction"><i class="fa fa-check"></i><b>13.7</b> Prediction</a></li>
<li class="chapter" data-level="13.8" data-path="linear-regression-models.html"><a href="linear-regression-models.html#interaction-terms"><i class="fa fa-check"></i><b>13.8</b> Interaction Terms</a></li>
<li class="chapter" data-level="13.9" data-path="linear-regression-models.html"><a href="linear-regression-models.html#variable-transformation"><i class="fa fa-check"></i><b>13.9</b> Variable Transformation</a></li>
<li class="chapter" data-level="13.10" data-path="linear-regression-models.html"><a href="linear-regression-models.html#polynomial-regression"><i class="fa fa-check"></i><b>13.10</b> Polynomial Regression</a></li>
<li class="chapter" data-level="13.11" data-path="linear-regression-models.html"><a href="linear-regression-models.html#stepwise-regression"><i class="fa fa-check"></i><b>13.11</b> Stepwise regression</a></li>
<li class="chapter" data-level="13.12" data-path="linear-regression-models.html"><a href="linear-regression-models.html#best-subset"><i class="fa fa-check"></i><b>13.12</b> Best subset</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="logistic-regression-model.html"><a href="logistic-regression-model.html"><i class="fa fa-check"></i><b>14</b> Logistic Regression Model</a></li>
<li class="chapter" data-level="15" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>15</b> k-means Clustering</a><ul>
<li class="chapter" data-level="15.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-4"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications"><i class="fa fa-check"></i><b>15.2</b> Applications</a><ul>
<li class="chapter" data-level="15.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#cluster-analysis"><i class="fa fa-check"></i><b>15.2.1</b> Cluster Analysis</a></li>
<li class="chapter" data-level="15.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#image-segementation"><i class="fa fa-check"></i><b>15.2.2</b> Image Segementation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>16</b> Neural Networks</a><ul>
<li class="chapter" data-level="16.1" data-path="neural-networks.html"><a href="neural-networks.html#introduction-5"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="neural-networks.html"><a href="neural-networks.html#regression-predict-the-strength-of-concrete"><i class="fa fa-check"></i><b>16.2</b> Regression: Predict the Strength of Concrete</a><ul>
<li class="chapter" data-level="16.2.1" data-path="neural-networks.html"><a href="neural-networks.html#data"><i class="fa fa-check"></i><b>16.2.1</b> Data</a></li>
<li class="chapter" data-level="16.2.2" data-path="neural-networks.html"><a href="neural-networks.html#training-a-model"><i class="fa fa-check"></i><b>16.2.2</b> Training a model</a></li>
<li class="chapter" data-level="16.2.3" data-path="neural-networks.html"><a href="neural-networks.html#understanding-the-model"><i class="fa fa-check"></i><b>16.2.3</b> Understanding the model</a></li>
<li class="chapter" data-level="16.2.4" data-path="neural-networks.html"><a href="neural-networks.html#evaluating-the-performance"><i class="fa fa-check"></i><b>16.2.4</b> Evaluating the Performance</a></li>
<li class="chapter" data-level="16.2.5" data-path="neural-networks.html"><a href="neural-networks.html#improving-the-model"><i class="fa fa-check"></i><b>16.2.5</b> Improving the Model</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="neural-networks.html"><a href="neural-networks.html#classification"><i class="fa fa-check"></i><b>16.3</b> Classification</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html"><i class="fa fa-check"></i><b>17</b> Evaluating Model Performance</a><ul>
<li class="chapter" data-level="17.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#accuracy-and-confusion-matrix"><i class="fa fa-check"></i><b>17.1</b> Accuracy and Confusion Matrix</a></li>
<li class="chapter" data-level="17.2" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#other-measures-of-performance"><i class="fa fa-check"></i><b>17.2</b> Other Measures of Performance</a><ul>
<li class="chapter" data-level="17.2.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#the-kappa-statistic"><i class="fa fa-check"></i><b>17.2.1</b> The kappa statistic</a></li>
<li class="chapter" data-level="17.2.2" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>17.2.2</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="17.2.3" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#roc-and-auc"><i class="fa fa-check"></i><b>17.2.3</b> ROC and AUC</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#estimating-future-performances"><i class="fa fa-check"></i><b>17.3</b> Estimating future performances</a><ul>
<li class="chapter" data-level="17.3.1" data-path="evaluating-model-performance-1.html"><a href="evaluating-model-performance-1.html#cross-validation"><i class="fa fa-check"></i><b>17.3.1</b> Cross-validation</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 362 R for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression-models" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Linear Regression Models</h1>
<p>Reference: R cookbook <a href="https://rc2e.com/linearregressionandanova" class="uri">https://rc2e.com/linearregressionandanova</a>, any linear regression textbooks.</p>
<p>Packages used in this chapter:</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="linear-regression-models.html#cb406-1"></a><span class="kw">library</span>(tidyverse) <span class="co"># contains ggplot2 and dplyr</span></span>
<span id="cb406-2"><a href="linear-regression-models.html#cb406-2"></a><span class="kw">library</span>(nycflights13) <span class="co"># contains the dataset &quot;flights&quot;</span></span>
<span id="cb406-3"><a href="linear-regression-models.html#cb406-3"></a><span class="kw">library</span>(bestglm) <span class="co"># find the best subset model</span></span></code></pre></div>
<p>Regression analysis is the study of the relationship between the responses and the covariates.</p>
<p>Linear regression is one of the most used statistical techniques when the response is continuous. It can be used for prediction and explain variation in the response variable.</p>
<p>Two types of variables:</p>
<ol style="list-style-type: decimal">
<li>response = dependent variable, usually denoted by <span class="math inline">\(Y\)</span></li>
<li>explanatory variable = independent variable = covariate = predictor = feature, usually denoted by <span class="math inline">\(X_1,\ldots,X_p\)</span></li>
</ol>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">13.1</span> Simple Linear Regression</h2>
<p>Suppose we have data <span class="math inline">\((x_1,y_1),\ldots,(x_n,y_n)\)</span>. A simple linear regression specifies that
<span class="math display">\[\begin{equation*}
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i,
\end{equation*}\]</span>
where <span class="math inline">\(E(\varepsilon_i) = 0\)</span>, <span class="math inline">\(Var(\varepsilon_i) = \sigma^2 &gt; 0\)</span>, and <span class="math inline">\(\varepsilon_i\)</span>’s are i.i.d.
Taking expectation on both sides of the above equation, we see that
<span class="math display">\[\begin{equation*}
E(Y|X=x) = \beta_0 + \beta_1 x.
\end{equation*}\]</span>
Hence, the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span> is linear in <span class="math inline">\(x\)</span>.</p>
<p>In the model:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the intercept (mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(x=0\)</span>)</li>
<li><span class="math inline">\(\beta_1\)</span> is the slope (which is the change in the mean of <span class="math inline">\(Y\)</span> for <span class="math inline">\(1\)</span> unit increase in <span class="math inline">\(x\)</span>)</li>
<li><span class="math inline">\(\varepsilon\)</span> is the error term (anything that is not explained by <span class="math inline">\(x\)</span>). It is the vertical distance between <span class="math inline">\(y\)</span> and the conditional mean <span class="math inline">\(E(Y|X=x)\)</span>.</li>
<li><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown parameters to be estimated from data</li>
</ul>
<p>How to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>? Recall that a line can be specified by the intercept (<span class="math inline">\(\beta_0\)</span>) and slope (<span class="math inline">\(\beta_1\)</span>). Therefore, intuitively, we want to find a line that best fits the points. How to define the “best fit”?</p>
<p><strong>Answer: Method of least square</strong></p>
<p>We can minimize the residual sum of squares, which is defined by
<span class="math display">\[\begin{equation*}
\sum^n_{i=1} [ y_i - (\beta_0 + \beta_1 x_i)]^2.
\end{equation*}\]</span>
The least square estimator for <span class="math inline">\((\beta_0, \beta_1)\)</span> is defined as the minimizer of the residual sum of squares. That is,
<span class="math display">\[\begin{equation*}
(\hat{\beta}_0, \hat{\beta}_1) := \text{argmin}_{\beta_0, \beta_1} \sum^n_{i=1} [ y_i - (\beta_0 + \beta_1 x_i)]^2.
\end{equation*}\]</span></p>
<p>The minimizer can be found by</p>
<ol style="list-style-type: decimal">
<li>differentiating the objection function with respect to <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></li>
<li>setting the resulting expressions to <span class="math inline">\(0\)</span></li>
<li>solving the simultaneous equations</li>
</ol>
<p>The above steps lead to a closed-form formula for <span class="math inline">\((\hat{\beta}_0, \hat{\beta}_1)\)</span> in terms of <span class="math inline">\(x\)</span>’s and <span class="math inline">\(y\)</span>’s, which is a special case of the formula given in Section 11.3 Multiple Linear Regression.</p>
<p>The residual is <span class="math inline">\(\hat{\varepsilon}_i = y_i - \hat{y}_i\)</span>, where <span class="math inline">\(\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i\)</span>.</p>
<p>To perform linear regression in R, we use <code>lm()</code>.</p>
<p><strong>Examples in R</strong></p>
<p>Recall the dataset <code>flights</code> in the package <code>nycflights13</code>.</p>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="linear-regression-models.html#cb407-1"></a><span class="co"># y = arr_delay, x = dep_delay</span></span>
<span id="cb407-2"><a href="linear-regression-models.html#cb407-2"></a><span class="kw">lm</span>(arr_delay <span class="op">~</span><span class="st"> </span>dep_delay, <span class="dt">data =</span> flights)</span>
<span id="cb407-3"><a href="linear-regression-models.html#cb407-3"></a><span class="co">## </span></span>
<span id="cb407-4"><a href="linear-regression-models.html#cb407-4"></a><span class="co">## Call:</span></span>
<span id="cb407-5"><a href="linear-regression-models.html#cb407-5"></a><span class="co">## lm(formula = arr_delay ~ dep_delay, data = flights)</span></span>
<span id="cb407-6"><a href="linear-regression-models.html#cb407-6"></a><span class="co">## </span></span>
<span id="cb407-7"><a href="linear-regression-models.html#cb407-7"></a><span class="co">## Coefficients:</span></span>
<span id="cb407-8"><a href="linear-regression-models.html#cb407-8"></a><span class="co">## (Intercept)    dep_delay  </span></span>
<span id="cb407-9"><a href="linear-regression-models.html#cb407-9"></a><span class="co">##      -5.899        1.019</span></span></code></pre></div>
<p>The regression equation is</p>
<p><span class="math display">\[\begin{equation*}
\text{arr delay} = -5.899 + 1.019 \text{dep delay} + \varepsilon.
\end{equation*}\]</span></p>
<p>From this equation, we can see that for <span class="math inline">\(1\)</span> minute increase in departure delay, the arrival delay will increase by <span class="math inline">\(1.019\)</span> minute on average. When there is no departure delay, the flights arrive earlier on average (a negative arrival delay means there was an early arrival).</p>
<p>Perform a simple linear regression without a dataset</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="linear-regression-models.html#cb408-1"></a><span class="co"># Example 1</span></span>
<span id="cb408-2"><a href="linear-regression-models.html#cb408-2"></a><span class="kw">lm</span>(flights<span class="op">$</span>arr_delay <span class="op">~</span><span class="st"> </span>flights<span class="op">$</span>dep_delay)</span>
<span id="cb408-3"><a href="linear-regression-models.html#cb408-3"></a><span class="co">## </span></span>
<span id="cb408-4"><a href="linear-regression-models.html#cb408-4"></a><span class="co">## Call:</span></span>
<span id="cb408-5"><a href="linear-regression-models.html#cb408-5"></a><span class="co">## lm(formula = flights$arr_delay ~ flights$dep_delay)</span></span>
<span id="cb408-6"><a href="linear-regression-models.html#cb408-6"></a><span class="co">## </span></span>
<span id="cb408-7"><a href="linear-regression-models.html#cb408-7"></a><span class="co">## Coefficients:</span></span>
<span id="cb408-8"><a href="linear-regression-models.html#cb408-8"></a><span class="co">##       (Intercept)  flights$dep_delay  </span></span>
<span id="cb408-9"><a href="linear-regression-models.html#cb408-9"></a><span class="co">##            -5.899              1.019</span></span></code></pre></div>
<p>Another example</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="linear-regression-models.html#cb409-1"></a><span class="co"># Example 2</span></span>
<span id="cb409-2"><a href="linear-regression-models.html#cb409-2"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb409-3"><a href="linear-regression-models.html#cb409-3"></a>x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb409-4"><a href="linear-regression-models.html#cb409-4"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb409-5"><a href="linear-regression-models.html#cb409-5"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x)</span>
<span id="cb409-6"><a href="linear-regression-models.html#cb409-6"></a><span class="co">## </span></span>
<span id="cb409-7"><a href="linear-regression-models.html#cb409-7"></a><span class="co">## Call:</span></span>
<span id="cb409-8"><a href="linear-regression-models.html#cb409-8"></a><span class="co">## lm(formula = y ~ x)</span></span>
<span id="cb409-9"><a href="linear-regression-models.html#cb409-9"></a><span class="co">## </span></span>
<span id="cb409-10"><a href="linear-regression-models.html#cb409-10"></a><span class="co">## Coefficients:</span></span>
<span id="cb409-11"><a href="linear-regression-models.html#cb409-11"></a><span class="co">## (Intercept)            x  </span></span>
<span id="cb409-12"><a href="linear-regression-models.html#cb409-12"></a><span class="co">##      0.9623       0.9989</span></span></code></pre></div>
<p>To visualize the regression line using <code>ggplot()</code>, use <code>geom_smooth()</code> and set <code>method = lm</code>.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="linear-regression-models.html#cb410-1"></a><span class="kw">ggplot</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y=</span> y)) <span class="op">+</span></span>
<span id="cb410-2"><a href="linear-regression-models.html#cb410-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb410-3"><a href="linear-regression-models.html#cb410-3"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> lm)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-450-1.png" width="55%" style="display: block; margin: auto;" /></p>
<p>To remove the confidence interval, set <code>se = FALSE</code> in <code>geom_smooth()</code>.</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="linear-regression-models.html#cb411-1"></a><span class="kw">ggplot</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y=</span> y)) <span class="op">+</span></span>
<span id="cb411-2"><a href="linear-regression-models.html#cb411-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb411-3"><a href="linear-regression-models.html#cb411-3"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> lm, <span class="dt">se =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-451-1.png" width="55%" style="display: block; margin: auto;" /></p>
<p>Remark: You have seen <code>geom_smooth()</code> in Assignment 3.</p>
</div>
<div id="smoothed-conditional-means" class="section level2">
<h2><span class="header-section-number">13.2</span> Smoothed Conditional Means</h2>
<p>In simple linear regression, a linear relationship between the response and predictor is assumed. In many cases, this assumption may not hold. In those situations, we may fit a curve to the data. For example, we can use <code>geom_smooth()</code> (without setting <code>method = lm</code>) to visualize the fitting of a function through the points of a scatterplot that “best represents” the relationship bewteen the response and the predictor without assuming the linear relationship.</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="linear-regression-models.html#cb412-1"></a>cars <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb412-2"><a href="linear-regression-models.html#cb412-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> speed, <span class="dt">y =</span> dist)) <span class="op">+</span></span>
<span id="cb412-3"><a href="linear-regression-models.html#cb412-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb412-4"><a href="linear-regression-models.html#cb412-4"></a><span class="st">  </span><span class="kw">geom_smooth</span>()</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-452-1.png" width="55%" style="display: block; margin: auto;" /></p>
<p>You can see the message saying <code>geom_smooth()</code> is using <code>method = "loess"</code>. loess stands for locally estimated scatterplot smoothing. You may also see “lowess”, which stands for
locally weighted scatterplot smoothing. Basically, a locally weighted regression solves a separate weighted least squares problem at each target point <span class="math inline">\(x_0\)</span>:
<span class="math display">\[\begin{equation*}
\min_{\alpha(x_0), \beta(x_0)} \sum^n_{i=1} K_\lambda(x_0, x_i)[y_i - \alpha(x_0) - \beta(x_0) x_i]^2,
\end{equation*}\]</span>
where <span class="math inline">\(K_\lambda(x_0, x_i) = K(|x_i-x_0|/\lambda)\)</span> for some kernel function <span class="math inline">\(K\)</span> and <span class="math inline">\(\lambda\)</span> is a positive number. The estimate is <span class="math inline">\(\hat{f}(x_0) = \hat{\alpha}(x_0) + \hat{\beta}(x_0)x_0\)</span>. The idea is that data points that are close to <span class="math inline">\(x_0\)</span> will have larger weights <span class="math inline">\(K_\lambda(x_0, x_i)\)</span> so that the points near <span class="math inline">\(x_0\)</span> are more important in estimating <span class="math inline">\(\alpha(x_0)\)</span> and <span class="math inline">\(\beta(x_0)\)</span>. This is the meaning of “locally weighted”.</p>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2><span class="header-section-number">13.3</span> Multiple Linear Regression</h2>
<p>We can add more predictors to explain the response variable better:
<span class="math display">\[\begin{equation*}
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \varepsilon_i.
\end{equation*}\]</span>
Suppose we have <span class="math inline">\(n\)</span> data, then we can use the matrix notation to represent our model:
<span class="math display">\[\begin{equation*}
Y = X \beta + \varepsilon,
\end{equation*}\]</span>
where
<span class="math display">\[\begin{equation*}
y = \left( 
\begin{array}{c}
y_1 \\
\vdots \\
y_n 
\end{array}
\right), \quad 
X = \left(
\begin{array}{cccc}
1 &amp; x_{11} &amp; \ldots &amp; x_{1p}\\
1 &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n1} &amp; \ldots &amp; x_{np}
\end{array}
\right), \quad \text{and } 
\varepsilon 
= \left( 
\begin{array}{c}
\varepsilon_1 \\
\vdots \\
\varepsilon_n 
\end{array}
\right).
\end{equation*}\]</span>
As in simple linear regression, we estimate <span class="math inline">\(\beta\)</span> by minimizing the residual sum of squares:
<span class="math display">\[\begin{equation*}
\sum^n_{i=1}(y_i - \beta_0 - \beta_1 x_{i1} - \beta_2 x_{i2} - \ldots - \beta_p x_{ip})^2.
\end{equation*}\]</span>
The least square estimator for <span class="math inline">\(\beta\)</span> is
<span class="math display">\[\begin{equation*}
\hat{\beta} = (X^T X)^{-1}X^T Y.
\end{equation*}\]</span></p>
<p><strong>Examples in R</strong></p>
<p>Without a dataframe:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="linear-regression-models.html#cb413-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb413-2"><a href="linear-regression-models.html#cb413-2"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb413-3"><a href="linear-regression-models.html#cb413-3"></a>x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb413-4"><a href="linear-regression-models.html#cb413-4"></a>x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb413-5"><a href="linear-regression-models.html#cb413-5"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb413-6"><a href="linear-regression-models.html#cb413-6"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2)</span>
<span id="cb413-7"><a href="linear-regression-models.html#cb413-7"></a><span class="co">## </span></span>
<span id="cb413-8"><a href="linear-regression-models.html#cb413-8"></a><span class="co">## Call:</span></span>
<span id="cb413-9"><a href="linear-regression-models.html#cb413-9"></a><span class="co">## lm(formula = y ~ x1 + x2)</span></span>
<span id="cb413-10"><a href="linear-regression-models.html#cb413-10"></a><span class="co">## </span></span>
<span id="cb413-11"><a href="linear-regression-models.html#cb413-11"></a><span class="co">## Coefficients:</span></span>
<span id="cb413-12"><a href="linear-regression-models.html#cb413-12"></a><span class="co">## (Intercept)           x1           x2  </span></span>
<span id="cb413-13"><a href="linear-regression-models.html#cb413-13"></a><span class="co">##       1.025        2.021        2.947</span></span></code></pre></div>
<p>With a dataframe:</p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="linear-regression-models.html#cb414-1"></a>new_data &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">response =</span> y, <span class="dt">cov1 =</span> x1, <span class="dt">cov2 =</span> x2)</span>
<span id="cb414-2"><a href="linear-regression-models.html#cb414-2"></a></span>
<span id="cb414-3"><a href="linear-regression-models.html#cb414-3"></a><span class="co"># use the column names of your dataframe</span></span>
<span id="cb414-4"><a href="linear-regression-models.html#cb414-4"></a><span class="kw">lm</span>(response <span class="op">~</span><span class="st"> </span>cov1 <span class="op">+</span><span class="st"> </span>cov2, <span class="dt">data =</span> new_data)</span>
<span id="cb414-5"><a href="linear-regression-models.html#cb414-5"></a><span class="co">## </span></span>
<span id="cb414-6"><a href="linear-regression-models.html#cb414-6"></a><span class="co">## Call:</span></span>
<span id="cb414-7"><a href="linear-regression-models.html#cb414-7"></a><span class="co">## lm(formula = response ~ cov1 + cov2, data = new_data)</span></span>
<span id="cb414-8"><a href="linear-regression-models.html#cb414-8"></a><span class="co">## </span></span>
<span id="cb414-9"><a href="linear-regression-models.html#cb414-9"></a><span class="co">## Coefficients:</span></span>
<span id="cb414-10"><a href="linear-regression-models.html#cb414-10"></a><span class="co">## (Intercept)         cov1         cov2  </span></span>
<span id="cb414-11"><a href="linear-regression-models.html#cb414-11"></a><span class="co">##       1.025        2.021        2.947</span></span></code></pre></div>
<p>To obtain more information about the model:</p>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="linear-regression-models.html#cb415-1"></a><span class="co"># assign the model object to a variable </span></span>
<span id="cb415-2"><a href="linear-regression-models.html#cb415-2"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2)</span>
<span id="cb415-3"><a href="linear-regression-models.html#cb415-3"></a></span>
<span id="cb415-4"><a href="linear-regression-models.html#cb415-4"></a><span class="co"># summary is one of the most important functions for linear regression</span></span>
<span id="cb415-5"><a href="linear-regression-models.html#cb415-5"></a><span class="kw">summary</span>(fit)</span>
<span id="cb415-6"><a href="linear-regression-models.html#cb415-6"></a><span class="co">## </span></span>
<span id="cb415-7"><a href="linear-regression-models.html#cb415-7"></a><span class="co">## Call:</span></span>
<span id="cb415-8"><a href="linear-regression-models.html#cb415-8"></a><span class="co">## lm(formula = y ~ x1 + x2)</span></span>
<span id="cb415-9"><a href="linear-regression-models.html#cb415-9"></a><span class="co">## </span></span>
<span id="cb415-10"><a href="linear-regression-models.html#cb415-10"></a><span class="co">## Residuals:</span></span>
<span id="cb415-11"><a href="linear-regression-models.html#cb415-11"></a><span class="co">##      Min       1Q   Median       3Q      Max </span></span>
<span id="cb415-12"><a href="linear-regression-models.html#cb415-12"></a><span class="co">## -2.94359 -0.43645  0.00202  0.63692  2.63941 </span></span>
<span id="cb415-13"><a href="linear-regression-models.html#cb415-13"></a><span class="co">## </span></span>
<span id="cb415-14"><a href="linear-regression-models.html#cb415-14"></a><span class="co">## Coefficients:</span></span>
<span id="cb415-15"><a href="linear-regression-models.html#cb415-15"></a><span class="co">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb415-16"><a href="linear-regression-models.html#cb415-16"></a><span class="co">## (Intercept)   1.0254     0.1052   9.747 4.71e-16 ***</span></span>
<span id="cb415-17"><a href="linear-regression-models.html#cb415-17"></a><span class="co">## x1            2.0211     0.1168  17.311  &lt; 2e-16 ***</span></span>
<span id="cb415-18"><a href="linear-regression-models.html#cb415-18"></a><span class="co">## x2            2.9465     0.1095  26.914  &lt; 2e-16 ***</span></span>
<span id="cb415-19"><a href="linear-regression-models.html#cb415-19"></a><span class="co">## ---</span></span>
<span id="cb415-20"><a href="linear-regression-models.html#cb415-20"></a><span class="co">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb415-21"><a href="linear-regression-models.html#cb415-21"></a><span class="co">## </span></span>
<span id="cb415-22"><a href="linear-regression-models.html#cb415-22"></a><span class="co">## Residual standard error: 1.043 on 97 degrees of freedom</span></span>
<span id="cb415-23"><a href="linear-regression-models.html#cb415-23"></a><span class="co">## Multiple R-squared:  0.9134, Adjusted R-squared:  0.9116 </span></span>
<span id="cb415-24"><a href="linear-regression-models.html#cb415-24"></a><span class="co">## F-statistic: 511.6 on 2 and 97 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>From the summary, you can find</p>
<ol style="list-style-type: decimal">
<li><p>Estimates of the regression coefficients. The <span class="math inline">\(p\)</span>-value indicates if the regression coefficient is significantly different from <span class="math inline">\(0\)</span>. If the <span class="math inline">\(p\)</span>-value is smaller than <span class="math inline">\(0.05\)</span>, then we reject the null hypothesis that the regression coefficient is equal to <span class="math inline">\(0\)</span> at <span class="math inline">\(0.05\)</span> significance level.</p></li>
<li><p><span class="math inline">\(R^2\)</span> is a measure of the variance of <span class="math inline">\(y\)</span> that is explained by the model. The higher the <span class="math inline">\(R^2\)</span> is, the “better” is your model. However, adding additional variables will always increase <span class="math inline">\(R^2\)</span> while this may not improve the model in the sense that it may not improve your prediction for new data. The adjusted <span class="math inline">\(R^2\)</span> accounts for the number of variables in the model and is a better measure of the model’s quality.</p></li>
<li><p><span class="math inline">\(F\)</span>-statistic tells you whether your model is statistically significant or not. The null hypothesis is that all coefficients are zero and the alternative hypothesis is that not all coefficients are zero.</p></li>
</ol>
<p>Objects in <code>fit</code></p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="linear-regression-models.html#cb416-1"></a><span class="kw">names</span>(fit)</span>
<span id="cb416-2"><a href="linear-regression-models.html#cb416-2"></a><span class="co">##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;          &quot;fitted.values&quot; &quot;assign&quot;       </span></span>
<span id="cb416-3"><a href="linear-regression-models.html#cb416-3"></a><span class="co">##  [7] &quot;qr&quot;            &quot;df.residual&quot;   &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;</span></span></code></pre></div>
<p>Extract the coefficients</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="linear-regression-models.html#cb417-1"></a>fit<span class="op">$</span>coefficients <span class="co"># or coef(fit)</span></span>
<span id="cb417-2"><a href="linear-regression-models.html#cb417-2"></a><span class="co">## (Intercept)          x1          x2 </span></span>
<span id="cb417-3"><a href="linear-regression-models.html#cb417-3"></a><span class="co">##    1.025353    2.021110    2.946533</span></span></code></pre></div>
<p>Extract the residuals</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="linear-regression-models.html#cb418-1"></a>fit<span class="op">$</span>residuals <span class="co"># or resid(fit)</span></span></code></pre></div>
<p>Confidence intervals for regression coefficients</p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="linear-regression-models.html#cb419-1"></a><span class="kw">confint</span>(fit)</span>
<span id="cb419-2"><a href="linear-regression-models.html#cb419-2"></a><span class="co">##                 2.5 %   97.5 %</span></span>
<span id="cb419-3"><a href="linear-regression-models.html#cb419-3"></a><span class="co">## (Intercept) 0.8165723 1.234135</span></span>
<span id="cb419-4"><a href="linear-regression-models.html#cb419-4"></a><span class="co">## x1          1.7893884 2.252832</span></span>
<span id="cb419-5"><a href="linear-regression-models.html#cb419-5"></a><span class="co">## x2          2.7292486 3.163818</span></span></code></pre></div>
</div>
<div id="example-diamonds" class="section level2">
<h2><span class="header-section-number">13.4</span> Example: <code>diamonds</code></h2>
<p>Consider the <code>diamonds</code> dataset in <code>ggplot2</code>. Let’s try to predict the price of an diamond based on its characteristics. In <code>diamonds</code>, <code>cut</code>, <code>color</code> and <code>clarity</code> are categorical features. To use them in regression, a standard way is to use the dummy coding discussed in the kNN chapter. The <code>lm()</code> function can handle this automatically as long as the variable is of a factor type.</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="linear-regression-models.html#cb420-1"></a><span class="co"># When the variable is an ordered factor, the names in the output of </span></span>
<span id="cb420-2"><a href="linear-regression-models.html#cb420-2"></a><span class="co"># lm() are weird. So I turn them into an unordered factor first.</span></span>
<span id="cb420-3"><a href="linear-regression-models.html#cb420-3"></a>diamonds2 &lt;-<span class="st"> </span>diamonds</span>
<span id="cb420-4"><a href="linear-regression-models.html#cb420-4"></a>diamonds2<span class="op">$</span>cut &lt;-<span class="st"> </span><span class="kw">factor</span>(diamonds2<span class="op">$</span>cut, <span class="dt">order =</span> <span class="ot">FALSE</span>)</span>
<span id="cb420-5"><a href="linear-regression-models.html#cb420-5"></a>diamonds2<span class="op">$</span>clarity &lt;-<span class="st"> </span><span class="kw">factor</span>(diamonds2<span class="op">$</span>clarity, <span class="dt">order =</span> <span class="ot">FALSE</span>)</span>
<span id="cb420-6"><a href="linear-regression-models.html#cb420-6"></a>diamonds2<span class="op">$</span>color &lt;-<span class="st"> </span><span class="kw">factor</span>(diamonds2<span class="op">$</span>color, <span class="dt">order =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p>Let’s try <code>price ~ cut</code>. The variable <code>cut</code> takes <span class="math inline">\(5\)</span> values: Fair, Good, Very Good, Premium and Ideal. Therefore, we expect to have 5 regresion coefficients (1 for intercept, 4 for <code>cut</code>).</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="linear-regression-models.html#cb421-1"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>cut, <span class="dt">data =</span> diamonds2)</span>
<span id="cb421-2"><a href="linear-regression-models.html#cb421-2"></a><span class="kw">summary</span>(fit)</span>
<span id="cb421-3"><a href="linear-regression-models.html#cb421-3"></a><span class="co">## </span></span>
<span id="cb421-4"><a href="linear-regression-models.html#cb421-4"></a><span class="co">## Call:</span></span>
<span id="cb421-5"><a href="linear-regression-models.html#cb421-5"></a><span class="co">## lm(formula = price ~ cut, data = diamonds2)</span></span>
<span id="cb421-6"><a href="linear-regression-models.html#cb421-6"></a><span class="co">## </span></span>
<span id="cb421-7"><a href="linear-regression-models.html#cb421-7"></a><span class="co">## Residuals:</span></span>
<span id="cb421-8"><a href="linear-regression-models.html#cb421-8"></a><span class="co">##    Min     1Q Median     3Q    Max </span></span>
<span id="cb421-9"><a href="linear-regression-models.html#cb421-9"></a><span class="co">##  -4258  -2741  -1494   1360  15348 </span></span>
<span id="cb421-10"><a href="linear-regression-models.html#cb421-10"></a><span class="co">## </span></span>
<span id="cb421-11"><a href="linear-regression-models.html#cb421-11"></a><span class="co">## Coefficients:</span></span>
<span id="cb421-12"><a href="linear-regression-models.html#cb421-12"></a><span class="co">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb421-13"><a href="linear-regression-models.html#cb421-13"></a><span class="co">## (Intercept)   4358.76      98.79  44.122  &lt; 2e-16 ***</span></span>
<span id="cb421-14"><a href="linear-regression-models.html#cb421-14"></a><span class="co">## cutGood       -429.89     113.85  -3.776 0.000160 ***</span></span>
<span id="cb421-15"><a href="linear-regression-models.html#cb421-15"></a><span class="co">## cutVery Good  -377.00     105.16  -3.585 0.000338 ***</span></span>
<span id="cb421-16"><a href="linear-regression-models.html#cb421-16"></a><span class="co">## cutPremium     225.50     104.40   2.160 0.030772 *  </span></span>
<span id="cb421-17"><a href="linear-regression-models.html#cb421-17"></a><span class="co">## cutIdeal      -901.22     102.41  -8.800  &lt; 2e-16 ***</span></span>
<span id="cb421-18"><a href="linear-regression-models.html#cb421-18"></a><span class="co">## ---</span></span>
<span id="cb421-19"><a href="linear-regression-models.html#cb421-19"></a><span class="co">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb421-20"><a href="linear-regression-models.html#cb421-20"></a><span class="co">## </span></span>
<span id="cb421-21"><a href="linear-regression-models.html#cb421-21"></a><span class="co">## Residual standard error: 3964 on 53935 degrees of freedom</span></span>
<span id="cb421-22"><a href="linear-regression-models.html#cb421-22"></a><span class="co">## Multiple R-squared:  0.01286,    Adjusted R-squared:  0.01279 </span></span>
<span id="cb421-23"><a href="linear-regression-models.html#cb421-23"></a><span class="co">## F-statistic: 175.7 on 4 and 53935 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>From the output, we see that we have <span class="math inline">\(4\)</span> additional predictors named <code>cutGood</code>, <code>cutVery Good</code>, <code>cutPremium</code> and <code>cutIdeal</code> corresponding to the values <code>cut</code> can take. We do not see <code>cutFair</code> because the value fair has been used as the baseline for the regression model. That means if <code>cut</code> is fair, then the mean price is just the intercept value. If <code>cut</code> is Good, then the mean price is <span class="math inline">\(4358.76-429.89.\)</span></p>
<p>The results for the regression coefficients do not make sense because they imply that diamonds with ideal cut are the cheapest on average. The problem is that we have not “controlled for other variables”. The meaning of “control for other variables” is that we should also put in other relevant variables so that the interpretation of the regression coefficients will make sense. For example, if we include <code>carat</code>, then the interpretation of the regression coefficient for <code>cutGood</code> is that fixing the weight of the diamond, diamonds with good cut is how much more expensive than diamonds with fair cut on average (because the baseline value is fair).</p>
<p>Now, let’s try to add <code>carat</code>.</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="linear-regression-models.html#cb422-1"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>cut <span class="op">+</span><span class="st"> </span>carat, <span class="dt">data =</span> diamonds2)</span>
<span id="cb422-2"><a href="linear-regression-models.html#cb422-2"></a><span class="kw">summary</span>(fit)</span>
<span id="cb422-3"><a href="linear-regression-models.html#cb422-3"></a><span class="co">## </span></span>
<span id="cb422-4"><a href="linear-regression-models.html#cb422-4"></a><span class="co">## Call:</span></span>
<span id="cb422-5"><a href="linear-regression-models.html#cb422-5"></a><span class="co">## lm(formula = price ~ cut + carat, data = diamonds2)</span></span>
<span id="cb422-6"><a href="linear-regression-models.html#cb422-6"></a><span class="co">## </span></span>
<span id="cb422-7"><a href="linear-regression-models.html#cb422-7"></a><span class="co">## Residuals:</span></span>
<span id="cb422-8"><a href="linear-regression-models.html#cb422-8"></a><span class="co">##      Min       1Q   Median       3Q      Max </span></span>
<span id="cb422-9"><a href="linear-regression-models.html#cb422-9"></a><span class="co">## -17540.7   -791.6    -37.6    522.1  12721.4 </span></span>
<span id="cb422-10"><a href="linear-regression-models.html#cb422-10"></a><span class="co">## </span></span>
<span id="cb422-11"><a href="linear-regression-models.html#cb422-11"></a><span class="co">## Coefficients:</span></span>
<span id="cb422-12"><a href="linear-regression-models.html#cb422-12"></a><span class="co">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb422-13"><a href="linear-regression-models.html#cb422-13"></a><span class="co">## (Intercept)  -3875.47      40.41  -95.91   &lt;2e-16 ***</span></span>
<span id="cb422-14"><a href="linear-regression-models.html#cb422-14"></a><span class="co">## cutGood       1120.33      43.50   25.75   &lt;2e-16 ***</span></span>
<span id="cb422-15"><a href="linear-regression-models.html#cb422-15"></a><span class="co">## cutVery Good  1510.14      40.24   37.53   &lt;2e-16 ***</span></span>
<span id="cb422-16"><a href="linear-regression-models.html#cb422-16"></a><span class="co">## cutPremium    1439.08      39.87   36.10   &lt;2e-16 ***</span></span>
<span id="cb422-17"><a href="linear-regression-models.html#cb422-17"></a><span class="co">## cutIdeal      1800.92      39.34   45.77   &lt;2e-16 ***</span></span>
<span id="cb422-18"><a href="linear-regression-models.html#cb422-18"></a><span class="co">## carat         7871.08      13.98  563.04   &lt;2e-16 ***</span></span>
<span id="cb422-19"><a href="linear-regression-models.html#cb422-19"></a><span class="co">## ---</span></span>
<span id="cb422-20"><a href="linear-regression-models.html#cb422-20"></a><span class="co">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb422-21"><a href="linear-regression-models.html#cb422-21"></a><span class="co">## </span></span>
<span id="cb422-22"><a href="linear-regression-models.html#cb422-22"></a><span class="co">## Residual standard error: 1511 on 53934 degrees of freedom</span></span>
<span id="cb422-23"><a href="linear-regression-models.html#cb422-23"></a><span class="co">## Multiple R-squared:  0.8565, Adjusted R-squared:  0.8565 </span></span>
<span id="cb422-24"><a href="linear-regression-models.html#cb422-24"></a><span class="co">## F-statistic: 6.437e+04 on 5 and 53934 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>It makes more sense now although the coefficient of <code>cutVery good</code> is still higher than that of <code>cutPremium</code>.</p>
<p>We now add <code>clarity</code> and <code>color</code>.</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="linear-regression-models.html#cb423-1"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>cut <span class="op">+</span><span class="st"> </span>carat <span class="op">+</span><span class="st"> </span>clarity <span class="op">+</span><span class="st"> </span>color, <span class="dt">data =</span> diamonds2)</span>
<span id="cb423-2"><a href="linear-regression-models.html#cb423-2"></a><span class="kw">summary</span>(fit)</span>
<span id="cb423-3"><a href="linear-regression-models.html#cb423-3"></a><span class="co">## </span></span>
<span id="cb423-4"><a href="linear-regression-models.html#cb423-4"></a><span class="co">## Call:</span></span>
<span id="cb423-5"><a href="linear-regression-models.html#cb423-5"></a><span class="co">## lm(formula = price ~ cut + carat + clarity + color, data = diamonds2)</span></span>
<span id="cb423-6"><a href="linear-regression-models.html#cb423-6"></a><span class="co">## </span></span>
<span id="cb423-7"><a href="linear-regression-models.html#cb423-7"></a><span class="co">## Residuals:</span></span>
<span id="cb423-8"><a href="linear-regression-models.html#cb423-8"></a><span class="co">##      Min       1Q   Median       3Q      Max </span></span>
<span id="cb423-9"><a href="linear-regression-models.html#cb423-9"></a><span class="co">## -16813.5   -680.4   -197.6    466.4  10394.9 </span></span>
<span id="cb423-10"><a href="linear-regression-models.html#cb423-10"></a><span class="co">## </span></span>
<span id="cb423-11"><a href="linear-regression-models.html#cb423-11"></a><span class="co">## Coefficients:</span></span>
<span id="cb423-12"><a href="linear-regression-models.html#cb423-12"></a><span class="co">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb423-13"><a href="linear-regression-models.html#cb423-13"></a><span class="co">## (Intercept)  -7362.80      51.68 -142.46   &lt;2e-16 ***</span></span>
<span id="cb423-14"><a href="linear-regression-models.html#cb423-14"></a><span class="co">## cutGood        655.77      33.63   19.50   &lt;2e-16 ***</span></span>
<span id="cb423-15"><a href="linear-regression-models.html#cb423-15"></a><span class="co">## cutVery Good   848.72      31.28   27.14   &lt;2e-16 ***</span></span>
<span id="cb423-16"><a href="linear-regression-models.html#cb423-16"></a><span class="co">## cutPremium     869.40      30.93   28.11   &lt;2e-16 ***</span></span>
<span id="cb423-17"><a href="linear-regression-models.html#cb423-17"></a><span class="co">## cutIdeal       998.25      30.66   32.56   &lt;2e-16 ***</span></span>
<span id="cb423-18"><a href="linear-regression-models.html#cb423-18"></a><span class="co">## carat         8886.13      12.03  738.44   &lt;2e-16 ***</span></span>
<span id="cb423-19"><a href="linear-regression-models.html#cb423-19"></a><span class="co">## claritySI2    2625.95      44.79   58.63   &lt;2e-16 ***</span></span>
<span id="cb423-20"><a href="linear-regression-models.html#cb423-20"></a><span class="co">## claritySI1    3573.69      44.60   80.13   &lt;2e-16 ***</span></span>
<span id="cb423-21"><a href="linear-regression-models.html#cb423-21"></a><span class="co">## clarityVS2    4217.83      44.84   94.06   &lt;2e-16 ***</span></span>
<span id="cb423-22"><a href="linear-regression-models.html#cb423-22"></a><span class="co">## clarityVS1    4534.88      45.54   99.59   &lt;2e-16 ***</span></span>
<span id="cb423-23"><a href="linear-regression-models.html#cb423-23"></a><span class="co">## clarityVVS2   4967.20      46.89  105.93   &lt;2e-16 ***</span></span>
<span id="cb423-24"><a href="linear-regression-models.html#cb423-24"></a><span class="co">## clarityVVS1   5072.03      48.21  105.20   &lt;2e-16 ***</span></span>
<span id="cb423-25"><a href="linear-regression-models.html#cb423-25"></a><span class="co">## clarityIF     5419.65      52.14  103.95   &lt;2e-16 ***</span></span>
<span id="cb423-26"><a href="linear-regression-models.html#cb423-26"></a><span class="co">## colorE        -211.68      18.32  -11.56   &lt;2e-16 ***</span></span>
<span id="cb423-27"><a href="linear-regression-models.html#cb423-27"></a><span class="co">## colorF        -303.31      18.51  -16.39   &lt;2e-16 ***</span></span>
<span id="cb423-28"><a href="linear-regression-models.html#cb423-28"></a><span class="co">## colorG        -506.20      18.12  -27.93   &lt;2e-16 ***</span></span>
<span id="cb423-29"><a href="linear-regression-models.html#cb423-29"></a><span class="co">## colorH        -978.70      19.27  -50.78   &lt;2e-16 ***</span></span>
<span id="cb423-30"><a href="linear-regression-models.html#cb423-30"></a><span class="co">## colorI       -1440.30      21.65  -66.54   &lt;2e-16 ***</span></span>
<span id="cb423-31"><a href="linear-regression-models.html#cb423-31"></a><span class="co">## colorJ       -2325.22      26.72  -87.01   &lt;2e-16 ***</span></span>
<span id="cb423-32"><a href="linear-regression-models.html#cb423-32"></a><span class="co">## ---</span></span>
<span id="cb423-33"><a href="linear-regression-models.html#cb423-33"></a><span class="co">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb423-34"><a href="linear-regression-models.html#cb423-34"></a><span class="co">## </span></span>
<span id="cb423-35"><a href="linear-regression-models.html#cb423-35"></a><span class="co">## Residual standard error: 1157 on 53921 degrees of freedom</span></span>
<span id="cb423-36"><a href="linear-regression-models.html#cb423-36"></a><span class="co">## Multiple R-squared:  0.9159, Adjusted R-squared:  0.9159 </span></span>
<span id="cb423-37"><a href="linear-regression-models.html#cb423-37"></a><span class="co">## F-statistic: 3.264e+04 on 18 and 53921 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Now, in terms of the relative magnitude, all the regression coefficients make sense. Note that the baseline value of <code>color</code> is D (which is the best) so that any other colors should have negative coefficients. For <code>cut</code> and <code>clarity</code>, the baseline values are the worst so that the other variables should have positive coefficients.</p>
</div>
<div id="categorical-predictors" class="section level2">
<h2><span class="header-section-number">13.5</span> Categorical Predictors</h2>
<p>See the <code>diamonds</code> example.</p>
</div>
<div id="compare-models-using-anova" class="section level2">
<h2><span class="header-section-number">13.6</span> Compare models using ANOVA</h2>
<p>If you want to compare if the difference between two models are statistically significant or not, you can use <code>anova()</code>. When you compare two models using <code>anova()</code>, <strong>one model must be contained within the other</strong>.</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="linear-regression-models.html#cb424-1"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb424-2"><a href="linear-regression-models.html#cb424-2"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb424-3"><a href="linear-regression-models.html#cb424-3"></a>x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb424-4"><a href="linear-regression-models.html#cb424-4"></a>x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb424-5"><a href="linear-regression-models.html#cb424-5"></a>x3 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb424-6"><a href="linear-regression-models.html#cb424-6"></a>x4 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb424-7"><a href="linear-regression-models.html#cb424-7"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb424-8"><a href="linear-regression-models.html#cb424-8"></a></span>
<span id="cb424-9"><a href="linear-regression-models.html#cb424-9"></a>m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1)</span>
<span id="cb424-10"><a href="linear-regression-models.html#cb424-10"></a>m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2)</span>
<span id="cb424-11"><a href="linear-regression-models.html#cb424-11"></a>m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x3)</span>
<span id="cb424-12"><a href="linear-regression-models.html#cb424-12"></a>m4 &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x3 <span class="op">+</span><span class="st"> </span>x4)</span></code></pre></div>
<p><code>m1</code> is contained within <code>m2</code>, <code>m2</code> is contained within <code>m3</code>, and <code>m3</code> is contained within <code>m4</code>.</p>
<p>Compare <code>m1</code> and <code>m2</code>:</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="linear-regression-models.html#cb425-1"></a><span class="kw">anova</span>(m1, m2)</span>
<span id="cb425-2"><a href="linear-regression-models.html#cb425-2"></a><span class="co">## Analysis of Variance Table</span></span>
<span id="cb425-3"><a href="linear-regression-models.html#cb425-3"></a><span class="co">## </span></span>
<span id="cb425-4"><a href="linear-regression-models.html#cb425-4"></a><span class="co">## Model 1: y ~ x1</span></span>
<span id="cb425-5"><a href="linear-regression-models.html#cb425-5"></a><span class="co">## Model 2: y ~ x1 + x2</span></span>
<span id="cb425-6"><a href="linear-regression-models.html#cb425-6"></a><span class="co">##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    </span></span>
<span id="cb425-7"><a href="linear-regression-models.html#cb425-7"></a><span class="co">## 1     98 618.26                                  </span></span>
<span id="cb425-8"><a href="linear-regression-models.html#cb425-8"></a><span class="co">## 2     97 112.56  1     505.7 435.79 &lt; 2.2e-16 ***</span></span>
<span id="cb425-9"><a href="linear-regression-models.html#cb425-9"></a><span class="co">## ---</span></span>
<span id="cb425-10"><a href="linear-regression-models.html#cb425-10"></a><span class="co">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span></code></pre></div>
<p>The difference is statistically significant, implying <code>x2</code> should be necessary.</p>
<p>Compare <code>m2</code> and <code>m4</code>:</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="linear-regression-models.html#cb426-1"></a><span class="kw">anova</span>(m2, m4)</span>
<span id="cb426-2"><a href="linear-regression-models.html#cb426-2"></a><span class="co">## Analysis of Variance Table</span></span>
<span id="cb426-3"><a href="linear-regression-models.html#cb426-3"></a><span class="co">## </span></span>
<span id="cb426-4"><a href="linear-regression-models.html#cb426-4"></a><span class="co">## Model 1: y ~ x1 + x2</span></span>
<span id="cb426-5"><a href="linear-regression-models.html#cb426-5"></a><span class="co">## Model 2: y ~ x1 + x2 + x3 + x4</span></span>
<span id="cb426-6"><a href="linear-regression-models.html#cb426-6"></a><span class="co">##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)</span></span>
<span id="cb426-7"><a href="linear-regression-models.html#cb426-7"></a><span class="co">## 1     97 112.56                           </span></span>
<span id="cb426-8"><a href="linear-regression-models.html#cb426-8"></a><span class="co">## 2     95 110.16  2    2.4024 1.0359 0.3589</span></span></code></pre></div>
<p>The difference is not statistically significant, implying <code>x3</code> and <code>x4</code> may not be necessary.</p>
<p>Compare <code>m3</code> and <code>m4</code>:</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="linear-regression-models.html#cb427-1"></a><span class="kw">anova</span>(m3, m4)</span>
<span id="cb427-2"><a href="linear-regression-models.html#cb427-2"></a><span class="co">## Analysis of Variance Table</span></span>
<span id="cb427-3"><a href="linear-regression-models.html#cb427-3"></a><span class="co">## </span></span>
<span id="cb427-4"><a href="linear-regression-models.html#cb427-4"></a><span class="co">## Model 1: y ~ x1 + x2 + x3</span></span>
<span id="cb427-5"><a href="linear-regression-models.html#cb427-5"></a><span class="co">## Model 2: y ~ x1 + x2 + x3 + x4</span></span>
<span id="cb427-6"><a href="linear-regression-models.html#cb427-6"></a><span class="co">##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)</span></span>
<span id="cb427-7"><a href="linear-regression-models.html#cb427-7"></a><span class="co">## 1     96 110.78                           </span></span>
<span id="cb427-8"><a href="linear-regression-models.html#cb427-8"></a><span class="co">## 2     95 110.16  1   0.61992 0.5346 0.4665</span></span></code></pre></div>
<p>The difference is not statistically significant, implying <code>x4</code> may not be necessary.</p>
</div>
<div id="prediction" class="section level2">
<h2><span class="header-section-number">13.7</span> Prediction</h2>
<p>Let’s use the <code>flights</code> dataset again.</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="linear-regression-models.html#cb428-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb428-2"><a href="linear-regression-models.html#cb428-2"></a>random_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(flights), <span class="dt">size =</span> <span class="kw">nrow</span>(flights) <span class="op">*</span><span class="st"> </span><span class="fl">0.7</span>)</span>
<span id="cb428-3"><a href="linear-regression-models.html#cb428-3"></a>flights_train &lt;-<span class="st"> </span>flights[random_index, ]</span>
<span id="cb428-4"><a href="linear-regression-models.html#cb428-4"></a>flights_test &lt;-<span class="st"> </span>flights[<span class="op">-</span>random_index, ]</span>
<span id="cb428-5"><a href="linear-regression-models.html#cb428-5"></a></span>
<span id="cb428-6"><a href="linear-regression-models.html#cb428-6"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(arr_delay <span class="op">~</span><span class="st"> </span>dep_delay, <span class="dt">data =</span> flights_train)</span></code></pre></div>
<p>Prediction</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="linear-regression-models.html#cb429-1"></a><span class="co"># prediction</span></span>
<span id="cb429-2"><a href="linear-regression-models.html#cb429-2"></a><span class="kw">predict</span>(fit, flights_test)</span></code></pre></div>
<p>Since the response is continuous, we cannot use “accuracy” to measure the performance of our prediction. One possible measure is to use correlation. The higher the correlation, the better the prediction.</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="linear-regression-models.html#cb430-1"></a><span class="kw">cor</span>(<span class="kw">predict</span>(fit, flights_test), flights_test<span class="op">$</span>arr_delay, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</span>
<span id="cb430-2"><a href="linear-regression-models.html#cb430-2"></a><span class="co">## [1] 0.9171425</span></span></code></pre></div>
<p>Since there are missing values, we set <code>use = "complete.obs"</code>.</p>
</div>
<div id="interaction-terms" class="section level2">
<h2><span class="header-section-number">13.8</span> Interaction Terms</h2>
<p>When the effect on the response of one predictor variable depends the levels or values of the other predictor variables, we can include interaction terms. For example, if we have two predictors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, we can include an additional interaction term <span class="math inline">\(X_1 X_2\)</span>:
<span class="math display">\[\begin{equation*}
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \varepsilon_i.
\end{equation*}\]</span></p>
<p>To perform the following regression
<span class="math display">\[\begin{equation*}
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \varepsilon_i,
\end{equation*}\]</span>
use <code>lm(y ~ x1 * x2)</code>.</p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="linear-regression-models.html#cb431-1"></a><span class="co"># simulation</span></span>
<span id="cb431-2"><a href="linear-regression-models.html#cb431-2"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb431-3"><a href="linear-regression-models.html#cb431-3"></a>x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb431-4"><a href="linear-regression-models.html#cb431-4"></a>x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb431-5"><a href="linear-regression-models.html#cb431-5"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb431-6"><a href="linear-regression-models.html#cb431-6"></a></span>
<span id="cb431-7"><a href="linear-regression-models.html#cb431-7"></a><span class="co"># estimation</span></span>
<span id="cb431-8"><a href="linear-regression-models.html#cb431-8"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2)</span>
<span id="cb431-9"><a href="linear-regression-models.html#cb431-9"></a><span class="co">## </span></span>
<span id="cb431-10"><a href="linear-regression-models.html#cb431-10"></a><span class="co">## Call:</span></span>
<span id="cb431-11"><a href="linear-regression-models.html#cb431-11"></a><span class="co">## lm(formula = y ~ x1 * x2)</span></span>
<span id="cb431-12"><a href="linear-regression-models.html#cb431-12"></a><span class="co">## </span></span>
<span id="cb431-13"><a href="linear-regression-models.html#cb431-13"></a><span class="co">## Coefficients:</span></span>
<span id="cb431-14"><a href="linear-regression-models.html#cb431-14"></a><span class="co">## (Intercept)           x1           x2        x1:x2  </span></span>
<span id="cb431-15"><a href="linear-regression-models.html#cb431-15"></a><span class="co">##       1.031        1.966        2.972        3.760</span></span></code></pre></div>
<p>To perform the following regression
<span class="math display">\[\begin{equation*}
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3}  + \beta_4 X_{i1} X_{i2} + \beta_5 X_{i1}X_{i3} + \beta_6 X_{i2} X_{i3} + \beta_7 X_{i1} X_{i2} X_{i3} + \varepsilon_i,
\end{equation*}\]</span>
use <code>lm(y ~ x1 * x2 * x3)</code>.</p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="linear-regression-models.html#cb432-1"></a><span class="co"># simulation</span></span>
<span id="cb432-2"><a href="linear-regression-models.html#cb432-2"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb432-3"><a href="linear-regression-models.html#cb432-3"></a>x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb432-4"><a href="linear-regression-models.html#cb432-4"></a>x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb432-5"><a href="linear-regression-models.html#cb432-5"></a>x3 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb432-6"><a href="linear-regression-models.html#cb432-6"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2 <span class="op">*</span><span class="st"> </span>x3 <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb432-7"><a href="linear-regression-models.html#cb432-7"></a></span>
<span id="cb432-8"><a href="linear-regression-models.html#cb432-8"></a><span class="co"># estimation</span></span>
<span id="cb432-9"><a href="linear-regression-models.html#cb432-9"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2 <span class="op">*</span><span class="st"> </span>x3)</span>
<span id="cb432-10"><a href="linear-regression-models.html#cb432-10"></a><span class="kw">summary</span>(fit)</span>
<span id="cb432-11"><a href="linear-regression-models.html#cb432-11"></a><span class="co">## </span></span>
<span id="cb432-12"><a href="linear-regression-models.html#cb432-12"></a><span class="co">## Call:</span></span>
<span id="cb432-13"><a href="linear-regression-models.html#cb432-13"></a><span class="co">## lm(formula = y ~ x1 * x2 * x3)</span></span>
<span id="cb432-14"><a href="linear-regression-models.html#cb432-14"></a><span class="co">## </span></span>
<span id="cb432-15"><a href="linear-regression-models.html#cb432-15"></a><span class="co">## Residuals:</span></span>
<span id="cb432-16"><a href="linear-regression-models.html#cb432-16"></a><span class="co">##      Min       1Q   Median       3Q      Max </span></span>
<span id="cb432-17"><a href="linear-regression-models.html#cb432-17"></a><span class="co">## -2.31655 -0.51032 -0.00288  0.76844  2.01342 </span></span>
<span id="cb432-18"><a href="linear-regression-models.html#cb432-18"></a><span class="co">## </span></span>
<span id="cb432-19"><a href="linear-regression-models.html#cb432-19"></a><span class="co">## Coefficients:</span></span>
<span id="cb432-20"><a href="linear-regression-models.html#cb432-20"></a><span class="co">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb432-21"><a href="linear-regression-models.html#cb432-21"></a><span class="co">## (Intercept)  1.050754   0.098109  10.710   &lt;2e-16 ***</span></span>
<span id="cb432-22"><a href="linear-regression-models.html#cb432-22"></a><span class="co">## x1           1.862611   0.113359  16.431   &lt;2e-16 ***</span></span>
<span id="cb432-23"><a href="linear-regression-models.html#cb432-23"></a><span class="co">## x2           2.942130   0.104931  28.039   &lt;2e-16 ***</span></span>
<span id="cb432-24"><a href="linear-regression-models.html#cb432-24"></a><span class="co">## x3           0.034585   0.096500   0.358   0.7209    </span></span>
<span id="cb432-25"><a href="linear-regression-models.html#cb432-25"></a><span class="co">## x1:x2        3.636972   0.143910  25.272   &lt;2e-16 ***</span></span>
<span id="cb432-26"><a href="linear-regression-models.html#cb432-26"></a><span class="co">## x1:x3       -0.246646   0.118356  -2.084   0.0399 *  </span></span>
<span id="cb432-27"><a href="linear-regression-models.html#cb432-27"></a><span class="co">## x2:x3        0.002958   0.113553   0.026   0.9793    </span></span>
<span id="cb432-28"><a href="linear-regression-models.html#cb432-28"></a><span class="co">## x1:x2:x3     1.873330   0.140592  13.325   &lt;2e-16 ***</span></span>
<span id="cb432-29"><a href="linear-regression-models.html#cb432-29"></a><span class="co">## ---</span></span>
<span id="cb432-30"><a href="linear-regression-models.html#cb432-30"></a><span class="co">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb432-31"><a href="linear-regression-models.html#cb432-31"></a><span class="co">## </span></span>
<span id="cb432-32"><a href="linear-regression-models.html#cb432-32"></a><span class="co">## Residual standard error: 0.9552 on 92 degrees of freedom</span></span>
<span id="cb432-33"><a href="linear-regression-models.html#cb432-33"></a><span class="co">## Multiple R-squared:  0.9625, Adjusted R-squared:  0.9597 </span></span>
<span id="cb432-34"><a href="linear-regression-models.html#cb432-34"></a><span class="co">## F-statistic: 337.6 on 7 and 92 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>We notice that the coefficients of <code>x3</code> and <code>x2:x3</code> are not significantly different from <span class="math inline">\(0\)</span>. This makes sense because our simulation model does not include these two predictors. However, we see that <code>x1:x3</code> is significantly different from <span class="math inline">\(0\)</span> (at the <span class="math inline">\(0.05\)</span> level) even though we did not include this predictor in the simulation model. This is because we will commit Type I error.</p>
<p>We can also include a specific interaction by using <code>:</code> instead of <code>*</code>. For example, to fit
<span class="math display">\[\begin{equation*}
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2}   + \beta_3 X_{i1} X_{i2} + \beta_4 X_{i1} X_{i2} X_{i3} + \varepsilon_i,
\end{equation*}\]</span>
we can use <code>lm(y ~ x1 + x2 + x1:x2 + x1:x2:x3)</code>.</p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="linear-regression-models.html#cb433-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb433-2"><a href="linear-regression-models.html#cb433-2"></a>x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb433-3"><a href="linear-regression-models.html#cb433-3"></a>x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb433-4"><a href="linear-regression-models.html#cb433-4"></a>x3 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb433-5"><a href="linear-regression-models.html#cb433-5"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2 <span class="op">*</span><span class="st"> </span>x3 <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb433-6"><a href="linear-regression-models.html#cb433-6"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x1<span class="op">:</span>x2 <span class="op">+</span><span class="st"> </span>x1<span class="op">:</span>x2<span class="op">:</span>x3)</span>
<span id="cb433-7"><a href="linear-regression-models.html#cb433-7"></a><span class="co">## </span></span>
<span id="cb433-8"><a href="linear-regression-models.html#cb433-8"></a><span class="co">## Call:</span></span>
<span id="cb433-9"><a href="linear-regression-models.html#cb433-9"></a><span class="co">## lm(formula = y ~ x1 + x2 + x1:x2 + x1:x2:x3)</span></span>
<span id="cb433-10"><a href="linear-regression-models.html#cb433-10"></a><span class="co">## </span></span>
<span id="cb433-11"><a href="linear-regression-models.html#cb433-11"></a><span class="co">## Coefficients:</span></span>
<span id="cb433-12"><a href="linear-regression-models.html#cb433-12"></a><span class="co">## (Intercept)           x1           x2        x1:x2     x1:x2:x3  </span></span>
<span id="cb433-13"><a href="linear-regression-models.html#cb433-13"></a><span class="co">##       1.061        1.853        2.981        3.598        1.967</span></span></code></pre></div>
</div>
<div id="variable-transformation" class="section level2">
<h2><span class="header-section-number">13.9</span> Variable Transformation</h2>
<p>We can transform our response or predictors. For example, the model
<span class="math display">\[\begin{equation*}
\log Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \varepsilon_i
\end{equation*}\]</span>
is a special case of the genearl linear regression model because we can define <span class="math inline">\(Y_i&#39;\)</span> as <span class="math inline">\(\log Y_i\)</span>. Then, we obtain
<span class="math display">\[\begin{equation*}
 Y_i&#39; = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \varepsilon_i.
\end{equation*}\]</span></p>
<p>Let’s generate some data in which <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> do not have a linear relationship.</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="linear-regression-models.html#cb434-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb434-2"><a href="linear-regression-models.html#cb434-2"></a>x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb434-3"><a href="linear-regression-models.html#cb434-3"></a>y &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x  <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span></code></pre></div>
<p><img src="Book_files/figure-html/unnamed-chunk-475-1.png" width="55%" style="display: block; margin: auto;" /></p>
<p>To perform regression for the transformed variable:</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="linear-regression-models.html#cb435-1"></a><span class="kw">lm</span>(<span class="kw">log</span>(y) <span class="op">~</span><span class="st"> </span>x)</span>
<span id="cb435-2"><a href="linear-regression-models.html#cb435-2"></a><span class="co">## </span></span>
<span id="cb435-3"><a href="linear-regression-models.html#cb435-3"></a><span class="co">## Call:</span></span>
<span id="cb435-4"><a href="linear-regression-models.html#cb435-4"></a><span class="co">## lm(formula = log(y) ~ x)</span></span>
<span id="cb435-5"><a href="linear-regression-models.html#cb435-5"></a><span class="co">## </span></span>
<span id="cb435-6"><a href="linear-regression-models.html#cb435-6"></a><span class="co">## Coefficients:</span></span>
<span id="cb435-7"><a href="linear-regression-models.html#cb435-7"></a><span class="co">## (Intercept)            x  </span></span>
<span id="cb435-8"><a href="linear-regression-models.html#cb435-8"></a><span class="co">##      0.9623       1.9979</span></span></code></pre></div>
</div>
<div id="polynomial-regression" class="section level2">
<h2><span class="header-section-number">13.10</span> Polynomial Regression</h2>
<p>Polynomial regression models are special cases of the general linear regression model. They contain higher-order terms of the predictor variables, making the response function nonlinear. Suppose we only have one predictor <span class="math inline">\(X_{1}\)</span>. An example of a polynomial regression model is
<span class="math display">\[\begin{equation*}
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i1}^2 + \varepsilon_i.
\end{equation*}\]</span>
It is a special case of the general linear regression model because we can define <span class="math inline">\(X_{i2}\)</span> as <span class="math inline">\(X_{i1}^2\)</span> and write
<span class="math display">\[\begin{equation*}
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \varepsilon_i.
\end{equation*}\]</span></p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="linear-regression-models.html#cb436-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb436-2"><a href="linear-regression-models.html#cb436-2"></a>x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb436-3"><a href="linear-regression-models.html#cb436-3"></a>x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb436-4"><a href="linear-regression-models.html#cb436-4"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x1<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb436-5"><a href="linear-regression-models.html#cb436-5"></a></span>
<span id="cb436-6"><a href="linear-regression-models.html#cb436-6"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x1, <span class="dv">2</span>, <span class="dt">raw =</span> <span class="ot">TRUE</span>) <span class="op">+</span><span class="st"> </span>x2)</span>
<span id="cb436-7"><a href="linear-regression-models.html#cb436-7"></a><span class="co">## </span></span>
<span id="cb436-8"><a href="linear-regression-models.html#cb436-8"></a><span class="co">## Call:</span></span>
<span id="cb436-9"><a href="linear-regression-models.html#cb436-9"></a><span class="co">## lm(formula = y ~ poly(x1, 2, raw = TRUE) + x2)</span></span>
<span id="cb436-10"><a href="linear-regression-models.html#cb436-10"></a><span class="co">## </span></span>
<span id="cb436-11"><a href="linear-regression-models.html#cb436-11"></a><span class="co">## Coefficients:</span></span>
<span id="cb436-12"><a href="linear-regression-models.html#cb436-12"></a><span class="co">##              (Intercept)  poly(x1, 2, raw = TRUE)1  poly(x1, 2, raw = TRUE)2  </span></span>
<span id="cb436-13"><a href="linear-regression-models.html#cb436-13"></a><span class="co">##                    1.044                     2.025                     2.976  </span></span>
<span id="cb436-14"><a href="linear-regression-models.html#cb436-14"></a><span class="co">##                       x2  </span></span>
<span id="cb436-15"><a href="linear-regression-models.html#cb436-15"></a><span class="co">##                   -2.058</span></span></code></pre></div>
<p>We have to set <code>raw = TRUE</code> in <code>ploy()</code>.</p>
<p>Alternatively, include <code>I(x1^2)</code> not <code>x1^2</code>:</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="linear-regression-models.html#cb437-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb437-2"><a href="linear-regression-models.html#cb437-2"></a>x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb437-3"><a href="linear-regression-models.html#cb437-3"></a>x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb437-4"><a href="linear-regression-models.html#cb437-4"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x1<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb437-5"><a href="linear-regression-models.html#cb437-5"></a></span>
<span id="cb437-6"><a href="linear-regression-models.html#cb437-6"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(x1<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>x2)</span>
<span id="cb437-7"><a href="linear-regression-models.html#cb437-7"></a><span class="co">## </span></span>
<span id="cb437-8"><a href="linear-regression-models.html#cb437-8"></a><span class="co">## Call:</span></span>
<span id="cb437-9"><a href="linear-regression-models.html#cb437-9"></a><span class="co">## lm(formula = y ~ x1 + I(x1^2) + x2)</span></span>
<span id="cb437-10"><a href="linear-regression-models.html#cb437-10"></a><span class="co">## </span></span>
<span id="cb437-11"><a href="linear-regression-models.html#cb437-11"></a><span class="co">## Coefficients:</span></span>
<span id="cb437-12"><a href="linear-regression-models.html#cb437-12"></a><span class="co">## (Intercept)           x1      I(x1^2)           x2  </span></span>
<span id="cb437-13"><a href="linear-regression-models.html#cb437-13"></a><span class="co">##       1.044        2.025        2.976       -2.058</span></span></code></pre></div>
<p>Note that <code>lm(y ~ x1 + x1^2 + x2)</code> does not do what you think.</p>
<p>If you want to perform
<span class="math display">\[\begin{equation*}
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i1}^3 + \varepsilon,
\end{equation*}\]</span>
you can use <code>lm(y ~ x1 + I(x1^3))</code>.</p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="linear-regression-models.html#cb438-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb438-2"><a href="linear-regression-models.html#cb438-2"></a>x1 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb438-3"><a href="linear-regression-models.html#cb438-3"></a>y &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x1<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb438-4"><a href="linear-regression-models.html#cb438-4"></a></span>
<span id="cb438-5"><a href="linear-regression-models.html#cb438-5"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(x1<span class="op">^</span><span class="dv">3</span>))</span>
<span id="cb438-6"><a href="linear-regression-models.html#cb438-6"></a><span class="co">## </span></span>
<span id="cb438-7"><a href="linear-regression-models.html#cb438-7"></a><span class="co">## Call:</span></span>
<span id="cb438-8"><a href="linear-regression-models.html#cb438-8"></a><span class="co">## lm(formula = y ~ x1 + I(x1^3))</span></span>
<span id="cb438-9"><a href="linear-regression-models.html#cb438-9"></a><span class="co">## </span></span>
<span id="cb438-10"><a href="linear-regression-models.html#cb438-10"></a><span class="co">## Coefficients:</span></span>
<span id="cb438-11"><a href="linear-regression-models.html#cb438-11"></a><span class="co">## (Intercept)           x1      I(x1^3)  </span></span>
<span id="cb438-12"><a href="linear-regression-models.html#cb438-12"></a><span class="co">##      0.9623       2.0014       2.9990</span></span></code></pre></div>
</div>
<div id="stepwise-regression" class="section level2">
<h2><span class="header-section-number">13.11</span> Stepwise regression</h2>
<p>Suppose that there are <span class="math inline">\(p\)</span> predictors, <span class="math inline">\(x_1,\ldots,x_p\)</span>. In many cases, not all of them are useful to model and predict the response. That is, we may only want to use a subset of them to form a model. Stepwise regression is one of the methods to achieve this. To perform stepwise regression, you can use <code>step()</code> in R. This function uses AIC as a criterion.</p>
<p>AIC is defined as
<span class="math display">\[\begin{equation*}
AIC = -2\log L(\hat{\beta}) + 2k,
\end{equation*}\]</span>
where <span class="math inline">\(k\)</span> is the number of parameters and <span class="math inline">\(L(\hat{\beta})\)</span> is the likelihood function evaluated at the MLE <span class="math inline">\(\hat{\beta}\)</span>. As you can see, there are two components in the formula. We want a large likelihood while keeping a model simple (= fewer parameters = smaller <span class="math inline">\(k\)</span>). Therefore, a model with a smaller AIC is preferred. You can think of the term <span class="math inline">\(2k\)</span> as a penalty that discourages us to use too many predictors. This is because while using more predictors can always increase the value of the likelihood function, it will result in overfitting.</p>
<p>We will illustrate <code>step()</code> using the prostate cancer dataset <code>zprostate</code> in the package <code>bestglm</code>.</p>
<p>Details of the dataset: a study of 97 men with prostate cancer examined the correlation between PSA (prostate specific antigen) and a number of clinical measurements: lcavol, lweight, lbph, svi, lcp, gleason, pgg45.</p>
<p><strong>Backward stepwise regression</strong>: starts with the full model (or a model with many variables) and <strong>removes</strong> the variable that results in the largest AIC until no variables can be removed.</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="linear-regression-models.html#cb439-1"></a>full_model &lt;-<span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span>. , <span class="dt">data =</span> zprostate[, <span class="dv">-10</span>])</span>
<span id="cb439-2"><a href="linear-regression-models.html#cb439-2"></a><span class="kw">step</span>(full_model, <span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>)</span>
<span id="cb439-3"><a href="linear-regression-models.html#cb439-3"></a><span class="co">## Start:  AIC=-60.78</span></span>
<span id="cb439-4"><a href="linear-regression-models.html#cb439-4"></a><span class="co">## lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + </span></span>
<span id="cb439-5"><a href="linear-regression-models.html#cb439-5"></a><span class="co">##     pgg45</span></span>
<span id="cb439-6"><a href="linear-regression-models.html#cb439-6"></a><span class="co">## </span></span>
<span id="cb439-7"><a href="linear-regression-models.html#cb439-7"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb439-8"><a href="linear-regression-models.html#cb439-8"></a><span class="co">## - gleason  1    0.0491 43.108 -62.668</span></span>
<span id="cb439-9"><a href="linear-regression-models.html#cb439-9"></a><span class="co">## - pgg45    1    0.5102 43.569 -61.636</span></span>
<span id="cb439-10"><a href="linear-regression-models.html#cb439-10"></a><span class="co">## - lcp      1    0.6814 43.740 -61.256</span></span>
<span id="cb439-11"><a href="linear-regression-models.html#cb439-11"></a><span class="co">## &lt;none&gt;                 43.058 -60.779</span></span>
<span id="cb439-12"><a href="linear-regression-models.html#cb439-12"></a><span class="co">## - lbph     1    1.3646 44.423 -59.753</span></span>
<span id="cb439-13"><a href="linear-regression-models.html#cb439-13"></a><span class="co">## - age      1    1.7981 44.857 -58.810</span></span>
<span id="cb439-14"><a href="linear-regression-models.html#cb439-14"></a><span class="co">## - lweight  1    4.6907 47.749 -52.749</span></span>
<span id="cb439-15"><a href="linear-regression-models.html#cb439-15"></a><span class="co">## - svi      1    4.8803 47.939 -52.364</span></span>
<span id="cb439-16"><a href="linear-regression-models.html#cb439-16"></a><span class="co">## - lcavol   1   20.1994 63.258 -25.467</span></span>
<span id="cb439-17"><a href="linear-regression-models.html#cb439-17"></a><span class="co">## </span></span>
<span id="cb439-18"><a href="linear-regression-models.html#cb439-18"></a><span class="co">## Step:  AIC=-62.67</span></span>
<span id="cb439-19"><a href="linear-regression-models.html#cb439-19"></a><span class="co">## lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45</span></span>
<span id="cb439-20"><a href="linear-regression-models.html#cb439-20"></a><span class="co">## </span></span>
<span id="cb439-21"><a href="linear-regression-models.html#cb439-21"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb439-22"><a href="linear-regression-models.html#cb439-22"></a><span class="co">## - lcp      1    0.6684 43.776 -63.176</span></span>
<span id="cb439-23"><a href="linear-regression-models.html#cb439-23"></a><span class="co">## &lt;none&gt;                 43.108 -62.668</span></span>
<span id="cb439-24"><a href="linear-regression-models.html#cb439-24"></a><span class="co">## - pgg45    1    1.1987 44.306 -62.008</span></span>
<span id="cb439-25"><a href="linear-regression-models.html#cb439-25"></a><span class="co">## - lbph     1    1.3844 44.492 -61.602</span></span>
<span id="cb439-26"><a href="linear-regression-models.html#cb439-26"></a><span class="co">## - age      1    1.7579 44.865 -60.791</span></span>
<span id="cb439-27"><a href="linear-regression-models.html#cb439-27"></a><span class="co">## - lweight  1    4.6429 47.751 -54.746</span></span>
<span id="cb439-28"><a href="linear-regression-models.html#cb439-28"></a><span class="co">## - svi      1    4.8333 47.941 -54.360</span></span>
<span id="cb439-29"><a href="linear-regression-models.html#cb439-29"></a><span class="co">## - lcavol   1   21.3191 64.427 -25.691</span></span>
<span id="cb439-30"><a href="linear-regression-models.html#cb439-30"></a><span class="co">## </span></span>
<span id="cb439-31"><a href="linear-regression-models.html#cb439-31"></a><span class="co">## Step:  AIC=-63.18</span></span>
<span id="cb439-32"><a href="linear-regression-models.html#cb439-32"></a><span class="co">## lpsa ~ lcavol + lweight + age + lbph + svi + pgg45</span></span>
<span id="cb439-33"><a href="linear-regression-models.html#cb439-33"></a><span class="co">## </span></span>
<span id="cb439-34"><a href="linear-regression-models.html#cb439-34"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb439-35"><a href="linear-regression-models.html#cb439-35"></a><span class="co">## - pgg45    1    0.6607 44.437 -63.723</span></span>
<span id="cb439-36"><a href="linear-regression-models.html#cb439-36"></a><span class="co">## &lt;none&gt;                 43.776 -63.176</span></span>
<span id="cb439-37"><a href="linear-regression-models.html#cb439-37"></a><span class="co">## - lbph     1    1.3329 45.109 -62.266</span></span>
<span id="cb439-38"><a href="linear-regression-models.html#cb439-38"></a><span class="co">## - age      1    1.4878 45.264 -61.934</span></span>
<span id="cb439-39"><a href="linear-regression-models.html#cb439-39"></a><span class="co">## - svi      1    4.1766 47.953 -56.336</span></span>
<span id="cb439-40"><a href="linear-regression-models.html#cb439-40"></a><span class="co">## - lweight  1    4.6553 48.431 -55.373</span></span>
<span id="cb439-41"><a href="linear-regression-models.html#cb439-41"></a><span class="co">## - lcavol   1   22.7555 66.531 -24.572</span></span>
<span id="cb439-42"><a href="linear-regression-models.html#cb439-42"></a><span class="co">## </span></span>
<span id="cb439-43"><a href="linear-regression-models.html#cb439-43"></a><span class="co">## Step:  AIC=-63.72</span></span>
<span id="cb439-44"><a href="linear-regression-models.html#cb439-44"></a><span class="co">## lpsa ~ lcavol + lweight + age + lbph + svi</span></span>
<span id="cb439-45"><a href="linear-regression-models.html#cb439-45"></a><span class="co">## </span></span>
<span id="cb439-46"><a href="linear-regression-models.html#cb439-46"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb439-47"><a href="linear-regression-models.html#cb439-47"></a><span class="co">## &lt;none&gt;                 44.437 -63.723</span></span>
<span id="cb439-48"><a href="linear-regression-models.html#cb439-48"></a><span class="co">## - age      1    1.1588 45.595 -63.226</span></span>
<span id="cb439-49"><a href="linear-regression-models.html#cb439-49"></a><span class="co">## - lbph     1    1.5087 45.945 -62.484</span></span>
<span id="cb439-50"><a href="linear-regression-models.html#cb439-50"></a><span class="co">## - lweight  1    4.3140 48.751 -56.735</span></span>
<span id="cb439-51"><a href="linear-regression-models.html#cb439-51"></a><span class="co">## - svi      1    5.8509 50.288 -53.724</span></span>
<span id="cb439-52"><a href="linear-regression-models.html#cb439-52"></a><span class="co">## - lcavol   1   25.9427 70.379 -21.119</span></span>
<span id="cb439-53"><a href="linear-regression-models.html#cb439-53"></a><span class="co">## </span></span>
<span id="cb439-54"><a href="linear-regression-models.html#cb439-54"></a><span class="co">## Call:</span></span>
<span id="cb439-55"><a href="linear-regression-models.html#cb439-55"></a><span class="co">## lm(formula = lpsa ~ lcavol + lweight + age + lbph + svi, data = zprostate[, </span></span>
<span id="cb439-56"><a href="linear-regression-models.html#cb439-56"></a><span class="co">##     -10])</span></span>
<span id="cb439-57"><a href="linear-regression-models.html#cb439-57"></a><span class="co">## </span></span>
<span id="cb439-58"><a href="linear-regression-models.html#cb439-58"></a><span class="co">## Coefficients:</span></span>
<span id="cb439-59"><a href="linear-regression-models.html#cb439-59"></a><span class="co">## (Intercept)       lcavol      lweight          age         lbph          svi  </span></span>
<span id="cb439-60"><a href="linear-regression-models.html#cb439-60"></a><span class="co">##      2.4784       0.6412       0.2520      -0.1224       0.1469       0.2960</span></span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>From the full model, removing <code>gleason</code> results in a model with the smallest AIC. Therefore, the algorithm first removes <code>gleason</code>.</p></li>
<li><p>Next, removing <code>lcp</code> results in a model with the smallest AIC. Therefore, <code>lcp</code> is removed in the next step.</p></li>
<li><p><code>pgg45</code> is removed because removing it will continue to reduce the AIC.</p></li>
<li><p>Finally, removing any variable will not reduce the AIC. Therefore, the algorithm stops and the “best” model is <code>lpsa ~ lcavol + lweight + age + lbph + svi</code>.</p></li>
</ol>
<p><strong>Forward stepwise regression:</strong> starts with the intercept (or a few variables) and adds new ones until the model cannot be improved (in the sense that the AIC will not decrease).</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="linear-regression-models.html#cb440-1"></a>null_model &lt;-<span class="st"> </span><span class="kw">lm</span>(lpsa <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> zprostate[, <span class="dv">-10</span>]) <span class="co"># the model with intercept only</span></span>
<span id="cb440-2"><a href="linear-regression-models.html#cb440-2"></a><span class="kw">step</span>(null_model, <span class="dt">direction =</span> <span class="st">&quot;forward&quot;</span>, <span class="dt">scope =</span> <span class="kw">formula</span>(full_model))</span>
<span id="cb440-3"><a href="linear-regression-models.html#cb440-3"></a><span class="co">## Start:  AIC=28.84</span></span>
<span id="cb440-4"><a href="linear-regression-models.html#cb440-4"></a><span class="co">## lpsa ~ 1</span></span>
<span id="cb440-5"><a href="linear-regression-models.html#cb440-5"></a><span class="co">## </span></span>
<span id="cb440-6"><a href="linear-regression-models.html#cb440-6"></a><span class="co">##           Df Sum of Sq     RSS     AIC</span></span>
<span id="cb440-7"><a href="linear-regression-models.html#cb440-7"></a><span class="co">## + lcavol   1    69.003  58.915 -44.366</span></span>
<span id="cb440-8"><a href="linear-regression-models.html#cb440-8"></a><span class="co">## + svi      1    41.011  86.907  -6.658</span></span>
<span id="cb440-9"><a href="linear-regression-models.html#cb440-9"></a><span class="co">## + lcp      1    38.528  89.389  -3.926</span></span>
<span id="cb440-10"><a href="linear-regression-models.html#cb440-10"></a><span class="co">## + lweight  1    24.019 103.899  10.665</span></span>
<span id="cb440-11"><a href="linear-regression-models.html#cb440-11"></a><span class="co">## + pgg45    1    22.814 105.103  11.783</span></span>
<span id="cb440-12"><a href="linear-regression-models.html#cb440-12"></a><span class="co">## + gleason  1    17.416 110.502  16.641</span></span>
<span id="cb440-13"><a href="linear-regression-models.html#cb440-13"></a><span class="co">## + lbph     1     4.136 123.782  27.650</span></span>
<span id="cb440-14"><a href="linear-regression-models.html#cb440-14"></a><span class="co">## + age      1     3.679 124.239  28.007</span></span>
<span id="cb440-15"><a href="linear-regression-models.html#cb440-15"></a><span class="co">## &lt;none&gt;                 127.918  28.838</span></span>
<span id="cb440-16"><a href="linear-regression-models.html#cb440-16"></a><span class="co">## </span></span>
<span id="cb440-17"><a href="linear-regression-models.html#cb440-17"></a><span class="co">## Step:  AIC=-44.37</span></span>
<span id="cb440-18"><a href="linear-regression-models.html#cb440-18"></a><span class="co">## lpsa ~ lcavol</span></span>
<span id="cb440-19"><a href="linear-regression-models.html#cb440-19"></a><span class="co">## </span></span>
<span id="cb440-20"><a href="linear-regression-models.html#cb440-20"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb440-21"><a href="linear-regression-models.html#cb440-21"></a><span class="co">## + lweight  1    7.1726 51.742 -54.958</span></span>
<span id="cb440-22"><a href="linear-regression-models.html#cb440-22"></a><span class="co">## + svi      1    5.2375 53.677 -51.397</span></span>
<span id="cb440-23"><a href="linear-regression-models.html#cb440-23"></a><span class="co">## + lbph     1    3.2658 55.649 -47.898</span></span>
<span id="cb440-24"><a href="linear-regression-models.html#cb440-24"></a><span class="co">## + pgg45    1    1.6980 57.217 -45.203</span></span>
<span id="cb440-25"><a href="linear-regression-models.html#cb440-25"></a><span class="co">## &lt;none&gt;                 58.915 -44.366</span></span>
<span id="cb440-26"><a href="linear-regression-models.html#cb440-26"></a><span class="co">## + lcp      1    0.6562 58.259 -43.452</span></span>
<span id="cb440-27"><a href="linear-regression-models.html#cb440-27"></a><span class="co">## + gleason  1    0.4156 58.499 -43.053</span></span>
<span id="cb440-28"><a href="linear-regression-models.html#cb440-28"></a><span class="co">## + age      1    0.0025 58.912 -42.370</span></span>
<span id="cb440-29"><a href="linear-regression-models.html#cb440-29"></a><span class="co">## </span></span>
<span id="cb440-30"><a href="linear-regression-models.html#cb440-30"></a><span class="co">## Step:  AIC=-54.96</span></span>
<span id="cb440-31"><a href="linear-regression-models.html#cb440-31"></a><span class="co">## lpsa ~ lcavol + lweight</span></span>
<span id="cb440-32"><a href="linear-regression-models.html#cb440-32"></a><span class="co">## </span></span>
<span id="cb440-33"><a href="linear-regression-models.html#cb440-33"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb440-34"><a href="linear-regression-models.html#cb440-34"></a><span class="co">## + svi      1    5.1737 46.568 -63.177</span></span>
<span id="cb440-35"><a href="linear-regression-models.html#cb440-35"></a><span class="co">## + pgg45    1    1.8158 49.926 -56.424</span></span>
<span id="cb440-36"><a href="linear-regression-models.html#cb440-36"></a><span class="co">## &lt;none&gt;                 51.742 -54.958</span></span>
<span id="cb440-37"><a href="linear-regression-models.html#cb440-37"></a><span class="co">## + lcp      1    0.8187 50.923 -54.506</span></span>
<span id="cb440-38"><a href="linear-regression-models.html#cb440-38"></a><span class="co">## + gleason  1    0.7163 51.026 -54.311</span></span>
<span id="cb440-39"><a href="linear-regression-models.html#cb440-39"></a><span class="co">## + age      1    0.6456 51.097 -54.176</span></span>
<span id="cb440-40"><a href="linear-regression-models.html#cb440-40"></a><span class="co">## + lbph     1    0.4440 51.298 -53.794</span></span>
<span id="cb440-41"><a href="linear-regression-models.html#cb440-41"></a><span class="co">## </span></span>
<span id="cb440-42"><a href="linear-regression-models.html#cb440-42"></a><span class="co">## Step:  AIC=-63.18</span></span>
<span id="cb440-43"><a href="linear-regression-models.html#cb440-43"></a><span class="co">## lpsa ~ lcavol + lweight + svi</span></span>
<span id="cb440-44"><a href="linear-regression-models.html#cb440-44"></a><span class="co">## </span></span>
<span id="cb440-45"><a href="linear-regression-models.html#cb440-45"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb440-46"><a href="linear-regression-models.html#cb440-46"></a><span class="co">## + lbph     1   0.97296 45.595 -63.226</span></span>
<span id="cb440-47"><a href="linear-regression-models.html#cb440-47"></a><span class="co">## &lt;none&gt;                 46.568 -63.177</span></span>
<span id="cb440-48"><a href="linear-regression-models.html#cb440-48"></a><span class="co">## + age      1   0.62301 45.945 -62.484</span></span>
<span id="cb440-49"><a href="linear-regression-models.html#cb440-49"></a><span class="co">## + pgg45    1   0.50069 46.068 -62.226</span></span>
<span id="cb440-50"><a href="linear-regression-models.html#cb440-50"></a><span class="co">## + gleason  1   0.34449 46.224 -61.898</span></span>
<span id="cb440-51"><a href="linear-regression-models.html#cb440-51"></a><span class="co">## + lcp      1   0.06937 46.499 -61.322</span></span>
<span id="cb440-52"><a href="linear-regression-models.html#cb440-52"></a><span class="co">## </span></span>
<span id="cb440-53"><a href="linear-regression-models.html#cb440-53"></a><span class="co">## Step:  AIC=-63.23</span></span>
<span id="cb440-54"><a href="linear-regression-models.html#cb440-54"></a><span class="co">## lpsa ~ lcavol + lweight + svi + lbph</span></span>
<span id="cb440-55"><a href="linear-regression-models.html#cb440-55"></a><span class="co">## </span></span>
<span id="cb440-56"><a href="linear-regression-models.html#cb440-56"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb440-57"><a href="linear-regression-models.html#cb440-57"></a><span class="co">## + age      1   1.15879 44.437 -63.723</span></span>
<span id="cb440-58"><a href="linear-regression-models.html#cb440-58"></a><span class="co">## &lt;none&gt;                 45.595 -63.226</span></span>
<span id="cb440-59"><a href="linear-regression-models.html#cb440-59"></a><span class="co">## + pgg45    1   0.33173 45.264 -61.934</span></span>
<span id="cb440-60"><a href="linear-regression-models.html#cb440-60"></a><span class="co">## + gleason  1   0.20691 45.389 -61.667</span></span>
<span id="cb440-61"><a href="linear-regression-models.html#cb440-61"></a><span class="co">## + lcp      1   0.10115 45.494 -61.441</span></span>
<span id="cb440-62"><a href="linear-regression-models.html#cb440-62"></a><span class="co">## </span></span>
<span id="cb440-63"><a href="linear-regression-models.html#cb440-63"></a><span class="co">## Step:  AIC=-63.72</span></span>
<span id="cb440-64"><a href="linear-regression-models.html#cb440-64"></a><span class="co">## lpsa ~ lcavol + lweight + svi + lbph + age</span></span>
<span id="cb440-65"><a href="linear-regression-models.html#cb440-65"></a><span class="co">## </span></span>
<span id="cb440-66"><a href="linear-regression-models.html#cb440-66"></a><span class="co">##           Df Sum of Sq    RSS     AIC</span></span>
<span id="cb440-67"><a href="linear-regression-models.html#cb440-67"></a><span class="co">## &lt;none&gt;                 44.437 -63.723</span></span>
<span id="cb440-68"><a href="linear-regression-models.html#cb440-68"></a><span class="co">## + pgg45    1   0.66071 43.776 -63.176</span></span>
<span id="cb440-69"><a href="linear-regression-models.html#cb440-69"></a><span class="co">## + gleason  1   0.47674 43.960 -62.769</span></span>
<span id="cb440-70"><a href="linear-regression-models.html#cb440-70"></a><span class="co">## + lcp      1   0.13040 44.306 -62.008</span></span>
<span id="cb440-71"><a href="linear-regression-models.html#cb440-71"></a><span class="co">## </span></span>
<span id="cb440-72"><a href="linear-regression-models.html#cb440-72"></a><span class="co">## Call:</span></span>
<span id="cb440-73"><a href="linear-regression-models.html#cb440-73"></a><span class="co">## lm(formula = lpsa ~ lcavol + lweight + svi + lbph + age, data = zprostate[, </span></span>
<span id="cb440-74"><a href="linear-regression-models.html#cb440-74"></a><span class="co">##     -10])</span></span>
<span id="cb440-75"><a href="linear-regression-models.html#cb440-75"></a><span class="co">## </span></span>
<span id="cb440-76"><a href="linear-regression-models.html#cb440-76"></a><span class="co">## Coefficients:</span></span>
<span id="cb440-77"><a href="linear-regression-models.html#cb440-77"></a><span class="co">## (Intercept)       lcavol      lweight          svi         lbph          age  </span></span>
<span id="cb440-78"><a href="linear-regression-models.html#cb440-78"></a><span class="co">##      2.4784       0.6412       0.2520       0.2960       0.1469      -0.1224</span></span></code></pre></div>
<p>Remark: there is also an option <code>direction = "both"</code>.</p>
</div>
<div id="best-subset" class="section level2">
<h2><span class="header-section-number">13.12</span> Best subset</h2>
<p>Given <span class="math inline">\(p\)</span> predictors, the best subset problem is to find out of all the <span class="math inline">\(2^p\)</span> subsets, the best subset according to some goodness-of-fit criterion. Some examples of goodness-of-fit criteria are AIC and BIC. We will use the package <code>bestglm</code>. The function to find out the best subset is also called <code>bestglm()</code>.</p>
<p>BIC is defined as
<span class="math display">\[\begin{equation*}
BIC = -2\log L(\hat{\beta}) + k \log (n),
\end{equation*}\]</span>
where <span class="math inline">\(k\)</span> is the number of parameters, <span class="math inline">\(n\)</span> is the number of observations, and <span class="math inline">\(L(\hat{\beta})\)</span> is the likelihood function evaluated at the MLE. You can think of <span class="math inline">\(k \log (n)\)</span> as a penalty to discourage us to use too many predictors to avoid overfitting.</p>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb441-1"><a href="linear-regression-models.html#cb441-1"></a><span class="kw">library</span>(bestglm)</span>
<span id="cb441-2"><a href="linear-regression-models.html#cb441-2"></a></span>
<span id="cb441-3"><a href="linear-regression-models.html#cb441-3"></a><span class="co"># the last column ind</span></span>
<span id="cb441-4"><a href="linear-regression-models.html#cb441-4"></a><span class="kw">bestglm</span>(zprostate[, <span class="dv">-10</span>], <span class="dt">IC =</span> <span class="st">&quot;BIC&quot;</span>)</span>
<span id="cb441-5"><a href="linear-regression-models.html#cb441-5"></a><span class="co">## BIC</span></span>
<span id="cb441-6"><a href="linear-regression-models.html#cb441-6"></a><span class="co">## BICq equivalent for q in (0.0561398731346802, 0.759703855996616)</span></span>
<span id="cb441-7"><a href="linear-regression-models.html#cb441-7"></a><span class="co">## Best Model:</span></span>
<span id="cb441-8"><a href="linear-regression-models.html#cb441-8"></a><span class="co">##              Estimate Std. Error   t value     Pr(&gt;|t|)</span></span>
<span id="cb441-9"><a href="linear-regression-models.html#cb441-9"></a><span class="co">## (Intercept) 2.4783869 0.07184863 34.494560 8.637858e-55</span></span>
<span id="cb441-10"><a href="linear-regression-models.html#cb441-10"></a><span class="co">## lcavol      0.6197821 0.08823567  7.024168 3.493564e-10</span></span>
<span id="cb441-11"><a href="linear-regression-models.html#cb441-11"></a><span class="co">## lweight     0.2835097 0.07524408  3.767867 2.887126e-04</span></span>
<span id="cb441-12"><a href="linear-regression-models.html#cb441-12"></a><span class="co">## svi         0.2755825 0.08573414  3.214385 1.797619e-03</span></span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Book.pdf", "Book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
